{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "1. https://github.com/ashishpatel26/LLM-Finetuning/blob/main/7.FineTune_LLAMA2_with_QLORA.ipynb  \n",
    "2. https://huggingface.co/blog/4bit-transformers-bitsandbytes  \n",
    "3. https://pytorch.org/blog/finetune-llms/  \n",
    "4. https://colab.research.google.com/drive/1vIjBtePIZwUaHWfjfNHzBjwuXOyU_ugD?usp=sharing#scrollTo=6k_nL6xJMZW2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight and Bias Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"QLoRA_Instruction_finetune_08\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = load_dataset(\"aeolian83/OpenOrca-gugugo-ko_test\", cache_dir=\"/mnt/t7/.cache/huggingface/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.shuffle(seed=2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 1008132\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 1232162\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'system_prompt', 'response', 'id'],\n",
       "    num_rows: 1008132\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 806505\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 201627\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '다음 질문에 답하세요: 1~5점 척도(1은 가장 불리한 점수, 5는 가장 유리한 점수)로 이 리뷰를 평가한다면? \"안녕하세요, 이 제품을 사용해보고 싶습니다.\"\\n답변:',\n",
       " 'system_prompt': '당신은 인공지능 비서입니다. 사용자가 과제를 부여할 것입니다. 당신의 목표는 가능한 한 충실하게 과제를 완료하는 것입니다. 과제를 수행하는 동안 단계별로 생각하고 단계를 정당화하세요.',\n",
       " 'response': '1\\n\\n단계별 프로세스:\\n\\n1. 리뷰를 이해합니다: 사용자가 작성한 리뷰 \"안녕하세요, 이 제품을 사용해보고 싶습니다.\"는 실제로 리뷰가 아니라 제품이나 서비스를 사용해보고 싶다는 의사를 표현한 문구입니다. 리뷰에는 일반적으로 특정 제품이나 서비스를 사용한 후의 피드백이나 의견이 포함됩니다.\\n\\n2. 평가 기준: 일반적으로 1~5점 척도에서 5는 가장 긍정적이고 1은 가장 부정적이거나 가장 호의적이지 않은 평가입니다.\\n\\n3. 리뷰 분석: 사용자가 제품이나 서비스를 아직 사용해보지 않았기 때문에 사용자의 경험에 대한 긍정적 또는 부정적 피드백을 제공하지 않았기 때문에 평점을 부여할 수 없습니다. \\n\\n4. 결론: 따라서 해당 제품/서비스에 대한 사용자의 경험에 대한 구체적인 정보가 없기 때문에 가장 불리한 평점인 1점을 부여합니다. 그러나 이는 제품이나 서비스에 대한 부정적인 평가를 반영하는 것이 아니라 리뷰에 정보가 없는 콘텐츠가 부족하다는 것을 반영할 뿐입니다.',\n",
       " 'id': 't0.263840'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"aeolian83/OpenOrca-gugugo-ko_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/llama-2-ko-7b\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for 4-bit QLoRA Training(4bit QLoRA 학습을 위한 설정)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Nvidia의 Ampere 아키텍처 이후 가속기는 bf16으로 속도 향상을 꾀할수 있다. \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# bnb_4bit_quant_type=\"nf4\" 설정상 기본값은 bnb_4bit_quant_type=\"fp4\"이나 허깅페이스 저자들에 의하면\n",
    "# 경험적 결과로 \"nf4\"가 결과가 더 좋았다고 한다. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# bnb_4bit_use_double_quant=True로 하면 매개변수단 0.4bit을 추가로 절약 할 수 있다고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb707e0c161c4de68edb6394ce9580c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# model.config.pretraining_tp = 1\n",
    "# 종종 QLoRA 코드에 이 코드가 보이는데 병렬 학습에 쓰이는 코드로 보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_model_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드를 쓰지 않는 경우(물론 패딩 토큰을 별도로 사용하는 경우에 해당됨) loss가 0으로 떨어지는 경우가 있다함\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(46337, 4096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) # pad_token이 추가되었으므로 embedding과 language modeling head를 resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    system_prompt = f\"### system prompt: {sample['system_prompt']}\"\n",
    "    input = f\"### question: {sample['question']}\" if len(sample[\"question\"]) > 0 else None\n",
    "    output = f\"### response: {sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [system_prompt, input, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_instruction(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d543794b3db48c5aa652528ca09b8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/806505 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ko_instruction_01['train'].map(template_dataset, remove_columns=list(ko_instruction_01['train'].features), num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### system prompt: 당신은 인공지능 비서입니다. 사용자가 과제를 부여할 것입니다. 당신의 목표는 가능한 한 충실하게 과제를 완료하는 것입니다. 과제를 수행하는 동안 단계별로 생각하고 단계를 정당화하세요.\\n\\n### question: 다음 질문에 답하세요: 1~5점 척도(1은 가장 불리한 점수, 5는 가장 유리한 점수)로 이 리뷰를 평가한다면? \"안녕하세요, 이 제품을 사용해보고 싶습니다.\"\\n답변:\\n\\n### response: 1\\n\\n단계별 프로세스:\\n\\n1. 리뷰를 이해합니다: 사용자가 작성한 리뷰 \"안녕하세요, 이 제품을 사용해보고 싶습니다.\"는 실제로 리뷰가 아니라 제품이나 서비스를 사용해보고 싶다는 의사를 표현한 문구입니다. 리뷰에는 일반적으로 특정 제품이나 서비스를 사용한 후의 피드백이나 의견이 포함됩니다.\\n\\n2. 평가 기준: 일반적으로 1~5점 척도에서 5는 가장 긍정적이고 1은 가장 부정적이거나 가장 호의적이지 않은 평가입니다.\\n\\n3. 리뷰 분석: 사용자가 제품이나 서비스를 아직 사용해보지 않았기 때문에 사용자의 경험에 대한 긍정적 또는 부정적 피드백을 제공하지 않았기 때문에 평점을 부여할 수 없습니다. \\n\\n4. 결론: 따라서 해당 제품/서비스에 대한 사용자의 경험에 대한 구체적인 정보가 없기 때문에 가장 불리한 평점인 1점을 부여합니다. 그러나 이는 제품이나 서비스에 대한 부정적인 평가를 반영하는 것이 아니라 리뷰에 정보가 없는 콘텐츠가 부족하다는 것을 반영할 뿐입니다.</s>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Argument Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/gugugo-experi_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = checkpoint_dir\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 500\n",
    "save_total_limit=5\n",
    "num_train_epochs = 0.9\n",
    "logging_steps = 20\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb554afc33a4f2b998f30976a46e255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/806505 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/00_QLoRa_fine_tune/wandb/run-20240506_173412-1x0jxwqz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08/runs/1x0jxwqz' target=\"_blank\">fragrant-frog-3</a></strong> to <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08/runs/1x0jxwqz' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08/runs/1x0jxwqz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654f7179d98943248fb94a715ac0bf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/362927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0349, 'grad_norm': 0.53125, 'learning_rate': 2.3524098182303667e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1966, 'grad_norm': 0.64453125, 'learning_rate': 2.3512735804839805e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2557, 'grad_norm': 0.51171875, 'learning_rate': 2.3501373427375943e-05, 'epoch': 0.8}\n",
      "{'loss': 1.296, 'grad_norm': 0.640625, 'learning_rate': 2.3490011049912084e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0313, 'grad_norm': 0.7734375, 'learning_rate': 2.3478648672448226e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2645, 'grad_norm': 0.9140625, 'learning_rate': 2.3467286294984363e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1992, 'grad_norm': 1.203125, 'learning_rate': 2.3455923917520505e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1567, 'grad_norm': 0.462890625, 'learning_rate': 2.3444561540056643e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1183, 'grad_norm': 0.625, 'learning_rate': 2.343319916259278e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0736, 'grad_norm': 0.92578125, 'learning_rate': 2.342183678512892e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1836, 'grad_norm': 0.7265625, 'learning_rate': 2.3410474407665063e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2316, 'grad_norm': 0.435546875, 'learning_rate': 2.33991120302012e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2294, 'grad_norm': 0.53125, 'learning_rate': 2.338774965273734e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1849, 'grad_norm': 0.6171875, 'learning_rate': 2.337638727527348e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0322, 'grad_norm': 0.52734375, 'learning_rate': 2.3365024897809618e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3641, 'grad_norm': 0.77734375, 'learning_rate': 2.335366252034576e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1201, 'grad_norm': 0.6796875, 'learning_rate': 2.3342300142881897e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1087, 'grad_norm': 0.4453125, 'learning_rate': 2.3330937765418038e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2506, 'grad_norm': 0.65234375, 'learning_rate': 2.3319575387954176e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0119, 'grad_norm': 1.03125, 'learning_rate': 2.3308213010490314e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2527, 'grad_norm': 0.51953125, 'learning_rate': 2.329685063302646e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1129, 'grad_norm': 0.91796875, 'learning_rate': 2.3285488255562596e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2012, 'grad_norm': 0.6171875, 'learning_rate': 2.3274125878098734e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2374, 'grad_norm': 0.515625, 'learning_rate': 2.3262763500634875e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1162, 'grad_norm': 1.3203125, 'learning_rate': 2.3251401123171013e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4249, 'grad_norm': 0.734375, 'learning_rate': 2.324003874570715e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1613, 'grad_norm': 0.6640625, 'learning_rate': 2.3228676368243292e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2463, 'grad_norm': 0.3984375, 'learning_rate': 2.3217313990779434e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3271, 'grad_norm': 0.76953125, 'learning_rate': 2.320595161331557e-05, 'epoch': 0.8}\n",
      "{'loss': 1.052, 'grad_norm': 0.9453125, 'learning_rate': 2.319458923585171e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3381, 'grad_norm': 0.484375, 'learning_rate': 2.318322685838785e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1833, 'grad_norm': 0.84375, 'learning_rate': 2.3171864480923992e-05, 'epoch': 0.8}\n",
      "{'loss': 1.24, 'grad_norm': 0.51171875, 'learning_rate': 2.316050210346013e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1762, 'grad_norm': 0.57421875, 'learning_rate': 2.3149139725996267e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0722, 'grad_norm': 0.99609375, 'learning_rate': 2.313777734853241e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2813, 'grad_norm': 0.515625, 'learning_rate': 2.3126414971068547e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1941, 'grad_norm': 0.79296875, 'learning_rate': 2.3115052593604684e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2727, 'grad_norm': 0.435546875, 'learning_rate': 2.310369021614083e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1533, 'grad_norm': 0.60546875, 'learning_rate': 2.3092327838676967e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9838, 'grad_norm': 0.7421875, 'learning_rate': 2.3080965461213105e-05, 'epoch': 0.8}\n",
      "{'loss': 1.387, 'grad_norm': 0.65625, 'learning_rate': 2.3069603083749246e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1738, 'grad_norm': 0.859375, 'learning_rate': 2.3058240706285384e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1168, 'grad_norm': 0.47265625, 'learning_rate': 2.3046878328821525e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2016, 'grad_norm': 0.4921875, 'learning_rate': 2.3035515951357663e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0039, 'grad_norm': 0.90234375, 'learning_rate': 2.3024153573893804e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1939, 'grad_norm': 0.53515625, 'learning_rate': 2.3012791196429942e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1737, 'grad_norm': 1.015625, 'learning_rate': 2.300142881896608e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1728, 'grad_norm': 0.53125, 'learning_rate': 2.299006644150222e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1838, 'grad_norm': 0.65234375, 'learning_rate': 2.2978704064038362e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0123, 'grad_norm': 1.1484375, 'learning_rate': 2.29673416865745e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2913, 'grad_norm': 0.494140625, 'learning_rate': 2.2955979309110638e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1713, 'grad_norm': 0.609375, 'learning_rate': 2.294461693164678e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1763, 'grad_norm': 0.48828125, 'learning_rate': 2.2933254554182917e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2305, 'grad_norm': 0.76171875, 'learning_rate': 2.2921892176719055e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9746, 'grad_norm': 0.53125, 'learning_rate': 2.29105297992552e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2763, 'grad_norm': 0.7265625, 'learning_rate': 2.2899167421791338e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1676, 'grad_norm': 0.8828125, 'learning_rate': 2.2887805044327475e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2308, 'grad_norm': 0.50390625, 'learning_rate': 2.2876442666863617e-05, 'epoch': 0.8}\n",
      "{'loss': 1.4059, 'grad_norm': 0.53125, 'learning_rate': 2.2865080289399754e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0617, 'grad_norm': 1.078125, 'learning_rate': 2.2853717911935896e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3119, 'grad_norm': 0.54296875, 'learning_rate': 2.2842355534472034e-05, 'epoch': 0.8}\n",
      "{'loss': 1.177, 'grad_norm': 0.6875, 'learning_rate': 2.2830993157008175e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0943, 'grad_norm': 0.50390625, 'learning_rate': 2.2819630779544313e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1359, 'grad_norm': 0.62890625, 'learning_rate': 2.280826840208045e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0296, 'grad_norm': 0.546875, 'learning_rate': 2.2796906024616592e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3382, 'grad_norm': 0.56640625, 'learning_rate': 2.2785543647152733e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1029, 'grad_norm': 0.8671875, 'learning_rate': 2.277418126968887e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2048, 'grad_norm': 0.45703125, 'learning_rate': 2.276281889222501e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2038, 'grad_norm': 0.78125, 'learning_rate': 2.275145651476115e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0279, 'grad_norm': 0.81640625, 'learning_rate': 2.2740094137297288e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3222, 'grad_norm': 0.62109375, 'learning_rate': 2.272873175983343e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1442, 'grad_norm': 0.66015625, 'learning_rate': 2.271736938236957e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2522, 'grad_norm': 0.54296875, 'learning_rate': 2.2706007004905708e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1898, 'grad_norm': 0.51171875, 'learning_rate': 2.2694644627441846e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9242, 'grad_norm': 0.423828125, 'learning_rate': 2.2683282249977987e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3385, 'grad_norm': 0.47265625, 'learning_rate': 2.2671919872514125e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2526, 'grad_norm': 0.7421875, 'learning_rate': 2.2660557495050266e-05, 'epoch': 0.8}\n",
      "{'loss': 1.188, 'grad_norm': 0.5078125, 'learning_rate': 2.2649195117586404e-05, 'epoch': 0.8}\n",
      "{'loss': 1.123, 'grad_norm': 0.46484375, 'learning_rate': 2.2637832740122545e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1429, 'grad_norm': 0.9765625, 'learning_rate': 2.2626470362658683e-05, 'epoch': 0.8}\n",
      "{'loss': 1.303, 'grad_norm': 0.58203125, 'learning_rate': 2.261510798519482e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1711, 'grad_norm': 0.70703125, 'learning_rate': 2.2603745607730962e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2651, 'grad_norm': 0.63671875, 'learning_rate': 2.2592383230267104e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1746, 'grad_norm': 0.546875, 'learning_rate': 2.258102085280324e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0262, 'grad_norm': 0.62890625, 'learning_rate': 2.256965847533938e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3517, 'grad_norm': 0.51171875, 'learning_rate': 2.255829609787552e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2368, 'grad_norm': 0.54296875, 'learning_rate': 2.254693372041166e-05, 'epoch': 0.8}\n",
      "{'loss': 1.127, 'grad_norm': 0.486328125, 'learning_rate': 2.25355713429478e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2713, 'grad_norm': 0.81640625, 'learning_rate': 2.252420896548394e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9799, 'grad_norm': 1.25, 'learning_rate': 2.251284658802008e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2616, 'grad_norm': 0.494140625, 'learning_rate': 2.2501484210556217e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2017, 'grad_norm': 0.8203125, 'learning_rate': 2.2490121833092358e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2539, 'grad_norm': 0.55078125, 'learning_rate': 2.24787594556285e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2362, 'grad_norm': 0.546875, 'learning_rate': 2.2467397078164637e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1004, 'grad_norm': 0.625, 'learning_rate': 2.2456034700700775e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3588, 'grad_norm': 0.59765625, 'learning_rate': 2.2444672323236916e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1759, 'grad_norm': 0.578125, 'learning_rate': 2.2433309945773054e-05, 'epoch': 0.8}\n",
      "{'loss': 1.152, 'grad_norm': 0.41796875, 'learning_rate': 2.2421947568309192e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2847, 'grad_norm': 0.60546875, 'learning_rate': 2.2410585190845333e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0238, 'grad_norm': 1.203125, 'learning_rate': 2.2399222813381474e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3637, 'grad_norm': 0.55859375, 'learning_rate': 2.2387860435917612e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1227, 'grad_norm': 0.66015625, 'learning_rate': 2.237649805845375e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2414, 'grad_norm': 0.5546875, 'learning_rate': 2.236513568098989e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3207, 'grad_norm': 0.703125, 'learning_rate': 2.2353773303526032e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9666, 'grad_norm': 0.6875, 'learning_rate': 2.234241092606217e-05, 'epoch': 0.8}\n",
      "{'loss': 1.4026, 'grad_norm': 0.546875, 'learning_rate': 2.233104854859831e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2989, 'grad_norm': 1.046875, 'learning_rate': 2.231968617113445e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1186, 'grad_norm': 0.75, 'learning_rate': 2.2308323793670587e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2061, 'grad_norm': 0.7109375, 'learning_rate': 2.229696141620673e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1219, 'grad_norm': 0.7890625, 'learning_rate': 2.228559903874287e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3391, 'grad_norm': 0.765625, 'learning_rate': 2.2274236661279008e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1165, 'grad_norm': 0.8125, 'learning_rate': 2.2262874283815145e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0273, 'grad_norm': 0.458984375, 'learning_rate': 2.2251511906351287e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2195, 'grad_norm': 0.78515625, 'learning_rate': 2.2240149528887425e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1419, 'grad_norm': 0.7265625, 'learning_rate': 2.2228787151423566e-05, 'epoch': 0.8}\n",
      "{'loss': 1.4196, 'grad_norm': 0.65625, 'learning_rate': 2.2217424773959704e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1388, 'grad_norm': 0.734375, 'learning_rate': 2.2206062396495845e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0827, 'grad_norm': 0.55078125, 'learning_rate': 2.2194700019031983e-05, 'epoch': 0.8}\n",
      "{'loss': 1.199, 'grad_norm': 0.55078125, 'learning_rate': 2.218333764156812e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0909, 'grad_norm': 1.0390625, 'learning_rate': 2.2171975264104262e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2831, 'grad_norm': 0.55859375, 'learning_rate': 2.2160612886640403e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1448, 'grad_norm': 1.078125, 'learning_rate': 2.214925050917654e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2079, 'grad_norm': 0.46484375, 'learning_rate': 2.2137888131712682e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1564, 'grad_norm': 0.57421875, 'learning_rate': 2.212652575424882e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0076, 'grad_norm': 0.90234375, 'learning_rate': 2.2115163376784958e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3862, 'grad_norm': 0.5078125, 'learning_rate': 2.21038009993211e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0769, 'grad_norm': 0.8203125, 'learning_rate': 2.209243862185724e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0847, 'grad_norm': 0.6953125, 'learning_rate': 2.2081076244393378e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1747, 'grad_norm': 0.6171875, 'learning_rate': 2.2069713866929516e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0775, 'grad_norm': 0.77734375, 'learning_rate': 2.2058351489465657e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2913, 'grad_norm': 0.63671875, 'learning_rate': 2.2046989112001795e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2186, 'grad_norm': 0.71484375, 'learning_rate': 2.2035626734537936e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2465, 'grad_norm': 0.439453125, 'learning_rate': 2.2024264357074074e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3558, 'grad_norm': 0.58203125, 'learning_rate': 2.2012901979610216e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0275, 'grad_norm': 1.3984375, 'learning_rate': 2.2001539602146353e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2255, 'grad_norm': 0.578125, 'learning_rate': 2.199017722468249e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1378, 'grad_norm': 0.78125, 'learning_rate': 2.1978814847218632e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0906, 'grad_norm': 0.490234375, 'learning_rate': 2.1967452469754774e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1459, 'grad_norm': 0.5390625, 'learning_rate': 2.195609009229091e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1454, 'grad_norm': 0.65625, 'learning_rate': 2.1944727714827053e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3153, 'grad_norm': 0.48046875, 'learning_rate': 2.193336533736319e-05, 'epoch': 0.8}\n",
      "{'loss': 1.174, 'grad_norm': 0.734375, 'learning_rate': 2.192200295989933e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2759, 'grad_norm': 0.435546875, 'learning_rate': 2.191064058243547e-05, 'epoch': 0.8}\n",
      "{'loss': 1.225, 'grad_norm': 0.59765625, 'learning_rate': 2.189927820497161e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0965, 'grad_norm': 1.2734375, 'learning_rate': 2.188791582750775e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2012, 'grad_norm': 0.59765625, 'learning_rate': 2.1876553450043887e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0108, 'grad_norm': 0.859375, 'learning_rate': 2.1865191072580028e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0183, 'grad_norm': 0.56640625, 'learning_rate': 2.1853828695116166e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3021, 'grad_norm': 0.51171875, 'learning_rate': 2.1842466317652307e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0099, 'grad_norm': 0.66015625, 'learning_rate': 2.1831103940188445e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3257, 'grad_norm': 0.45703125, 'learning_rate': 2.1819741562724586e-05, 'epoch': 0.8}\n",
      "{'loss': 1.165, 'grad_norm': 1.0234375, 'learning_rate': 2.1808379185260724e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2688, 'grad_norm': 0.48828125, 'learning_rate': 2.1797016807796862e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2601, 'grad_norm': 0.62890625, 'learning_rate': 2.1785654430333006e-05, 'epoch': 0.8}\n",
      "{'loss': 1.175, 'grad_norm': 1.015625, 'learning_rate': 2.1774292052869144e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1887, 'grad_norm': 0.54296875, 'learning_rate': 2.1762929675405282e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1248, 'grad_norm': 0.734375, 'learning_rate': 2.1751567297941423e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1702, 'grad_norm': 0.455078125, 'learning_rate': 2.174020492047756e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3117, 'grad_norm': 0.70703125, 'learning_rate': 2.17288425430137e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9415, 'grad_norm': 0.53515625, 'learning_rate': 2.171748016554984e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2933, 'grad_norm': 0.52734375, 'learning_rate': 2.170611778808598e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2956, 'grad_norm': 0.58203125, 'learning_rate': 2.169475541062212e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2302, 'grad_norm': 0.5390625, 'learning_rate': 2.1683393033158257e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1418, 'grad_norm': 0.62890625, 'learning_rate': 2.16720306556944e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0042, 'grad_norm': 0.69921875, 'learning_rate': 2.166066827823054e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2843, 'grad_norm': 0.52734375, 'learning_rate': 2.1649305900766678e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1826, 'grad_norm': 1.1171875, 'learning_rate': 2.1637943523302816e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2663, 'grad_norm': 0.53125, 'learning_rate': 2.1626581145838957e-05, 'epoch': 0.81}\n",
      "{'loss': 1.338, 'grad_norm': 0.66015625, 'learning_rate': 2.1615218768375095e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0017, 'grad_norm': 1.21875, 'learning_rate': 2.1603856390911232e-05, 'epoch': 0.81}\n",
      "{'loss': 1.402, 'grad_norm': 0.5234375, 'learning_rate': 2.1592494013447377e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1519, 'grad_norm': 0.58984375, 'learning_rate': 2.1581131635983515e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1371, 'grad_norm': 0.65234375, 'learning_rate': 2.1569769258519653e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2126, 'grad_norm': 0.90625, 'learning_rate': 2.1558406881055794e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0528, 'grad_norm': 0.72265625, 'learning_rate': 2.1547044503591932e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2521, 'grad_norm': 0.58203125, 'learning_rate': 2.1535682126128073e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1436, 'grad_norm': 0.6015625, 'learning_rate': 2.152431974866421e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2174, 'grad_norm': 0.55078125, 'learning_rate': 2.1512957371200352e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2029, 'grad_norm': 0.6640625, 'learning_rate': 2.150159499373649e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9642, 'grad_norm': 0.86328125, 'learning_rate': 2.1490232616272628e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3166, 'grad_norm': 0.55078125, 'learning_rate': 2.147887023880877e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1776, 'grad_norm': 0.75390625, 'learning_rate': 2.146750786134491e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1805, 'grad_norm': 0.435546875, 'learning_rate': 2.1456145483881048e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1913, 'grad_norm': 0.93359375, 'learning_rate': 2.1444783106417186e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1049, 'grad_norm': 1.1171875, 'learning_rate': 2.1433420728953327e-05, 'epoch': 0.81}\n",
      "{'loss': 1.289, 'grad_norm': 0.609375, 'learning_rate': 2.1422058351489465e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0849, 'grad_norm': 0.875, 'learning_rate': 2.1410695974025606e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3004, 'grad_norm': 0.84375, 'learning_rate': 2.1399333596561748e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3342, 'grad_norm': 0.72265625, 'learning_rate': 2.1387971219097886e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1022, 'grad_norm': 0.96875, 'learning_rate': 2.1376608841634023e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2112, 'grad_norm': 0.57421875, 'learning_rate': 2.1365246464170165e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2423, 'grad_norm': 0.8671875, 'learning_rate': 2.1353884086706303e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2577, 'grad_norm': 0.498046875, 'learning_rate': 2.1342521709242444e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2829, 'grad_norm': 0.6875, 'learning_rate': 2.133115933177858e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2257, 'grad_norm': 0.59765625, 'learning_rate': 2.1319796954314723e-05, 'epoch': 0.81}\n",
      "{'loss': 1.4402, 'grad_norm': 1.125, 'learning_rate': 2.130843457685086e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0808, 'grad_norm': 0.92578125, 'learning_rate': 2.1297072199387e-05, 'epoch': 0.81}\n",
      "{'loss': 1.247, 'grad_norm': 0.51171875, 'learning_rate': 2.1285709821923143e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1054, 'grad_norm': 0.5546875, 'learning_rate': 2.127434744445928e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1604, 'grad_norm': 1.296875, 'learning_rate': 2.126298506699542e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2789, 'grad_norm': 0.73046875, 'learning_rate': 2.1251622689531557e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1326, 'grad_norm': 0.8984375, 'learning_rate': 2.1240260312067698e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1599, 'grad_norm': 1.0234375, 'learning_rate': 2.1228897934603836e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3633, 'grad_norm': 0.93359375, 'learning_rate': 2.1217535557139977e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0845, 'grad_norm': 0.85546875, 'learning_rate': 2.120617317967612e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2192, 'grad_norm': 0.8203125, 'learning_rate': 2.1194810802212256e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1252, 'grad_norm': 0.7421875, 'learning_rate': 2.1183448424748394e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0795, 'grad_norm': 0.42578125, 'learning_rate': 2.1172086047284535e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2199, 'grad_norm': 0.6640625, 'learning_rate': 2.1160723669820677e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1227, 'grad_norm': 0.8828125, 'learning_rate': 2.1149361292356814e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2637, 'grad_norm': 0.498046875, 'learning_rate': 2.1137998914892952e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2197, 'grad_norm': 0.859375, 'learning_rate': 2.1126636537429094e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1142, 'grad_norm': 0.498046875, 'learning_rate': 2.111527415996523e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2081, 'grad_norm': 0.52734375, 'learning_rate': 2.110391178250137e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1595, 'grad_norm': 0.94921875, 'learning_rate': 2.1092549405037514e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2723, 'grad_norm': 0.546875, 'learning_rate': 2.1081187027573652e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2748, 'grad_norm': 0.83203125, 'learning_rate': 2.106982465010979e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1004, 'grad_norm': 0.494140625, 'learning_rate': 2.1058462272645927e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1112, 'grad_norm': 0.59375, 'learning_rate': 2.104709989518207e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1626, 'grad_norm': 0.83203125, 'learning_rate': 2.1035737517718207e-05, 'epoch': 0.81}\n",
      "{'loss': 1.395, 'grad_norm': 0.5, 'learning_rate': 2.1024375140254348e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2351, 'grad_norm': 0.68359375, 'learning_rate': 2.101301276279049e-05, 'epoch': 0.81}\n",
      "{'loss': 1.28, 'grad_norm': 0.484375, 'learning_rate': 2.1001650385326627e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3466, 'grad_norm': 1.1640625, 'learning_rate': 2.0990288007862765e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0832, 'grad_norm': 0.94921875, 'learning_rate': 2.0978925630398906e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3448, 'grad_norm': 0.58984375, 'learning_rate': 2.0967563252935047e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1631, 'grad_norm': 0.7109375, 'learning_rate': 2.0956200875471185e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2491, 'grad_norm': 0.4296875, 'learning_rate': 2.0944838498007323e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2311, 'grad_norm': 0.6328125, 'learning_rate': 2.0933476120543464e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0578, 'grad_norm': 0.546875, 'learning_rate': 2.0922113743079602e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2746, 'grad_norm': 0.7734375, 'learning_rate': 2.091075136561574e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0893, 'grad_norm': 0.796875, 'learning_rate': 2.0899388988151884e-05, 'epoch': 0.81}\n",
      "{'loss': 1.119, 'grad_norm': 0.443359375, 'learning_rate': 2.0888026610688022e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2933, 'grad_norm': 0.6171875, 'learning_rate': 2.087666423322416e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0516, 'grad_norm': 1.03125, 'learning_rate': 2.0865301855760298e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2632, 'grad_norm': 0.50390625, 'learning_rate': 2.085393947829644e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1985, 'grad_norm': 0.70703125, 'learning_rate': 2.084257710083258e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2305, 'grad_norm': 0.55078125, 'learning_rate': 2.083121472336872e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1976, 'grad_norm': 0.65234375, 'learning_rate': 2.081985234590486e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0395, 'grad_norm': 1.46875, 'learning_rate': 2.0808489968440997e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2916, 'grad_norm': 0.546875, 'learning_rate': 2.0797127590977135e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0916, 'grad_norm': 0.65625, 'learning_rate': 2.0785765213513277e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1435, 'grad_norm': 0.404296875, 'learning_rate': 2.0774402836049418e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2261, 'grad_norm': 0.58984375, 'learning_rate': 2.0763040458585556e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9926, 'grad_norm': 0.80078125, 'learning_rate': 2.0751678081121694e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3211, 'grad_norm': 0.57421875, 'learning_rate': 2.0740315703657835e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2558, 'grad_norm': 0.80078125, 'learning_rate': 2.0728953326193973e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1295, 'grad_norm': 0.7890625, 'learning_rate': 2.0717590948730114e-05, 'epoch': 0.81}\n",
      "{'loss': 1.133, 'grad_norm': 0.52734375, 'learning_rate': 2.0706228571266255e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9449, 'grad_norm': 0.50390625, 'learning_rate': 2.0694866193802393e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1607, 'grad_norm': 0.52734375, 'learning_rate': 2.068350381633853e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1504, 'grad_norm': 0.74609375, 'learning_rate': 2.067214143887467e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0933, 'grad_norm': 0.52734375, 'learning_rate': 2.066077906141081e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1251, 'grad_norm': 0.546875, 'learning_rate': 2.064941668394695e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0969, 'grad_norm': 1.265625, 'learning_rate': 2.063805430648309e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3522, 'grad_norm': 0.54296875, 'learning_rate': 2.062669192901923e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0769, 'grad_norm': 0.68359375, 'learning_rate': 2.0615329551555368e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2058, 'grad_norm': 0.41796875, 'learning_rate': 2.0603967174091506e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2621, 'grad_norm': 0.6484375, 'learning_rate': 2.0592604796627647e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9556, 'grad_norm': 0.62109375, 'learning_rate': 2.058124241916379e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2667, 'grad_norm': 0.578125, 'learning_rate': 2.0569880041699926e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1764, 'grad_norm': 0.87109375, 'learning_rate': 2.0558517664236064e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2224, 'grad_norm': 0.45703125, 'learning_rate': 2.0547155286772205e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2929, 'grad_norm': 0.9375, 'learning_rate': 2.0535792909308343e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1081, 'grad_norm': 1.4296875, 'learning_rate': 2.0524430531844484e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3231, 'grad_norm': 0.515625, 'learning_rate': 2.0513068154380626e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0672, 'grad_norm': 0.7265625, 'learning_rate': 2.0501705776916764e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2371, 'grad_norm': 0.51171875, 'learning_rate': 2.04903433994529e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2563, 'grad_norm': 0.8515625, 'learning_rate': 2.0478981021989043e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9981, 'grad_norm': 0.984375, 'learning_rate': 2.0467618644525184e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1346, 'grad_norm': 0.5, 'learning_rate': 2.0456256267061322e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1041, 'grad_norm': 0.75, 'learning_rate': 2.044489388959746e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1943, 'grad_norm': 0.546875, 'learning_rate': 2.04335315121336e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1991, 'grad_norm': 0.62109375, 'learning_rate': 2.042216913466974e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1468, 'grad_norm': 4.28125, 'learning_rate': 2.0410806757205877e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3144, 'grad_norm': 0.56640625, 'learning_rate': 2.0399444379742018e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1804, 'grad_norm': 0.87890625, 'learning_rate': 2.038808200227816e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1255, 'grad_norm': 0.5078125, 'learning_rate': 2.0376719624814297e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3566, 'grad_norm': 0.56640625, 'learning_rate': 2.0365357247350435e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1488, 'grad_norm': 1.0546875, 'learning_rate': 2.0353994869886576e-05, 'epoch': 0.81}\n",
      "{'loss': 1.4385, 'grad_norm': 0.55859375, 'learning_rate': 2.0342632492422717e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1001, 'grad_norm': 0.71875, 'learning_rate': 2.0331270114958855e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3204, 'grad_norm': 0.671875, 'learning_rate': 2.0319907737494996e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2276, 'grad_norm': 0.6796875, 'learning_rate': 2.0308545360031134e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0461, 'grad_norm': 1.171875, 'learning_rate': 2.0297182982567272e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2253, 'grad_norm': 0.5703125, 'learning_rate': 2.0285820605103413e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0786, 'grad_norm': 0.7734375, 'learning_rate': 2.0274458227639555e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1264, 'grad_norm': 0.50390625, 'learning_rate': 2.0263095850175692e-05, 'epoch': 0.81}\n",
      "{'loss': 1.4141, 'grad_norm': 0.68359375, 'learning_rate': 2.025173347271183e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0251, 'grad_norm': 0.83984375, 'learning_rate': 2.024037109524797e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2623, 'grad_norm': 0.57421875, 'learning_rate': 2.022900871778411e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2254, 'grad_norm': 0.859375, 'learning_rate': 2.021764634032025e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1703, 'grad_norm': 0.482421875, 'learning_rate': 2.020628396285639e-05, 'epoch': 0.81}\n",
      "{'loss': 1.236, 'grad_norm': 0.64453125, 'learning_rate': 2.019492158539253e-05, 'epoch': 0.81}\n",
      "{'loss': 1.086, 'grad_norm': 1.1015625, 'learning_rate': 2.0183559207928668e-05, 'epoch': 0.81}\n",
      "{'loss': 1.27, 'grad_norm': 0.59765625, 'learning_rate': 2.0172196830464805e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1552, 'grad_norm': 0.5234375, 'learning_rate': 2.0160834453000947e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2339, 'grad_norm': 0.62890625, 'learning_rate': 2.0149472075537088e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2365, 'grad_norm': 0.6328125, 'learning_rate': 2.0138109698073226e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0309, 'grad_norm': 0.53125, 'learning_rate': 2.0126747320609367e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3328, 'grad_norm': 0.5234375, 'learning_rate': 2.0115384943145505e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1754, 'grad_norm': 0.9921875, 'learning_rate': 2.0104022565681643e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2633, 'grad_norm': 0.498046875, 'learning_rate': 2.0092660188217784e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2775, 'grad_norm': 0.6171875, 'learning_rate': 2.0081297810753925e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0414, 'grad_norm': 1.0078125, 'learning_rate': 2.0069935433290063e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3985, 'grad_norm': 0.5625, 'learning_rate': 2.00585730558262e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3099, 'grad_norm': 0.72265625, 'learning_rate': 2.0047210678362342e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1801, 'grad_norm': 0.4453125, 'learning_rate': 2.003584830089848e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2579, 'grad_norm': 0.60546875, 'learning_rate': 2.002448592343462e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9769, 'grad_norm': 0.69140625, 'learning_rate': 2.001312354597076e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2723, 'grad_norm': 0.50390625, 'learning_rate': 2.00017611685069e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2029, 'grad_norm': 0.5859375, 'learning_rate': 1.9990398791043038e-05, 'epoch': 0.81}\n",
      "{'loss': 1.195, 'grad_norm': 0.435546875, 'learning_rate': 1.9979036413579176e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2105, 'grad_norm': 0.671875, 'learning_rate': 1.9967674036115317e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0937, 'grad_norm': 0.875, 'learning_rate': 1.995631165865146e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3262, 'grad_norm': 0.51171875, 'learning_rate': 1.9944949281187596e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0049, 'grad_norm': 0.84375, 'learning_rate': 1.9933586903723738e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2273, 'grad_norm': 0.4921875, 'learning_rate': 1.9922224526259875e-05, 'epoch': 0.81}\n",
      "{'loss': 1.223, 'grad_norm': 0.69140625, 'learning_rate': 1.9910862148796013e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0795, 'grad_norm': 1.0625, 'learning_rate': 1.9899499771332155e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2516, 'grad_norm': 0.50390625, 'learning_rate': 1.9888137393868296e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1898, 'grad_norm': 0.90625, 'learning_rate': 1.9876775016404434e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2135, 'grad_norm': 0.5546875, 'learning_rate': 1.986541263894057e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2322, 'grad_norm': 0.61328125, 'learning_rate': 1.9854050261476713e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0983, 'grad_norm': 1.1484375, 'learning_rate': 1.984268788401285e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3293, 'grad_norm': 0.5, 'learning_rate': 1.9831325506548992e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1059, 'grad_norm': 0.828125, 'learning_rate': 1.981996312908513e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3733, 'grad_norm': 0.48828125, 'learning_rate': 1.980860075162127e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1777, 'grad_norm': 0.59375, 'learning_rate': 1.979723837415741e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0967, 'grad_norm': 0.94140625, 'learning_rate': 1.9785875996693547e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3964, 'grad_norm': 0.5625, 'learning_rate': 1.977451361922969e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0644, 'grad_norm': 0.65625, 'learning_rate': 1.976315124176583e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1518, 'grad_norm': 0.462890625, 'learning_rate': 1.9751788864301967e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1988, 'grad_norm': 0.62109375, 'learning_rate': 1.9740426486838108e-05, 'epoch': 0.81}\n",
      "{'loss': 1.074, 'grad_norm': 0.89453125, 'learning_rate': 1.9729064109374246e-05, 'epoch': 0.81}\n",
      "{'loss': 1.314, 'grad_norm': 0.53125, 'learning_rate': 1.9717701731910384e-05, 'epoch': 0.81}\n",
      "{'loss': 1.316, 'grad_norm': 0.7265625, 'learning_rate': 1.9706339354446525e-05, 'epoch': 0.81}\n",
      "{'loss': 1.233, 'grad_norm': 0.515625, 'learning_rate': 1.9694976976982666e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1753, 'grad_norm': 0.80859375, 'learning_rate': 1.9683614599518804e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0825, 'grad_norm': 0.57421875, 'learning_rate': 1.9672252222054942e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2773, 'grad_norm': 0.52734375, 'learning_rate': 1.9660889844591083e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1241, 'grad_norm': 0.84765625, 'learning_rate': 1.9649527467127225e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1878, 'grad_norm': 0.431640625, 'learning_rate': 1.9638165089663362e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1857, 'grad_norm': 0.640625, 'learning_rate': 1.96268027121995e-05, 'epoch': 0.81}\n",
      "{'loss': 1.177, 'grad_norm': 0.82421875, 'learning_rate': 1.961544033473564e-05, 'epoch': 0.81}\n",
      "{'loss': 1.323, 'grad_norm': 0.5703125, 'learning_rate': 1.960407795727178e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1845, 'grad_norm': 0.62109375, 'learning_rate': 1.9592715579807917e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0987, 'grad_norm': 0.470703125, 'learning_rate': 1.9581353202344062e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2194, 'grad_norm': 0.57421875, 'learning_rate': 1.95699908248802e-05, 'epoch': 0.81}\n",
      "{'loss': 0.987, 'grad_norm': 0.765625, 'learning_rate': 1.9558628447416338e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1658, 'grad_norm': 0.578125, 'learning_rate': 1.954726606995248e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1352, 'grad_norm': 1.09375, 'learning_rate': 1.9535903692488617e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2801, 'grad_norm': 0.462890625, 'learning_rate': 1.9524541315024758e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2097, 'grad_norm': 0.5859375, 'learning_rate': 1.9513178937560896e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9188, 'grad_norm': 1.21875, 'learning_rate': 1.9501816560097037e-05, 'epoch': 0.81}\n",
      "{'loss': 1.3338, 'grad_norm': 0.6015625, 'learning_rate': 1.9490454182633175e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2286, 'grad_norm': 0.65234375, 'learning_rate': 1.9479091805169313e-05, 'epoch': 0.81}\n",
      "{'loss': 1.2103, 'grad_norm': 0.640625, 'learning_rate': 1.9467729427705454e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3135, 'grad_norm': 0.78125, 'learning_rate': 1.9456367050241595e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0036, 'grad_norm': 1.0, 'learning_rate': 1.9445004672777733e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3072, 'grad_norm': 0.5, 'learning_rate': 1.943364229531387e-05, 'epoch': 0.82}\n",
      "{'loss': 1.107, 'grad_norm': 0.87890625, 'learning_rate': 1.9422279917850012e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1344, 'grad_norm': 0.53515625, 'learning_rate': 1.941091754038615e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2549, 'grad_norm': 0.498046875, 'learning_rate': 1.939955516292229e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0098, 'grad_norm': 1.171875, 'learning_rate': 1.9388192785458433e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3205, 'grad_norm': 0.6875, 'learning_rate': 1.937683040799457e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1169, 'grad_norm': 0.83203125, 'learning_rate': 1.9365468030530708e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0713, 'grad_norm': 0.427734375, 'learning_rate': 1.935410565306685e-05, 'epoch': 0.82}\n",
      "{'loss': 1.258, 'grad_norm': 0.7734375, 'learning_rate': 1.9342743275602987e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1085, 'grad_norm': 0.984375, 'learning_rate': 1.933138089813913e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3328, 'grad_norm': 0.57421875, 'learning_rate': 1.9320018520675266e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1344, 'grad_norm': 0.75, 'learning_rate': 1.9308656143211408e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1583, 'grad_norm': 0.5546875, 'learning_rate': 1.9297293765747546e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1581, 'grad_norm': 0.5703125, 'learning_rate': 1.9285931388283683e-05, 'epoch': 0.82}\n",
      "{'loss': 1.028, 'grad_norm': 1.1875, 'learning_rate': 1.9274569010819825e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3554, 'grad_norm': 1.0703125, 'learning_rate': 1.9263206633355966e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2144, 'grad_norm': 0.8125, 'learning_rate': 1.9251844255892104e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2709, 'grad_norm': 0.5625, 'learning_rate': 1.924048187842824e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2313, 'grad_norm': 0.88671875, 'learning_rate': 1.9229119500964383e-05, 'epoch': 0.82}\n",
      "{'loss': 0.984, 'grad_norm': 1.140625, 'learning_rate': 1.921775712350052e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2897, 'grad_norm': 0.609375, 'learning_rate': 1.9206394746036662e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1652, 'grad_norm': 0.640625, 'learning_rate': 1.9195032368572803e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1271, 'grad_norm': 0.5546875, 'learning_rate': 1.918366999110894e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2481, 'grad_norm': 0.62109375, 'learning_rate': 1.917230761364508e-05, 'epoch': 0.82}\n",
      "{'loss': 1.015, 'grad_norm': 0.921875, 'learning_rate': 1.916094523618122e-05, 'epoch': 0.82}\n",
      "{'loss': 1.435, 'grad_norm': 0.58984375, 'learning_rate': 1.9149582858717358e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2707, 'grad_norm': 0.703125, 'learning_rate': 1.91382204812535e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2127, 'grad_norm': 0.5703125, 'learning_rate': 1.9126858103789637e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1654, 'grad_norm': 0.72265625, 'learning_rate': 1.911549572632578e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0662, 'grad_norm': 0.7109375, 'learning_rate': 1.9104133348861916e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2477, 'grad_norm': 0.609375, 'learning_rate': 1.9092770971398054e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2605, 'grad_norm': 0.52734375, 'learning_rate': 1.9081408593934195e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1665, 'grad_norm': 0.46484375, 'learning_rate': 1.9070046216470337e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1532, 'grad_norm': 0.6015625, 'learning_rate': 1.9058683839006474e-05, 'epoch': 0.82}\n",
      "{'loss': 1.116, 'grad_norm': 0.91015625, 'learning_rate': 1.9047321461542612e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1973, 'grad_norm': 0.54296875, 'learning_rate': 1.9035959084078753e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1077, 'grad_norm': 0.8046875, 'learning_rate': 1.902459670661489e-05, 'epoch': 0.82}\n",
      "{'loss': 1.196, 'grad_norm': 0.447265625, 'learning_rate': 1.9013234329151033e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2447, 'grad_norm': 0.58984375, 'learning_rate': 1.9001871951687174e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9418, 'grad_norm': 1.25, 'learning_rate': 1.899050957422331e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2652, 'grad_norm': 0.5546875, 'learning_rate': 1.897914719675945e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1491, 'grad_norm': 0.75, 'learning_rate': 1.896778481929559e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1977, 'grad_norm': 0.451171875, 'learning_rate': 1.8956422441831732e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2727, 'grad_norm': 0.6015625, 'learning_rate': 1.894506006436787e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1729, 'grad_norm': 1.1953125, 'learning_rate': 1.8933697686904008e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3858, 'grad_norm': 0.65625, 'learning_rate': 1.892233530944015e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1933, 'grad_norm': 0.69140625, 'learning_rate': 1.8910972931976287e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0928, 'grad_norm': 0.51953125, 'learning_rate': 1.8899610554512425e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1335, 'grad_norm': 0.609375, 'learning_rate': 1.8888248177048566e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0412, 'grad_norm': 0.82421875, 'learning_rate': 1.8876885799584707e-05, 'epoch': 0.82}\n",
      "{'loss': 1.15, 'grad_norm': 0.75390625, 'learning_rate': 1.8865523422120845e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2533, 'grad_norm': 0.875, 'learning_rate': 1.8854161044656983e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1988, 'grad_norm': 0.5625, 'learning_rate': 1.8842798667193124e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2255, 'grad_norm': 0.6796875, 'learning_rate': 1.8831436289729265e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0316, 'grad_norm': 0.42578125, 'learning_rate': 1.8820073912265403e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2576, 'grad_norm': 0.419921875, 'learning_rate': 1.8808711534801544e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2938, 'grad_norm': 1.453125, 'learning_rate': 1.8797349157337682e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1845, 'grad_norm': 0.484375, 'learning_rate': 1.878598677987382e-05, 'epoch': 0.82}\n",
      "{'loss': 1.134, 'grad_norm': 0.76171875, 'learning_rate': 1.877462440240996e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9819, 'grad_norm': 0.49609375, 'learning_rate': 1.8763262024946103e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2056, 'grad_norm': 0.490234375, 'learning_rate': 1.875189964748224e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1328, 'grad_norm': 0.76171875, 'learning_rate': 1.874053727001838e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2101, 'grad_norm': 0.578125, 'learning_rate': 1.872917489255452e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2117, 'grad_norm': 0.60546875, 'learning_rate': 1.8717812515090657e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1156, 'grad_norm': 0.859375, 'learning_rate': 1.87064501376268e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3698, 'grad_norm': 0.6171875, 'learning_rate': 1.8695087760162937e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2019, 'grad_norm': 0.71484375, 'learning_rate': 1.8683725382699078e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1767, 'grad_norm': 0.490234375, 'learning_rate': 1.8672363005235216e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2676, 'grad_norm': 0.84765625, 'learning_rate': 1.8661000627771353e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9091, 'grad_norm': 0.81640625, 'learning_rate': 1.8649638250307495e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2322, 'grad_norm': 0.73828125, 'learning_rate': 1.8638275872843636e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0936, 'grad_norm': 0.7421875, 'learning_rate': 1.8626913495379774e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1216, 'grad_norm': 0.50390625, 'learning_rate': 1.8615551117915915e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3156, 'grad_norm': 0.66796875, 'learning_rate': 1.8604188740452053e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0686, 'grad_norm': 1.25, 'learning_rate': 1.859282636298819e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3758, 'grad_norm': 0.62109375, 'learning_rate': 1.8581463985524332e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1045, 'grad_norm': 0.76953125, 'learning_rate': 1.8570101608060473e-05, 'epoch': 0.82}\n",
      "{'loss': 1.138, 'grad_norm': 0.59375, 'learning_rate': 1.855873923059661e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3148, 'grad_norm': 0.62109375, 'learning_rate': 1.854737685313275e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0659, 'grad_norm': 0.82421875, 'learning_rate': 1.853601447566889e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3863, 'grad_norm': 0.55859375, 'learning_rate': 1.8524652098205028e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1295, 'grad_norm': 0.6875, 'learning_rate': 1.851328972074117e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2289, 'grad_norm': 0.74609375, 'learning_rate': 1.8501927343277307e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2767, 'grad_norm': 0.69140625, 'learning_rate': 1.849056496581345e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0712, 'grad_norm': 1.21875, 'learning_rate': 1.8479202588349586e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2098, 'grad_norm': 0.5859375, 'learning_rate': 1.8467840210885724e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0921, 'grad_norm': 0.71484375, 'learning_rate': 1.845647783342187e-05, 'epoch': 0.82}\n",
      "{'loss': 1.112, 'grad_norm': 0.455078125, 'learning_rate': 1.8445115455958007e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9985, 'grad_norm': 0.64453125, 'learning_rate': 1.8433753078494144e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1057, 'grad_norm': 1.0390625, 'learning_rate': 1.8422390701030286e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1996, 'grad_norm': 0.75390625, 'learning_rate': 1.8411028323566424e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1616, 'grad_norm': 0.8125, 'learning_rate': 1.839966594610256e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1528, 'grad_norm': 0.69140625, 'learning_rate': 1.8388303568638703e-05, 'epoch': 0.82}\n",
      "{'loss': 1.139, 'grad_norm': 0.65234375, 'learning_rate': 1.8376941191174844e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1389, 'grad_norm': 0.7578125, 'learning_rate': 1.8365578813710982e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2307, 'grad_norm': 0.50390625, 'learning_rate': 1.835421643624712e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3235, 'grad_norm': 0.80859375, 'learning_rate': 1.834285405878326e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2665, 'grad_norm': 0.490234375, 'learning_rate': 1.8331491681319402e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1834, 'grad_norm': 0.66015625, 'learning_rate': 1.832012930385554e-05, 'epoch': 0.82}\n",
      "{'loss': 1.143, 'grad_norm': 0.70703125, 'learning_rate': 1.8308766926391678e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3552, 'grad_norm': 0.61328125, 'learning_rate': 1.829740454892782e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1451, 'grad_norm': 0.94921875, 'learning_rate': 1.8286042171463957e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2655, 'grad_norm': 0.484375, 'learning_rate': 1.8274679794000095e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3809, 'grad_norm': 0.55078125, 'learning_rate': 1.826331741653624e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2542, 'grad_norm': 0.9296875, 'learning_rate': 1.8251955039072377e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3115, 'grad_norm': 0.57421875, 'learning_rate': 1.8240592661608515e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1654, 'grad_norm': 0.71875, 'learning_rate': 1.8229230284144656e-05, 'epoch': 0.82}\n",
      "{'loss': 1.308, 'grad_norm': 0.515625, 'learning_rate': 1.8217867906680794e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2479, 'grad_norm': 0.73046875, 'learning_rate': 1.8206505529216932e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0121, 'grad_norm': 1.40625, 'learning_rate': 1.8195143151753073e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2811, 'grad_norm': 0.85546875, 'learning_rate': 1.8183780774289215e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1429, 'grad_norm': 0.68359375, 'learning_rate': 1.8172418396825352e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0106, 'grad_norm': 0.62109375, 'learning_rate': 1.816105601936149e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2588, 'grad_norm': 0.703125, 'learning_rate': 1.814969364189763e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1311, 'grad_norm': 1.125, 'learning_rate': 1.8138331264433773e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3393, 'grad_norm': 0.5703125, 'learning_rate': 1.812696888696991e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1839, 'grad_norm': 0.7265625, 'learning_rate': 1.811560650950605e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2258, 'grad_norm': 0.47265625, 'learning_rate': 1.810424413204219e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2719, 'grad_norm': 0.59765625, 'learning_rate': 1.8092881754578328e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1152, 'grad_norm': 0.71875, 'learning_rate': 1.8081519377114465e-05, 'epoch': 0.82}\n",
      "{'loss': 1.5681, 'grad_norm': 0.474609375, 'learning_rate': 1.807015699965061e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2001, 'grad_norm': 0.6015625, 'learning_rate': 1.8058794622186748e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2674, 'grad_norm': 0.5, 'learning_rate': 1.8047432244722886e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2408, 'grad_norm': 0.6875, 'learning_rate': 1.8036069867259027e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1204, 'grad_norm': 0.8828125, 'learning_rate': 1.8024707489795165e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3219, 'grad_norm': 0.74609375, 'learning_rate': 1.8013345112331306e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0617, 'grad_norm': 0.828125, 'learning_rate': 1.8001982734867444e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1171, 'grad_norm': 0.55078125, 'learning_rate': 1.7990620357403585e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2119, 'grad_norm': 0.6171875, 'learning_rate': 1.7979257979939723e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0474, 'grad_norm': 1.078125, 'learning_rate': 1.796789560247586e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2625, 'grad_norm': 0.6640625, 'learning_rate': 1.7956533225012002e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0668, 'grad_norm': 0.78125, 'learning_rate': 1.7945170847548143e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1637, 'grad_norm': 0.462890625, 'learning_rate': 1.793380847008428e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1951, 'grad_norm': 0.671875, 'learning_rate': 1.792244609262042e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9341, 'grad_norm': 0.671875, 'learning_rate': 1.791108371515656e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2953, 'grad_norm': 0.55859375, 'learning_rate': 1.7899721337692698e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1648, 'grad_norm': 0.71875, 'learning_rate': 1.788835896022884e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2328, 'grad_norm': 0.51953125, 'learning_rate': 1.787699658276498e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1353, 'grad_norm': 0.7421875, 'learning_rate': 1.786563420530112e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9565, 'grad_norm': 1.1015625, 'learning_rate': 1.7854271827837256e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3031, 'grad_norm': 0.63671875, 'learning_rate': 1.7842909450373398e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1681, 'grad_norm': 0.65234375, 'learning_rate': 1.7831547072909535e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1919, 'grad_norm': 0.49609375, 'learning_rate': 1.7820184695445677e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2366, 'grad_norm': 0.7890625, 'learning_rate': 1.7808822317981815e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1627, 'grad_norm': 0.9921875, 'learning_rate': 1.7797459940517956e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3122, 'grad_norm': 0.51171875, 'learning_rate': 1.7786097563054094e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2124, 'grad_norm': 0.65625, 'learning_rate': 1.777473518559023e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1488, 'grad_norm': 0.59765625, 'learning_rate': 1.7763372808126376e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1259, 'grad_norm': 0.7734375, 'learning_rate': 1.7752010430662514e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0627, 'grad_norm': 1.453125, 'learning_rate': 1.7740648053198652e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1974, 'grad_norm': 0.50390625, 'learning_rate': 1.772928567573479e-05, 'epoch': 0.82}\n",
      "{'loss': 1.14, 'grad_norm': 0.625, 'learning_rate': 1.771792329827093e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1145, 'grad_norm': 0.4375, 'learning_rate': 1.770656092080707e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2032, 'grad_norm': 0.6328125, 'learning_rate': 1.769519854334321e-05, 'epoch': 0.82}\n",
      "{'loss': 1.066, 'grad_norm': 1.3203125, 'learning_rate': 1.768383616587935e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3387, 'grad_norm': 0.5390625, 'learning_rate': 1.767247378841549e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1635, 'grad_norm': 0.796875, 'learning_rate': 1.7661111410951627e-05, 'epoch': 0.82}\n",
      "{'loss': 1.135, 'grad_norm': 0.404296875, 'learning_rate': 1.7649749033487768e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2449, 'grad_norm': 0.65234375, 'learning_rate': 1.763838665602391e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0287, 'grad_norm': 0.76953125, 'learning_rate': 1.7627024278560047e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2196, 'grad_norm': 0.5078125, 'learning_rate': 1.7615661901096185e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1451, 'grad_norm': 0.734375, 'learning_rate': 1.7604299523632326e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1494, 'grad_norm': 0.515625, 'learning_rate': 1.7592937146168464e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1958, 'grad_norm': 0.53125, 'learning_rate': 1.7581574768704602e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0381, 'grad_norm': 0.796875, 'learning_rate': 1.7570212391240747e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.32, 'grad_norm': 0.5703125, 'learning_rate': 1.7558850013776885e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1261, 'grad_norm': 1.0078125, 'learning_rate': 1.7547487636313022e-05, 'epoch': 0.82}\n",
      "{'loss': 1.143, 'grad_norm': 0.63671875, 'learning_rate': 1.7536125258849164e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2104, 'grad_norm': 1.03125, 'learning_rate': 1.75247628813853e-05, 'epoch': 0.82}\n",
      "{'loss': 1.001, 'grad_norm': 0.65625, 'learning_rate': 1.7513400503921443e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2328, 'grad_norm': 0.65234375, 'learning_rate': 1.750203812645758e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1926, 'grad_norm': 0.72265625, 'learning_rate': 1.7490675748993722e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1166, 'grad_norm': 0.484375, 'learning_rate': 1.747931337152986e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1335, 'grad_norm': 0.58984375, 'learning_rate': 1.7467950994065998e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9814, 'grad_norm': 0.84765625, 'learning_rate': 1.745658861660214e-05, 'epoch': 0.82}\n",
      "{'loss': 1.4799, 'grad_norm': 0.69921875, 'learning_rate': 1.744522623913828e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0828, 'grad_norm': 0.69140625, 'learning_rate': 1.7433863861674418e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1603, 'grad_norm': 0.4453125, 'learning_rate': 1.7422501484210556e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1574, 'grad_norm': 0.69140625, 'learning_rate': 1.7411139106746697e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0066, 'grad_norm': 0.63671875, 'learning_rate': 1.7399776729282835e-05, 'epoch': 0.82}\n",
      "{'loss': 1.3523, 'grad_norm': 0.40625, 'learning_rate': 1.7388414351818976e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2173, 'grad_norm': 0.609375, 'learning_rate': 1.7377051974355117e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1015, 'grad_norm': 0.4375, 'learning_rate': 1.7365689596891255e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1448, 'grad_norm': 0.59765625, 'learning_rate': 1.7354327219427393e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0204, 'grad_norm': 0.609375, 'learning_rate': 1.7342964841963534e-05, 'epoch': 0.82}\n",
      "{'loss': 1.4496, 'grad_norm': 0.73828125, 'learning_rate': 1.7331602464499672e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1578, 'grad_norm': 0.79296875, 'learning_rate': 1.7320240087035813e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1579, 'grad_norm': 0.4609375, 'learning_rate': 1.730887770957195e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1434, 'grad_norm': 0.74609375, 'learning_rate': 1.7297515332108093e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1411, 'grad_norm': 1.59375, 'learning_rate': 1.728615295464423e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3525, 'grad_norm': 0.67578125, 'learning_rate': 1.7274790577180368e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1688, 'grad_norm': 0.77734375, 'learning_rate': 1.726342819971651e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0781, 'grad_norm': 0.59375, 'learning_rate': 1.725206582225265e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0927, 'grad_norm': 0.77734375, 'learning_rate': 1.724070344478879e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0559, 'grad_norm': 0.828125, 'learning_rate': 1.7229341067324926e-05, 'epoch': 0.82}\n",
      "{'loss': 1.4361, 'grad_norm': 0.609375, 'learning_rate': 1.7217978689861068e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1288, 'grad_norm': 1.1015625, 'learning_rate': 1.7206616312397206e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2518, 'grad_norm': 0.640625, 'learning_rate': 1.7195253934933347e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1441, 'grad_norm': 0.7265625, 'learning_rate': 1.7183891557469488e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1165, 'grad_norm': 1.0234375, 'learning_rate': 1.7172529180005626e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3152, 'grad_norm': 0.5703125, 'learning_rate': 1.7161166802541764e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2032, 'grad_norm': 0.734375, 'learning_rate': 1.7149804425077905e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1742, 'grad_norm': 0.6328125, 'learning_rate': 1.7138442047614043e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1358, 'grad_norm': 0.6328125, 'learning_rate': 1.7127079670150184e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8879, 'grad_norm': 0.62109375, 'learning_rate': 1.7115717292686322e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2995, 'grad_norm': 0.5859375, 'learning_rate': 1.7104354915222463e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1917, 'grad_norm': 0.6953125, 'learning_rate': 1.70929925377586e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2964, 'grad_norm': 0.46484375, 'learning_rate': 1.708163016029474e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3073, 'grad_norm': 0.65234375, 'learning_rate': 1.707026778283088e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0254, 'grad_norm': 0.98046875, 'learning_rate': 1.705890540536702e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3294, 'grad_norm': 0.4765625, 'learning_rate': 1.704754302790316e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1992, 'grad_norm': 1.0546875, 'learning_rate': 1.7036180650439297e-05, 'epoch': 0.83}\n",
      "{'loss': 1.082, 'grad_norm': 0.65234375, 'learning_rate': 1.702481827297544e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1858, 'grad_norm': 0.55859375, 'learning_rate': 1.7013455895511576e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0048, 'grad_norm': 0.9140625, 'learning_rate': 1.7002093518047717e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3003, 'grad_norm': 0.6796875, 'learning_rate': 1.699073114058386e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0694, 'grad_norm': 0.65625, 'learning_rate': 1.6979368763119997e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1797, 'grad_norm': 0.51953125, 'learning_rate': 1.6968006385656134e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4159, 'grad_norm': 0.59765625, 'learning_rate': 1.6956644008192276e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0962, 'grad_norm': 0.71484375, 'learning_rate': 1.6945281630728417e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1859, 'grad_norm': 0.4921875, 'learning_rate': 1.6933919253264555e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1475, 'grad_norm': 0.87109375, 'learning_rate': 1.6922556875800693e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1932, 'grad_norm': 0.50390625, 'learning_rate': 1.6911194498336834e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3676, 'grad_norm': 0.66015625, 'learning_rate': 1.689983212087297e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0084, 'grad_norm': 0.66015625, 'learning_rate': 1.688846974340911e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3223, 'grad_norm': 0.69921875, 'learning_rate': 1.687710736594525e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2029, 'grad_norm': 1.4140625, 'learning_rate': 1.6865744988481392e-05, 'epoch': 0.83}\n",
      "{'loss': 1.193, 'grad_norm': 0.47265625, 'learning_rate': 1.685438261101753e-05, 'epoch': 0.83}\n",
      "{'loss': 1.243, 'grad_norm': 0.76171875, 'learning_rate': 1.6843020233553668e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9165, 'grad_norm': 0.953125, 'learning_rate': 1.683165785608981e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3284, 'grad_norm': 0.59375, 'learning_rate': 1.682029547862595e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1929, 'grad_norm': 1.0, 'learning_rate': 1.6808933101162088e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1808, 'grad_norm': 0.58984375, 'learning_rate': 1.679757072369823e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2473, 'grad_norm': 0.66796875, 'learning_rate': 1.6786208346234367e-05, 'epoch': 0.83}\n",
      "{'loss': 1.151, 'grad_norm': 0.828125, 'learning_rate': 1.6774845968770505e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3574, 'grad_norm': 0.63671875, 'learning_rate': 1.6763483591306646e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3347, 'grad_norm': 0.703125, 'learning_rate': 1.6752121213842787e-05, 'epoch': 0.83}\n",
      "{'loss': 1.247, 'grad_norm': 0.451171875, 'learning_rate': 1.6740758836378925e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1608, 'grad_norm': 0.7265625, 'learning_rate': 1.6729396458915063e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0146, 'grad_norm': 0.62109375, 'learning_rate': 1.6718034081451204e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3726, 'grad_norm': 0.4921875, 'learning_rate': 1.6706671703987342e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1995, 'grad_norm': 0.84765625, 'learning_rate': 1.6695309326523484e-05, 'epoch': 0.83}\n",
      "{'loss': 1.138, 'grad_norm': 0.4609375, 'learning_rate': 1.668394694905962e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2189, 'grad_norm': 0.7265625, 'learning_rate': 1.6672584571595763e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1275, 'grad_norm': 0.8671875, 'learning_rate': 1.66612221941319e-05, 'epoch': 0.83}\n",
      "{'loss': 1.447, 'grad_norm': 0.5078125, 'learning_rate': 1.664985981666804e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1054, 'grad_norm': 0.5859375, 'learning_rate': 1.663849743920418e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1924, 'grad_norm': 0.4765625, 'learning_rate': 1.662713506174032e-05, 'epoch': 0.83}\n",
      "{'loss': 1.193, 'grad_norm': 0.7578125, 'learning_rate': 1.661577268427646e-05, 'epoch': 0.83}\n",
      "{'loss': 1.052, 'grad_norm': 0.8203125, 'learning_rate': 1.66044103068126e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3155, 'grad_norm': 0.578125, 'learning_rate': 1.6593047929348738e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1597, 'grad_norm': 0.875, 'learning_rate': 1.6581685551884876e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2346, 'grad_norm': 0.625, 'learning_rate': 1.6570323174421017e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2164, 'grad_norm': 0.59765625, 'learning_rate': 1.6558960796957158e-05, 'epoch': 0.83}\n",
      "{'loss': 1.037, 'grad_norm': 1.2265625, 'learning_rate': 1.6547598419493296e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3345, 'grad_norm': 0.69140625, 'learning_rate': 1.6536236042029434e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1023, 'grad_norm': 0.625, 'learning_rate': 1.6524873664565575e-05, 'epoch': 0.83}\n",
      "{'loss': 1.148, 'grad_norm': 0.54296875, 'learning_rate': 1.6513511287101713e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1103, 'grad_norm': 0.74609375, 'learning_rate': 1.6502148909637854e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0724, 'grad_norm': 1.046875, 'learning_rate': 1.6490786532173992e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3673, 'grad_norm': 0.5234375, 'learning_rate': 1.6479424154710133e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2021, 'grad_norm': 0.73828125, 'learning_rate': 1.646806177724627e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2046, 'grad_norm': 0.51953125, 'learning_rate': 1.645669939978241e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1398, 'grad_norm': 0.5703125, 'learning_rate': 1.644533702231855e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0284, 'grad_norm': 1.046875, 'learning_rate': 1.643397464485469e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1664, 'grad_norm': 0.6484375, 'learning_rate': 1.642261226739083e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0336, 'grad_norm': 0.70703125, 'learning_rate': 1.641124988992697e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1999, 'grad_norm': 0.48828125, 'learning_rate': 1.639988751246311e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1737, 'grad_norm': 0.5, 'learning_rate': 1.6388525134999246e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0702, 'grad_norm': 0.703125, 'learning_rate': 1.6377162757535387e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4588, 'grad_norm': 0.62890625, 'learning_rate': 1.636580038007153e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2311, 'grad_norm': 0.796875, 'learning_rate': 1.6354438002607667e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2294, 'grad_norm': 0.45703125, 'learning_rate': 1.6343075625143804e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2411, 'grad_norm': 0.75, 'learning_rate': 1.6331713247679946e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0501, 'grad_norm': 1.3359375, 'learning_rate': 1.6320350870216084e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4245, 'grad_norm': 0.59375, 'learning_rate': 1.6308988492752225e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1027, 'grad_norm': 0.66015625, 'learning_rate': 1.6297626115288363e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3072, 'grad_norm': 0.5546875, 'learning_rate': 1.6286263737824504e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2553, 'grad_norm': 0.5703125, 'learning_rate': 1.6274901360360642e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1075, 'grad_norm': 0.96484375, 'learning_rate': 1.626353898289678e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1848, 'grad_norm': 0.70703125, 'learning_rate': 1.6252176605432924e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0689, 'grad_norm': 0.8515625, 'learning_rate': 1.6240814227969062e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1631, 'grad_norm': 0.46484375, 'learning_rate': 1.62294518505052e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2069, 'grad_norm': 0.75, 'learning_rate': 1.621808947304134e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0504, 'grad_norm': 1.1328125, 'learning_rate': 1.620672709557748e-05, 'epoch': 0.83}\n",
      "{'loss': 1.456, 'grad_norm': 0.6953125, 'learning_rate': 1.6195364718113617e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1724, 'grad_norm': 0.640625, 'learning_rate': 1.6184002340649758e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1592, 'grad_norm': 0.423828125, 'learning_rate': 1.61726399631859e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1956, 'grad_norm': 0.625, 'learning_rate': 1.6161277585722037e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1362, 'grad_norm': 0.8125, 'learning_rate': 1.6149915208258175e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2753, 'grad_norm': 0.6171875, 'learning_rate': 1.6138552830794316e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1304, 'grad_norm': 0.67578125, 'learning_rate': 1.6127190453330458e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1647, 'grad_norm': 0.63671875, 'learning_rate': 1.6115828075866595e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1967, 'grad_norm': 0.625, 'learning_rate': 1.6104465698402733e-05, 'epoch': 0.83}\n",
      "{'loss': 0.974, 'grad_norm': 0.73046875, 'learning_rate': 1.6093103320938875e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3828, 'grad_norm': 0.482421875, 'learning_rate': 1.6081740943475012e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0496, 'grad_norm': 0.93359375, 'learning_rate': 1.607037856601115e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1736, 'grad_norm': 0.498046875, 'learning_rate': 1.6059016188547295e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3585, 'grad_norm': 0.64453125, 'learning_rate': 1.6047653811083433e-05, 'epoch': 0.83}\n",
      "{'loss': 1.013, 'grad_norm': 1.625, 'learning_rate': 1.603629143361957e-05, 'epoch': 0.83}\n",
      "{'loss': 1.199, 'grad_norm': 0.55859375, 'learning_rate': 1.6024929056155712e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9295, 'grad_norm': 0.8203125, 'learning_rate': 1.601356667869185e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1916, 'grad_norm': 0.4765625, 'learning_rate': 1.600220430122799e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2459, 'grad_norm': 0.9453125, 'learning_rate': 1.599084192376413e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1124, 'grad_norm': 0.66796875, 'learning_rate': 1.597947954630027e-05, 'epoch': 0.83}\n",
      "{'loss': 1.243, 'grad_norm': 0.50390625, 'learning_rate': 1.5968117168836408e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1035, 'grad_norm': 1.2890625, 'learning_rate': 1.5956754791372546e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0873, 'grad_norm': 0.62109375, 'learning_rate': 1.5945392413908687e-05, 'epoch': 0.83}\n",
      "{'loss': 1.192, 'grad_norm': 0.7421875, 'learning_rate': 1.5934030036444828e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9875, 'grad_norm': 0.97265625, 'learning_rate': 1.5922667658980966e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3751, 'grad_norm': 0.55859375, 'learning_rate': 1.5911305281517104e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0302, 'grad_norm': 0.703125, 'learning_rate': 1.5899942904053245e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0772, 'grad_norm': 0.494140625, 'learning_rate': 1.5888580526589383e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2151, 'grad_norm': 0.765625, 'learning_rate': 1.5877218149125524e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0935, 'grad_norm': 1.0703125, 'learning_rate': 1.5865855771661665e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2406, 'grad_norm': 0.50390625, 'learning_rate': 1.5854493394197803e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0636, 'grad_norm': 0.7421875, 'learning_rate': 1.584313101673394e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1666, 'grad_norm': 0.455078125, 'learning_rate': 1.5831768639270082e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2528, 'grad_norm': 0.80859375, 'learning_rate': 1.582040626180622e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9951, 'grad_norm': 1.0703125, 'learning_rate': 1.580904388434236e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2722, 'grad_norm': 0.66015625, 'learning_rate': 1.57976815068785e-05, 'epoch': 0.83}\n",
      "{'loss': 1.154, 'grad_norm': 0.71484375, 'learning_rate': 1.578631912941464e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1129, 'grad_norm': 0.59765625, 'learning_rate': 1.577495675195078e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2381, 'grad_norm': 0.63671875, 'learning_rate': 1.5763594374486916e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0188, 'grad_norm': 0.85546875, 'learning_rate': 1.5752231997023058e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3452, 'grad_norm': 0.5703125, 'learning_rate': 1.57408696195592e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3122, 'grad_norm': 0.84375, 'learning_rate': 1.5729507242095337e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3694, 'grad_norm': 0.427734375, 'learning_rate': 1.5718144864631475e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1372, 'grad_norm': 0.609375, 'learning_rate': 1.5706782487167616e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9758, 'grad_norm': 1.71875, 'learning_rate': 1.5695420109703754e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4938, 'grad_norm': 0.5859375, 'learning_rate': 1.5684057732239895e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1713, 'grad_norm': 0.6328125, 'learning_rate': 1.5672695354776036e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1118, 'grad_norm': 0.498046875, 'learning_rate': 1.5661332977312174e-05, 'epoch': 0.83}\n",
      "{'loss': 1.198, 'grad_norm': 0.6328125, 'learning_rate': 1.5649970599848312e-05, 'epoch': 0.83}\n",
      "{'loss': 1.021, 'grad_norm': 0.859375, 'learning_rate': 1.5638608222384453e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4252, 'grad_norm': 1.1640625, 'learning_rate': 1.5627245844920594e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0802, 'grad_norm': 0.640625, 'learning_rate': 1.5615883467456732e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0768, 'grad_norm': 0.58984375, 'learning_rate': 1.560452108999287e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1689, 'grad_norm': 0.6328125, 'learning_rate': 1.559315871252901e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9733, 'grad_norm': 1.0625, 'learning_rate': 1.558179633506515e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2901, 'grad_norm': 0.875, 'learning_rate': 1.5570433957601287e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0755, 'grad_norm': 0.76171875, 'learning_rate': 1.5559071580137428e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2566, 'grad_norm': 0.52734375, 'learning_rate': 1.554770920267357e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2408, 'grad_norm': 0.65234375, 'learning_rate': 1.5536346825209707e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0432, 'grad_norm': 0.73046875, 'learning_rate': 1.5524984447745845e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3062, 'grad_norm': 0.6171875, 'learning_rate': 1.5513622070281986e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1333, 'grad_norm': 0.62890625, 'learning_rate': 1.5502259692818124e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0532, 'grad_norm': 0.703125, 'learning_rate': 1.5490897315354265e-05, 'epoch': 0.83}\n",
      "{'loss': 1.186, 'grad_norm': 0.640625, 'learning_rate': 1.5479534937890407e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0429, 'grad_norm': 0.7265625, 'learning_rate': 1.5468172560426545e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2555, 'grad_norm': 0.75390625, 'learning_rate': 1.5456810182962682e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1933, 'grad_norm': 0.8515625, 'learning_rate': 1.5445447805498824e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1423, 'grad_norm': 0.64453125, 'learning_rate': 1.5434085428034965e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1376, 'grad_norm': 0.546875, 'learning_rate': 1.5422723050571103e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1718, 'grad_norm': 1.2265625, 'learning_rate': 1.541136067310724e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2906, 'grad_norm': 0.70703125, 'learning_rate': 1.5399998295643382e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1399, 'grad_norm': 0.68359375, 'learning_rate': 1.538863591817952e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2339, 'grad_norm': 0.47265625, 'learning_rate': 1.5377273540715658e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2285, 'grad_norm': 0.56640625, 'learning_rate': 1.53659111632518e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0053, 'grad_norm': 0.73046875, 'learning_rate': 1.535454878578794e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2894, 'grad_norm': 0.6875, 'learning_rate': 1.5343186408324078e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1104, 'grad_norm': 0.49609375, 'learning_rate': 1.5331824030860216e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0493, 'grad_norm': 0.5546875, 'learning_rate': 1.5320461653396357e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2377, 'grad_norm': 0.90234375, 'learning_rate': 1.5309099275932498e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1584, 'grad_norm': 0.82421875, 'learning_rate': 1.5297736898468636e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2684, 'grad_norm': 0.66015625, 'learning_rate': 1.5286374521004777e-05, 'epoch': 0.83}\n",
      "{'loss': 1.198, 'grad_norm': 0.90625, 'learning_rate': 1.5275012143540915e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1226, 'grad_norm': 0.4765625, 'learning_rate': 1.5263649766077053e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1094, 'grad_norm': 0.62890625, 'learning_rate': 1.5252287388613193e-05, 'epoch': 0.83}\n",
      "{'loss': 1.143, 'grad_norm': 0.5703125, 'learning_rate': 1.5240925011149334e-05, 'epoch': 0.83}\n",
      "{'loss': 1.269, 'grad_norm': 0.494140625, 'learning_rate': 1.5229562633685473e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0712, 'grad_norm': 0.7109375, 'learning_rate': 1.5218200256221613e-05, 'epoch': 0.83}\n",
      "{'loss': 1.225, 'grad_norm': 0.46484375, 'learning_rate': 1.520683787875775e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1677, 'grad_norm': 0.5859375, 'learning_rate': 1.519547550129389e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1001, 'grad_norm': 0.7734375, 'learning_rate': 1.5184113123830032e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1374, 'grad_norm': 0.61328125, 'learning_rate': 1.5172750746366171e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0905, 'grad_norm': 0.77734375, 'learning_rate': 1.516138836890231e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1632, 'grad_norm': 0.4765625, 'learning_rate': 1.5150025991438449e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2576, 'grad_norm': 0.66796875, 'learning_rate': 1.5138663613974588e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1203, 'grad_norm': 1.2578125, 'learning_rate': 1.5127301236510728e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2875, 'grad_norm': 0.578125, 'learning_rate': 1.5115938859046869e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1633, 'grad_norm': 0.6171875, 'learning_rate': 1.5104576481583008e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1467, 'grad_norm': 0.52734375, 'learning_rate': 1.5093214104119146e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1818, 'grad_norm': 0.62109375, 'learning_rate': 1.5081851726655286e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9694, 'grad_norm': 1.2890625, 'learning_rate': 1.5070489349191424e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3714, 'grad_norm': 0.58203125, 'learning_rate': 1.5059126971727567e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1802, 'grad_norm': 0.6875, 'learning_rate': 1.5047764594263704e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2074, 'grad_norm': 0.53125, 'learning_rate': 1.5036402216799844e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2209, 'grad_norm': 0.59765625, 'learning_rate': 1.5025039839335984e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0243, 'grad_norm': 1.046875, 'learning_rate': 1.5013677461872121e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2689, 'grad_norm': 0.59765625, 'learning_rate': 1.5002315084408261e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2192, 'grad_norm': 1.3828125, 'learning_rate': 1.4990952706944402e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2195, 'grad_norm': 0.4765625, 'learning_rate': 1.4979590329480542e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3445, 'grad_norm': 0.5859375, 'learning_rate': 1.4968227952016681e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1372, 'grad_norm': 0.99609375, 'learning_rate': 1.495686557455282e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3717, 'grad_norm': 0.6796875, 'learning_rate': 1.4945503197088959e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0886, 'grad_norm': 0.5625, 'learning_rate': 1.49341408196251e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2655, 'grad_norm': 0.46484375, 'learning_rate': 1.492277844216124e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2847, 'grad_norm': 0.6328125, 'learning_rate': 1.4911416064697379e-05, 'epoch': 0.83}\n",
      "{'loss': 1.1704, 'grad_norm': 1.03125, 'learning_rate': 1.4900053687233517e-05, 'epoch': 0.83}\n",
      "{'loss': 1.2857, 'grad_norm': 0.63671875, 'learning_rate': 1.4888691309769656e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2864, 'grad_norm': 1.125, 'learning_rate': 1.4877328932305794e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1761, 'grad_norm': 0.466796875, 'learning_rate': 1.4865966554841937e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1441, 'grad_norm': 0.76171875, 'learning_rate': 1.4854604177378075e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0837, 'grad_norm': 0.87890625, 'learning_rate': 1.4843241799914215e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1849, 'grad_norm': 0.462890625, 'learning_rate': 1.4831879422450354e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1704, 'grad_norm': 0.73046875, 'learning_rate': 1.4820517044986492e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2211, 'grad_norm': 0.486328125, 'learning_rate': 1.4809154667522635e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9632, 'grad_norm': 0.65234375, 'learning_rate': 1.4797792290058773e-05, 'epoch': 0.84}\n",
      "{'loss': 1.073, 'grad_norm': 0.6015625, 'learning_rate': 1.4786429912594912e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3185, 'grad_norm': 0.6328125, 'learning_rate': 1.4775067535131052e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2066, 'grad_norm': 1.265625, 'learning_rate': 1.476370515766719e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1781, 'grad_norm': 0.53515625, 'learning_rate': 1.475234278020333e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3828, 'grad_norm': 0.54296875, 'learning_rate': 1.474098040273947e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8973, 'grad_norm': 0.65234375, 'learning_rate': 1.472961802527561e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2831, 'grad_norm': 0.65234375, 'learning_rate': 1.471825564781175e-05, 'epoch': 0.84}\n",
      "{'loss': 1.286, 'grad_norm': 0.640625, 'learning_rate': 1.4706893270347888e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2175, 'grad_norm': 0.50390625, 'learning_rate': 1.4695530892884027e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2885, 'grad_norm': 0.54296875, 'learning_rate': 1.4684168515420168e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1129, 'grad_norm': 0.8984375, 'learning_rate': 1.4672806137956308e-05, 'epoch': 0.84}\n",
      "{'loss': 1.361, 'grad_norm': 0.5078125, 'learning_rate': 1.4661443760492446e-05, 'epoch': 0.84}\n",
      "{'loss': 1.183, 'grad_norm': 0.67578125, 'learning_rate': 1.4650081383028585e-05, 'epoch': 0.84}\n",
      "{'loss': 1.136, 'grad_norm': 0.51171875, 'learning_rate': 1.4638719005564725e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1832, 'grad_norm': 0.70703125, 'learning_rate': 1.4627356628100863e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0391, 'grad_norm': 0.82421875, 'learning_rate': 1.4615994250637006e-05, 'epoch': 0.84}\n",
      "{'loss': 1.408, 'grad_norm': 0.765625, 'learning_rate': 1.4604631873173143e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1531, 'grad_norm': 0.68359375, 'learning_rate': 1.4593269495709283e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1295, 'grad_norm': 0.546875, 'learning_rate': 1.4581907118245423e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2204, 'grad_norm': 0.74609375, 'learning_rate': 1.457054474078156e-05, 'epoch': 0.84}\n",
      "{'loss': 1.074, 'grad_norm': 0.72265625, 'learning_rate': 1.45591823633177e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2303, 'grad_norm': 0.62109375, 'learning_rate': 1.4547819985853841e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0903, 'grad_norm': 0.74609375, 'learning_rate': 1.453645760838998e-05, 'epoch': 0.84}\n",
      "{'loss': 1.243, 'grad_norm': 0.400390625, 'learning_rate': 1.452509523092612e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2041, 'grad_norm': 0.54296875, 'learning_rate': 1.4513732853462258e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9743, 'grad_norm': 0.466796875, 'learning_rate': 1.4502370475998398e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2432, 'grad_norm': 0.70703125, 'learning_rate': 1.4491008098534539e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0133, 'grad_norm': 0.62890625, 'learning_rate': 1.4479645721070679e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1049, 'grad_norm': 0.53125, 'learning_rate': 1.4468283343606816e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1843, 'grad_norm': 0.5859375, 'learning_rate': 1.4456920966142956e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1912, 'grad_norm': 1.2578125, 'learning_rate': 1.4445558588679095e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4412, 'grad_norm': 0.5703125, 'learning_rate': 1.4434196211215233e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1908, 'grad_norm': 0.6875, 'learning_rate': 1.4422833833751376e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0938, 'grad_norm': 0.515625, 'learning_rate': 1.4411471456287514e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2185, 'grad_norm': 0.71484375, 'learning_rate': 1.4400109078823654e-05, 'epoch': 0.84}\n",
      "{'loss': 1.184, 'grad_norm': 0.6328125, 'learning_rate': 1.4388746701359793e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2607, 'grad_norm': 0.80078125, 'learning_rate': 1.4377384323895931e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0612, 'grad_norm': 1.2421875, 'learning_rate': 1.4366021946432074e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2662, 'grad_norm': 0.474609375, 'learning_rate': 1.4354659568968212e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1909, 'grad_norm': 0.6640625, 'learning_rate': 1.4343297191504351e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0776, 'grad_norm': 1.0859375, 'learning_rate': 1.4331934814040491e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2947, 'grad_norm': 0.578125, 'learning_rate': 1.4320572436576629e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2142, 'grad_norm': 0.5859375, 'learning_rate': 1.4309210059112768e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2008, 'grad_norm': 0.51171875, 'learning_rate': 1.429784768164891e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0157, 'grad_norm': 1.0390625, 'learning_rate': 1.4286485304185049e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9381, 'grad_norm': 0.7109375, 'learning_rate': 1.4275122926721187e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3036, 'grad_norm': 0.5859375, 'learning_rate': 1.4263760549257327e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1698, 'grad_norm': 0.5703125, 'learning_rate': 1.4252398171793466e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1362, 'grad_norm': 0.4140625, 'learning_rate': 1.4241035794329607e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2514, 'grad_norm': 0.59375, 'learning_rate': 1.4229673416865747e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0729, 'grad_norm': 0.72265625, 'learning_rate': 1.4218311039401885e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2899, 'grad_norm': 0.55859375, 'learning_rate': 1.4206948661938024e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1742, 'grad_norm': 0.78515625, 'learning_rate': 1.4195586284474164e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1706, 'grad_norm': 0.388671875, 'learning_rate': 1.4184223907010302e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2666, 'grad_norm': 0.69921875, 'learning_rate': 1.4172861529546445e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2009, 'grad_norm': 0.58203125, 'learning_rate': 1.4161499152082582e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5343, 'grad_norm': 0.51171875, 'learning_rate': 1.4150136774618722e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1665, 'grad_norm': 0.7890625, 'learning_rate': 1.4138774397154862e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1772, 'grad_norm': 0.96484375, 'learning_rate': 1.4127412019691e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2779, 'grad_norm': 0.46875, 'learning_rate': 1.4116049642227142e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0624, 'grad_norm': 2.0625, 'learning_rate': 1.410468726476328e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3326, 'grad_norm': 0.58984375, 'learning_rate': 1.409332488729942e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1224, 'grad_norm': 0.73046875, 'learning_rate': 1.4081962509835558e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1676, 'grad_norm': 0.47265625, 'learning_rate': 1.4070600132371697e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1685, 'grad_norm': 0.734375, 'learning_rate': 1.4059237754907837e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9561, 'grad_norm': 2.53125, 'learning_rate': 1.4047875377443978e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3137, 'grad_norm': 1.0625, 'learning_rate': 1.4036512999980118e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1487, 'grad_norm': 0.6484375, 'learning_rate': 1.4025150622516255e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1738, 'grad_norm': 0.48828125, 'learning_rate': 1.4013788245052395e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2294, 'grad_norm': 0.70703125, 'learning_rate': 1.4002425867588534e-05, 'epoch': 0.84}\n",
      "{'loss': 1.016, 'grad_norm': 0.58203125, 'learning_rate': 1.3991063490124676e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3441, 'grad_norm': 0.71875, 'learning_rate': 1.3979701112660815e-05, 'epoch': 0.84}\n",
      "{'loss': 1.142, 'grad_norm': 0.71875, 'learning_rate': 1.3968338735196953e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2101, 'grad_norm': 0.57421875, 'learning_rate': 1.3956976357733093e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1805, 'grad_norm': 0.6484375, 'learning_rate': 1.3945613980269232e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1428, 'grad_norm': 0.84765625, 'learning_rate': 1.393425160280537e-05, 'epoch': 0.84}\n",
      "{'loss': 1.37, 'grad_norm': 0.58203125, 'learning_rate': 1.3922889225341513e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0937, 'grad_norm': 0.7734375, 'learning_rate': 1.3911526847877651e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1711, 'grad_norm': 0.49609375, 'learning_rate': 1.390016447041379e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2354, 'grad_norm': 0.7265625, 'learning_rate': 1.3888802092949928e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1267, 'grad_norm': 0.86328125, 'learning_rate': 1.3877439715486068e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2918, 'grad_norm': 0.53125, 'learning_rate': 1.3866077338022209e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0641, 'grad_norm': 0.68359375, 'learning_rate': 1.3854714960558349e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1827, 'grad_norm': 0.447265625, 'learning_rate': 1.3843352583094488e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2911, 'grad_norm': 0.59375, 'learning_rate': 1.3831990205630626e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0158, 'grad_norm': 0.87109375, 'learning_rate': 1.3820627828166766e-05, 'epoch': 0.84}\n",
      "{'loss': 1.386, 'grad_norm': 0.53125, 'learning_rate': 1.3809265450702905e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1305, 'grad_norm': 0.62109375, 'learning_rate': 1.3797903073239046e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2333, 'grad_norm': 0.453125, 'learning_rate': 1.3786540695775186e-05, 'epoch': 0.84}\n",
      "{'loss': 1.185, 'grad_norm': 0.63671875, 'learning_rate': 1.3775178318311324e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0852, 'grad_norm': 0.921875, 'learning_rate': 1.3763815940847463e-05, 'epoch': 0.84}\n",
      "{'loss': 1.4241, 'grad_norm': 0.76953125, 'learning_rate': 1.3752453563383603e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0887, 'grad_norm': 0.8125, 'learning_rate': 1.3741091185919744e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2171, 'grad_norm': 0.48828125, 'learning_rate': 1.3729728808455884e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1856, 'grad_norm': 0.6875, 'learning_rate': 1.3718366430992021e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9309, 'grad_norm': 0.98046875, 'learning_rate': 1.3707004053528161e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1993, 'grad_norm': 0.53515625, 'learning_rate': 1.3695641676064299e-05, 'epoch': 0.84}\n",
      "{'loss': 1.17, 'grad_norm': 0.57421875, 'learning_rate': 1.3684279298600438e-05, 'epoch': 0.84}\n",
      "{'loss': 1.26, 'grad_norm': 0.53515625, 'learning_rate': 1.367291692113658e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2473, 'grad_norm': 0.5859375, 'learning_rate': 1.366155454367272e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9696, 'grad_norm': 1.75, 'learning_rate': 1.3650192166208859e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3821, 'grad_norm': 0.59765625, 'learning_rate': 1.3638829788744997e-05, 'epoch': 0.84}\n",
      "{'loss': 1.221, 'grad_norm': 0.828125, 'learning_rate': 1.3627467411281136e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1929, 'grad_norm': 0.451171875, 'learning_rate': 1.3616105033817276e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2376, 'grad_norm': 0.8046875, 'learning_rate': 1.3604742656353417e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0545, 'grad_norm': 1.515625, 'learning_rate': 1.3593380278889557e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.228, 'grad_norm': 0.765625, 'learning_rate': 1.3582017901425694e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1097, 'grad_norm': 0.83203125, 'learning_rate': 1.3570655523961834e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1475, 'grad_norm': 0.439453125, 'learning_rate': 1.3559293146497973e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2312, 'grad_norm': 0.64453125, 'learning_rate': 1.3547930769034115e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1653, 'grad_norm': 1.3125, 'learning_rate': 1.3536568391570254e-05, 'epoch': 0.84}\n",
      "{'loss': 1.208, 'grad_norm': 0.5, 'learning_rate': 1.3525206014106392e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2065, 'grad_norm': 0.69140625, 'learning_rate': 1.3513843636642532e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2049, 'grad_norm': 0.47265625, 'learning_rate': 1.350248125917867e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2643, 'grad_norm': 0.61328125, 'learning_rate': 1.3491118881714809e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1039, 'grad_norm': 0.671875, 'learning_rate': 1.347975650425095e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3231, 'grad_norm': 0.59765625, 'learning_rate': 1.346839412678709e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1074, 'grad_norm': 0.734375, 'learning_rate': 1.345703174932323e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1636, 'grad_norm': 0.5703125, 'learning_rate': 1.3445669371859367e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2598, 'grad_norm': 0.70703125, 'learning_rate': 1.3434306994395507e-05, 'epoch': 0.84}\n",
      "{'loss': 1.078, 'grad_norm': 0.984375, 'learning_rate': 1.3422944616931648e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2326, 'grad_norm': 0.58203125, 'learning_rate': 1.3411582239467788e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1504, 'grad_norm': 0.91796875, 'learning_rate': 1.3400219862003927e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2713, 'grad_norm': 0.6328125, 'learning_rate': 1.3388857484540065e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1359, 'grad_norm': 0.71875, 'learning_rate': 1.3377495107076205e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1067, 'grad_norm': 0.76171875, 'learning_rate': 1.3366132729612344e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3045, 'grad_norm': 0.61328125, 'learning_rate': 1.3354770352148485e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0997, 'grad_norm': 0.62109375, 'learning_rate': 1.3343407974684625e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2305, 'grad_norm': 0.4921875, 'learning_rate': 1.3332045597220763e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1078, 'grad_norm': 0.7421875, 'learning_rate': 1.3320683219756902e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9832, 'grad_norm': 1.234375, 'learning_rate': 1.330932084229304e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2141, 'grad_norm': 0.48828125, 'learning_rate': 1.3297958464829183e-05, 'epoch': 0.84}\n",
      "{'loss': 1.366, 'grad_norm': 0.74609375, 'learning_rate': 1.3286596087365321e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0209, 'grad_norm': 0.484375, 'learning_rate': 1.327523370990146e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2072, 'grad_norm': 0.65234375, 'learning_rate': 1.32638713324376e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9705, 'grad_norm': 0.640625, 'learning_rate': 1.3252508954973738e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3039, 'grad_norm': 0.5546875, 'learning_rate': 1.3241146577509877e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1949, 'grad_norm': 0.765625, 'learning_rate': 1.3229784200046019e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1907, 'grad_norm': 0.578125, 'learning_rate': 1.3218421822582158e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1102, 'grad_norm': 0.72265625, 'learning_rate': 1.3207059445118298e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1208, 'grad_norm': 1.0625, 'learning_rate': 1.3195697067654436e-05, 'epoch': 0.84}\n",
      "{'loss': 1.4077, 'grad_norm': 0.484375, 'learning_rate': 1.3184334690190575e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1594, 'grad_norm': 0.6171875, 'learning_rate': 1.3172972312726716e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2369, 'grad_norm': 0.515625, 'learning_rate': 1.3161609935262856e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3141, 'grad_norm': 0.58984375, 'learning_rate': 1.3150247557798996e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0755, 'grad_norm': 0.8515625, 'learning_rate': 1.3138885180335133e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2922, 'grad_norm': 0.7734375, 'learning_rate': 1.3127522802871273e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2813, 'grad_norm': 0.6328125, 'learning_rate': 1.3116160425407412e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3056, 'grad_norm': 0.53125, 'learning_rate': 1.3104798047943554e-05, 'epoch': 0.84}\n",
      "{'loss': 1.225, 'grad_norm': 0.7734375, 'learning_rate': 1.3093435670479692e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0539, 'grad_norm': 0.671875, 'learning_rate': 1.3082073293015831e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2476, 'grad_norm': 0.69140625, 'learning_rate': 1.307071091555197e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9927, 'grad_norm': 0.56640625, 'learning_rate': 1.3059348538088109e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2221, 'grad_norm': 0.400390625, 'learning_rate': 1.3047986160624251e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2887, 'grad_norm': 0.62890625, 'learning_rate': 1.303662378316039e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1124, 'grad_norm': 0.83984375, 'learning_rate': 1.3025261405696529e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2826, 'grad_norm': 0.734375, 'learning_rate': 1.3013899028232668e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0673, 'grad_norm': 0.7734375, 'learning_rate': 1.3002536650768806e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2093, 'grad_norm': 0.484375, 'learning_rate': 1.2991174273304946e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1094, 'grad_norm': 0.98828125, 'learning_rate': 1.2979811895841087e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1374, 'grad_norm': 1.1171875, 'learning_rate': 1.2968449518377227e-05, 'epoch': 0.84}\n",
      "{'loss': 1.38, 'grad_norm': 0.53515625, 'learning_rate': 1.2957087140913366e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1463, 'grad_norm': 0.8046875, 'learning_rate': 1.2945724763449504e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3292, 'grad_norm': 0.578125, 'learning_rate': 1.2934362385985644e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2235, 'grad_norm': 0.76171875, 'learning_rate': 1.2923000008521785e-05, 'epoch': 0.84}\n",
      "{'loss': 1.234, 'grad_norm': 1.15625, 'learning_rate': 1.2911637631057924e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2856, 'grad_norm': 0.5390625, 'learning_rate': 1.2900275253594062e-05, 'epoch': 0.84}\n",
      "{'loss': 1.201, 'grad_norm': 0.81640625, 'learning_rate': 1.2888912876130202e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1496, 'grad_norm': 0.515625, 'learning_rate': 1.2877550498666341e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1409, 'grad_norm': 0.4921875, 'learning_rate': 1.2866188121202479e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0412, 'grad_norm': 1.1796875, 'learning_rate': 1.2854825743738622e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3659, 'grad_norm': 0.61328125, 'learning_rate': 1.284346336627476e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1361, 'grad_norm': 1.0078125, 'learning_rate': 1.28321009888109e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9982, 'grad_norm': 0.453125, 'learning_rate': 1.2820738611347039e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1324, 'grad_norm': 0.62109375, 'learning_rate': 1.2809376233883177e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0596, 'grad_norm': 1.4765625, 'learning_rate': 1.279801385641932e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2808, 'grad_norm': 0.89453125, 'learning_rate': 1.2786651478955458e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0374, 'grad_norm': 0.85546875, 'learning_rate': 1.2775289101491597e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1874, 'grad_norm': 0.48828125, 'learning_rate': 1.2763926724027737e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1593, 'grad_norm': 0.66796875, 'learning_rate': 1.2752564346563875e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1219, 'grad_norm': 0.96484375, 'learning_rate': 1.2741201969100014e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3623, 'grad_norm': 0.5390625, 'learning_rate': 1.2729839591636155e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1342, 'grad_norm': 0.69921875, 'learning_rate': 1.2718477214172295e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0736, 'grad_norm': 0.4609375, 'learning_rate': 1.2707114836708433e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3404, 'grad_norm': 0.515625, 'learning_rate': 1.2695752459244572e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0589, 'grad_norm': 1.09375, 'learning_rate': 1.2684390081780712e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3261, 'grad_norm': 0.56640625, 'learning_rate': 1.267302770431685e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0872, 'grad_norm': 0.84765625, 'learning_rate': 1.2661665326852993e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1325, 'grad_norm': 0.5, 'learning_rate': 1.265030294938913e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2964, 'grad_norm': 0.74609375, 'learning_rate': 1.263894057192527e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0837, 'grad_norm': 1.3203125, 'learning_rate': 1.262757819446141e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2936, 'grad_norm': 0.546875, 'learning_rate': 1.2616215816997548e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1454, 'grad_norm': 0.5859375, 'learning_rate': 1.260485343953369e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2715, 'grad_norm': 0.39453125, 'learning_rate': 1.2593491062069828e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2536, 'grad_norm': 0.625, 'learning_rate': 1.2582128684605968e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1168, 'grad_norm': 1.0625, 'learning_rate': 1.2570766307142107e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3338, 'grad_norm': 0.515625, 'learning_rate': 1.2559403929678245e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1029, 'grad_norm': 0.94921875, 'learning_rate': 1.2548041552214385e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1655, 'grad_norm': 0.59765625, 'learning_rate': 1.2536679174750526e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1149, 'grad_norm': 0.6875, 'learning_rate': 1.2525316797286666e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0825, 'grad_norm': 0.94921875, 'learning_rate': 1.2513954419822803e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3084, 'grad_norm': 0.828125, 'learning_rate': 1.2502592042358943e-05, 'epoch': 0.85}\n",
      "{'loss': 1.158, 'grad_norm': 0.6875, 'learning_rate': 1.2491229664895084e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1343, 'grad_norm': 0.4453125, 'learning_rate': 1.2479867287431222e-05, 'epoch': 0.85}\n",
      "{'loss': 1.187, 'grad_norm': 0.55078125, 'learning_rate': 1.2468504909967362e-05, 'epoch': 0.85}\n",
      "{'loss': 0.999, 'grad_norm': 0.82421875, 'learning_rate': 1.2457142532503501e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4194, 'grad_norm': 0.60546875, 'learning_rate': 1.244578015503964e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1526, 'grad_norm': 0.6875, 'learning_rate': 1.243441777757578e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9847, 'grad_norm': 0.54296875, 'learning_rate': 1.242305540011192e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1266, 'grad_norm': 0.69140625, 'learning_rate': 1.241169302264806e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9922, 'grad_norm': 1.4765625, 'learning_rate': 1.2400330645184199e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2756, 'grad_norm': 0.482421875, 'learning_rate': 1.2388968267720338e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1464, 'grad_norm': 0.84375, 'learning_rate': 1.2377605890256478e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1666, 'grad_norm': 0.5078125, 'learning_rate': 1.2366243512792618e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1377, 'grad_norm': 0.64453125, 'learning_rate': 1.2354881135328757e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0142, 'grad_norm': 1.0546875, 'learning_rate': 1.2343518757864895e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4022, 'grad_norm': 0.6171875, 'learning_rate': 1.2332156380401036e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1975, 'grad_norm': 0.60546875, 'learning_rate': 1.2320794002937174e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0958, 'grad_norm': 0.49609375, 'learning_rate': 1.2309431625473315e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2254, 'grad_norm': 0.58984375, 'learning_rate': 1.2298069248009455e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9841, 'grad_norm': 0.69140625, 'learning_rate': 1.2286706870545593e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2269, 'grad_norm': 0.421875, 'learning_rate': 1.2275344493081734e-05, 'epoch': 0.85}\n",
      "{'loss': 1.152, 'grad_norm': 0.8359375, 'learning_rate': 1.2263982115617872e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1048, 'grad_norm': 1.1015625, 'learning_rate': 1.2252619738154011e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2429, 'grad_norm': 0.81640625, 'learning_rate': 1.2241257360690153e-05, 'epoch': 0.85}\n",
      "{'loss': 1.021, 'grad_norm': 0.8515625, 'learning_rate': 1.222989498322629e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2937, 'grad_norm': 0.765625, 'learning_rate': 1.221853260576243e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9633, 'grad_norm': 0.71484375, 'learning_rate': 1.220717022829857e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1863, 'grad_norm': 0.453125, 'learning_rate': 1.2195807850834709e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1909, 'grad_norm': 0.60546875, 'learning_rate': 1.2184445473370849e-05, 'epoch': 0.85}\n",
      "{'loss': 1.032, 'grad_norm': 0.66796875, 'learning_rate': 1.2173083095906988e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3297, 'grad_norm': 0.5859375, 'learning_rate': 1.2161720718443128e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1905, 'grad_norm': 0.91796875, 'learning_rate': 1.2150358340979267e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1381, 'grad_norm': 0.5546875, 'learning_rate': 1.2138995963515407e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2075, 'grad_norm': 0.75, 'learning_rate': 1.2127633586051545e-05, 'epoch': 0.85}\n",
      "{'loss': 1.154, 'grad_norm': 1.15625, 'learning_rate': 1.2116271208587686e-05, 'epoch': 0.85}\n",
      "{'loss': 1.287, 'grad_norm': 0.55859375, 'learning_rate': 1.2104908831123826e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1429, 'grad_norm': 0.7890625, 'learning_rate': 1.2093546453659963e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1537, 'grad_norm': 0.4765625, 'learning_rate': 1.2082184076196105e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1712, 'grad_norm': 0.62890625, 'learning_rate': 1.2070821698732242e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0205, 'grad_norm': 0.828125, 'learning_rate': 1.2059459321268384e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3817, 'grad_norm': 0.51953125, 'learning_rate': 1.2048096943804523e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1995, 'grad_norm': 0.734375, 'learning_rate': 1.2036734566340661e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1058, 'grad_norm': 0.43359375, 'learning_rate': 1.2025372188876802e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4488, 'grad_norm': 0.75390625, 'learning_rate': 1.201400981141294e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0878, 'grad_norm': 0.5, 'learning_rate': 1.200264743394908e-05, 'epoch': 0.85}\n",
      "{'loss': 1.356, 'grad_norm': 0.671875, 'learning_rate': 1.199128505648522e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1553, 'grad_norm': 0.86328125, 'learning_rate': 1.1979922679021359e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1851, 'grad_norm': 0.625, 'learning_rate': 1.1968560301557498e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3021, 'grad_norm': 0.54296875, 'learning_rate': 1.1957197924093638e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0107, 'grad_norm': 0.76171875, 'learning_rate': 1.1945835546629777e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2614, 'grad_norm': 0.546875, 'learning_rate': 1.1934473169165915e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1431, 'grad_norm': 0.765625, 'learning_rate': 1.1923110791702057e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1768, 'grad_norm': 0.498046875, 'learning_rate': 1.1911748414238196e-05, 'epoch': 0.85}\n",
      "{'loss': 1.282, 'grad_norm': 0.578125, 'learning_rate': 1.1900386036774336e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0517, 'grad_norm': 0.53515625, 'learning_rate': 1.1889023659310475e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2548, 'grad_norm': 0.5625, 'learning_rate': 1.1877661281846613e-05, 'epoch': 0.85}\n",
      "{'loss': 1.109, 'grad_norm': 0.62890625, 'learning_rate': 1.1866298904382754e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1496, 'grad_norm': 0.73046875, 'learning_rate': 1.1854936526918894e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1102, 'grad_norm': 0.7890625, 'learning_rate': 1.1843574149455032e-05, 'epoch': 0.85}\n",
      "{'loss': 1.263, 'grad_norm': 0.859375, 'learning_rate': 1.1832211771991173e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3475, 'grad_norm': 0.87109375, 'learning_rate': 1.182084939452731e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1507, 'grad_norm': 0.7890625, 'learning_rate': 1.180948701706345e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1534, 'grad_norm': 0.5625, 'learning_rate': 1.179812463959959e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3242, 'grad_norm': 0.5703125, 'learning_rate': 1.178676226213573e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0335, 'grad_norm': 0.94140625, 'learning_rate': 1.177539988467187e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3814, 'grad_norm': 0.546875, 'learning_rate': 1.1764037507208009e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1492, 'grad_norm': 0.72265625, 'learning_rate': 1.1752675129744148e-05, 'epoch': 0.85}\n",
      "{'loss': 1.115, 'grad_norm': 0.42578125, 'learning_rate': 1.1741312752280288e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1527, 'grad_norm': 0.51953125, 'learning_rate': 1.1729950374816427e-05, 'epoch': 0.85}\n",
      "{'loss': 1.017, 'grad_norm': 0.78125, 'learning_rate': 1.1718587997352567e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2679, 'grad_norm': 0.76171875, 'learning_rate': 1.1707225619888706e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1855, 'grad_norm': 0.74609375, 'learning_rate': 1.1695863242424846e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2076, 'grad_norm': 0.72265625, 'learning_rate': 1.1684500864960984e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1856, 'grad_norm': 0.578125, 'learning_rate': 1.1673138487497125e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0788, 'grad_norm': 0.83984375, 'learning_rate': 1.1661776110033265e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2032, 'grad_norm': 0.66015625, 'learning_rate': 1.1650413732569404e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2588, 'grad_norm': 0.83203125, 'learning_rate': 1.1639051355105544e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1067, 'grad_norm': 0.447265625, 'learning_rate': 1.1627688977641681e-05, 'epoch': 0.85}\n",
      "{'loss': 1.164, 'grad_norm': 0.59765625, 'learning_rate': 1.1616326600177823e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0882, 'grad_norm': 1.1328125, 'learning_rate': 1.160496422271396e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3911, 'grad_norm': 0.5546875, 'learning_rate': 1.15936018452501e-05, 'epoch': 0.85}\n",
      "{'loss': 1.189, 'grad_norm': 0.8515625, 'learning_rate': 1.1582239467786241e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0174, 'grad_norm': 0.4765625, 'learning_rate': 1.157087709032238e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1453, 'grad_norm': 0.8203125, 'learning_rate': 1.1559514712858519e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9961, 'grad_norm': 0.6015625, 'learning_rate': 1.1548152335394658e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2671, 'grad_norm': 0.734375, 'learning_rate': 1.1536789957930798e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1136, 'grad_norm': 0.71875, 'learning_rate': 1.1525427580466937e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2964, 'grad_norm': 0.5390625, 'learning_rate': 1.1514065203003077e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3827, 'grad_norm': 0.50390625, 'learning_rate': 1.1502702825539216e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0736, 'grad_norm': 1.1328125, 'learning_rate': 1.1491340448075356e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3072, 'grad_norm': 0.53515625, 'learning_rate': 1.1479978070611496e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1825, 'grad_norm': 0.84765625, 'learning_rate': 1.1468615693147635e-05, 'epoch': 0.85}\n",
      "{'loss': 1.237, 'grad_norm': 0.53125, 'learning_rate': 1.1457253315683775e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1449, 'grad_norm': 0.6875, 'learning_rate': 1.1445890938219914e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0197, 'grad_norm': 0.80859375, 'learning_rate': 1.1434528560756052e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2367, 'grad_norm': 0.54296875, 'learning_rate': 1.1423166183292193e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0625, 'grad_norm': 0.6484375, 'learning_rate': 1.1411803805828331e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1201, 'grad_norm': 0.5390625, 'learning_rate': 1.140044142836447e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2078, 'grad_norm': 0.59765625, 'learning_rate': 1.1389079050900612e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1452, 'grad_norm': 0.94921875, 'learning_rate': 1.137771667343675e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2802, 'grad_norm': 0.58203125, 'learning_rate': 1.1366354295972891e-05, 'epoch': 0.85}\n",
      "{'loss': 1.111, 'grad_norm': 0.63671875, 'learning_rate': 1.1354991918509029e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2255, 'grad_norm': 0.4453125, 'learning_rate': 1.1343629541045168e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2123, 'grad_norm': 0.921875, 'learning_rate': 1.1332267163581308e-05, 'epoch': 0.85}\n",
      "{'loss': 1.028, 'grad_norm': 0.87109375, 'learning_rate': 1.1320904786117448e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2867, 'grad_norm': 0.51171875, 'learning_rate': 1.1309542408653587e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0357, 'grad_norm': 0.73828125, 'learning_rate': 1.1298180031189727e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2809, 'grad_norm': 0.4453125, 'learning_rate': 1.1286817653725866e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2341, 'grad_norm': 0.63671875, 'learning_rate': 1.1275455276262006e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0858, 'grad_norm': 1.15625, 'learning_rate': 1.1264092898798145e-05, 'epoch': 0.85}\n",
      "{'loss': 1.195, 'grad_norm': 0.82421875, 'learning_rate': 1.1252730521334285e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1019, 'grad_norm': 0.953125, 'learning_rate': 1.1241368143870424e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1745, 'grad_norm': 0.494140625, 'learning_rate': 1.1230005766406564e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2373, 'grad_norm': 0.65625, 'learning_rate': 1.1218643388942702e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0655, 'grad_norm': 0.82421875, 'learning_rate': 1.1207281011478843e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4096, 'grad_norm': 0.734375, 'learning_rate': 1.1195918634014983e-05, 'epoch': 0.85}\n",
      "{'loss': 1.168, 'grad_norm': 0.625, 'learning_rate': 1.118455625655112e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1213, 'grad_norm': 0.4921875, 'learning_rate': 1.1173193879087262e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2348, 'grad_norm': 0.66796875, 'learning_rate': 1.11618315016234e-05, 'epoch': 0.85}\n",
      "{'loss': 1.024, 'grad_norm': 1.078125, 'learning_rate': 1.1150469124159539e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3684, 'grad_norm': 0.6328125, 'learning_rate': 1.1139106746695679e-05, 'epoch': 0.85}\n",
      "{'loss': 1.232, 'grad_norm': 0.74609375, 'learning_rate': 1.1127744369231818e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1931, 'grad_norm': 0.482421875, 'learning_rate': 1.111638199176796e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1933, 'grad_norm': 0.77734375, 'learning_rate': 1.1105019614304097e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1206, 'grad_norm': 0.7890625, 'learning_rate': 1.1093657236840237e-05, 'epoch': 0.85}\n",
      "{'loss': 1.275, 'grad_norm': 0.65234375, 'learning_rate': 1.1082294859376376e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1283, 'grad_norm': 0.80078125, 'learning_rate': 1.1070932481912516e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1506, 'grad_norm': 0.53515625, 'learning_rate': 1.1059570104448655e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2334, 'grad_norm': 0.498046875, 'learning_rate': 1.1048207726984795e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0807, 'grad_norm': 0.78515625, 'learning_rate': 1.1036845349520935e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2692, 'grad_norm': 0.5546875, 'learning_rate': 1.1025482972057072e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1734, 'grad_norm': 0.90234375, 'learning_rate': 1.1014120594593214e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1265, 'grad_norm': 0.486328125, 'learning_rate': 1.1002758217129353e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2648, 'grad_norm': 0.765625, 'learning_rate': 1.0991395839665491e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0446, 'grad_norm': 0.625, 'learning_rate': 1.0980033462201632e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3282, 'grad_norm': 0.921875, 'learning_rate': 1.096867108473777e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1543, 'grad_norm': 0.63671875, 'learning_rate': 1.0957308707273911e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2262, 'grad_norm': 0.46875, 'learning_rate': 1.094594632981005e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2032, 'grad_norm': 0.59765625, 'learning_rate': 1.0934583952346189e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0628, 'grad_norm': 0.86328125, 'learning_rate': 1.092322157488233e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2953, 'grad_norm': 0.55078125, 'learning_rate': 1.0911859197418468e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1225, 'grad_norm': 0.80859375, 'learning_rate': 1.0900496819954607e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2866, 'grad_norm': 0.578125, 'learning_rate': 1.0889134442490747e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2465, 'grad_norm': 0.984375, 'learning_rate': 1.0877772065026887e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0781, 'grad_norm': 1.0703125, 'learning_rate': 1.0866409687563026e-05, 'epoch': 0.85}\n",
      "{'loss': 1.283, 'grad_norm': 0.58203125, 'learning_rate': 1.0855047310099166e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1528, 'grad_norm': 0.65234375, 'learning_rate': 1.0843684932635305e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0818, 'grad_norm': 0.6171875, 'learning_rate': 1.0832322555171445e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2682, 'grad_norm': 1.046875, 'learning_rate': 1.0820960177707584e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9963, 'grad_norm': 0.9140625, 'learning_rate': 1.0809597800243724e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4548, 'grad_norm': 0.50390625, 'learning_rate': 1.0798235422779863e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1713, 'grad_norm': 0.62890625, 'learning_rate': 1.0786873045316003e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1305, 'grad_norm': 0.5703125, 'learning_rate': 1.077551066785214e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0929, 'grad_norm': 0.765625, 'learning_rate': 1.0764148290388282e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1223, 'grad_norm': 1.046875, 'learning_rate': 1.075278591292442e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4231, 'grad_norm': 0.78515625, 'learning_rate': 1.074142353546056e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1429, 'grad_norm': 0.8046875, 'learning_rate': 1.07300611579967e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1885, 'grad_norm': 0.60546875, 'learning_rate': 1.0718698780532839e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0657, 'grad_norm': 0.80859375, 'learning_rate': 1.070733640306898e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9256, 'grad_norm': 1.078125, 'learning_rate': 1.0695974025605118e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2998, 'grad_norm': 0.5546875, 'learning_rate': 1.0684611648141257e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1531, 'grad_norm': 0.65625, 'learning_rate': 1.0673249270677398e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2697, 'grad_norm': 0.490234375, 'learning_rate': 1.0661886893213536e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0797, 'grad_norm': 0.53125, 'learning_rate': 1.0650524515749676e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1501, 'grad_norm': 0.95703125, 'learning_rate': 1.0639162138285815e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4053, 'grad_norm': 0.5703125, 'learning_rate': 1.0627799760821955e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1873, 'grad_norm': 0.78515625, 'learning_rate': 1.0616437383358094e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2652, 'grad_norm': 0.51953125, 'learning_rate': 1.0605075005894234e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1387, 'grad_norm': 0.5703125, 'learning_rate': 1.0593712628430374e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1662, 'grad_norm': 1.171875, 'learning_rate': 1.0582350250966511e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3651, 'grad_norm': 0.66796875, 'learning_rate': 1.0570987873502653e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1933, 'grad_norm': 0.51171875, 'learning_rate': 1.055962549603879e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1275, 'grad_norm': 0.53515625, 'learning_rate': 1.0548263118574932e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3213, 'grad_norm': 0.62109375, 'learning_rate': 1.0536900741111071e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0909, 'grad_norm': 0.890625, 'learning_rate': 1.052553836364721e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3807, 'grad_norm': 0.58984375, 'learning_rate': 1.051417598618335e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1471, 'grad_norm': 0.70703125, 'learning_rate': 1.0502813608719488e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1813, 'grad_norm': 0.546875, 'learning_rate': 1.0491451231255628e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2003, 'grad_norm': 0.73046875, 'learning_rate': 1.0480088853791769e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0707, 'grad_norm': 0.7890625, 'learning_rate': 1.0468726476327907e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2933, 'grad_norm': 0.5546875, 'learning_rate': 1.0457364098864046e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1311, 'grad_norm': 0.5859375, 'learning_rate': 1.0446001721400186e-05, 'epoch': 0.85}\n",
      "{'loss': 1.188, 'grad_norm': 0.54296875, 'learning_rate': 1.0434639343936326e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3877, 'grad_norm': 0.76171875, 'learning_rate': 1.0423276966472465e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0821, 'grad_norm': 0.79296875, 'learning_rate': 1.0411914589008605e-05, 'epoch': 0.85}\n",
      "{'loss': 1.3237, 'grad_norm': 0.734375, 'learning_rate': 1.0400552211544744e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1331, 'grad_norm': 0.78515625, 'learning_rate': 1.0389189834080884e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1762, 'grad_norm': 0.412109375, 'learning_rate': 1.0377827456617023e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1882, 'grad_norm': 0.59375, 'learning_rate': 1.0366465079153161e-05, 'epoch': 0.85}\n",
      "{'loss': 1.082, 'grad_norm': 0.95703125, 'learning_rate': 1.0355102701689302e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4057, 'grad_norm': 0.5546875, 'learning_rate': 1.0343740324225442e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1294, 'grad_norm': 0.6796875, 'learning_rate': 1.033237794676158e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2498, 'grad_norm': 0.423828125, 'learning_rate': 1.0321015569297721e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0946, 'grad_norm': 0.7265625, 'learning_rate': 1.0309653191833859e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1571, 'grad_norm': 1.1484375, 'learning_rate': 1.029829081437e-05, 'epoch': 0.86}\n",
      "{'loss': 1.3062, 'grad_norm': 0.49609375, 'learning_rate': 1.028692843690614e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0633, 'grad_norm': 1.0390625, 'learning_rate': 1.0275566059442278e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1816, 'grad_norm': 0.482421875, 'learning_rate': 1.0264203681978419e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1899, 'grad_norm': 0.73828125, 'learning_rate': 1.0252841304514557e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0328, 'grad_norm': 1.0546875, 'learning_rate': 1.0241478927050696e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1977, 'grad_norm': 0.5078125, 'learning_rate': 1.0230116549586836e-05, 'epoch': 0.86}\n",
      "{'loss': 1.2138, 'grad_norm': 0.6484375, 'learning_rate': 1.0218754172122975e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0864, 'grad_norm': 0.462890625, 'learning_rate': 1.0207391794659115e-05, 'epoch': 0.86}\n",
      "{'loss': 1.2058, 'grad_norm': 0.7890625, 'learning_rate': 1.0196029417195254e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0308, 'grad_norm': 0.69921875, 'learning_rate': 1.0184667039731394e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2904, 'grad_norm': 0.58984375, 'learning_rate': 1.0173304662267533e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1232, 'grad_norm': 1.0078125, 'learning_rate': 1.0161942284803673e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1241, 'grad_norm': 0.47265625, 'learning_rate': 1.0150579907339813e-05, 'epoch': 0.86}\n",
      "{'loss': 1.3101, 'grad_norm': 0.72265625, 'learning_rate': 1.0139217529875952e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0225, 'grad_norm': 1.046875, 'learning_rate': 1.0127855152412092e-05, 'epoch': 0.86}\n",
      "{'loss': 1.4849, 'grad_norm': 0.46875, 'learning_rate': 1.011649277494823e-05, 'epoch': 0.86}\n",
      "{'loss': 1.2072, 'grad_norm': 0.71484375, 'learning_rate': 1.010513039748437e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1914, 'grad_norm': 0.5859375, 'learning_rate': 1.009376802002051e-05, 'epoch': 0.86}\n",
      "{'loss': 1.3901, 'grad_norm': 0.4609375, 'learning_rate': 1.0082405642556648e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0962, 'grad_norm': 0.60546875, 'learning_rate': 1.007104326509279e-05, 'epoch': 0.86}\n",
      "{'loss': 1.2623, 'grad_norm': 0.55859375, 'learning_rate': 1.0059680887628927e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1122, 'grad_norm': 0.7578125, 'learning_rate': 1.0048318510165067e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1877, 'grad_norm': 0.48046875, 'learning_rate': 1.0036956132701206e-05, 'epoch': 0.86}\n",
      "{'loss': 1.2258, 'grad_norm': 0.609375, 'learning_rate': 1.0025593755237346e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0526, 'grad_norm': 0.91015625, 'learning_rate': 1.0014231377773487e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1294, 'grad_norm': 0.60546875, 'learning_rate': 1.0002869000309625e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0633, 'grad_norm': 0.51171875, 'learning_rate': 9.991506622845765e-06, 'epoch': 0.86}\n",
      "{'loss': 1.324, 'grad_norm': 0.6484375, 'learning_rate': 9.980144245381904e-06, 'epoch': 0.86}\n",
      "{'loss': 1.127, 'grad_norm': 1.0234375, 'learning_rate': 9.968781867918044e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1867, 'grad_norm': 1.0078125, 'learning_rate': 9.957419490454183e-06, 'epoch': 0.86}\n",
      "{'loss': 1.4081, 'grad_norm': 0.58203125, 'learning_rate': 9.946057112990323e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0982, 'grad_norm': 0.765625, 'learning_rate': 9.934694735526462e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0815, 'grad_norm': 0.490234375, 'learning_rate': 9.9233323580626e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1371, 'grad_norm': 0.578125, 'learning_rate': 9.911969980598741e-06, 'epoch': 0.86}\n",
      "{'loss': 1.073, 'grad_norm': 1.3828125, 'learning_rate': 9.900607603134881e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4372, 'grad_norm': 0.68359375, 'learning_rate': 9.88924522567102e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0565, 'grad_norm': 0.68359375, 'learning_rate': 9.87788284820716e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2063, 'grad_norm': 0.51953125, 'learning_rate': 9.866520470743298e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2419, 'grad_norm': 0.6640625, 'learning_rate': 9.85515809327944e-06, 'epoch': 0.86}\n",
      "{'loss': 0.9716, 'grad_norm': 0.57421875, 'learning_rate': 9.843795715815577e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2696, 'grad_norm': 0.5078125, 'learning_rate': 9.832433338351717e-06, 'epoch': 0.86}\n",
      "{'loss': 0.966, 'grad_norm': 0.98828125, 'learning_rate': 9.821070960887858e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2577, 'grad_norm': 0.546875, 'learning_rate': 9.809708583423996e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2708, 'grad_norm': 0.73046875, 'learning_rate': 9.798346205960135e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0694, 'grad_norm': 0.62109375, 'learning_rate': 9.786983828496275e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3304, 'grad_norm': 0.625, 'learning_rate': 9.775621451032414e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2178, 'grad_norm': 0.61328125, 'learning_rate': 9.764259073568554e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0698, 'grad_norm': 0.5703125, 'learning_rate': 9.752896696104693e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1011, 'grad_norm': 0.78515625, 'learning_rate': 9.741534318640833e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1469, 'grad_norm': 0.95703125, 'learning_rate': 9.730171941176972e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3586, 'grad_norm': 0.52734375, 'learning_rate': 9.718809563713112e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1788, 'grad_norm': 0.6875, 'learning_rate': 9.707447186249252e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2617, 'grad_norm': 0.51953125, 'learning_rate': 9.696084808785391e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0921, 'grad_norm': 0.5, 'learning_rate': 9.68472243132153e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0081, 'grad_norm': 0.98046875, 'learning_rate': 9.673360053857669e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3648, 'grad_norm': 0.55078125, 'learning_rate': 9.66199767639381e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2273, 'grad_norm': 0.94921875, 'learning_rate': 9.650635298929948e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2102, 'grad_norm': 0.5390625, 'learning_rate': 9.639272921466087e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1921, 'grad_norm': 0.7265625, 'learning_rate': 9.627910544002228e-06, 'epoch': 0.86}\n",
      "{'loss': 1.066, 'grad_norm': 1.0078125, 'learning_rate': 9.616548166538366e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2098, 'grad_norm': 0.54296875, 'learning_rate': 9.605185789074508e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2502, 'grad_norm': 0.62109375, 'learning_rate': 9.593823411610645e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1329, 'grad_norm': 0.55859375, 'learning_rate': 9.582461034146785e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1739, 'grad_norm': 0.72265625, 'learning_rate': 9.571098656682924e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0462, 'grad_norm': 1.140625, 'learning_rate': 9.559736279219064e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3165, 'grad_norm': 0.61328125, 'learning_rate': 9.548373901755204e-06, 'epoch': 0.86}\n",
      "{'loss': 0.9949, 'grad_norm': 0.52734375, 'learning_rate': 9.537011524291343e-06, 'epoch': 0.86}\n",
      "{'loss': 1.222, 'grad_norm': 0.443359375, 'learning_rate': 9.525649146827483e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1798, 'grad_norm': 0.52734375, 'learning_rate': 9.514286769363622e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0891, 'grad_norm': 0.82421875, 'learning_rate': 9.502924391899762e-06, 'epoch': 0.86}\n",
      "{'loss': 1.274, 'grad_norm': 0.5234375, 'learning_rate': 9.491562014435901e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0917, 'grad_norm': 0.8515625, 'learning_rate': 9.480199636972041e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2338, 'grad_norm': 0.470703125, 'learning_rate': 9.46883725950818e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3227, 'grad_norm': 0.5625, 'learning_rate': 9.457474882044318e-06, 'epoch': 0.86}\n",
      "{'loss': 0.9889, 'grad_norm': 0.66015625, 'learning_rate': 9.44611250458046e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3165, 'grad_norm': 0.52734375, 'learning_rate': 9.434750127116599e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1235, 'grad_norm': 0.671875, 'learning_rate': 9.423387749652737e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1408, 'grad_norm': 0.53515625, 'learning_rate': 9.412025372188878e-06, 'epoch': 0.86}\n",
      "{'loss': 1.4074, 'grad_norm': 0.87890625, 'learning_rate': 9.400662994725016e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0261, 'grad_norm': 8.0, 'learning_rate': 9.389300617261156e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3628, 'grad_norm': 0.54296875, 'learning_rate': 9.377938239797295e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1016, 'grad_norm': 1.0546875, 'learning_rate': 9.366575862333435e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1752, 'grad_norm': 0.5078125, 'learning_rate': 9.355213484869576e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2247, 'grad_norm': 0.65625, 'learning_rate': 9.343851107405714e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0473, 'grad_norm': 0.5625, 'learning_rate': 9.332488729941853e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3018, 'grad_norm': 0.515625, 'learning_rate': 9.321126352477993e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1044, 'grad_norm': 1.03125, 'learning_rate': 9.309763975014132e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2279, 'grad_norm': 0.5390625, 'learning_rate': 9.298401597550272e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1821, 'grad_norm': 0.5390625, 'learning_rate': 9.287039220086411e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0717, 'grad_norm': 1.0234375, 'learning_rate': 9.275676842622551e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2964, 'grad_norm': 0.71484375, 'learning_rate': 9.264314465158689e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1725, 'grad_norm': 1.0078125, 'learning_rate': 9.25295208769483e-06, 'epoch': 0.86}\n",
      "{'loss': 1.296, 'grad_norm': 0.4921875, 'learning_rate': 9.24158971023097e-06, 'epoch': 0.86}\n",
      "{'loss': 1.181, 'grad_norm': 0.59765625, 'learning_rate': 9.23022733276711e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0489, 'grad_norm': 0.9453125, 'learning_rate': 9.218864955303249e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2421, 'grad_norm': 0.609375, 'learning_rate': 9.207502577839387e-06, 'epoch': 0.86}\n",
      "{'loss': 1.212, 'grad_norm': 0.89453125, 'learning_rate': 9.196140200375528e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1213, 'grad_norm': 0.515625, 'learning_rate': 9.184777822911666e-06, 'epoch': 0.86}\n",
      "{'loss': 1.222, 'grad_norm': 0.482421875, 'learning_rate': 9.173415445447805e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0756, 'grad_norm': 0.48046875, 'learning_rate': 9.162053067983947e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2829, 'grad_norm': 0.52734375, 'learning_rate': 9.150690690520084e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0977, 'grad_norm': 0.8125, 'learning_rate': 9.139328313056224e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1512, 'grad_norm': 0.55078125, 'learning_rate': 9.127965935592363e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3201, 'grad_norm': 0.62890625, 'learning_rate': 9.116603558128503e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0257, 'grad_norm': 0.44140625, 'learning_rate': 9.105241180664643e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2601, 'grad_norm': 0.62890625, 'learning_rate': 9.093878803200782e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2303, 'grad_norm': 0.81640625, 'learning_rate': 9.082516425736922e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2258, 'grad_norm': 0.57421875, 'learning_rate': 9.071154048273061e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1723, 'grad_norm': 0.515625, 'learning_rate': 9.0597916708092e-06, 'epoch': 0.86}\n",
      "{'loss': 0.962, 'grad_norm': 0.98046875, 'learning_rate': 9.04842929334534e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4523, 'grad_norm': 0.57421875, 'learning_rate': 9.03706691588148e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2242, 'grad_norm': 0.8515625, 'learning_rate': 9.02570453841762e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2004, 'grad_norm': 0.48046875, 'learning_rate': 9.014342160953757e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2628, 'grad_norm': 0.67578125, 'learning_rate': 9.002979783489899e-06, 'epoch': 0.86}\n",
      "{'loss': 0.9786, 'grad_norm': 0.578125, 'learning_rate': 8.991617406026036e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2885, 'grad_norm': 0.4453125, 'learning_rate': 8.980255028562176e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1851, 'grad_norm': 0.76171875, 'learning_rate': 8.968892651098317e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1328, 'grad_norm': 0.455078125, 'learning_rate': 8.957530273634455e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2054, 'grad_norm': 0.59375, 'learning_rate': 8.946167896170596e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1229, 'grad_norm': 0.66796875, 'learning_rate': 8.934805518706734e-06, 'epoch': 0.86}\n",
      "{'loss': 1.271, 'grad_norm': 0.58203125, 'learning_rate': 8.923443141242874e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1813, 'grad_norm': 0.6875, 'learning_rate': 8.912080763779015e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2253, 'grad_norm': 0.478515625, 'learning_rate': 8.900718386315153e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1988, 'grad_norm': 0.765625, 'learning_rate': 8.889356008851292e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0309, 'grad_norm': 0.953125, 'learning_rate': 8.877993631387432e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2341, 'grad_norm': 0.57421875, 'learning_rate': 8.866631253923571e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0525, 'grad_norm': 0.6953125, 'learning_rate': 8.855268876459711e-06, 'epoch': 0.86}\n",
      "{'loss': 1.111, 'grad_norm': 0.60546875, 'learning_rate': 8.84390649899585e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2063, 'grad_norm': 0.77734375, 'learning_rate': 8.83254412153199e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0732, 'grad_norm': 1.3125, 'learning_rate': 8.82118174406813e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2328, 'grad_norm': 0.72265625, 'learning_rate': 8.809819366604269e-06, 'epoch': 0.86}\n",
      "{'loss': 1.186, 'grad_norm': 0.703125, 'learning_rate': 8.798456989140407e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2279, 'grad_norm': 0.49609375, 'learning_rate': 8.787094611676548e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2114, 'grad_norm': 0.62109375, 'learning_rate': 8.775732234212688e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0996, 'grad_norm': 0.87890625, 'learning_rate': 8.764369856748826e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3518, 'grad_norm': 0.609375, 'learning_rate': 8.753007479284967e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1928, 'grad_norm': 0.8046875, 'learning_rate': 8.741645101821105e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1219, 'grad_norm': 0.51171875, 'learning_rate': 8.730282724357244e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2409, 'grad_norm': 0.5078125, 'learning_rate': 8.718920346893386e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1525, 'grad_norm': 2.03125, 'learning_rate': 8.707557969429523e-06, 'epoch': 0.86}\n",
      "{'loss': 1.4512, 'grad_norm': 0.59765625, 'learning_rate': 8.696195591965663e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1898, 'grad_norm': 0.765625, 'learning_rate': 8.684833214501802e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1558, 'grad_norm': 0.486328125, 'learning_rate': 8.673470837037942e-06, 'epoch': 0.86}\n",
      "{'loss': 1.186, 'grad_norm': 0.59765625, 'learning_rate': 8.662108459574082e-06, 'epoch': 0.86}\n",
      "{'loss': 1.227, 'grad_norm': 1.203125, 'learning_rate': 8.650746082110221e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2994, 'grad_norm': 0.7109375, 'learning_rate': 8.63938370464636e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0533, 'grad_norm': 0.71484375, 'learning_rate': 8.6280213271825e-06, 'epoch': 0.86}\n",
      "{'loss': 1.301, 'grad_norm': 0.4453125, 'learning_rate': 8.61665894971864e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2427, 'grad_norm': 0.71875, 'learning_rate': 8.60529657225478e-06, 'epoch': 0.86}\n",
      "{'loss': 0.9961, 'grad_norm': 0.90234375, 'learning_rate': 8.593934194790919e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3001, 'grad_norm': 0.6171875, 'learning_rate': 8.582571817327058e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1205, 'grad_norm': 0.51171875, 'learning_rate': 8.571209439863196e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2234, 'grad_norm': 0.53125, 'learning_rate': 8.559847062399338e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2777, 'grad_norm': 0.6015625, 'learning_rate': 8.548484684935475e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2221, 'grad_norm': 0.734375, 'learning_rate': 8.537122307471617e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3357, 'grad_norm': 0.53515625, 'learning_rate': 8.525759930007756e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1034, 'grad_norm': 0.8125, 'learning_rate': 8.514397552543894e-06, 'epoch': 0.86}\n",
      "{'loss': 1.076, 'grad_norm': 0.5234375, 'learning_rate': 8.503035175080035e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1329, 'grad_norm': 0.62109375, 'learning_rate': 8.491672797616173e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0019, 'grad_norm': 0.86328125, 'learning_rate': 8.480310420152313e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2138, 'grad_norm': 0.62109375, 'learning_rate': 8.468948042688452e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2319, 'grad_norm': 0.9765625, 'learning_rate': 8.457585665224592e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2024, 'grad_norm': 0.5, 'learning_rate': 8.446223287760731e-06, 'epoch': 0.86}\n",
      "{'loss': 1.28, 'grad_norm': 0.5703125, 'learning_rate': 8.434860910296871e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0741, 'grad_norm': 0.9140625, 'learning_rate': 8.42349853283301e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3546, 'grad_norm': 0.58203125, 'learning_rate': 8.41213615536915e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2099, 'grad_norm': 0.93359375, 'learning_rate': 8.40077377790529e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1761, 'grad_norm': 0.439453125, 'learning_rate': 8.389411400441429e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2763, 'grad_norm': 0.875, 'learning_rate': 8.378049022977569e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0679, 'grad_norm': 1.1171875, 'learning_rate': 8.366686645513708e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2942, 'grad_norm': 0.640625, 'learning_rate': 8.355324268049846e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0675, 'grad_norm': 0.69140625, 'learning_rate': 8.343961890585987e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2156, 'grad_norm': 0.4296875, 'learning_rate': 8.332599513122127e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2393, 'grad_norm': 1.078125, 'learning_rate': 8.321237135658265e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0646, 'grad_norm': 1.046875, 'learning_rate': 8.309874758194406e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3868, 'grad_norm': 0.4921875, 'learning_rate': 8.298512380730544e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1799, 'grad_norm': 0.65234375, 'learning_rate': 8.287150003266683e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2246, 'grad_norm': 0.58203125, 'learning_rate': 8.275787625802823e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2307, 'grad_norm': 0.76953125, 'learning_rate': 8.264425248338962e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0235, 'grad_norm': 0.82421875, 'learning_rate': 8.253062870875104e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3353, 'grad_norm': 0.62890625, 'learning_rate': 8.241700493411241e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1544, 'grad_norm': 0.7265625, 'learning_rate': 8.230338115947381e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1071, 'grad_norm': 0.60546875, 'learning_rate': 8.21897573848352e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3033, 'grad_norm': 0.56640625, 'learning_rate': 8.20761336101966e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1344, 'grad_norm': 1.015625, 'learning_rate': 8.1962509835558e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1649, 'grad_norm': 0.515625, 'learning_rate': 8.18488860609194e-06, 'epoch': 0.86}\n",
      "{'loss': 0.9037, 'grad_norm': 0.8984375, 'learning_rate': 8.173526228628079e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1325, 'grad_norm': 0.392578125, 'learning_rate': 8.162163851164217e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1736, 'grad_norm': 0.92578125, 'learning_rate': 8.150801473700358e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0737, 'grad_norm': 0.8125, 'learning_rate': 8.139439096236497e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2352, 'grad_norm': 0.55078125, 'learning_rate': 8.128076718772637e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0075, 'grad_norm': 0.59765625, 'learning_rate': 8.116714341308777e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1931, 'grad_norm': 0.62890625, 'learning_rate': 8.105351963844914e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1771, 'grad_norm': 0.62890625, 'learning_rate': 8.093989586381056e-06, 'epoch': 0.86}\n",
      "{'loss': 0.8746, 'grad_norm': 0.8984375, 'learning_rate': 8.082627208917193e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3068, 'grad_norm': 0.54296875, 'learning_rate': 8.071264831453333e-06, 'epoch': 0.86}\n",
      "{'loss': 1.312, 'grad_norm': 0.7421875, 'learning_rate': 8.059902453989474e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2896, 'grad_norm': 0.6796875, 'learning_rate': 8.048540076525612e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2025, 'grad_norm': 0.703125, 'learning_rate': 8.037177699061752e-06, 'epoch': 0.86}\n",
      "{'loss': 1.0365, 'grad_norm': 0.671875, 'learning_rate': 8.025815321597891e-06, 'epoch': 0.86}\n",
      "{'loss': 1.2158, 'grad_norm': 0.6015625, 'learning_rate': 8.01445294413403e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2467, 'grad_norm': 0.69921875, 'learning_rate': 8.00309056667017e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1674, 'grad_norm': 0.482421875, 'learning_rate': 7.99172818920631e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1651, 'grad_norm': 0.51171875, 'learning_rate': 7.98036581174245e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9756, 'grad_norm': 0.6171875, 'learning_rate': 7.969003434278589e-06, 'epoch': 0.87}\n",
      "{'loss': 1.212, 'grad_norm': 0.5234375, 'learning_rate': 7.957641056814728e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1834, 'grad_norm': 0.64453125, 'learning_rate': 7.946278679350868e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1584, 'grad_norm': 0.50390625, 'learning_rate': 7.934916301887008e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2537, 'grad_norm': 0.703125, 'learning_rate': 7.923553924423147e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1392, 'grad_norm': 0.60546875, 'learning_rate': 7.912191546959285e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2266, 'grad_norm': 0.48046875, 'learning_rate': 7.900829169495426e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1591, 'grad_norm': 0.6796875, 'learning_rate': 7.889466792031564e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2426, 'grad_norm': 0.498046875, 'learning_rate': 7.878104414567705e-06, 'epoch': 0.87}\n",
      "{'loss': 1.265, 'grad_norm': 0.6953125, 'learning_rate': 7.866742037103845e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0251, 'grad_norm': 1.1875, 'learning_rate': 7.855379659639983e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3854, 'grad_norm': 0.5078125, 'learning_rate': 7.844017282176124e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0559, 'grad_norm': 0.74609375, 'learning_rate': 7.832654904712262e-06, 'epoch': 0.87}\n",
      "{'loss': 1.385, 'grad_norm': 0.97265625, 'learning_rate': 7.821292527248401e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1701, 'grad_norm': 0.5859375, 'learning_rate': 7.809930149784541e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0043, 'grad_norm': 0.83984375, 'learning_rate': 7.79856777232068e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3571, 'grad_norm': 0.5625, 'learning_rate': 7.78720539485682e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1528, 'grad_norm': 0.69921875, 'learning_rate': 7.77584301739296e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1924, 'grad_norm': 0.58984375, 'learning_rate': 7.764480639929099e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0774, 'grad_norm': 0.59765625, 'learning_rate': 7.753118262465239e-06, 'epoch': 0.87}\n",
      "{'loss': 0.8549, 'grad_norm': 0.69140625, 'learning_rate': 7.741755885001378e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2439, 'grad_norm': 0.671875, 'learning_rate': 7.730393507537518e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1558, 'grad_norm': 0.62109375, 'learning_rate': 7.719031130073657e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2482, 'grad_norm': 0.427734375, 'learning_rate': 7.707668752609797e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1436, 'grad_norm': 0.67578125, 'learning_rate': 7.696306375145935e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0944, 'grad_norm': 0.62109375, 'learning_rate': 7.684943997682076e-06, 'epoch': 0.87}\n",
      "{'loss': 1.4157, 'grad_norm': 0.447265625, 'learning_rate': 7.673581620218216e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2048, 'grad_norm': 0.765625, 'learning_rate': 7.662219242754353e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2433, 'grad_norm': 0.61328125, 'learning_rate': 7.650856865290495e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2779, 'grad_norm': 0.6640625, 'learning_rate': 7.639494487826632e-06, 'epoch': 0.87}\n",
      "{'loss': 1.066, 'grad_norm': 0.64453125, 'learning_rate': 7.628132110362772e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2579, 'grad_norm': 0.53125, 'learning_rate': 7.616769732898912e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0847, 'grad_norm': 0.82421875, 'learning_rate': 7.605407355435051e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2017, 'grad_norm': 0.57421875, 'learning_rate': 7.5940449779711915e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2001, 'grad_norm': 0.6328125, 'learning_rate': 7.582682600507331e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1393, 'grad_norm': 1.1875, 'learning_rate': 7.57132022304347e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3301, 'grad_norm': 0.5, 'learning_rate': 7.55995784557961e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1564, 'grad_norm': 0.75, 'learning_rate': 7.548595468115749e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1099, 'grad_norm': 0.51953125, 'learning_rate': 7.5372330906518876e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1551, 'grad_norm': 0.7109375, 'learning_rate': 7.525870713188028e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0132, 'grad_norm': 0.765625, 'learning_rate': 7.5145083357241675e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3209, 'grad_norm': 0.6015625, 'learning_rate': 7.503145958260306e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1063, 'grad_norm': 0.953125, 'learning_rate': 7.491783580796447e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2097, 'grad_norm': 0.498046875, 'learning_rate': 7.480421203332585e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2023, 'grad_norm': 0.8984375, 'learning_rate': 7.469058825868726e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0962, 'grad_norm': 1.0546875, 'learning_rate': 7.457696448404864e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3321, 'grad_norm': 0.6328125, 'learning_rate': 7.446334070941004e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1681, 'grad_norm': 1.1953125, 'learning_rate': 7.434971693477144e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2108, 'grad_norm': 0.4375, 'learning_rate': 7.423609316013283e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3354, 'grad_norm': 0.6328125, 'learning_rate': 7.412246938549422e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0032, 'grad_norm': 0.67578125, 'learning_rate': 7.400884561085562e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2916, 'grad_norm': 1.03125, 'learning_rate': 7.389522183621702e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9986, 'grad_norm': 0.90625, 'learning_rate': 7.37815980615784e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1538, 'grad_norm': 0.498046875, 'learning_rate': 7.366797428693981e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1501, 'grad_norm': 1.140625, 'learning_rate': 7.3554350512301195e-06, 'epoch': 0.87}\n",
      "{'loss': 0.975, 'grad_norm': 0.984375, 'learning_rate': 7.344072673766258e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2488, 'grad_norm': 0.765625, 'learning_rate': 7.332710296302399e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2183, 'grad_norm': 0.69921875, 'learning_rate': 7.321347918838538e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3253, 'grad_norm': 0.44921875, 'learning_rate': 7.3099855413746785e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2029, 'grad_norm': 0.53515625, 'learning_rate': 7.298623163910817e-06, 'epoch': 0.87}\n",
      "{'loss': 0.988, 'grad_norm': 0.546875, 'learning_rate': 7.287260786446956e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3938, 'grad_norm': 0.59375, 'learning_rate': 7.275898408983096e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0868, 'grad_norm': 0.7734375, 'learning_rate': 7.264536031519235e-06, 'epoch': 0.87}\n",
      "{'loss': 1.259, 'grad_norm': 0.69921875, 'learning_rate': 7.2531736540553746e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3082, 'grad_norm': 0.65625, 'learning_rate': 7.241811276591515e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1288, 'grad_norm': 0.7421875, 'learning_rate': 7.230448899127654e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3627, 'grad_norm': 0.640625, 'learning_rate': 7.219086521663792e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0879, 'grad_norm': 1.015625, 'learning_rate': 7.207724144199933e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0984, 'grad_norm': 0.73046875, 'learning_rate': 7.196361766736072e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1681, 'grad_norm': 0.67578125, 'learning_rate': 7.184999389272212e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9915, 'grad_norm': 0.5859375, 'learning_rate': 7.173637011808351e-06, 'epoch': 0.87}\n",
      "{'loss': 1.392, 'grad_norm': 0.56640625, 'learning_rate': 7.16227463434449e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1743, 'grad_norm': 0.58984375, 'learning_rate': 7.1509122568806305e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1438, 'grad_norm': 0.515625, 'learning_rate': 7.139549879416769e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2603, 'grad_norm': 0.61328125, 'learning_rate': 7.128187501952909e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9827, 'grad_norm': 0.625, 'learning_rate': 7.116825124489049e-06, 'epoch': 0.87}\n",
      "{'loss': 1.398, 'grad_norm': 0.61328125, 'learning_rate': 7.105462747025188e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1133, 'grad_norm': 0.703125, 'learning_rate': 7.0941003695613266e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2748, 'grad_norm': 0.470703125, 'learning_rate': 7.082737992097467e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1926, 'grad_norm': 0.57421875, 'learning_rate': 7.071375614633606e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0425, 'grad_norm': 0.96484375, 'learning_rate': 7.060013237169746e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2688, 'grad_norm': 0.55859375, 'learning_rate': 7.048650859705886e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2, 'grad_norm': 0.6875, 'learning_rate': 7.037288482242024e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0925, 'grad_norm': 0.71875, 'learning_rate': 7.025926104778165e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3521, 'grad_norm': 0.55078125, 'learning_rate': 7.014563727314303e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9693, 'grad_norm': 0.93359375, 'learning_rate': 7.003201349850443e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2294, 'grad_norm': 0.48046875, 'learning_rate': 6.991838972386583e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2042, 'grad_norm': 0.82421875, 'learning_rate': 6.980476594922722e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1751, 'grad_norm': 0.458984375, 'learning_rate': 6.969114217458861e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2672, 'grad_norm': 0.80859375, 'learning_rate': 6.957751839995001e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9804, 'grad_norm': 0.96484375, 'learning_rate': 6.94638946253114e-06, 'epoch': 0.87}\n",
      "{'loss': 1.319, 'grad_norm': 0.96484375, 'learning_rate': 6.93502708506728e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9776, 'grad_norm': 0.578125, 'learning_rate': 6.92366470760342e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2115, 'grad_norm': 0.578125, 'learning_rate': 6.9123023301395585e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3372, 'grad_norm': 0.61328125, 'learning_rate': 6.900939952675699e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1379, 'grad_norm': 0.67578125, 'learning_rate': 6.889577575211838e-06, 'epoch': 0.87}\n",
      "{'loss': 1.416, 'grad_norm': 0.55859375, 'learning_rate': 6.878215197747976e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2317, 'grad_norm': 0.953125, 'learning_rate': 6.866852820284117e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2004, 'grad_norm': 0.41796875, 'learning_rate': 6.855490442820256e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1703, 'grad_norm': 0.8984375, 'learning_rate': 6.844128065356395e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0738, 'grad_norm': 1.34375, 'learning_rate': 6.832765687892535e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3551, 'grad_norm': 0.50390625, 'learning_rate': 6.821403310428674e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2077, 'grad_norm': 2.03125, 'learning_rate': 6.8100409329648136e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0674, 'grad_norm': 0.478515625, 'learning_rate': 6.798678555500954e-06, 'epoch': 0.87}\n",
      "{'loss': 1.254, 'grad_norm': 0.734375, 'learning_rate': 6.787316178037093e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1769, 'grad_norm': 1.03125, 'learning_rate': 6.775953800573233e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4, 'grad_norm': 0.5703125, 'learning_rate': 6.764591423109372e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1144, 'grad_norm': 0.498046875, 'learning_rate': 6.7532290456455105e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2359, 'grad_norm': 0.41796875, 'learning_rate': 6.741866668181651e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1329, 'grad_norm': 0.796875, 'learning_rate': 6.73050429071779e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0488, 'grad_norm': 0.90625, 'learning_rate': 6.719141913253929e-06, 'epoch': 0.87}\n",
      "{'loss': 1.323, 'grad_norm': 0.51953125, 'learning_rate': 6.7077795357900695e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1721, 'grad_norm': 1.0234375, 'learning_rate': 6.696417158326208e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1435, 'grad_norm': 0.4609375, 'learning_rate': 6.685054780862347e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2172, 'grad_norm': 0.7265625, 'learning_rate': 6.673692403398487e-06, 'epoch': 0.87}\n",
      "{'loss': 1.198, 'grad_norm': 1.5234375, 'learning_rate': 6.662330025934627e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3829, 'grad_norm': 0.57421875, 'learning_rate': 6.650967648470767e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1804, 'grad_norm': 0.73046875, 'learning_rate': 6.639605271006906e-06, 'epoch': 0.87}\n",
      "{'loss': 1.164, 'grad_norm': 0.44140625, 'learning_rate': 6.628242893543045e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2638, 'grad_norm': 0.62109375, 'learning_rate': 6.616880516079185e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1525, 'grad_norm': 0.80078125, 'learning_rate': 6.605518138615325e-06, 'epoch': 0.87}\n",
      "{'loss': 1.4147, 'grad_norm': 0.5859375, 'learning_rate': 6.594155761151463e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1283, 'grad_norm': 0.875, 'learning_rate': 6.582793383687604e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2198, 'grad_norm': 0.8515625, 'learning_rate': 6.571431006223742e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2176, 'grad_norm': 0.8515625, 'learning_rate': 6.560068628759881e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0791, 'grad_norm': 0.5390625, 'learning_rate': 6.5487062512960215e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3199, 'grad_norm': 0.76171875, 'learning_rate': 6.537343873832161e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2232, 'grad_norm': 0.58203125, 'learning_rate': 6.5259814963683014e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2912, 'grad_norm': 0.53125, 'learning_rate': 6.51461911890444e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2204, 'grad_norm': 0.5859375, 'learning_rate': 6.503256741440579e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1311, 'grad_norm': 0.91015625, 'learning_rate': 6.491894363976719e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3717, 'grad_norm': 0.5625, 'learning_rate': 6.480531986512858e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2045, 'grad_norm': 0.69140625, 'learning_rate': 6.4691696090489975e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3242, 'grad_norm': 0.515625, 'learning_rate': 6.457807231585138e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2676, 'grad_norm': 0.7421875, 'learning_rate': 6.446444854121277e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0713, 'grad_norm': 0.828125, 'learning_rate': 6.435082476657415e-06, 'epoch': 0.87}\n",
      "{'loss': 1.475, 'grad_norm': 0.6015625, 'learning_rate': 6.423720099193556e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1793, 'grad_norm': 0.6328125, 'learning_rate': 6.412357721729695e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3024, 'grad_norm': 0.45703125, 'learning_rate': 6.400995344265834e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1087, 'grad_norm': 0.88671875, 'learning_rate': 6.389632966801974e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0329, 'grad_norm': 1.1875, 'learning_rate': 6.378270589338113e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2222, 'grad_norm': 0.58203125, 'learning_rate': 6.366908211874253e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1026, 'grad_norm': 0.69921875, 'learning_rate': 6.355545834410392e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2581, 'grad_norm': 0.51171875, 'learning_rate': 6.344183456946532e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2459, 'grad_norm': 0.57421875, 'learning_rate': 6.332821079482672e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0382, 'grad_norm': 1.0859375, 'learning_rate': 6.321458702018811e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2292, 'grad_norm': 0.55078125, 'learning_rate': 6.3100963245549495e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3117, 'grad_norm': 0.93359375, 'learning_rate': 6.29873394709109e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2374, 'grad_norm': 0.60546875, 'learning_rate': 6.2873715696272286e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2571, 'grad_norm': 0.6875, 'learning_rate': 6.276009192163368e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0463, 'grad_norm': 0.60546875, 'learning_rate': 6.2646468146995085e-06, 'epoch': 0.87}\n",
      "{'loss': 1.4213, 'grad_norm': 0.5625, 'learning_rate': 6.253284437235647e-06, 'epoch': 0.87}\n",
      "{'loss': 1.136, 'grad_norm': 0.83203125, 'learning_rate': 6.241922059771787e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3293, 'grad_norm': 0.439453125, 'learning_rate': 6.230559682307926e-06, 'epoch': 0.87}\n",
      "{'loss': 1.145, 'grad_norm': 0.7734375, 'learning_rate': 6.219197304844066e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1902, 'grad_norm': 0.64453125, 'learning_rate': 6.207834927380205e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3464, 'grad_norm': 0.53125, 'learning_rate': 6.196472549916345e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0206, 'grad_norm': 1.1796875, 'learning_rate': 6.1851101724524845e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2829, 'grad_norm': 0.82421875, 'learning_rate': 6.173747794988623e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1442, 'grad_norm': 0.6953125, 'learning_rate': 6.162385417524763e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9992, 'grad_norm': 0.8046875, 'learning_rate': 6.151023040060902e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2257, 'grad_norm': 0.53515625, 'learning_rate': 6.139660662597043e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0694, 'grad_norm': 0.90625, 'learning_rate': 6.128298285133181e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1848, 'grad_norm': 0.5703125, 'learning_rate': 6.116935907669321e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3985, 'grad_norm': 0.66015625, 'learning_rate': 6.1055735302054605e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0776, 'grad_norm': 1.1484375, 'learning_rate': 6.0942111527416e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3029, 'grad_norm': 0.490234375, 'learning_rate': 6.08284877527774e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1458, 'grad_norm': 0.921875, 'learning_rate': 6.071486397813879e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1498, 'grad_norm': 0.44921875, 'learning_rate': 6.060124020350019e-06, 'epoch': 0.87}\n",
      "{'loss': 1.219, 'grad_norm': 0.65234375, 'learning_rate': 6.048761642886157e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1584, 'grad_norm': 1.7265625, 'learning_rate': 6.037399265422297e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3112, 'grad_norm': 0.546875, 'learning_rate': 6.0260368879584365e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0653, 'grad_norm': 0.6328125, 'learning_rate': 6.014674510494577e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2349, 'grad_norm': 0.51171875, 'learning_rate': 6.003312133030716e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3219, 'grad_norm': 0.66015625, 'learning_rate': 5.991949755566855e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1688, 'grad_norm': 1.65625, 'learning_rate': 5.980587378102995e-06, 'epoch': 0.87}\n",
      "{'loss': 1.4418, 'grad_norm': 0.54296875, 'learning_rate': 5.969225000639133e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1802, 'grad_norm': 0.515625, 'learning_rate': 5.957862623175274e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0941, 'grad_norm': 0.42578125, 'learning_rate': 5.946500245711413e-06, 'epoch': 0.87}\n",
      "{'loss': 1.195, 'grad_norm': 1.0, 'learning_rate': 5.935137868247553e-06, 'epoch': 0.87}\n",
      "{'loss': 1.062, 'grad_norm': 0.92578125, 'learning_rate': 5.9237754907836916e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2579, 'grad_norm': 1.0390625, 'learning_rate': 5.912413113319831e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2325, 'grad_norm': 0.73828125, 'learning_rate': 5.901050735855971e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1942, 'grad_norm': 0.57421875, 'learning_rate': 5.88968835839211e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3072, 'grad_norm': 0.58984375, 'learning_rate': 5.87832598092825e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1427, 'grad_norm': 0.70703125, 'learning_rate': 5.866963603464389e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2843, 'grad_norm': 0.546875, 'learning_rate': 5.855601226000529e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2012, 'grad_norm': 0.87109375, 'learning_rate': 5.8442388485366676e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1629, 'grad_norm': 0.498046875, 'learning_rate': 5.832876471072807e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1611, 'grad_norm': 0.84375, 'learning_rate': 5.8215140936089475e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9897, 'grad_norm': 0.91796875, 'learning_rate': 5.810151716145087e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3039, 'grad_norm': 0.5234375, 'learning_rate': 5.798789338681226e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2292, 'grad_norm': 0.71484375, 'learning_rate': 5.787426961217365e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2886, 'grad_norm': 0.53515625, 'learning_rate': 5.776064583753505e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0921, 'grad_norm': 1.078125, 'learning_rate': 5.764702206289644e-06, 'epoch': 0.87}\n",
      "{'loss': 1.112, 'grad_norm': 0.95703125, 'learning_rate': 5.753339828825784e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3131, 'grad_norm': 0.53515625, 'learning_rate': 5.7419774513619235e-06, 'epoch': 0.87}\n",
      "{'loss': 1.0847, 'grad_norm': 0.65234375, 'learning_rate': 5.730615073898063e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1508, 'grad_norm': 0.42578125, 'learning_rate': 5.719252696434202e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3287, 'grad_norm': 0.73828125, 'learning_rate': 5.707890318970341e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0842, 'grad_norm': 0.84765625, 'learning_rate': 5.696527941506481e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3264, 'grad_norm': 0.578125, 'learning_rate': 5.685165564042621e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2133, 'grad_norm': 0.75, 'learning_rate': 5.67380318657876e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1818, 'grad_norm': 0.43359375, 'learning_rate': 5.6624408091148995e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2832, 'grad_norm': 0.6484375, 'learning_rate': 5.651078431651039e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0166, 'grad_norm': 0.765625, 'learning_rate': 5.639716054187178e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3487, 'grad_norm': 0.73046875, 'learning_rate': 5.628353676723318e-06, 'epoch': 0.88}\n",
      "{'loss': 1.192, 'grad_norm': 0.75, 'learning_rate': 5.616991299259458e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2016, 'grad_norm': 0.447265625, 'learning_rate': 5.605628921795597e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2773, 'grad_norm': 0.91015625, 'learning_rate': 5.594266544331736e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0201, 'grad_norm': 0.61328125, 'learning_rate': 5.5829041668678755e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3848, 'grad_norm': 0.54296875, 'learning_rate': 5.571541789404015e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0686, 'grad_norm': 1.09375, 'learning_rate': 5.560179411940155e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0941, 'grad_norm': 0.55078125, 'learning_rate': 5.548817034476294e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2553, 'grad_norm': 0.5390625, 'learning_rate': 5.537454657012434e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0047, 'grad_norm': 0.84375, 'learning_rate': 5.526092279548573e-06, 'epoch': 0.88}\n",
      "{'loss': 1.299, 'grad_norm': 0.53515625, 'learning_rate': 5.514729902084712e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1391, 'grad_norm': 1.4140625, 'learning_rate': 5.5033675246208515e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0247, 'grad_norm': 0.49609375, 'learning_rate': 5.492005147156992e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2517, 'grad_norm': 0.6953125, 'learning_rate': 5.480642769693131e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0797, 'grad_norm': 0.52734375, 'learning_rate': 5.46928039222927e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3031, 'grad_norm': 0.515625, 'learning_rate': 5.45791801476541e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2334, 'grad_norm': 1.6328125, 'learning_rate': 5.446555637301549e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0967, 'grad_norm': 0.51953125, 'learning_rate': 5.435193259837689e-06, 'epoch': 0.88}\n",
      "{'loss': 1.189, 'grad_norm': 0.58203125, 'learning_rate': 5.423830882373828e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0058, 'grad_norm': 0.92578125, 'learning_rate': 5.412468504909968e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2295, 'grad_norm': 0.54296875, 'learning_rate': 5.401106127446107e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1527, 'grad_norm': 0.94140625, 'learning_rate': 5.389743749982246e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1743, 'grad_norm': 0.50390625, 'learning_rate': 5.378381372518386e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2941, 'grad_norm': 0.6484375, 'learning_rate': 5.367018995054525e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0641, 'grad_norm': 0.8828125, 'learning_rate': 5.355656617590665e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2752, 'grad_norm': 0.83984375, 'learning_rate': 5.344294240126804e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9267, 'grad_norm': 0.83203125, 'learning_rate': 5.332931862662944e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2597, 'grad_norm': 0.72265625, 'learning_rate': 5.321569485199083e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1758, 'grad_norm': 0.63671875, 'learning_rate': 5.310207107735222e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1307, 'grad_norm': 1.34375, 'learning_rate': 5.2988447302713625e-06, 'epoch': 0.88}\n",
      "{'loss': 1.293, 'grad_norm': 0.8125, 'learning_rate': 5.287482352807502e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1353, 'grad_norm': 0.640625, 'learning_rate': 5.276119975343642e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1092, 'grad_norm': 0.5234375, 'learning_rate': 5.26475759787978e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2669, 'grad_norm': 0.60546875, 'learning_rate': 5.25339522041592e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9508, 'grad_norm': 0.51171875, 'learning_rate': 5.242032842952059e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3771, 'grad_norm': 0.6015625, 'learning_rate': 5.230670465488199e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1338, 'grad_norm': 0.625, 'learning_rate': 5.2193080880243385e-06, 'epoch': 0.88}\n",
      "{'loss': 1.217, 'grad_norm': 0.51953125, 'learning_rate': 5.207945710560478e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0138, 'grad_norm': 0.5703125, 'learning_rate': 5.196583333096618e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0282, 'grad_norm': 0.63671875, 'learning_rate': 5.185220955632756e-06, 'epoch': 0.88}\n",
      "{'loss': 1.391, 'grad_norm': 0.6953125, 'learning_rate': 5.173858578168897e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1876, 'grad_norm': 0.7421875, 'learning_rate': 5.162496200705036e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1609, 'grad_norm': 0.50390625, 'learning_rate': 5.151133823241176e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2651, 'grad_norm': 0.59375, 'learning_rate': 5.1397714457773145e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0871, 'grad_norm': 0.46875, 'learning_rate': 5.128409068313454e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1809, 'grad_norm': 0.62890625, 'learning_rate': 5.117046690849594e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0624, 'grad_norm': 0.92578125, 'learning_rate': 5.105684313385733e-06, 'epoch': 0.88}\n",
      "{'loss': 1.4588, 'grad_norm': 0.486328125, 'learning_rate': 5.094321935921873e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2364, 'grad_norm': 0.61328125, 'learning_rate': 5.082959558458012e-06, 'epoch': 0.88}\n",
      "{'loss': 1.148, 'grad_norm': 0.8125, 'learning_rate': 5.071597180994152e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2963, 'grad_norm': 0.625, 'learning_rate': 5.0602348035302905e-06, 'epoch': 0.88}\n",
      "{'loss': 1.138, 'grad_norm': 0.65625, 'learning_rate': 5.04887242606643e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2187, 'grad_norm': 0.65234375, 'learning_rate': 5.03751004860257e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2543, 'grad_norm': 0.6015625, 'learning_rate': 5.026147671138709e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1514, 'grad_norm': 0.78125, 'learning_rate': 5.014785293674849e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2926, 'grad_norm': 0.5546875, 'learning_rate': 5.003422916210988e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0856, 'grad_norm': 0.55859375, 'learning_rate': 4.992060538747128e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2761, 'grad_norm': 0.59765625, 'learning_rate': 4.980698161283267e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1433, 'grad_norm': 0.65625, 'learning_rate': 4.969335783819407e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0429, 'grad_norm': 0.796875, 'learning_rate': 4.957973406355546e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3927, 'grad_norm': 0.484375, 'learning_rate': 4.946611028891686e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2502, 'grad_norm': 0.97265625, 'learning_rate': 4.935248651427825e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3744, 'grad_norm': 0.46875, 'learning_rate': 4.923886273963964e-06, 'epoch': 0.88}\n",
      "{'loss': 1.341, 'grad_norm': 0.6796875, 'learning_rate': 4.912523896500104e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0733, 'grad_norm': 0.7578125, 'learning_rate': 4.901161519036243e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3278, 'grad_norm': 0.62890625, 'learning_rate': 4.889799141572383e-06, 'epoch': 0.88}\n",
      "{'loss': 1.223, 'grad_norm': 0.97265625, 'learning_rate': 4.878436764108522e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1743, 'grad_norm': 0.609375, 'learning_rate': 4.867074386644662e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1814, 'grad_norm': 0.70703125, 'learning_rate': 4.855712009180801e-06, 'epoch': 0.88}\n",
      "{'loss': 1.047, 'grad_norm': 0.81640625, 'learning_rate': 4.844349631716941e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2554, 'grad_norm': 0.6328125, 'learning_rate': 4.832987254253081e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2502, 'grad_norm': 0.74609375, 'learning_rate': 4.821624876789219e-06, 'epoch': 0.88}\n",
      "{'loss': 1.262, 'grad_norm': 0.54296875, 'learning_rate': 4.810262499325359e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2858, 'grad_norm': 0.8828125, 'learning_rate': 4.798900121861498e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9403, 'grad_norm': 1.0859375, 'learning_rate': 4.787537744397638e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4369, 'grad_norm': 0.7109375, 'learning_rate': 4.7761753669337775e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0457, 'grad_norm': 0.9296875, 'learning_rate': 4.764812989469917e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0931, 'grad_norm': 0.578125, 'learning_rate': 4.753450612006057e-06, 'epoch': 0.88}\n",
      "{'loss': 1.4026, 'grad_norm': 0.6171875, 'learning_rate': 4.742088234542196e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1032, 'grad_norm': 1.140625, 'learning_rate': 4.730725857078335e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2805, 'grad_norm': 0.490234375, 'learning_rate': 4.719363479614474e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0201, 'grad_norm': 0.6328125, 'learning_rate': 4.708001102150615e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2175, 'grad_norm': 0.6484375, 'learning_rate': 4.6966387246867535e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1115, 'grad_norm': 0.5625, 'learning_rate': 4.685276347222893e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0268, 'grad_norm': 0.78125, 'learning_rate': 4.673913969759033e-06, 'epoch': 0.88}\n",
      "{'loss': 1.443, 'grad_norm': 0.5703125, 'learning_rate': 4.662551592295172e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0895, 'grad_norm': 0.9765625, 'learning_rate': 4.651189214831312e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2121, 'grad_norm': 0.48828125, 'learning_rate': 4.639826837367451e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1569, 'grad_norm': 0.73828125, 'learning_rate': 4.628464459903591e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0888, 'grad_norm': 1.453125, 'learning_rate': 4.6171020824397295e-06, 'epoch': 0.88}\n",
      "{'loss': 1.249, 'grad_norm': 0.4765625, 'learning_rate': 4.605739704975869e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1919, 'grad_norm': 0.7109375, 'learning_rate': 4.5943773275120086e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1121, 'grad_norm': 0.546875, 'learning_rate': 4.583014950048148e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2112, 'grad_norm': 0.71484375, 'learning_rate': 4.571652572584288e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0996, 'grad_norm': 0.8671875, 'learning_rate': 4.560290195120427e-06, 'epoch': 0.88}\n",
      "{'loss': 1.4976, 'grad_norm': 0.447265625, 'learning_rate': 4.548927817656567e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2297, 'grad_norm': 1.03125, 'learning_rate': 4.537565440192706e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0804, 'grad_norm': 0.5078125, 'learning_rate': 4.526203062728845e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1609, 'grad_norm': 0.6328125, 'learning_rate': 4.514840685264985e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0827, 'grad_norm': 0.79296875, 'learning_rate': 4.503478307801125e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2597, 'grad_norm': 0.447265625, 'learning_rate': 4.492115930337264e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1663, 'grad_norm': 0.8828125, 'learning_rate': 4.480753552873403e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2175, 'grad_norm': 0.4296875, 'learning_rate': 4.469391175409543e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2133, 'grad_norm': 0.70703125, 'learning_rate': 4.458028797945682e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0887, 'grad_norm': 0.86328125, 'learning_rate': 4.446666420481822e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3812, 'grad_norm': 0.59765625, 'learning_rate': 4.435304043017961e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1307, 'grad_norm': 0.703125, 'learning_rate': 4.423941665554101e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0899, 'grad_norm': 0.546875, 'learning_rate': 4.41257928809024e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3062, 'grad_norm': 0.5390625, 'learning_rate': 4.401216910626379e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1001, 'grad_norm': 0.76171875, 'learning_rate': 4.38985453316252e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3365, 'grad_norm': 0.55859375, 'learning_rate': 4.378492155698659e-06, 'epoch': 0.88}\n",
      "{'loss': 1.12, 'grad_norm': 0.90234375, 'learning_rate': 4.367129778234798e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3354, 'grad_norm': 0.5703125, 'learning_rate': 4.355767400770937e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2304, 'grad_norm': 0.484375, 'learning_rate': 4.344405023307077e-06, 'epoch': 0.88}\n",
      "{'loss': 1.078, 'grad_norm': 1.3515625, 'learning_rate': 4.3330426458432165e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2105, 'grad_norm': 0.62890625, 'learning_rate': 4.321680268379356e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0943, 'grad_norm': 0.62890625, 'learning_rate': 4.310317890915496e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0736, 'grad_norm': 0.4296875, 'learning_rate': 4.298955513451635e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1332, 'grad_norm': 0.59765625, 'learning_rate': 4.287593135987774e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0206, 'grad_norm': 0.6640625, 'learning_rate': 4.276230758523913e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2746, 'grad_norm': 0.51171875, 'learning_rate': 4.264868381060053e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1554, 'grad_norm': 0.75390625, 'learning_rate': 4.253506003596193e-06, 'epoch': 0.88}\n",
      "{'loss': 1.165, 'grad_norm': 0.451171875, 'learning_rate': 4.242143626132332e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2683, 'grad_norm': 0.671875, 'learning_rate': 4.230781248668472e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9317, 'grad_norm': 1.1015625, 'learning_rate': 4.219418871204611e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4079, 'grad_norm': 0.6484375, 'learning_rate': 4.20805649374075e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1607, 'grad_norm': 0.78515625, 'learning_rate': 4.19669411627689e-06, 'epoch': 0.88}\n",
      "{'loss': 1.267, 'grad_norm': 0.43359375, 'learning_rate': 4.18533173881303e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0694, 'grad_norm': 0.62890625, 'learning_rate': 4.173969361349169e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1433, 'grad_norm': 1.1171875, 'learning_rate': 4.162606983885308e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3096, 'grad_norm': 0.51953125, 'learning_rate': 4.1512446064214476e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0904, 'grad_norm': 0.734375, 'learning_rate': 4.139882228957587e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2552, 'grad_norm': 0.458984375, 'learning_rate': 4.128519851493727e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0884, 'grad_norm': 0.61328125, 'learning_rate': 4.117157474029866e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9744, 'grad_norm': 1.0234375, 'learning_rate': 4.105795096566006e-06, 'epoch': 0.88}\n",
      "{'loss': 1.4075, 'grad_norm': 0.64453125, 'learning_rate': 4.094432719102145e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0986, 'grad_norm': 0.96484375, 'learning_rate': 4.083070341638284e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2477, 'grad_norm': 0.490234375, 'learning_rate': 4.0717079641744236e-06, 'epoch': 0.88}\n",
      "{'loss': 1.28, 'grad_norm': 1.0, 'learning_rate': 4.060345586710564e-06, 'epoch': 0.88}\n",
      "{'loss': 1.052, 'grad_norm': 0.76171875, 'learning_rate': 4.0489832092467035e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3025, 'grad_norm': 0.55078125, 'learning_rate': 4.037620831782842e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1573, 'grad_norm': 0.796875, 'learning_rate': 4.026258454318982e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1831, 'grad_norm': 0.5546875, 'learning_rate': 4.014896076855121e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1532, 'grad_norm': 0.79296875, 'learning_rate': 4.003533699391261e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9257, 'grad_norm': 1.140625, 'learning_rate': 3.9921713219274e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3046, 'grad_norm': 0.7109375, 'learning_rate': 3.98080894446354e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0929, 'grad_norm': 1.0234375, 'learning_rate': 3.9694465669996795e-06, 'epoch': 0.88}\n",
      "{'loss': 1.257, 'grad_norm': 0.443359375, 'learning_rate': 3.958084189535818e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1457, 'grad_norm': 0.609375, 'learning_rate': 3.946721812071958e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1558, 'grad_norm': 1.2265625, 'learning_rate': 3.935359434608097e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2273, 'grad_norm': 0.45703125, 'learning_rate': 3.923997057144238e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1494, 'grad_norm': 0.7265625, 'learning_rate': 3.912634679680376e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2386, 'grad_norm': 0.515625, 'learning_rate': 3.901272302216516e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3495, 'grad_norm': 0.578125, 'learning_rate': 3.8899099247526555e-06, 'epoch': 0.88}\n",
      "{'loss': 0.995, 'grad_norm': 1.5703125, 'learning_rate': 3.878547547288794e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3309, 'grad_norm': 0.55859375, 'learning_rate': 3.867185169824935e-06, 'epoch': 0.88}\n",
      "{'loss': 1.164, 'grad_norm': 0.84375, 'learning_rate': 3.855822792361074e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0741, 'grad_norm': 0.5390625, 'learning_rate': 3.844460414897214e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1755, 'grad_norm': 0.56640625, 'learning_rate': 3.833098037433352e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0064, 'grad_norm': 1.078125, 'learning_rate': 3.821735659969492e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3458, 'grad_norm': 0.77734375, 'learning_rate': 3.810373282505632e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2162, 'grad_norm': 0.94921875, 'learning_rate': 3.7990109050417715e-06, 'epoch': 0.88}\n",
      "{'loss': 1.374, 'grad_norm': 0.51171875, 'learning_rate': 3.7876485275779106e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2073, 'grad_norm': 0.58984375, 'learning_rate': 3.77628615011405e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0565, 'grad_norm': 0.75390625, 'learning_rate': 3.7649237726501897e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2373, 'grad_norm': 0.59765625, 'learning_rate': 3.753561395186329e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1015, 'grad_norm': 0.66015625, 'learning_rate': 3.7421990177224684e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1016, 'grad_norm': 0.5234375, 'learning_rate': 3.730836640258608e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1323, 'grad_norm': 0.5546875, 'learning_rate': 3.7194742627947475e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9788, 'grad_norm': 1.0625, 'learning_rate': 3.7081118853308866e-06, 'epoch': 0.88}\n",
      "{'loss': 1.395, 'grad_norm': 0.5625, 'learning_rate': 3.696749507867026e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1669, 'grad_norm': 0.9140625, 'learning_rate': 3.685387130403166e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2649, 'grad_norm': 0.6328125, 'learning_rate': 3.674024752939305e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1412, 'grad_norm': 0.58203125, 'learning_rate': 3.6626623754754443e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9976, 'grad_norm': 0.85546875, 'learning_rate': 3.6512999980115843e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3785, 'grad_norm': 0.62109375, 'learning_rate': 3.639937620547724e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2292, 'grad_norm': 0.796875, 'learning_rate': 3.628575243083863e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2197, 'grad_norm': 0.50390625, 'learning_rate': 3.6172128656200025e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2155, 'grad_norm': 0.482421875, 'learning_rate': 3.605850488156142e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0398, 'grad_norm': 0.78515625, 'learning_rate': 3.5944881106922816e-06, 'epoch': 0.88}\n",
      "{'loss': 1.3994, 'grad_norm': 0.58984375, 'learning_rate': 3.5831257332284208e-06, 'epoch': 0.88}\n",
      "{'loss': 1.089, 'grad_norm': 0.78125, 'learning_rate': 3.5717633557645603e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1571, 'grad_norm': 0.5234375, 'learning_rate': 3.5604009783007e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1644, 'grad_norm': 0.609375, 'learning_rate': 3.549038600836839e-06, 'epoch': 0.88}\n",
      "{'loss': 0.9482, 'grad_norm': 1.265625, 'learning_rate': 3.5376762233729785e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1406, 'grad_norm': 0.640625, 'learning_rate': 3.526313845909118e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2288, 'grad_norm': 0.7421875, 'learning_rate': 3.514951468445258e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2005, 'grad_norm': 0.42578125, 'learning_rate': 3.5035890909813968e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1684, 'grad_norm': 0.62890625, 'learning_rate': 3.4922267135175367e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1986, 'grad_norm': 1.09375, 'learning_rate': 3.4808643360536763e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2155, 'grad_norm': 0.60546875, 'learning_rate': 3.469501958589815e-06, 'epoch': 0.88}\n",
      "{'loss': 1.1103, 'grad_norm': 0.42578125, 'learning_rate': 3.458139581125955e-06, 'epoch': 0.88}\n",
      "{'loss': 1.126, 'grad_norm': 0.50390625, 'learning_rate': 3.4467772036620945e-06, 'epoch': 0.88}\n",
      "{'loss': 1.2701, 'grad_norm': 0.88671875, 'learning_rate': 3.435414826198234e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0625, 'grad_norm': 0.6796875, 'learning_rate': 3.424052448734373e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3703, 'grad_norm': 0.62890625, 'learning_rate': 3.4126900712705127e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0577, 'grad_norm': 1.0703125, 'learning_rate': 3.4013276938066523e-06, 'epoch': 0.89}\n",
      "{'loss': 1.142, 'grad_norm': 0.4765625, 'learning_rate': 3.3899653163427922e-06, 'epoch': 0.89}\n",
      "{'loss': 1.04, 'grad_norm': 0.68359375, 'learning_rate': 3.378602938878931e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0168, 'grad_norm': 0.8125, 'learning_rate': 3.3672405614150705e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1567, 'grad_norm': 0.55078125, 'learning_rate': 3.3558781839512105e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0369, 'grad_norm': 0.75390625, 'learning_rate': 3.344515806487349e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2289, 'grad_norm': 0.486328125, 'learning_rate': 3.333153429023489e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2159, 'grad_norm': 0.6796875, 'learning_rate': 3.3217910515596287e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1248, 'grad_norm': 1.09375, 'learning_rate': 3.3104286740957682e-06, 'epoch': 0.89}\n",
      "{'loss': 1.439, 'grad_norm': 0.70703125, 'learning_rate': 3.2990662966319074e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1883, 'grad_norm': 0.6015625, 'learning_rate': 3.287703919168047e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1418, 'grad_norm': 0.57421875, 'learning_rate': 3.2763415417041865e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2918, 'grad_norm': 0.83984375, 'learning_rate': 3.2649791642403256e-06, 'epoch': 0.89}\n",
      "{'loss': 0.9687, 'grad_norm': 0.9296875, 'learning_rate': 3.253616786776465e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3367, 'grad_norm': 0.62890625, 'learning_rate': 3.2422544093126047e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1127, 'grad_norm': 0.74609375, 'learning_rate': 3.2308920318487442e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1662, 'grad_norm': 0.484375, 'learning_rate': 3.2195296543848833e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1602, 'grad_norm': 0.6015625, 'learning_rate': 3.208167276921023e-06, 'epoch': 0.89}\n",
      "{'loss': 0.9881, 'grad_norm': 0.98046875, 'learning_rate': 3.196804899457163e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3618, 'grad_norm': 0.50390625, 'learning_rate': 3.1854425219933024e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1928, 'grad_norm': 1.046875, 'learning_rate': 3.174080144529441e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1883, 'grad_norm': 0.48828125, 'learning_rate': 3.162717767065581e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1553, 'grad_norm': 0.6484375, 'learning_rate': 3.1513553896017206e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1815, 'grad_norm': 0.81640625, 'learning_rate': 3.1399930121378598e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3002, 'grad_norm': 0.57421875, 'learning_rate': 3.1286306346739993e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0926, 'grad_norm': 0.65625, 'learning_rate': 3.117268257210139e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3346, 'grad_norm': 0.609375, 'learning_rate': 3.105905879746278e-06, 'epoch': 0.89}\n",
      "{'loss': 1.226, 'grad_norm': 0.59765625, 'learning_rate': 3.094543502282418e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1985, 'grad_norm': 0.93359375, 'learning_rate': 3.083181124818557e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3353, 'grad_norm': 0.5078125, 'learning_rate': 3.0718187473546966e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1028, 'grad_norm': 0.6640625, 'learning_rate': 3.060456369890836e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1817, 'grad_norm': 0.455078125, 'learning_rate': 3.0490939924269753e-06, 'epoch': 0.89}\n",
      "{'loss': 1.277, 'grad_norm': 0.62890625, 'learning_rate': 3.0377316149631153e-06, 'epoch': 0.89}\n",
      "{'loss': 1.119, 'grad_norm': 0.89453125, 'learning_rate': 3.0263692374992544e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2591, 'grad_norm': 0.490234375, 'learning_rate': 3.015006860035394e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1653, 'grad_norm': 0.90625, 'learning_rate': 3.0036444825715335e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1338, 'grad_norm': 0.4140625, 'learning_rate': 2.992282105107673e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2033, 'grad_norm': 0.640625, 'learning_rate': 2.980919727643812e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0119, 'grad_norm': 0.796875, 'learning_rate': 2.969557350179952e-06, 'epoch': 0.89}\n",
      "{'loss': 1.4024, 'grad_norm': 0.5078125, 'learning_rate': 2.9581949727160913e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1773, 'grad_norm': 1.1640625, 'learning_rate': 2.9468325952522304e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1399, 'grad_norm': 0.546875, 'learning_rate': 2.9354702177883704e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3515, 'grad_norm': 0.7265625, 'learning_rate': 2.9241078403245095e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0573, 'grad_norm': 0.84375, 'learning_rate': 2.912745462860649e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2815, 'grad_norm': 0.59765625, 'learning_rate': 2.9013830853967886e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2032, 'grad_norm': 0.6640625, 'learning_rate': 2.890020707932928e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1878, 'grad_norm': 0.494140625, 'learning_rate': 2.8786583304690673e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1951, 'grad_norm': 0.55859375, 'learning_rate': 2.8672959530052072e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1394, 'grad_norm': 0.7578125, 'learning_rate': 2.8559335755413464e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2164, 'grad_norm': 0.546875, 'learning_rate': 2.844571198077486e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2149, 'grad_norm': 0.7265625, 'learning_rate': 2.8332088206136255e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1875, 'grad_norm': 0.56640625, 'learning_rate': 2.8218464431497646e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2009, 'grad_norm': 0.5625, 'learning_rate': 2.810484065685904e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0727, 'grad_norm': 1.734375, 'learning_rate': 2.7991216882220437e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3244, 'grad_norm': 0.51171875, 'learning_rate': 2.7877593107581832e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1961, 'grad_norm': 0.77734375, 'learning_rate': 2.7763969332943228e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1862, 'grad_norm': 0.474609375, 'learning_rate': 2.7650345558304623e-06, 'epoch': 0.89}\n",
      "{'loss': 1.23, 'grad_norm': 0.6875, 'learning_rate': 2.7536721783666014e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1331, 'grad_norm': 0.73046875, 'learning_rate': 2.742309800902741e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1572, 'grad_norm': 0.6953125, 'learning_rate': 2.7309474234388805e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0888, 'grad_norm': 0.8203125, 'learning_rate': 2.7195850459750197e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1452, 'grad_norm': 0.87890625, 'learning_rate': 2.7082226685111596e-06, 'epoch': 0.89}\n",
      "{'loss': 1.156, 'grad_norm': 0.703125, 'learning_rate': 2.6968602910472988e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0649, 'grad_norm': 1.0390625, 'learning_rate': 2.6854979135834383e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2639, 'grad_norm': 0.470703125, 'learning_rate': 2.674135536119578e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0695, 'grad_norm': 0.81640625, 'learning_rate': 2.6627731586557174e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1312, 'grad_norm': 0.443359375, 'learning_rate': 2.6514107811918565e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2415, 'grad_norm': 0.64453125, 'learning_rate': 2.640048403727996e-06, 'epoch': 0.89}\n",
      "{'loss': 0.9758, 'grad_norm': 0.5625, 'learning_rate': 2.6286860262641356e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2202, 'grad_norm': 0.490234375, 'learning_rate': 2.6173236488002748e-06, 'epoch': 0.89}\n",
      "{'loss': 1.161, 'grad_norm': 0.71875, 'learning_rate': 2.6059612713364147e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0905, 'grad_norm': 0.5625, 'learning_rate': 2.594598893872554e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1351, 'grad_norm': 0.63671875, 'learning_rate': 2.5832365164086934e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0421, 'grad_norm': 1.015625, 'learning_rate': 2.571874138944833e-06, 'epoch': 0.89}\n",
      "{'loss': 1.22, 'grad_norm': 0.5546875, 'learning_rate': 2.5605117614809725e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0995, 'grad_norm': 1.0859375, 'learning_rate': 2.549149384017112e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0821, 'grad_norm': 0.45703125, 'learning_rate': 2.5377870065532516e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2078, 'grad_norm': 0.57421875, 'learning_rate': 2.5264246290893907e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1135, 'grad_norm': 1.140625, 'learning_rate': 2.5150622516255303e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2435, 'grad_norm': 0.609375, 'learning_rate': 2.50369987416167e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1888, 'grad_norm': 0.7890625, 'learning_rate': 2.492337496697809e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1406, 'grad_norm': 0.47265625, 'learning_rate': 2.480975119233949e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2727, 'grad_norm': 0.703125, 'learning_rate': 2.469612741770088e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0229, 'grad_norm': 0.77734375, 'learning_rate': 2.4582503643062276e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2929, 'grad_norm': 0.55078125, 'learning_rate': 2.446887986842367e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2137, 'grad_norm': 0.9140625, 'learning_rate': 2.4355256093785067e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2223, 'grad_norm': 0.4765625, 'learning_rate': 2.424163231914646e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2211, 'grad_norm': 0.53515625, 'learning_rate': 2.4128008544507854e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0924, 'grad_norm': 0.8828125, 'learning_rate': 2.401438476986925e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2535, 'grad_norm': 0.66796875, 'learning_rate': 2.390076099523064e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1379, 'grad_norm': 0.57421875, 'learning_rate': 2.378713722059204e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0845, 'grad_norm': 0.3984375, 'learning_rate': 2.367351344595343e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2628, 'grad_norm': 0.6171875, 'learning_rate': 2.3559889671314827e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1074, 'grad_norm': 0.9609375, 'learning_rate': 2.3446265896676222e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3466, 'grad_norm': 0.51953125, 'learning_rate': 2.3332642122037618e-06, 'epoch': 0.89}\n",
      "{'loss': 1.34, 'grad_norm': 0.9375, 'learning_rate': 2.321901834739901e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1756, 'grad_norm': 0.625, 'learning_rate': 2.3105394572760404e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3066, 'grad_norm': 0.6484375, 'learning_rate': 2.29917707981218e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1126, 'grad_norm': 1.46875, 'learning_rate': 2.2878147023483195e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2284, 'grad_norm': 0.5234375, 'learning_rate': 2.276452324884459e-06, 'epoch': 0.89}\n",
      "{'loss': 1.189, 'grad_norm': 0.66796875, 'learning_rate': 2.2650899474205982e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1722, 'grad_norm': 0.455078125, 'learning_rate': 2.253727569956738e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2577, 'grad_norm': 0.73046875, 'learning_rate': 2.2423651924928773e-06, 'epoch': 0.89}\n",
      "{'loss': 0.9779, 'grad_norm': 0.52734375, 'learning_rate': 2.231002815029017e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4341, 'grad_norm': 0.5546875, 'learning_rate': 2.2196404375651564e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1755, 'grad_norm': 0.70703125, 'learning_rate': 2.2082780601012955e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0504, 'grad_norm': 0.734375, 'learning_rate': 2.196915682637435e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1634, 'grad_norm': 0.6640625, 'learning_rate': 2.1855533051735746e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0925, 'grad_norm': 0.85546875, 'learning_rate': 2.174190927709714e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2862, 'grad_norm': 0.58984375, 'learning_rate': 2.1628285502458533e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1895, 'grad_norm': 0.7890625, 'learning_rate': 2.1514661727819933e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2808, 'grad_norm': 0.45703125, 'learning_rate': 2.1401037953181324e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2486, 'grad_norm': 0.6171875, 'learning_rate': 2.128741417854272e-06, 'epoch': 0.89}\n",
      "{'loss': 0.8894, 'grad_norm': 0.84375, 'learning_rate': 2.1173790403904115e-06, 'epoch': 0.89}\n",
      "{'loss': 1.313, 'grad_norm': 0.5078125, 'learning_rate': 2.1060166629265506e-06, 'epoch': 0.89}\n",
      "{'loss': 0.9951, 'grad_norm': 0.91015625, 'learning_rate': 2.09465428546269e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3304, 'grad_norm': 0.609375, 'learning_rate': 2.0832919079988297e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2697, 'grad_norm': 0.62890625, 'learning_rate': 2.0719295305349693e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1073, 'grad_norm': 0.6015625, 'learning_rate': 2.060567153071109e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2896, 'grad_norm': 0.49609375, 'learning_rate': 2.0492047756072484e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2271, 'grad_norm': 0.408203125, 'learning_rate': 2.0378423981433875e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1823, 'grad_norm': 0.4921875, 'learning_rate': 2.026480020679527e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2191, 'grad_norm': 0.88671875, 'learning_rate': 2.0151176432156666e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0381, 'grad_norm': 0.7734375, 'learning_rate': 2.0037552657518057e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2745, 'grad_norm': 0.53125, 'learning_rate': 1.9923928882879457e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1482, 'grad_norm': 0.64453125, 'learning_rate': 1.981030510824085e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2428, 'grad_norm': 0.52734375, 'learning_rate': 1.9696681333602244e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1697, 'grad_norm': 0.6328125, 'learning_rate': 1.958305755896364e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1475, 'grad_norm': 0.96875, 'learning_rate': 1.9469433784325035e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2988, 'grad_norm': 0.69140625, 'learning_rate': 1.9355810009686426e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0287, 'grad_norm': 0.73046875, 'learning_rate': 1.9242186235047826e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1134, 'grad_norm': 0.4765625, 'learning_rate': 1.9128562460409217e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1751, 'grad_norm': 0.68359375, 'learning_rate': 1.901493868577061e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0971, 'grad_norm': 0.73046875, 'learning_rate': 1.8901314911132006e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1851, 'grad_norm': 0.66796875, 'learning_rate': 1.87876911364934e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1755, 'grad_norm': 0.8671875, 'learning_rate': 1.8674067361854797e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1371, 'grad_norm': 0.6640625, 'learning_rate': 1.856044358721619e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2677, 'grad_norm': 0.6640625, 'learning_rate': 1.8446819812577585e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1184, 'grad_norm': 1.015625, 'learning_rate': 1.8333196037938979e-06, 'epoch': 0.89}\n",
      "{'loss': 1.345, 'grad_norm': 0.546875, 'learning_rate': 1.8219572263300374e-06, 'epoch': 0.89}\n",
      "{'loss': 1.093, 'grad_norm': 0.7109375, 'learning_rate': 1.8105948488661768e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1994, 'grad_norm': 0.40625, 'learning_rate': 1.799232471402316e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2845, 'grad_norm': 0.58203125, 'learning_rate': 1.7878700939384559e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1073, 'grad_norm': 1.09375, 'learning_rate': 1.7765077164745952e-06, 'epoch': 0.89}\n",
      "{'loss': 1.333, 'grad_norm': 0.59765625, 'learning_rate': 1.7651453390107347e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1379, 'grad_norm': 0.85546875, 'learning_rate': 1.753782961546874e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1418, 'grad_norm': 0.64453125, 'learning_rate': 1.7424205840830136e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2589, 'grad_norm': 0.58984375, 'learning_rate': 1.731058206619153e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0652, 'grad_norm': 1.25, 'learning_rate': 1.7196958291552927e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2205, 'grad_norm': 0.703125, 'learning_rate': 1.708333451691432e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0254, 'grad_norm': 0.6953125, 'learning_rate': 1.6969710742275712e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2995, 'grad_norm': 0.5234375, 'learning_rate': 1.685608696763711e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1019, 'grad_norm': 0.6171875, 'learning_rate': 1.6742463192998503e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0488, 'grad_norm': 1.109375, 'learning_rate': 1.6628839418359898e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3566, 'grad_norm': 0.44921875, 'learning_rate': 1.6515215643721292e-06, 'epoch': 0.89}\n",
      "{'loss': 1.206, 'grad_norm': 0.82421875, 'learning_rate': 1.640159186908269e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3162, 'grad_norm': 0.484375, 'learning_rate': 1.6287968094444083e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2483, 'grad_norm': 0.7421875, 'learning_rate': 1.6174344319805478e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0773, 'grad_norm': 0.58203125, 'learning_rate': 1.6060720545166872e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3792, 'grad_norm': 0.734375, 'learning_rate': 1.5947096770528265e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2543, 'grad_norm': 0.7890625, 'learning_rate': 1.583347299588966e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2767, 'grad_norm': 0.51953125, 'learning_rate': 1.5719849221251054e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2167, 'grad_norm': 0.74609375, 'learning_rate': 1.5606225446612451e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1152, 'grad_norm': 1.203125, 'learning_rate': 1.5492601671973845e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2269, 'grad_norm': 0.609375, 'learning_rate': 1.5378977897335238e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1219, 'grad_norm': 1.0625, 'learning_rate': 1.5265354122696634e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2159, 'grad_norm': 0.50390625, 'learning_rate': 1.5151730348058027e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1992, 'grad_norm': 0.58984375, 'learning_rate': 1.5038106573419422e-06, 'epoch': 0.89}\n",
      "{'loss': 0.9294, 'grad_norm': 0.953125, 'learning_rate': 1.4924482798780818e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1737, 'grad_norm': 0.6640625, 'learning_rate': 1.4810859024142213e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1061, 'grad_norm': 0.6015625, 'learning_rate': 1.4697235249503607e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2007, 'grad_norm': 0.474609375, 'learning_rate': 1.4583611474865002e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1586, 'grad_norm': 0.79296875, 'learning_rate': 1.4469987700226398e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0001, 'grad_norm': 0.83984375, 'learning_rate': 1.435636392558779e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1703, 'grad_norm': 0.95703125, 'learning_rate': 1.4242740150949184e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0148, 'grad_norm': 0.78125, 'learning_rate': 1.412911637631058e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1236, 'grad_norm': 0.494140625, 'learning_rate': 1.4015492601671973e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2882, 'grad_norm': 0.6484375, 'learning_rate': 1.3901868827033369e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0893, 'grad_norm': 0.6953125, 'learning_rate': 1.3788245052394764e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.448, 'grad_norm': 0.61328125, 'learning_rate': 1.3674621277756158e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1765, 'grad_norm': 1.140625, 'learning_rate': 1.3560997503117553e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3306, 'grad_norm': 0.5625, 'learning_rate': 1.3447373728478949e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1517, 'grad_norm': 0.51953125, 'learning_rate': 1.3333749953840342e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0768, 'grad_norm': 1.390625, 'learning_rate': 1.3220126179201735e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3641, 'grad_norm': 0.60546875, 'learning_rate': 1.310650240456313e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1696, 'grad_norm': 1.2578125, 'learning_rate': 1.2992878629924526e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1005, 'grad_norm': 0.4921875, 'learning_rate': 1.287925485528592e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1661, 'grad_norm': 0.5078125, 'learning_rate': 1.2765631080647315e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1382, 'grad_norm': 1.2578125, 'learning_rate': 1.265200730600871e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3849, 'grad_norm': 0.458984375, 'learning_rate': 1.2538383531370104e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1572, 'grad_norm': 0.71875, 'learning_rate': 1.24247597567315e-06, 'epoch': 0.89}\n",
      "{'loss': 1.193, 'grad_norm': 0.431640625, 'learning_rate': 1.2311135982092895e-06, 'epoch': 0.89}\n",
      "{'loss': 1.3316, 'grad_norm': 0.953125, 'learning_rate': 1.2197512207454288e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0804, 'grad_norm': 0.55078125, 'learning_rate': 1.2083888432815682e-06, 'epoch': 0.89}\n",
      "{'loss': 1.387, 'grad_norm': 0.53515625, 'learning_rate': 1.1970264658177077e-06, 'epoch': 0.89}\n",
      "{'loss': 1.29, 'grad_norm': 0.484375, 'learning_rate': 1.1856640883538473e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1404, 'grad_norm': 0.455078125, 'learning_rate': 1.1743017108899866e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1791, 'grad_norm': 0.6171875, 'learning_rate': 1.1629393334261262e-06, 'epoch': 0.89}\n",
      "{'loss': 1.1503, 'grad_norm': 0.8046875, 'learning_rate': 1.1515769559622657e-06, 'epoch': 0.89}\n",
      "{'loss': 1.2271, 'grad_norm': 0.57421875, 'learning_rate': 1.140214578498405e-06, 'epoch': 0.9}\n",
      "{'loss': 0.9874, 'grad_norm': 0.76171875, 'learning_rate': 1.1288522010345446e-06, 'epoch': 0.9}\n",
      "{'loss': 1.1614, 'grad_norm': 0.52734375, 'learning_rate': 1.117489823570684e-06, 'epoch': 0.9}\n",
      "{'loss': 1.2766, 'grad_norm': 0.6640625, 'learning_rate': 1.1061274461068235e-06, 'epoch': 0.9}\n",
      "{'loss': 1.1351, 'grad_norm': 1.0859375, 'learning_rate': 1.0947650686429628e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2837, 'grad_norm': 0.67578125, 'learning_rate': 1.0834026911791024e-06, 'epoch': 0.9}\n",
      "{'loss': 1.0815, 'grad_norm': 0.671875, 'learning_rate': 1.072040313715242e-06, 'epoch': 0.9}\n",
      "{'loss': 1.1613, 'grad_norm': 0.400390625, 'learning_rate': 1.0606779362513812e-06, 'epoch': 0.9}\n",
      "{'loss': 1.2489, 'grad_norm': 0.6328125, 'learning_rate': 1.0493155587875208e-06, 'epoch': 0.9}\n",
      "{'loss': 0.9531, 'grad_norm': 0.63671875, 'learning_rate': 1.0379531813236603e-06, 'epoch': 0.9}\n",
      "{'loss': 1.2966, 'grad_norm': 0.734375, 'learning_rate': 1.0265908038597997e-06, 'epoch': 0.9}\n",
      "{'loss': 1.1149, 'grad_norm': 0.63671875, 'learning_rate': 1.015228426395939e-06, 'epoch': 0.9}\n",
      "{'loss': 1.2621, 'grad_norm': 0.451171875, 'learning_rate': 1.0038660489320786e-06, 'epoch': 0.9}\n",
      "{'loss': 1.2336, 'grad_norm': 0.5390625, 'learning_rate': 9.925036714682181e-07, 'epoch': 0.9}\n",
      "{'loss': 1.051, 'grad_norm': 1.1640625, 'learning_rate': 9.811412940043574e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3283, 'grad_norm': 0.4609375, 'learning_rate': 9.69778916540497e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1359, 'grad_norm': 0.75390625, 'learning_rate': 9.584165390766365e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1245, 'grad_norm': 0.4921875, 'learning_rate': 9.47054161612776e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2807, 'grad_norm': 0.640625, 'learning_rate': 9.356917841489154e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0727, 'grad_norm': 0.625, 'learning_rate': 9.243294066850549e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3302, 'grad_norm': 0.75, 'learning_rate': 9.129670292211942e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2072, 'grad_norm': 0.734375, 'learning_rate': 9.016046517573337e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2179, 'grad_norm': 0.5, 'learning_rate': 8.902422742934732e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1422, 'grad_norm': 0.69140625, 'learning_rate': 8.788798968296126e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0421, 'grad_norm': 1.078125, 'learning_rate': 8.675175193657521e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3283, 'grad_norm': 0.609375, 'learning_rate': 8.561551419018916e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1051, 'grad_norm': 0.6796875, 'learning_rate': 8.447927644380311e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0719, 'grad_norm': 0.44921875, 'learning_rate': 8.334303869741705e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2359, 'grad_norm': 0.84765625, 'learning_rate': 8.220680095103101e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0834, 'grad_norm': 0.92578125, 'learning_rate': 8.107056320464495e-07, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3093, 'grad_norm': 0.5, 'learning_rate': 7.993432545825888e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1292, 'grad_norm': 0.68359375, 'learning_rate': 7.879808771187283e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1307, 'grad_norm': 0.46875, 'learning_rate': 7.766184996548678e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2661, 'grad_norm': 0.66796875, 'learning_rate': 7.652561221910073e-07, 'epoch': 0.9}\n",
      "{'loss': 1.061, 'grad_norm': 0.86328125, 'learning_rate': 7.538937447271467e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2208, 'grad_norm': 0.50390625, 'learning_rate': 7.425313672632863e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1065, 'grad_norm': 0.68359375, 'learning_rate': 7.311689897994256e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2842, 'grad_norm': 0.55078125, 'learning_rate': 7.198066123355652e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2045, 'grad_norm': 0.6015625, 'learning_rate': 7.084442348717046e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0571, 'grad_norm': 1.0703125, 'learning_rate': 6.97081857407844e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2026, 'grad_norm': 0.5546875, 'learning_rate': 6.857194799439836e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1764, 'grad_norm': 0.90234375, 'learning_rate': 6.743571024801229e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2314, 'grad_norm': 0.453125, 'learning_rate': 6.629947250162624e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1628, 'grad_norm': 0.8359375, 'learning_rate': 6.516323475524019e-07, 'epoch': 0.9}\n",
      "{'loss': 0.9926, 'grad_norm': 0.72265625, 'learning_rate': 6.402699700885414e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1608, 'grad_norm': 0.7421875, 'learning_rate': 6.289075926246809e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1011, 'grad_norm': 0.7578125, 'learning_rate': 6.175452151608202e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2209, 'grad_norm': 0.51171875, 'learning_rate': 6.061828376969597e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2149, 'grad_norm': 0.69140625, 'learning_rate': 5.948204602330992e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0609, 'grad_norm': 1.453125, 'learning_rate': 5.834580827692387e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3462, 'grad_norm': 0.48828125, 'learning_rate': 5.720957053053781e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2898, 'grad_norm': 0.65625, 'learning_rate': 5.607333278415176e-07, 'epoch': 0.9}\n",
      "{'loss': 1.181, 'grad_norm': 0.4609375, 'learning_rate': 5.49370950377657e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2376, 'grad_norm': 0.83984375, 'learning_rate': 5.380085729137966e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1288, 'grad_norm': 1.1171875, 'learning_rate': 5.26646195449936e-07, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3042, 'grad_norm': 0.50390625, 'learning_rate': 5.152838179860754e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2142, 'grad_norm': 0.72265625, 'learning_rate': 5.039214405222149e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3196, 'grad_norm': 0.51953125, 'learning_rate': 4.925590630583543e-07, 'epoch': 0.9}\n",
      "{'loss': 1.089, 'grad_norm': 0.5078125, 'learning_rate': 4.811966855944939e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0441, 'grad_norm': 1.34375, 'learning_rate': 4.698343081306332e-07, 'epoch': 0.9}\n",
      "{'loss': 1.386, 'grad_norm': 0.515625, 'learning_rate': 4.584719306667727e-07, 'epoch': 0.9}\n",
      "{'loss': 1.179, 'grad_norm': 0.828125, 'learning_rate': 4.471095532029122e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2427, 'grad_norm': 0.58984375, 'learning_rate': 4.357471757390517e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2359, 'grad_norm': 0.57421875, 'learning_rate': 4.2438479827519114e-07, 'epoch': 0.9}\n",
      "{'loss': 1.024, 'grad_norm': 1.34375, 'learning_rate': 4.1302242081133053e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3356, 'grad_norm': 0.64453125, 'learning_rate': 4.0166004334747e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1506, 'grad_norm': 0.73828125, 'learning_rate': 3.902976658836095e-07, 'epoch': 0.9}\n",
      "{'loss': 1.065, 'grad_norm': 0.76171875, 'learning_rate': 3.7893528841974896e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1799, 'grad_norm': 0.6484375, 'learning_rate': 3.675729109558884e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1066, 'grad_norm': 1.046875, 'learning_rate': 3.562105334920279e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3056, 'grad_norm': 0.5234375, 'learning_rate': 3.4484815602816734e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2128, 'grad_norm': 0.87109375, 'learning_rate': 3.3348577856430684e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2855, 'grad_norm': 0.47265625, 'learning_rate': 3.221234011004463e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2747, 'grad_norm': 0.8515625, 'learning_rate': 3.107610236365857e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1234, 'grad_norm': 1.3515625, 'learning_rate': 2.9939864617272516e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2535, 'grad_norm': 0.5625, 'learning_rate': 2.8803626870886466e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0977, 'grad_norm': 0.84375, 'learning_rate': 2.7667389124500416e-07, 'epoch': 0.9}\n",
      "{'loss': 1.15, 'grad_norm': 0.494140625, 'learning_rate': 2.653115137811436e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1975, 'grad_norm': 0.69921875, 'learning_rate': 2.5394913631728304e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0504, 'grad_norm': 0.9375, 'learning_rate': 2.425867588534225e-07, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2982, 'grad_norm': 0.65625, 'learning_rate': 2.3122438138956198e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1463, 'grad_norm': 0.69921875, 'learning_rate': 2.198620039257014e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1995, 'grad_norm': 0.490234375, 'learning_rate': 2.084996264618409e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0831, 'grad_norm': 0.59765625, 'learning_rate': 1.9713724899798036e-07, 'epoch': 0.9}\n",
      "{'loss': 1.0388, 'grad_norm': 0.91796875, 'learning_rate': 1.857748715341198e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3806, 'grad_norm': 0.46875, 'learning_rate': 1.7441249407025927e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1799, 'grad_norm': 0.6640625, 'learning_rate': 1.6305011660639871e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1103, 'grad_norm': 0.60546875, 'learning_rate': 1.516877391425382e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1816, 'grad_norm': 0.7265625, 'learning_rate': 1.4032536167867765e-07, 'epoch': 0.9}\n",
      "{'loss': 0.9829, 'grad_norm': 1.0625, 'learning_rate': 1.2896298421481712e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1696, 'grad_norm': 0.625, 'learning_rate': 1.1760060675095658e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1794, 'grad_norm': 0.78515625, 'learning_rate': 1.0623822928709603e-07, 'epoch': 0.9}\n",
      "{'loss': 1.1413, 'grad_norm': 0.40625, 'learning_rate': 9.48758518232355e-08, 'epoch': 0.9}\n",
      "{'loss': 1.2059, 'grad_norm': 0.6328125, 'learning_rate': 8.351347435937496e-08, 'epoch': 0.9}\n",
      "{'loss': 1.0406, 'grad_norm': 0.7109375, 'learning_rate': 7.215109689551443e-08, 'epoch': 0.9}\n",
      "{'loss': 1.2003, 'grad_norm': 0.5, 'learning_rate': 6.078871943165388e-08, 'epoch': 0.9}\n",
      "{'loss': 1.1715, 'grad_norm': 0.71484375, 'learning_rate': 4.942634196779335e-08, 'epoch': 0.9}\n",
      "{'loss': 1.1281, 'grad_norm': 0.486328125, 'learning_rate': 3.8063964503932807e-08, 'epoch': 0.9}\n",
      "{'loss': 1.3038, 'grad_norm': 0.6875, 'learning_rate': 2.6701587040072266e-08, 'epoch': 0.9}\n",
      "{'loss': 0.9898, 'grad_norm': 0.87109375, 'learning_rate': 1.5339209576211725e-08, 'epoch': 0.9}\n",
      "{'loss': 1.4222, 'grad_norm': 0.5390625, 'learning_rate': 3.976832112351189e-09, 'epoch': 0.9}\n",
      "{'train_runtime': 34616.5275, 'train_samples_per_second': 20.968, 'train_steps_per_second': 10.484, 'train_loss': 0.1352296082772137, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=362927, training_loss=0.1352296082772137, metrics={'train_runtime': 34616.5275, 'train_samples_per_second': 20.968, 'train_steps_per_second': 10.484, 'train_loss': 0.1352296082772137, 'epoch': 0.9})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "lora_model_save_dir = \"./results/gugugo-experi_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(lora_model_save_dir, save_embedding_layers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(lora_model_save_dir)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aeolian83/llama_ko_sft_gugugo_experi_01/commit/19dd71bb9c3aebf4c5be4ad2c4a15d34a7a999d6', commit_message='Upload tokenizer', commit_description='', oid='19dd71bb9c3aebf4c5be4ad2c4a15d34a7a999d6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('aeolian83/llama_ko_sft_gugugo_experi_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
