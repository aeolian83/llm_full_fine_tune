{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"Machin Translator_01\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "with open('./data/train_data_300.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "len(train_data)\n",
    "\n",
    "with open('./data/validation_data_28.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "len(test_data)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# DatasetDict로 \"train\"과 \"test\" 데이터셋 묶기\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Checkpoints Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../checkpoint/translate_machine_llama3ko_intsuct_origindata300_02\"\n",
    "\n",
    "# 디렉터리 이름에서 숫자를 추출하는 함수\n",
    "def extract_number(directory_name):\n",
    "    return int(directory_name.split('-')[-1])\n",
    "\n",
    "\n",
    "# 디렉터리 항목을 숫자 순서대로 정렬\n",
    "checkpoints = []\n",
    "with os.scandir(checkpoint_dir) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_dir():\n",
    "            checkpoints.append(entry.name)\n",
    "\n",
    "# 숫자 순서대로 정렬\n",
    "checkpoints.sort(key=extract_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "model_id = \"beomi/Llama-3-KoEn-8B-Instruct-preview\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\"\n",
    "\n",
    "def make_prompt(text):\n",
    "    return f'''Translate input sentence to Korean\n",
    "### Input: {text}\n",
    "### Translated:'''\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\(.*?\\)', '', text)\n",
    "\n",
    "\n",
    "def counter_terms(terms, text):\n",
    "    if not isinstance(terms, list):\n",
    "        terms = terms.split(\", \")\n",
    "\n",
    "    return sum(text.lower().count(term.lower()) for term in terms)\n",
    "\n",
    "\n",
    "def generate(inputs, tokenizer, model):\n",
    "    examples = []\n",
    "\n",
    "    for input in inputs:\n",
    "        prompt = make_prompt(input)\n",
    "        examples.append(prompt)\n",
    "\n",
    "    example_batch = tokenizer(examples, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**example_batch, max_new_tokens = 512, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "    outputs = [tokenizer.decode(t[len(tokenizer.encode(examples[i])):], skip_special_tokens=True) for i, t in enumerate(output_tokens)]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_score(datasets, outputs):\n",
    "    if not len(datasets[\"korean\"]) == len(outputs):\n",
    "        pass\n",
    "\n",
    "    inputs = datasets[\"english\"]\n",
    "    labels = datasets[\"korean\"]\n",
    "    terms = datasets[\"terms\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for input, output, label, term in zip(inputs, outputs, labels, terms):\n",
    "        # print(input)\n",
    "        # print(output)\n",
    "        # print(label)\n",
    "        # print(term)\n",
    "\n",
    "        label_clean = clean_text(label)\n",
    "        prediction_clean = clean_text(output)\n",
    "        result = metric.compute(predictions=[prediction_clean], references=[label_clean])\n",
    "\n",
    "        input_count = counter_terms(term, input)\n",
    "        predic_count = counter_terms(term, output)\n",
    "\n",
    "        weight = 1.0 if predic_count > input_count else predic_count / input_count\n",
    "\n",
    "        results.append((result['score'], weight, weight * result['score']))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map='auto', cache_dir=cache_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "bleu_scores = {}\n",
    "\n",
    "for checkpoint in checkpoints[10:20]:\n",
    "    print('#' * 50)\n",
    "    print(checkpoint)\n",
    "    print('#' * 50)\n",
    "\n",
    "    peft_model_id = os.path.join(checkpoint_dir, checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    loaded_model = PeftModel.from_pretrained(model=model, model_id=peft_model_id)\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(dataset_dict[\"test\"]), 6):\n",
    "        dataset = dataset_dict[\"test\"][i:i+6]\n",
    "\n",
    "        outputs = generate(dataset[\"english\"], tokenizer, loaded_model)\n",
    "\n",
    "        results += evaluate_score(dataset, outputs)\n",
    "\n",
    "    bleus, weights, weighted_bleus = zip(*results)\n",
    "    score = {\n",
    "        \"bleu\": np.mean(bleus),\n",
    "        \"weight\": np.mean(weights),\n",
    "        \"weighted_bleus\": np.mean(weighted_bleus)\n",
    "        }\n",
    "    bleu_scores[checkpoint] = score\n",
    "\n",
    "    del peft_model_id, tokenizer, loaded_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "df = pd.DataFrame.from_dict(bleu_scores, orient='index')\n",
    "\n",
    "# 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution Time: {end_time - start_time} seconds\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
