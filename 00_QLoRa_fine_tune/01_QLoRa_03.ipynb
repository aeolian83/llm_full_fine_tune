{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Environment & Huggingface, W&B login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"QLoRA_Instruction_finetune_03\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Load\n",
    "\n",
    "- 이 시스템에서는 작업을 외장하드에서 하고 있기 때문에 캐쉬폴더를 별도로 지정(In this system, tasks are being performed on an external hard drive, so a separate cache folder is specified.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = load_dataset(\"nlpai-lab/kullm-v2\", cache_dir=\"/mnt/t7/.cache/huggingface/datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.shuffle(seed=2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'instruction', 'input', 'output'],\n",
       "    num_rows: 152630\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.train_test_split(test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'instruction', 'input', 'output'],\n",
       "        num_rows: 15263\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'instruction', 'input', 'output'],\n",
       "        num_rows: 137367\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ko_instruction_01 = ko_instruction_01['train'].train_test_split(test_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Load\n",
    "\n",
    "- QLoRA로 finetune을 하기위해 일반 7B 모델을 BitsAndBytes를 통해 양자화(4bit quantization)하여 로드, 훈련속도 향상을 위해 데이터 타입을 fp32가 아니라 fp16으로 로드(차후에는 bf16으로 로드해서 훈련예정)\n",
    "- To finetune with QLoRA, a standard 7B model is quantized (4-bit quantization) through BitsAndBytes for loading, and to improve training speed, the data type is loaded as fp16 instead of fp32 (with plans to load and train with bf16 in the future).\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    ),\n",
    "```\n",
    "\n",
    "### *Reference\n",
    "\n",
    "- https://github.com/huggingface/peft/blob/main/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py\n",
    "- https://colab.research.google.com/drive/19AFEOrCI6-bc7h9RTso_NndRwXJRaJ25?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/experi_03\"\n",
    "model_id = \"beomi/llama-2-ko-7b\"\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit QLoRA 학습을 위한 설정\n",
    "bnb_4bit_compute_dtype = \"bfloat16\" # 코랩 무료버전에서 실행 시 \"float16\"를 사용하세요\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_4bit = True\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c2d0773be648bd9d67175efc1a65ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=device_map, cache_dir=\"/mnt/t7/.cache/huggingface/models\")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) PEFT훈련을 위한 CONFIG설정, 이 부분은 Trainable Parameter를 확인하지 않아도 QLoRA를 위해서는 무조건 해야 한다.(\"For PEFT training configuration, this step must be done unconditionally for QLoRA, even without checking for Trainable Parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"/mnt/t7/.cache/huggingface/models\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # 학습 진행 중 loss가 치솟다가 0.0으로 떨어지는 문제 해결을 위해 사용\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Prompt 설정(Prompt configuration)\n",
    "\n",
    "- Prompt는 아래와 같음(The prompt is as follows.)\n",
    "> \\#\\#\\# System: content  \n",
    "> \\#\\#\\# Human: content  \n",
    "> \\#\\#\\# Assistant: content  \n",
    "\n",
    "- Prompt에 맞춰서 Dataset을 재가공(Reprocessing the Dataset according to the Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    system_prompt = f\"### instruction: {sample['instruction']}\"\n",
    "    input = f\"### input: {sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    output = f\"### output: {sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [system_prompt, input, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_instruction(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407a56f49624465eb3b6f3695849ea67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/15263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ko_instruction_01['train'].map(template_dataset, remove_columns=list(ko_instruction_01['train'].features), num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 15263\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 주어진 데이터 세트에 대해 적절한 데이터 정리 기술을 수행합니다.\\n\\n### input: \"국가\", \"도시\", \"인구\" 열이 있는 데이터 집합\\n\\n### output: \\'국가\\', \\'도시\\', \\'인구\\' 열이 있는 주어진 데이터 집합에 대해 적절한 데이터 정리 기법을 수행하려면 다음 단계를 수행할 수 있습니다:\\n\\n1. **누락된 데이터**: 세 열 모두에서 누락된 데이터가 있는지 확인합니다. 빈 셀이 있는 경우 전체 행을 삭제하거나 데이터 집합에 대한 이해를 바탕으로 누락된 데이터를 적절한 값으로 채울 수 있습니다.\\n\\n2. **데이터 유형**: 각 열의 데이터 유형이 적절한지 확인합니다. 예를 들어, \\'국가\\' 및 \\'도시\\' 열의 데이터 유형은 문자열 또는 텍스트여야 하고 \\'인구\\'는 정수 또는 실수여야 합니다.\\n\\n3. **중복 데이터**를 클릭합니다: 데이터 집합에 중복되는 행이 있는지 확인하고 제거합니다.\\n\\n4. **일관성**: 열의 데이터가 일관성이 있는지 확인합니다. 예를 들어, \\'국가\\' 열에 전체 국가명이 포함된 경우 모든 행에서 국가명이 동일해야 합니다. \\'도시\\' 열도 마찬가지입니다.\\n\\n5. **이상값**: 분석에 오류를 일으킬 수 있는 \\'인구\\' 열에 이상값이 있는지 확인합니다. 통계적 방법이나 시각화 기법을 사용하여 이상값을 식별하고 제거해야 하는지 여부를 결정할 수 있습니다.\\n\\n6. **표준화**: 추가 분석 또는 모델링에 데이터를 사용해야 하는 경우, 요구사항에 따라 \\'모집단\\' 열의 값을 표준화하거나 정규화해야 할 수 있습니다.\\n\\n위의 단계를 수행한 후에는 추가 분석에 사용할 수 있는 깨끗한 데이터 세트가 준비되어 있어야 합니다.</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 전문 사진 작가처럼 행동하십시오. Amazon FBA 창고의 이미지를 캡처하는 프롬프트를 만들어야 합니다. 카메라 설정을 포함하세요. 영어로 작성하세요.\\n\\n### output: rompt: 아마존 FBA(아마존 주문 처리) 창고의 방대한 규모와 정돈된 혼란스러움을 보여주는 인상적인 이미지를 캡처하세요. 사진에는 엄청난 수의 제품, 복잡한 물류 시스템, 운영의 효율성이 강조되어야 합니다.카메라 설정:1. 카메라: Canon EOS 5D Mark IV 또는 이와 유사한 풀프레임 DSLR2. 렌즈: 광활한 창고를 담을 수 있는 Canon EF 16-35mm f/2.8L III USM 또는 동급의 광각 줌 렌즈3. ISO: 400-800(창고 내 사용 가능한 조명에 따라 다름)4. 셔터 속도: 1/100 - 1/200초(움직이는 작업자와 장비의 모션 블러를 방지하기 위해)5. 조리개: f/5.6 - f/8, 피사계 심도를 깊게 하여 창고 대부분에 초점 맞추기6. 삼각대: 필요한 경우 장노출 촬영 시 카메라를 안정화할 수 있는 견고한 삼각대7. 원격 셔터 릴리즈: 촬영 중 카메라 흔들림을 제거하기 위해컨베이어 벨트나 보관 선반과 같은 주요 선으로 샷을 구성하여 시청자의 시선을 이미지로 유도합니다. 움직이는 작업자, 제품을 운반하는 지게차, 작업 중인 로봇 시스템을 포함하여 역동적인 환경을 포착합니다. 사용 가능한 자연광이나 오버헤드 인공 조명을 활용하여 장면을 밝히고, 조명 조건이 좋지 않은 경우 HDR 이미지를 만들기 위해 브라케팅 노출을 사용하는 것도 고려하세요.최종 사진은 아마존 FBA 창고의 인상적인 규모와 복잡성을 전달하는 동시에 효율적이고 능률적인 운영 특성을 보여줄 수 있어야 합니다.</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 예, 계속\\n\\n### output: 각 개념에 대한 몇 가지 추가 연습 문제가 있습니다:수식 구문 이해를 위한 연습:* 다음 수식에서 구문 요소를 식별하세요:text(month(today()) & \"/\" & text(day(today()) & \"/\" & text(year(today()))간단한 수식 만들기 연습:* 연락처 개체에서 생년월일을 기준으로 연락처의 나이를 계산하는 수식 필드를 만듭니다.복잡한 수식 작성 연습:* 기회 단계가 \"마감 원\"으로 설정되어 있지 않은 경우 마감 날짜가 기회 생성 날짜로부터 최소 7일이 지나야 한다는 유효성 검사 규칙을 기회 개체에 만듭니다.공식 디버깅 연습:* 다음 수식에서 오류를 식별하고 수정된 버전을 제공하세요:IF(스테이지 = \"유망\" AND 금액 > 10000, \"고가\", \"저가\")수식 필드 사용 연습:* 리드 개체에서 여러 사용자 지정 필드의 값을 기반으로 리드 점수를 계산하는 수식 필드를 만듭니다.유효성 검사 규칙 사용 연습:* 연락처 개체에서 이메일 또는 전화 필드를 입력해야 하는 유효성 검사 규칙을 만듭니다.보고서 수식 사용 연습:* 계정 보고서에서 각 계정과 연결된 총 연락처 수를 계산하는 보고서 수식을 만듭니다.고급 수식 기법을 위한 연습:* 케이스 객체에서 케이스 생성 날짜와 가장 최근의 케이스 댓글 생성 날짜 사이의 일 수를 표시하는 수식 필드를 만듭니다.수식 작성 모범 사례 연습:* 기존 수식 필드 또는 유효성 검사 규칙을 검토하고 수식을 단순화하거나 최적화할 수 있는 기회를 파악합니다.</s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][3230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 초\\\\_total` 및 `밀리초`를 계산할 때 `uint32\\\\_t`로 명시적으로 캐스팅해야 하나요? 아니면 그냥 좋은 스타일인가요?\\n\\n### output: 이 경우 `uint32_t`로 명시적 형변환이 반드시 필요한 것은 아닙니다. uint64_t`인 `timestamp` 변수에 대한 나누기 및 모듈로 연산은 할당되는 32비트 변수에 맞게 자동으로 잘립니다.그러나 명시적 형변환을 사용하면 프로그래머의 의도를 명확히 하고 값이 의도적으로 32비트 값으로 잘린다는 것을 명확히 할 수 있으므로 좋은 스타일로 간주할 수 있습니다. 오버플로 또는 예기치 않은 동작이 발생할 가능성이 있는 다른 경우에는 잠재적인 문제를 피하기 위해 명시적 형변환이 중요할 수 있습니다.요약하면, 명시적 형변환이 반드시 필요한 것은 아니지만 코드를 더 읽기 쉽게 만들고 프로그래머의 의도를 전달하는 데 도움이 될 수 있습니다.</s>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][12312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = ko_instruction_01['test'].map(template_dataset, remove_columns=list(ko_instruction_01['test'].features), num_proc=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configure Train arguments & Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33df8a4242f84a43bbabcd1dd62405cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=checkpoint_dir,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3, # 가장 최근 체크포인트 3개만 저장합니다.\n",
    "    logging_steps=30,\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=3e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=2, # epochs 대신 max_steps을 기준으로 할 수 있습니다.\n",
    "    warmup_ratio=0.02,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\", # paged_adamw_8bit 사용시 메모리를 더 절약할 수 있지만 loss가 0으로 떨어지는 문제가 있습니다.\n",
    "    group_by_length=True,\n",
    "    fp16 = False, # 코랩 무료버전에서 실행 시 \"True\"를 사용하세요\n",
    "    bf16 = True, # 코랩 무료버전에서 실행 시 \"False\"를 사용하세요\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=1024,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_arguments,\n",
    "#     packing=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/00_QLoRa_fine_tune/wandb/run-20240425_135035-974at97u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_03/runs/974at97u' target=\"_blank\">mild-sponge-8</a></strong> to <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_03' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_03' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_03/runs/974at97u' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_03/runs/974at97u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9e34cf40ce46da91f52c12b2bdc8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2808, 'grad_norm': 0.400390625, 'learning_rate': 0.0003, 'epoch': 0.0}\n",
      "{'loss': 1.8199, 'grad_norm': 0.80078125, 'learning_rate': 0.0003, 'epoch': 0.0}\n",
      "{'loss': 1.9828, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 0.01}\n",
      "{'loss': 1.8847, 'grad_norm': 0.29296875, 'learning_rate': 0.0003, 'epoch': 0.01}\n",
      "{'loss': 2.031, 'grad_norm': 1.25, 'learning_rate': 0.0003, 'epoch': 0.01}\n",
      "{'loss': 1.9026, 'grad_norm': 0.392578125, 'learning_rate': 0.0003, 'epoch': 0.01}\n",
      "{'loss': 2.0661, 'grad_norm': 0.26171875, 'learning_rate': 0.0003, 'epoch': 0.01}\n",
      "{'loss': 1.8414, 'grad_norm': 0.7109375, 'learning_rate': 0.0003, 'epoch': 0.02}\n",
      "{'loss': 1.8244, 'grad_norm': 0.41796875, 'learning_rate': 0.0003, 'epoch': 0.02}\n",
      "{'loss': 1.8273, 'grad_norm': 2.15625, 'learning_rate': 0.0003, 'epoch': 0.02}\n",
      "{'loss': 1.9557, 'grad_norm': 0.365234375, 'learning_rate': 0.0003, 'epoch': 0.02}\n",
      "{'loss': 1.8285, 'grad_norm': 0.22265625, 'learning_rate': 0.0003, 'epoch': 0.02}\n",
      "{'loss': 1.7757, 'grad_norm': 0.5625, 'learning_rate': 0.0003, 'epoch': 0.03}\n",
      "{'loss': 1.783, 'grad_norm': 0.345703125, 'learning_rate': 0.0003, 'epoch': 0.03}\n",
      "{'loss': 1.8906, 'grad_norm': 1.171875, 'learning_rate': 0.0003, 'epoch': 0.03}\n",
      "{'loss': 1.8707, 'grad_norm': 0.29296875, 'learning_rate': 0.0003, 'epoch': 0.03}\n",
      "{'loss': 1.8268, 'grad_norm': 0.2099609375, 'learning_rate': 0.0003, 'epoch': 0.03}\n",
      "{'loss': 1.7992, 'grad_norm': 0.458984375, 'learning_rate': 0.0003, 'epoch': 0.04}\n",
      "{'loss': 1.7668, 'grad_norm': 0.236328125, 'learning_rate': 0.0003, 'epoch': 0.04}\n",
      "{'loss': 1.9003, 'grad_norm': 1.484375, 'learning_rate': 0.0003, 'epoch': 0.04}\n",
      "{'loss': 1.8268, 'grad_norm': 0.2490234375, 'learning_rate': 0.0003, 'epoch': 0.04}\n",
      "{'loss': 1.7953, 'grad_norm': 0.251953125, 'learning_rate': 0.0003, 'epoch': 0.04}\n",
      "{'loss': 1.8839, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 0.05}\n",
      "{'loss': 1.8641, 'grad_norm': 0.328125, 'learning_rate': 0.0003, 'epoch': 0.05}\n",
      "{'loss': 1.7137, 'grad_norm': 1.984375, 'learning_rate': 0.0003, 'epoch': 0.05}\n",
      "{'loss': 1.9354, 'grad_norm': 0.359375, 'learning_rate': 0.0003, 'epoch': 0.05}\n",
      "{'loss': 1.8359, 'grad_norm': 0.234375, 'learning_rate': 0.0003, 'epoch': 0.05}\n",
      "{'loss': 1.7583, 'grad_norm': 0.5703125, 'learning_rate': 0.0003, 'epoch': 0.06}\n",
      "{'loss': 1.7871, 'grad_norm': 0.291015625, 'learning_rate': 0.0003, 'epoch': 0.06}\n",
      "{'loss': 1.8671, 'grad_norm': 1.15625, 'learning_rate': 0.0003, 'epoch': 0.06}\n",
      "{'loss': 1.8401, 'grad_norm': 0.353515625, 'learning_rate': 0.0003, 'epoch': 0.06}\n",
      "{'loss': 1.8072, 'grad_norm': 0.216796875, 'learning_rate': 0.0003, 'epoch': 0.06}\n",
      "{'loss': 1.7817, 'grad_norm': 0.455078125, 'learning_rate': 0.0003, 'epoch': 0.06}\n",
      "{'loss': 1.867, 'grad_norm': 0.25390625, 'learning_rate': 0.0003, 'epoch': 0.07}\n",
      "{'loss': 1.9153, 'grad_norm': 0.97265625, 'learning_rate': 0.0003, 'epoch': 0.07}\n",
      "{'loss': 1.6909, 'grad_norm': 0.30078125, 'learning_rate': 0.0003, 'epoch': 0.07}\n",
      "{'loss': 1.7626, 'grad_norm': 0.380859375, 'learning_rate': 0.0003, 'epoch': 0.07}\n",
      "{'loss': 1.6956, 'grad_norm': 0.578125, 'learning_rate': 0.0003, 'epoch': 0.07}\n",
      "{'loss': 1.7053, 'grad_norm': 0.4296875, 'learning_rate': 0.0003, 'epoch': 0.08}\n",
      "{'loss': 1.9053, 'grad_norm': 1.34375, 'learning_rate': 0.0003, 'epoch': 0.08}\n",
      "{'loss': 1.7535, 'grad_norm': 0.5625, 'learning_rate': 0.0003, 'epoch': 0.08}\n",
      "{'loss': 1.5615, 'grad_norm': 0.29296875, 'learning_rate': 0.0003, 'epoch': 0.08}\n",
      "{'loss': 1.9643, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 0.08}\n",
      "{'loss': 1.7272, 'grad_norm': 0.330078125, 'learning_rate': 0.0003, 'epoch': 0.09}\n",
      "{'loss': 1.8112, 'grad_norm': 1.2265625, 'learning_rate': 0.0003, 'epoch': 0.09}\n",
      "{'loss': 1.84, 'grad_norm': 0.2890625, 'learning_rate': 0.0003, 'epoch': 0.09}\n",
      "{'loss': 1.8578, 'grad_norm': 0.2255859375, 'learning_rate': 0.0003, 'epoch': 0.09}\n",
      "{'loss': 1.7166, 'grad_norm': 0.66015625, 'learning_rate': 0.0003, 'epoch': 0.09}\n",
      "{'loss': 1.8088, 'grad_norm': 0.318359375, 'learning_rate': 0.0003, 'epoch': 0.1}\n",
      "{'loss': 1.7312, 'grad_norm': 1.09375, 'learning_rate': 0.0003, 'epoch': 0.1}\n",
      "{'loss': 1.8133, 'grad_norm': 0.359375, 'learning_rate': 0.0003, 'epoch': 0.1}\n",
      "{'loss': 1.6546, 'grad_norm': 0.30078125, 'learning_rate': 0.0003, 'epoch': 0.1}\n",
      "{'loss': 1.8252, 'grad_norm': 0.67578125, 'learning_rate': 0.0003, 'epoch': 0.1}\n",
      "{'loss': 1.6209, 'grad_norm': 0.31640625, 'learning_rate': 0.0003, 'epoch': 0.11}\n",
      "{'loss': 1.7971, 'grad_norm': 1.234375, 'learning_rate': 0.0003, 'epoch': 0.11}\n",
      "{'loss': 1.7491, 'grad_norm': 0.60546875, 'learning_rate': 0.0003, 'epoch': 0.11}\n",
      "{'loss': 1.8063, 'grad_norm': 0.33203125, 'learning_rate': 0.0003, 'epoch': 0.11}\n",
      "{'loss': 1.7838, 'grad_norm': 0.8515625, 'learning_rate': 0.0003, 'epoch': 0.11}\n",
      "{'loss': 1.6571, 'grad_norm': 0.462890625, 'learning_rate': 0.0003, 'epoch': 0.12}\n",
      "{'loss': 1.8678, 'grad_norm': 1.640625, 'learning_rate': 0.0003, 'epoch': 0.12}\n",
      "{'loss': 1.805, 'grad_norm': 0.32421875, 'learning_rate': 0.0003, 'epoch': 0.12}\n",
      "{'loss': 1.9131, 'grad_norm': 0.310546875, 'learning_rate': 0.0003, 'epoch': 0.12}\n",
      "{'loss': 1.792, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 0.12}\n",
      "{'loss': 1.7887, 'grad_norm': 0.33984375, 'learning_rate': 0.0003, 'epoch': 0.13}\n",
      "{'loss': 1.9199, 'grad_norm': 1.5859375, 'learning_rate': 0.0003, 'epoch': 0.13}\n",
      "{'loss': 1.8076, 'grad_norm': 0.44921875, 'learning_rate': 0.0003, 'epoch': 0.13}\n",
      "{'loss': 1.9245, 'grad_norm': 0.240234375, 'learning_rate': 0.0003, 'epoch': 0.13}\n",
      "{'loss': 1.8583, 'grad_norm': 0.49609375, 'learning_rate': 0.0003, 'epoch': 0.13}\n",
      "{'loss': 2.0197, 'grad_norm': 0.265625, 'learning_rate': 0.0003, 'epoch': 0.14}\n",
      "{'loss': 1.8949, 'grad_norm': 1.7890625, 'learning_rate': 0.0003, 'epoch': 0.14}\n",
      "{'loss': 1.7203, 'grad_norm': 0.5859375, 'learning_rate': 0.0003, 'epoch': 0.14}\n",
      "{'loss': 1.7917, 'grad_norm': 0.3203125, 'learning_rate': 0.0003, 'epoch': 0.14}\n",
      "{'loss': 1.8805, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 0.14}\n",
      "{'loss': 1.7711, 'grad_norm': 0.4609375, 'learning_rate': 0.0003, 'epoch': 0.15}\n",
      "{'loss': 1.7712, 'grad_norm': 1.75, 'learning_rate': 0.0003, 'epoch': 0.15}\n",
      "{'loss': 1.7644, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 0.15}\n",
      "{'loss': 1.6651, 'grad_norm': 0.224609375, 'learning_rate': 0.0003, 'epoch': 0.15}\n",
      "{'loss': 1.8319, 'grad_norm': 0.828125, 'learning_rate': 0.0003, 'epoch': 0.15}\n",
      "{'loss': 1.6364, 'grad_norm': 0.328125, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
      "{'loss': 1.869, 'grad_norm': 1.203125, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
      "{'loss': 1.7493, 'grad_norm': 0.38671875, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
      "{'loss': 1.7922, 'grad_norm': 0.3984375, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
      "{'loss': 1.8086, 'grad_norm': 0.353515625, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
      "{'loss': 1.6945, 'grad_norm': 0.2470703125, 'learning_rate': 0.0003, 'epoch': 0.17}\n",
      "{'loss': 1.8299, 'grad_norm': 1.0, 'learning_rate': 0.0003, 'epoch': 0.17}\n",
      "{'loss': 1.8605, 'grad_norm': 0.431640625, 'learning_rate': 0.0003, 'epoch': 0.17}\n",
      "{'loss': 1.7217, 'grad_norm': 0.3828125, 'learning_rate': 0.0003, 'epoch': 0.17}\n",
      "{'loss': 1.6985, 'grad_norm': 1.3359375, 'learning_rate': 0.0003, 'epoch': 0.17}\n",
      "{'loss': 1.8638, 'grad_norm': 0.306640625, 'learning_rate': 0.0003, 'epoch': 0.17}\n",
      "{'loss': 1.9249, 'grad_norm': 1.6875, 'learning_rate': 0.0003, 'epoch': 0.18}\n",
      "{'loss': 1.6373, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 0.18}\n",
      "{'loss': 1.7922, 'grad_norm': 0.265625, 'learning_rate': 0.0003, 'epoch': 0.18}\n",
      "{'loss': 1.8985, 'grad_norm': 0.439453125, 'learning_rate': 0.0003, 'epoch': 0.18}\n",
      "{'loss': 1.6223, 'grad_norm': 0.322265625, 'learning_rate': 0.0003, 'epoch': 0.18}\n",
      "{'loss': 1.7858, 'grad_norm': 1.359375, 'learning_rate': 0.0003, 'epoch': 0.19}\n",
      "{'loss': 1.5932, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 0.19}\n",
      "{'loss': 1.6548, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 0.19}\n",
      "{'loss': 1.6811, 'grad_norm': 0.81640625, 'learning_rate': 0.0003, 'epoch': 0.19}\n",
      "{'loss': 1.7695, 'grad_norm': 0.40625, 'learning_rate': 0.0003, 'epoch': 0.19}\n",
      "{'loss': 1.916, 'grad_norm': 1.4296875, 'learning_rate': 0.0003, 'epoch': 0.2}\n",
      "{'loss': 1.6891, 'grad_norm': 0.36328125, 'learning_rate': 0.0003, 'epoch': 0.2}\n",
      "{'loss': 1.7359, 'grad_norm': 0.21484375, 'learning_rate': 0.0003, 'epoch': 0.2}\n",
      "{'loss': 1.7286, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 0.2}\n",
      "{'loss': 1.7402, 'grad_norm': 0.3359375, 'learning_rate': 0.0003, 'epoch': 0.2}\n",
      "{'loss': 1.7173, 'grad_norm': 1.140625, 'learning_rate': 0.0003, 'epoch': 0.21}\n",
      "{'loss': 1.7523, 'grad_norm': 0.44921875, 'learning_rate': 0.0003, 'epoch': 0.21}\n",
      "{'loss': 1.7148, 'grad_norm': 0.32421875, 'learning_rate': 0.0003, 'epoch': 0.21}\n",
      "{'loss': 1.6164, 'grad_norm': 0.53515625, 'learning_rate': 0.0003, 'epoch': 0.21}\n",
      "{'loss': 1.5843, 'grad_norm': 0.337890625, 'learning_rate': 0.0003, 'epoch': 0.21}\n",
      "{'loss': 1.7123, 'grad_norm': 0.494140625, 'learning_rate': 0.0003, 'epoch': 0.22}\n",
      "{'loss': 1.6941, 'grad_norm': 0.34765625, 'learning_rate': 0.0003, 'epoch': 0.22}\n",
      "{'loss': 1.796, 'grad_norm': 0.44140625, 'learning_rate': 0.0003, 'epoch': 0.22}\n",
      "{'loss': 1.8442, 'grad_norm': 0.5859375, 'learning_rate': 0.0003, 'epoch': 0.22}\n",
      "{'loss': 1.7033, 'grad_norm': 0.275390625, 'learning_rate': 0.0003, 'epoch': 0.22}\n",
      "{'loss': 1.7569, 'grad_norm': 1.1015625, 'learning_rate': 0.0003, 'epoch': 0.23}\n",
      "{'loss': 1.6784, 'grad_norm': 0.4453125, 'learning_rate': 0.0003, 'epoch': 0.23}\n",
      "{'loss': 1.7903, 'grad_norm': 0.2421875, 'learning_rate': 0.0003, 'epoch': 0.23}\n",
      "{'loss': 1.7337, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 0.23}\n",
      "{'loss': 1.7708, 'grad_norm': 0.326171875, 'learning_rate': 0.0003, 'epoch': 0.23}\n",
      "{'loss': 1.8337, 'grad_norm': 1.6875, 'learning_rate': 0.0003, 'epoch': 0.24}\n",
      "{'loss': 1.6141, 'grad_norm': 0.3671875, 'learning_rate': 0.0003, 'epoch': 0.24}\n",
      "{'loss': 1.6609, 'grad_norm': 0.39453125, 'learning_rate': 0.0003, 'epoch': 0.24}\n",
      "{'loss': 1.7852, 'grad_norm': 0.9375, 'learning_rate': 0.0003, 'epoch': 0.24}\n",
      "{'loss': 1.7262, 'grad_norm': 0.349609375, 'learning_rate': 0.0003, 'epoch': 0.24}\n",
      "{'loss': 1.7289, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 0.25}\n",
      "{'loss': 1.6312, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 0.25}\n",
      "{'loss': 1.5174, 'grad_norm': 0.294921875, 'learning_rate': 0.0003, 'epoch': 0.25}\n",
      "{'loss': 1.8232, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 0.25}\n",
      "{'loss': 1.7243, 'grad_norm': 0.33984375, 'learning_rate': 0.0003, 'epoch': 0.25}\n",
      "{'loss': 1.7671, 'grad_norm': 0.8359375, 'learning_rate': 0.0003, 'epoch': 0.26}\n",
      "{'loss': 1.7453, 'grad_norm': 0.421875, 'learning_rate': 0.0003, 'epoch': 0.26}\n",
      "{'loss': 1.6731, 'grad_norm': 0.400390625, 'learning_rate': 0.0003, 'epoch': 0.26}\n",
      "{'loss': 1.8129, 'grad_norm': 0.9296875, 'learning_rate': 0.0003, 'epoch': 0.26}\n",
      "{'loss': 1.6601, 'grad_norm': 0.45703125, 'learning_rate': 0.0003, 'epoch': 0.26}\n",
      "{'loss': 1.673, 'grad_norm': 0.431640625, 'learning_rate': 0.0003, 'epoch': 0.27}\n",
      "{'loss': 1.7815, 'grad_norm': 0.384765625, 'learning_rate': 0.0003, 'epoch': 0.27}\n",
      "{'loss': 1.6039, 'grad_norm': 0.375, 'learning_rate': 0.0003, 'epoch': 0.27}\n",
      "{'loss': 1.8149, 'grad_norm': 0.98046875, 'learning_rate': 0.0003, 'epoch': 0.27}\n",
      "{'loss': 1.6672, 'grad_norm': 0.373046875, 'learning_rate': 0.0003, 'epoch': 0.27}\n",
      "{'loss': 1.7798, 'grad_norm': 1.3046875, 'learning_rate': 0.0003, 'epoch': 0.28}\n",
      "{'loss': 1.7807, 'grad_norm': 0.470703125, 'learning_rate': 0.0003, 'epoch': 0.28}\n",
      "{'loss': 1.7557, 'grad_norm': 0.29296875, 'learning_rate': 0.0003, 'epoch': 0.28}\n",
      "{'loss': 1.7799, 'grad_norm': 0.59765625, 'learning_rate': 0.0003, 'epoch': 0.28}\n",
      "{'loss': 1.5894, 'grad_norm': 0.337890625, 'learning_rate': 0.0003, 'epoch': 0.28}\n",
      "{'loss': 1.7364, 'grad_norm': 0.390625, 'learning_rate': 0.0003, 'epoch': 0.29}\n",
      "{'loss': 1.7929, 'grad_norm': 0.50390625, 'learning_rate': 0.0003, 'epoch': 0.29}\n",
      "{'loss': 1.659, 'grad_norm': 0.353515625, 'learning_rate': 0.0003, 'epoch': 0.29}\n",
      "{'loss': 1.729, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 0.29}\n",
      "{'loss': 1.6987, 'grad_norm': 0.4296875, 'learning_rate': 0.0003, 'epoch': 0.29}\n",
      "{'loss': 1.7334, 'grad_norm': 1.390625, 'learning_rate': 0.0003, 'epoch': 0.29}\n",
      "{'loss': 1.8061, 'grad_norm': 0.447265625, 'learning_rate': 0.0003, 'epoch': 0.3}\n",
      "{'loss': 1.7093, 'grad_norm': 0.494140625, 'learning_rate': 0.0003, 'epoch': 0.3}\n",
      "{'loss': 1.6901, 'grad_norm': 0.7734375, 'learning_rate': 0.0003, 'epoch': 0.3}\n",
      "{'loss': 1.8018, 'grad_norm': 0.310546875, 'learning_rate': 0.0003, 'epoch': 0.3}\n",
      "{'loss': 1.6653, 'grad_norm': 1.140625, 'learning_rate': 0.0003, 'epoch': 0.3}\n",
      "{'loss': 1.7079, 'grad_norm': 0.416015625, 'learning_rate': 0.0003, 'epoch': 0.31}\n",
      "{'loss': 1.7112, 'grad_norm': 0.30859375, 'learning_rate': 0.0003, 'epoch': 0.31}\n",
      "{'loss': 1.8132, 'grad_norm': 0.76171875, 'learning_rate': 0.0003, 'epoch': 0.31}\n",
      "{'loss': 1.6498, 'grad_norm': 0.416015625, 'learning_rate': 0.0003, 'epoch': 0.31}\n",
      "{'loss': 1.6079, 'grad_norm': 0.36328125, 'learning_rate': 0.0003, 'epoch': 0.31}\n",
      "{'loss': 1.7265, 'grad_norm': 0.86328125, 'learning_rate': 0.0003, 'epoch': 0.32}\n",
      "{'loss': 1.6948, 'grad_norm': 0.34375, 'learning_rate': 0.0003, 'epoch': 0.32}\n",
      "{'loss': 1.7075, 'grad_norm': 0.6015625, 'learning_rate': 0.0003, 'epoch': 0.32}\n",
      "{'loss': 1.8056, 'grad_norm': 0.458984375, 'learning_rate': 0.0003, 'epoch': 0.32}\n",
      "{'loss': 1.7127, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 0.32}\n",
      "{'loss': 1.657, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 0.33}\n",
      "{'loss': 1.6479, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 0.33}\n",
      "{'loss': 1.8162, 'grad_norm': 1.0625, 'learning_rate': 0.0003, 'epoch': 0.33}\n",
      "{'loss': 1.5867, 'grad_norm': 0.4140625, 'learning_rate': 0.0003, 'epoch': 0.33}\n",
      "{'loss': 1.5844, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 0.33}\n",
      "{'loss': 1.702, 'grad_norm': 0.51171875, 'learning_rate': 0.0003, 'epoch': 0.34}\n",
      "{'loss': 1.6147, 'grad_norm': 0.298828125, 'learning_rate': 0.0003, 'epoch': 0.34}\n",
      "{'loss': 1.7571, 'grad_norm': 0.828125, 'learning_rate': 0.0003, 'epoch': 0.34}\n",
      "{'loss': 1.6838, 'grad_norm': 0.330078125, 'learning_rate': 0.0003, 'epoch': 0.34}\n",
      "{'loss': 1.7251, 'grad_norm': 0.8046875, 'learning_rate': 0.0003, 'epoch': 0.34}\n",
      "{'loss': 1.84, 'grad_norm': 0.484375, 'learning_rate': 0.0003, 'epoch': 0.35}\n",
      "{'loss': 1.7291, 'grad_norm': 0.404296875, 'learning_rate': 0.0003, 'epoch': 0.35}\n",
      "{'loss': 1.8574, 'grad_norm': 0.9296875, 'learning_rate': 0.0003, 'epoch': 0.35}\n",
      "{'loss': 1.7716, 'grad_norm': 0.482421875, 'learning_rate': 0.0003, 'epoch': 0.35}\n",
      "{'loss': 1.5955, 'grad_norm': 0.9609375, 'learning_rate': 0.0003, 'epoch': 0.35}\n",
      "{'loss': 1.684, 'grad_norm': 0.6171875, 'learning_rate': 0.0003, 'epoch': 0.36}\n",
      "{'loss': 1.8006, 'grad_norm': 0.447265625, 'learning_rate': 0.0003, 'epoch': 0.36}\n",
      "{'loss': 1.6129, 'grad_norm': 0.64453125, 'learning_rate': 0.0003, 'epoch': 0.36}\n",
      "{'loss': 1.6483, 'grad_norm': 0.453125, 'learning_rate': 0.0003, 'epoch': 0.36}\n",
      "{'loss': 1.7419, 'grad_norm': 0.447265625, 'learning_rate': 0.0003, 'epoch': 0.36}\n",
      "{'loss': 1.7177, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 0.37}\n",
      "{'loss': 1.7492, 'grad_norm': 0.2890625, 'learning_rate': 0.0003, 'epoch': 0.37}\n",
      "{'loss': 1.8314, 'grad_norm': 0.78125, 'learning_rate': 0.0003, 'epoch': 0.37}\n",
      "{'loss': 1.7184, 'grad_norm': 0.328125, 'learning_rate': 0.0003, 'epoch': 0.37}\n",
      "{'loss': 1.8365, 'grad_norm': 1.09375, 'learning_rate': 0.0003, 'epoch': 0.37}\n",
      "{'loss': 1.6951, 'grad_norm': 0.443359375, 'learning_rate': 0.0003, 'epoch': 0.38}\n",
      "{'loss': 1.623, 'grad_norm': 0.38671875, 'learning_rate': 0.0003, 'epoch': 0.38}\n",
      "{'loss': 1.7325, 'grad_norm': 1.28125, 'learning_rate': 0.0003, 'epoch': 0.38}\n",
      "{'loss': 1.7035, 'grad_norm': 0.408203125, 'learning_rate': 0.0003, 'epoch': 0.38}\n",
      "{'loss': 1.7676, 'grad_norm': 2.0625, 'learning_rate': 0.0003, 'epoch': 0.38}\n",
      "{'loss': 1.6463, 'grad_norm': 0.431640625, 'learning_rate': 0.0003, 'epoch': 0.39}\n",
      "{'loss': 1.8628, 'grad_norm': 0.283203125, 'learning_rate': 0.0003, 'epoch': 0.39}\n",
      "{'loss': 1.8388, 'grad_norm': 0.65625, 'learning_rate': 0.0003, 'epoch': 0.39}\n",
      "{'loss': 1.6388, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 0.39}\n",
      "{'loss': 1.6385, 'grad_norm': 0.37109375, 'learning_rate': 0.0003, 'epoch': 0.39}\n",
      "{'loss': 1.7612, 'grad_norm': 0.462890625, 'learning_rate': 0.0003, 'epoch': 0.4}\n",
      "{'loss': 1.7068, 'grad_norm': 0.283203125, 'learning_rate': 0.0003, 'epoch': 0.4}\n",
      "{'loss': 1.882, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 0.4}\n",
      "{'loss': 1.7897, 'grad_norm': 0.734375, 'learning_rate': 0.0003, 'epoch': 0.4}\n",
      "{'loss': 1.8123, 'grad_norm': 0.9140625, 'learning_rate': 0.0003, 'epoch': 0.4}\n",
      "{'loss': 1.6504, 'grad_norm': 0.6953125, 'learning_rate': 0.0003, 'epoch': 0.4}\n",
      "{'loss': 1.7186, 'grad_norm': 0.310546875, 'learning_rate': 0.0003, 'epoch': 0.41}\n",
      "{'loss': 1.8358, 'grad_norm': 0.94140625, 'learning_rate': 0.0003, 'epoch': 0.41}\n",
      "{'loss': 1.7426, 'grad_norm': 0.3671875, 'learning_rate': 0.0003, 'epoch': 0.41}\n",
      "{'loss': 1.7978, 'grad_norm': 1.140625, 'learning_rate': 0.0003, 'epoch': 0.41}\n",
      "{'loss': 1.7713, 'grad_norm': 0.62890625, 'learning_rate': 0.0003, 'epoch': 0.41}\n",
      "{'loss': 1.8823, 'grad_norm': 0.3203125, 'learning_rate': 0.0003, 'epoch': 0.42}\n",
      "{'loss': 1.7824, 'grad_norm': 0.48046875, 'learning_rate': 0.0003, 'epoch': 0.42}\n",
      "{'loss': 1.6933, 'grad_norm': 0.392578125, 'learning_rate': 0.0003, 'epoch': 0.42}\n",
      "{'loss': 1.6182, 'grad_norm': 0.765625, 'learning_rate': 0.0003, 'epoch': 0.42}\n",
      "{'loss': 1.7263, 'grad_norm': 0.451171875, 'learning_rate': 0.0003, 'epoch': 0.42}\n",
      "{'loss': 1.6807, 'grad_norm': 0.318359375, 'learning_rate': 0.0003, 'epoch': 0.43}\n",
      "{'loss': 1.6461, 'grad_norm': 0.796875, 'learning_rate': 0.0003, 'epoch': 0.43}\n",
      "{'loss': 1.7224, 'grad_norm': 0.462890625, 'learning_rate': 0.0003, 'epoch': 0.43}\n",
      "{'loss': 1.7663, 'grad_norm': 0.94921875, 'learning_rate': 0.0003, 'epoch': 0.43}\n",
      "{'loss': 1.7095, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 0.43}\n",
      "{'loss': 1.7006, 'grad_norm': 0.279296875, 'learning_rate': 0.0003, 'epoch': 0.44}\n",
      "{'loss': 1.7492, 'grad_norm': 0.76953125, 'learning_rate': 0.0003, 'epoch': 0.44}\n",
      "{'loss': 1.748, 'grad_norm': 0.51171875, 'learning_rate': 0.0003, 'epoch': 0.44}\n",
      "{'loss': 1.7301, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 0.44}\n",
      "{'loss': 1.8774, 'grad_norm': 0.6171875, 'learning_rate': 0.0003, 'epoch': 0.44}\n",
      "{'loss': 1.6724, 'grad_norm': 0.458984375, 'learning_rate': 0.0003, 'epoch': 0.45}\n",
      "{'loss': 1.9205, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 0.45}\n",
      "{'loss': 1.6478, 'grad_norm': 0.427734375, 'learning_rate': 0.0003, 'epoch': 0.45}\n",
      "{'loss': 1.706, 'grad_norm': 1.2109375, 'learning_rate': 0.0003, 'epoch': 0.45}\n",
      "{'loss': 1.7472, 'grad_norm': 0.40625, 'learning_rate': 0.0003, 'epoch': 0.45}\n",
      "{'loss': 1.6142, 'grad_norm': 0.345703125, 'learning_rate': 0.0003, 'epoch': 0.46}\n",
      "{'loss': 1.7645, 'grad_norm': 1.0234375, 'learning_rate': 0.0003, 'epoch': 0.46}\n",
      "{'loss': 1.5519, 'grad_norm': 0.328125, 'learning_rate': 0.0003, 'epoch': 0.46}\n",
      "{'loss': 1.7073, 'grad_norm': 0.208984375, 'learning_rate': 0.0003, 'epoch': 0.46}\n",
      "{'loss': 1.7165, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 0.46}\n",
      "{'loss': 1.6563, 'grad_norm': 0.4140625, 'learning_rate': 0.0003, 'epoch': 0.47}\n",
      "{'loss': 1.7746, 'grad_norm': 0.73828125, 'learning_rate': 0.0003, 'epoch': 0.47}\n",
      "{'loss': 1.6348, 'grad_norm': 0.56640625, 'learning_rate': 0.0003, 'epoch': 0.47}\n",
      "{'loss': 1.6379, 'grad_norm': 0.59375, 'learning_rate': 0.0003, 'epoch': 0.47}\n",
      "{'loss': 1.7529, 'grad_norm': 0.50390625, 'learning_rate': 0.0003, 'epoch': 0.47}\n",
      "{'loss': 1.6608, 'grad_norm': 0.3671875, 'learning_rate': 0.0003, 'epoch': 0.48}\n",
      "{'loss': 1.7582, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 0.48}\n",
      "{'loss': 1.6152, 'grad_norm': 0.427734375, 'learning_rate': 0.0003, 'epoch': 0.48}\n",
      "{'loss': 1.7209, 'grad_norm': 1.7265625, 'learning_rate': 0.0003, 'epoch': 0.48}\n",
      "{'loss': 1.7316, 'grad_norm': 0.7421875, 'learning_rate': 0.0003, 'epoch': 0.48}\n",
      "{'loss': 1.6502, 'grad_norm': 0.376953125, 'learning_rate': 0.0003, 'epoch': 0.49}\n",
      "{'loss': 1.7805, 'grad_norm': 0.72265625, 'learning_rate': 0.0003, 'epoch': 0.49}\n",
      "{'loss': 1.76, 'grad_norm': 0.40234375, 'learning_rate': 0.0003, 'epoch': 0.49}\n",
      "{'loss': 1.9244, 'grad_norm': 0.9453125, 'learning_rate': 0.0003, 'epoch': 0.49}\n",
      "{'loss': 1.7708, 'grad_norm': 0.494140625, 'learning_rate': 0.0003, 'epoch': 0.49}\n",
      "{'loss': 1.6186, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 0.5}\n",
      "{'loss': 1.7277, 'grad_norm': 0.828125, 'learning_rate': 0.0003, 'epoch': 0.5}\n",
      "{'loss': 1.7598, 'grad_norm': 0.400390625, 'learning_rate': 0.0003, 'epoch': 0.5}\n",
      "{'loss': 1.5941, 'grad_norm': 0.875, 'learning_rate': 0.0003, 'epoch': 0.5}\n",
      "{'loss': 1.6971, 'grad_norm': 0.4296875, 'learning_rate': 0.0003, 'epoch': 0.5}\n",
      "{'loss': 1.6939, 'grad_norm': 0.314453125, 'learning_rate': 0.0003, 'epoch': 0.51}\n",
      "{'loss': 1.6509, 'grad_norm': 0.484375, 'learning_rate': 0.0003, 'epoch': 0.51}\n",
      "{'loss': 1.7528, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 0.51}\n",
      "{'loss': 1.6766, 'grad_norm': 1.1796875, 'learning_rate': 0.0003, 'epoch': 0.51}\n",
      "{'loss': 1.7271, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 0.51}\n",
      "{'loss': 1.8163, 'grad_norm': 0.447265625, 'learning_rate': 0.0003, 'epoch': 0.51}\n",
      "{'loss': 1.6805, 'grad_norm': 1.0859375, 'learning_rate': 0.0003, 'epoch': 0.52}\n",
      "{'loss': 1.6376, 'grad_norm': 0.5859375, 'learning_rate': 0.0003, 'epoch': 0.52}\n",
      "{'loss': 1.695, 'grad_norm': 1.5, 'learning_rate': 0.0003, 'epoch': 0.52}\n",
      "{'loss': 1.6044, 'grad_norm': 0.39453125, 'learning_rate': 0.0003, 'epoch': 0.52}\n",
      "{'loss': 1.604, 'grad_norm': 0.494140625, 'learning_rate': 0.0003, 'epoch': 0.52}\n",
      "{'loss': 1.642, 'grad_norm': 0.921875, 'learning_rate': 0.0003, 'epoch': 0.53}\n",
      "{'loss': 1.7753, 'grad_norm': 0.5703125, 'learning_rate': 0.0003, 'epoch': 0.53}\n",
      "{'loss': 1.7462, 'grad_norm': 2.015625, 'learning_rate': 0.0003, 'epoch': 0.53}\n",
      "{'loss': 1.7915, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 0.53}\n",
      "{'loss': 1.6855, 'grad_norm': 0.48046875, 'learning_rate': 0.0003, 'epoch': 0.53}\n",
      "{'loss': 1.7335, 'grad_norm': 0.99609375, 'learning_rate': 0.0003, 'epoch': 0.54}\n",
      "{'loss': 1.837, 'grad_norm': 0.369140625, 'learning_rate': 0.0003, 'epoch': 0.54}\n",
      "{'loss': 1.5951, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 0.54}\n",
      "{'loss': 1.8443, 'grad_norm': 0.78125, 'learning_rate': 0.0003, 'epoch': 0.54}\n",
      "{'loss': 1.7209, 'grad_norm': 0.4609375, 'learning_rate': 0.0003, 'epoch': 0.54}\n",
      "{'loss': 1.7398, 'grad_norm': 0.703125, 'learning_rate': 0.0003, 'epoch': 0.55}\n",
      "{'loss': 1.5759, 'grad_norm': 1.25, 'learning_rate': 0.0003, 'epoch': 0.55}\n",
      "{'loss': 1.6002, 'grad_norm': 0.20703125, 'learning_rate': 0.0003, 'epoch': 0.55}\n",
      "{'loss': 1.6453, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 0.55}\n",
      "{'loss': 1.6229, 'grad_norm': 0.341796875, 'learning_rate': 0.0003, 'epoch': 0.55}\n",
      "{'loss': 1.6408, 'grad_norm': 0.96875, 'learning_rate': 0.0003, 'epoch': 0.56}\n",
      "{'loss': 1.5656, 'grad_norm': 0.61328125, 'learning_rate': 0.0003, 'epoch': 0.56}\n",
      "{'loss': 1.6731, 'grad_norm': 1.25, 'learning_rate': 0.0003, 'epoch': 0.56}\n",
      "{'loss': 1.6727, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 0.56}\n",
      "{'loss': 1.7736, 'grad_norm': 0.5703125, 'learning_rate': 0.0003, 'epoch': 0.56}\n",
      "{'loss': 1.7947, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 0.57}\n",
      "{'loss': 1.699, 'grad_norm': 0.353515625, 'learning_rate': 0.0003, 'epoch': 0.57}\n",
      "{'loss': 1.6951, 'grad_norm': 1.125, 'learning_rate': 0.0003, 'epoch': 0.57}\n",
      "{'loss': 1.72, 'grad_norm': 0.9296875, 'learning_rate': 0.0003, 'epoch': 0.57}\n",
      "{'loss': 1.8808, 'grad_norm': 0.5078125, 'learning_rate': 0.0003, 'epoch': 0.57}\n",
      "{'loss': 1.6517, 'grad_norm': 0.4921875, 'learning_rate': 0.0003, 'epoch': 0.58}\n",
      "{'loss': 1.5381, 'grad_norm': 0.390625, 'learning_rate': 0.0003, 'epoch': 0.58}\n",
      "{'loss': 1.8005, 'grad_norm': 1.1171875, 'learning_rate': 0.0003, 'epoch': 0.58}\n",
      "{'loss': 1.6621, 'grad_norm': 0.431640625, 'learning_rate': 0.0003, 'epoch': 0.58}\n",
      "{'loss': 1.6106, 'grad_norm': 0.5078125, 'learning_rate': 0.0003, 'epoch': 0.58}\n",
      "{'loss': 1.6855, 'grad_norm': 0.78515625, 'learning_rate': 0.0003, 'epoch': 0.59}\n",
      "{'loss': 1.7993, 'grad_norm': 0.453125, 'learning_rate': 0.0003, 'epoch': 0.59}\n",
      "{'loss': 1.6845, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 0.59}\n",
      "{'loss': 1.8209, 'grad_norm': 1.0546875, 'learning_rate': 0.0003, 'epoch': 0.59}\n",
      "{'loss': 1.7089, 'grad_norm': 0.3125, 'learning_rate': 0.0003, 'epoch': 0.59}\n",
      "{'loss': 1.7159, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 0.6}\n",
      "{'loss': 1.6786, 'grad_norm': 0.55859375, 'learning_rate': 0.0003, 'epoch': 0.6}\n",
      "{'loss': 1.7263, 'grad_norm': 0.8046875, 'learning_rate': 0.0003, 'epoch': 0.6}\n",
      "{'loss': 1.7256, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 0.6}\n",
      "{'loss': 1.8227, 'grad_norm': 0.361328125, 'learning_rate': 0.0003, 'epoch': 0.6}\n",
      "{'loss': 1.6684, 'grad_norm': 0.73046875, 'learning_rate': 0.0003, 'epoch': 0.61}\n",
      "{'loss': 1.5235, 'grad_norm': 0.47265625, 'learning_rate': 0.0003, 'epoch': 0.61}\n",
      "{'loss': 1.7647, 'grad_norm': 0.421875, 'learning_rate': 0.0003, 'epoch': 0.61}\n",
      "{'loss': 1.6591, 'grad_norm': 0.5234375, 'learning_rate': 0.0003, 'epoch': 0.61}\n",
      "{'loss': 1.6774, 'grad_norm': 0.3984375, 'learning_rate': 0.0003, 'epoch': 0.61}\n",
      "{'loss': 1.9622, 'grad_norm': 0.83203125, 'learning_rate': 0.0003, 'epoch': 0.62}\n",
      "{'loss': 1.6922, 'grad_norm': 0.421875, 'learning_rate': 0.0003, 'epoch': 0.62}\n",
      "{'loss': 1.7896, 'grad_norm': 0.369140625, 'learning_rate': 0.0003, 'epoch': 0.62}\n",
      "{'loss': 1.6841, 'grad_norm': 0.82421875, 'learning_rate': 0.0003, 'epoch': 0.62}\n",
      "{'loss': 1.8638, 'grad_norm': 0.46875, 'learning_rate': 0.0003, 'epoch': 0.62}\n",
      "{'loss': 1.5938, 'grad_norm': 0.88671875, 'learning_rate': 0.0003, 'epoch': 0.63}\n",
      "{'loss': 1.6441, 'grad_norm': 0.462890625, 'learning_rate': 0.0003, 'epoch': 0.63}\n",
      "{'loss': 1.8084, 'grad_norm': 1.21875, 'learning_rate': 0.0003, 'epoch': 0.63}\n",
      "{'loss': 1.6092, 'grad_norm': 0.58984375, 'learning_rate': 0.0003, 'epoch': 0.63}\n",
      "{'loss': 1.6791, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 0.63}\n",
      "{'loss': 1.804, 'grad_norm': 0.63671875, 'learning_rate': 0.0003, 'epoch': 0.63}\n",
      "{'loss': 1.5649, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 0.64}\n",
      "{'loss': 1.7527, 'grad_norm': 1.5390625, 'learning_rate': 0.0003, 'epoch': 0.64}\n",
      "{'loss': 1.5738, 'grad_norm': 0.67578125, 'learning_rate': 0.0003, 'epoch': 0.64}\n",
      "{'loss': 1.6893, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 0.64}\n",
      "{'loss': 1.8198, 'grad_norm': 0.99609375, 'learning_rate': 0.0003, 'epoch': 0.64}\n",
      "{'loss': 1.5589, 'grad_norm': 0.5703125, 'learning_rate': 0.0003, 'epoch': 0.65}\n",
      "{'loss': 1.7083, 'grad_norm': 0.97265625, 'learning_rate': 0.0003, 'epoch': 0.65}\n",
      "{'loss': 1.6425, 'grad_norm': 0.416015625, 'learning_rate': 0.0003, 'epoch': 0.65}\n",
      "{'loss': 1.7845, 'grad_norm': 0.31640625, 'learning_rate': 0.0003, 'epoch': 0.65}\n",
      "{'loss': 1.7296, 'grad_norm': 1.15625, 'learning_rate': 0.0003, 'epoch': 0.65}\n",
      "{'loss': 1.6174, 'grad_norm': 0.4140625, 'learning_rate': 0.0003, 'epoch': 0.66}\n",
      "{'loss': 1.5651, 'grad_norm': 3.15625, 'learning_rate': 0.0003, 'epoch': 0.66}\n",
      "{'loss': 1.69, 'grad_norm': 0.55859375, 'learning_rate': 0.0003, 'epoch': 0.66}\n",
      "{'loss': 1.6597, 'grad_norm': 0.455078125, 'learning_rate': 0.0003, 'epoch': 0.66}\n",
      "{'loss': 1.8296, 'grad_norm': 0.828125, 'learning_rate': 0.0003, 'epoch': 0.66}\n",
      "{'loss': 1.5979, 'grad_norm': 0.498046875, 'learning_rate': 0.0003, 'epoch': 0.67}\n",
      "{'loss': 1.5654, 'grad_norm': 0.91796875, 'learning_rate': 0.0003, 'epoch': 0.67}\n",
      "{'loss': 1.6779, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 0.67}\n",
      "{'loss': 1.6204, 'grad_norm': 0.365234375, 'learning_rate': 0.0003, 'epoch': 0.67}\n",
      "{'loss': 1.8116, 'grad_norm': 0.72265625, 'learning_rate': 0.0003, 'epoch': 0.67}\n",
      "{'loss': 1.5851, 'grad_norm': 0.3359375, 'learning_rate': 0.0003, 'epoch': 0.68}\n",
      "{'loss': 1.6636, 'grad_norm': 1.1953125, 'learning_rate': 0.0003, 'epoch': 0.68}\n",
      "{'loss': 1.6939, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 0.68}\n",
      "{'loss': 1.5773, 'grad_norm': 0.361328125, 'learning_rate': 0.0003, 'epoch': 0.68}\n",
      "{'loss': 1.7786, 'grad_norm': 0.94921875, 'learning_rate': 0.0003, 'epoch': 0.68}\n",
      "{'loss': 1.6502, 'grad_norm': 0.419921875, 'learning_rate': 0.0003, 'epoch': 0.69}\n",
      "{'loss': 1.7831, 'grad_norm': 2.671875, 'learning_rate': 0.0003, 'epoch': 0.69}\n",
      "{'loss': 1.697, 'grad_norm': 0.64453125, 'learning_rate': 0.0003, 'epoch': 0.69}\n",
      "{'loss': 1.6331, 'grad_norm': 0.33984375, 'learning_rate': 0.0003, 'epoch': 0.69}\n",
      "{'loss': 1.6109, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 0.69}\n",
      "{'loss': 1.4278, 'grad_norm': 0.43359375, 'learning_rate': 0.0003, 'epoch': 0.7}\n",
      "{'loss': 1.7446, 'grad_norm': 1.0390625, 'learning_rate': 0.0003, 'epoch': 0.7}\n",
      "{'loss': 1.6122, 'grad_norm': 0.5859375, 'learning_rate': 0.0003, 'epoch': 0.7}\n",
      "{'loss': 1.5844, 'grad_norm': 0.9375, 'learning_rate': 0.0003, 'epoch': 0.7}\n",
      "{'loss': 1.7958, 'grad_norm': 0.890625, 'learning_rate': 0.0003, 'epoch': 0.7}\n",
      "{'loss': 1.5982, 'grad_norm': 0.396484375, 'learning_rate': 0.0003, 'epoch': 0.71}\n",
      "{'loss': 1.69, 'grad_norm': 0.5859375, 'learning_rate': 0.0003, 'epoch': 0.71}\n",
      "{'loss': 1.8215, 'grad_norm': 0.71484375, 'learning_rate': 0.0003, 'epoch': 0.71}\n",
      "{'loss': 1.6566, 'grad_norm': 0.369140625, 'learning_rate': 0.0003, 'epoch': 0.71}\n",
      "{'loss': 1.6909, 'grad_norm': 0.84765625, 'learning_rate': 0.0003, 'epoch': 0.71}\n",
      "{'loss': 1.5684, 'grad_norm': 0.609375, 'learning_rate': 0.0003, 'epoch': 0.72}\n",
      "{'loss': 1.6728, 'grad_norm': 0.60546875, 'learning_rate': 0.0003, 'epoch': 0.72}\n",
      "{'loss': 1.6399, 'grad_norm': 0.58984375, 'learning_rate': 0.0003, 'epoch': 0.72}\n",
      "{'loss': 1.6386, 'grad_norm': 0.42578125, 'learning_rate': 0.0003, 'epoch': 0.72}\n",
      "{'loss': 1.7316, 'grad_norm': 0.59765625, 'learning_rate': 0.0003, 'epoch': 0.72}\n",
      "{'loss': 1.6638, 'grad_norm': 0.38671875, 'learning_rate': 0.0003, 'epoch': 0.73}\n",
      "{'loss': 1.7159, 'grad_norm': 1.2265625, 'learning_rate': 0.0003, 'epoch': 0.73}\n",
      "{'loss': 1.7482, 'grad_norm': 0.5703125, 'learning_rate': 0.0003, 'epoch': 0.73}\n",
      "{'loss': 1.6016, 'grad_norm': 0.439453125, 'learning_rate': 0.0003, 'epoch': 0.73}\n",
      "{'loss': 1.7604, 'grad_norm': 0.67578125, 'learning_rate': 0.0003, 'epoch': 0.73}\n",
      "{'loss': 1.6096, 'grad_norm': 0.390625, 'learning_rate': 0.0003, 'epoch': 0.74}\n",
      "{'loss': 1.7351, 'grad_norm': 1.8125, 'learning_rate': 0.0003, 'epoch': 0.74}\n",
      "{'loss': 1.7755, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 0.74}\n",
      "{'loss': 1.6153, 'grad_norm': 0.408203125, 'learning_rate': 0.0003, 'epoch': 0.74}\n",
      "{'loss': 1.7762, 'grad_norm': 0.890625, 'learning_rate': 0.0003, 'epoch': 0.74}\n",
      "{'loss': 1.7206, 'grad_norm': 0.40625, 'learning_rate': 0.0003, 'epoch': 0.74}\n",
      "{'loss': 1.6804, 'grad_norm': 2.109375, 'learning_rate': 0.0003, 'epoch': 0.75}\n",
      "{'loss': 1.7262, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 0.75}\n",
      "{'loss': 1.5965, 'grad_norm': 0.59765625, 'learning_rate': 0.0003, 'epoch': 0.75}\n",
      "{'loss': 1.6952, 'grad_norm': 1.1171875, 'learning_rate': 0.0003, 'epoch': 0.75}\n",
      "{'loss': 1.7093, 'grad_norm': 0.68359375, 'learning_rate': 0.0003, 'epoch': 0.75}\n",
      "{'loss': 1.8586, 'grad_norm': 1.6015625, 'learning_rate': 0.0003, 'epoch': 0.76}\n",
      "{'loss': 1.6652, 'grad_norm': 0.94921875, 'learning_rate': 0.0003, 'epoch': 0.76}\n",
      "{'loss': 1.7786, 'grad_norm': 0.37890625, 'learning_rate': 0.0003, 'epoch': 0.76}\n",
      "{'loss': 1.7068, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 0.76}\n",
      "{'loss': 1.7752, 'grad_norm': 0.53515625, 'learning_rate': 0.0003, 'epoch': 0.76}\n",
      "{'loss': 1.743, 'grad_norm': 2.09375, 'learning_rate': 0.0003, 'epoch': 0.77}\n",
      "{'loss': 1.606, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 0.77}\n",
      "{'loss': 1.6005, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 0.77}\n",
      "{'loss': 1.6, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 0.77}\n",
      "{'loss': 1.6302, 'grad_norm': 0.466796875, 'learning_rate': 0.0003, 'epoch': 0.77}\n",
      "{'loss': 1.6856, 'grad_norm': 0.310546875, 'learning_rate': 0.0003, 'epoch': 0.78}\n",
      "{'loss': 1.6135, 'grad_norm': 0.9453125, 'learning_rate': 0.0003, 'epoch': 0.78}\n",
      "{'loss': 1.7807, 'grad_norm': 0.50390625, 'learning_rate': 0.0003, 'epoch': 0.78}\n",
      "{'loss': 1.6613, 'grad_norm': 1.1015625, 'learning_rate': 0.0003, 'epoch': 0.78}\n",
      "{'loss': 1.4862, 'grad_norm': 0.388671875, 'learning_rate': 0.0003, 'epoch': 0.78}\n",
      "{'loss': 1.5631, 'grad_norm': 0.12158203125, 'learning_rate': 0.0003, 'epoch': 0.79}\n",
      "{'loss': 1.7628, 'grad_norm': 0.578125, 'learning_rate': 0.0003, 'epoch': 0.79}\n",
      "{'loss': 1.6492, 'grad_norm': 0.447265625, 'learning_rate': 0.0003, 'epoch': 0.79}\n",
      "{'loss': 1.755, 'grad_norm': 0.7109375, 'learning_rate': 0.0003, 'epoch': 0.79}\n",
      "{'loss': 1.679, 'grad_norm': 0.44140625, 'learning_rate': 0.0003, 'epoch': 0.79}\n",
      "{'loss': 1.7257, 'grad_norm': 2.671875, 'learning_rate': 0.0003, 'epoch': 0.8}\n",
      "{'loss': 1.783, 'grad_norm': 0.498046875, 'learning_rate': 0.0003, 'epoch': 0.8}\n",
      "{'loss': 1.7191, 'grad_norm': 0.57421875, 'learning_rate': 0.0003, 'epoch': 0.8}\n",
      "{'loss': 1.6273, 'grad_norm': 0.78125, 'learning_rate': 0.0003, 'epoch': 0.8}\n",
      "{'loss': 1.4988, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 0.8}\n",
      "{'loss': 1.8396, 'grad_norm': 1.03125, 'learning_rate': 0.0003, 'epoch': 0.81}\n",
      "{'loss': 1.7905, 'grad_norm': 0.6796875, 'learning_rate': 0.0003, 'epoch': 0.81}\n",
      "{'loss': 1.6061, 'grad_norm': 0.42578125, 'learning_rate': 0.0003, 'epoch': 0.81}\n",
      "{'loss': 1.8669, 'grad_norm': 0.93359375, 'learning_rate': 0.0003, 'epoch': 0.81}\n",
      "{'loss': 1.4865, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 0.81}\n",
      "{'loss': 1.6435, 'grad_norm': 1.359375, 'learning_rate': 0.0003, 'epoch': 0.82}\n",
      "{'loss': 1.7568, 'grad_norm': 0.63671875, 'learning_rate': 0.0003, 'epoch': 0.82}\n",
      "{'loss': 1.633, 'grad_norm': 0.4375, 'learning_rate': 0.0003, 'epoch': 0.82}\n",
      "{'loss': 1.7567, 'grad_norm': 1.2421875, 'learning_rate': 0.0003, 'epoch': 0.82}\n",
      "{'loss': 1.6032, 'grad_norm': 0.375, 'learning_rate': 0.0003, 'epoch': 0.82}\n",
      "{'loss': 1.804, 'grad_norm': 1.8828125, 'learning_rate': 0.0003, 'epoch': 0.83}\n",
      "{'loss': 1.6136, 'grad_norm': 0.91015625, 'learning_rate': 0.0003, 'epoch': 0.83}\n",
      "{'loss': 1.7411, 'grad_norm': 0.421875, 'learning_rate': 0.0003, 'epoch': 0.83}\n",
      "{'loss': 1.9259, 'grad_norm': 0.88671875, 'learning_rate': 0.0003, 'epoch': 0.83}\n",
      "{'loss': 1.4933, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 0.83}\n",
      "{'loss': 1.6639, 'grad_norm': 0.94140625, 'learning_rate': 0.0003, 'epoch': 0.84}\n",
      "{'loss': 1.6029, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 0.84}\n",
      "{'loss': 1.7717, 'grad_norm': 0.296875, 'learning_rate': 0.0003, 'epoch': 0.84}\n",
      "{'loss': 1.7267, 'grad_norm': 0.68359375, 'learning_rate': 0.0003, 'epoch': 0.84}\n",
      "{'loss': 1.4145, 'grad_norm': 0.498046875, 'learning_rate': 0.0003, 'epoch': 0.84}\n",
      "{'loss': 1.6423, 'grad_norm': 0.77734375, 'learning_rate': 0.0003, 'epoch': 0.85}\n",
      "{'loss': 1.7528, 'grad_norm': 0.484375, 'learning_rate': 0.0003, 'epoch': 0.85}\n",
      "{'loss': 1.7416, 'grad_norm': 0.349609375, 'learning_rate': 0.0003, 'epoch': 0.85}\n",
      "{'loss': 1.7398, 'grad_norm': 0.6015625, 'learning_rate': 0.0003, 'epoch': 0.85}\n",
      "{'loss': 1.7487, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 0.85}\n",
      "{'loss': 1.6926, 'grad_norm': 0.8984375, 'learning_rate': 0.0003, 'epoch': 0.86}\n",
      "{'loss': 1.748, 'grad_norm': 0.51171875, 'learning_rate': 0.0003, 'epoch': 0.86}\n",
      "{'loss': 1.6623, 'grad_norm': 0.67578125, 'learning_rate': 0.0003, 'epoch': 0.86}\n",
      "{'loss': 1.74, 'grad_norm': 0.6953125, 'learning_rate': 0.0003, 'epoch': 0.86}\n",
      "{'loss': 1.6794, 'grad_norm': 0.5234375, 'learning_rate': 0.0003, 'epoch': 0.86}\n",
      "{'loss': 1.5245, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 0.86}\n",
      "{'loss': 1.6463, 'grad_norm': 0.69921875, 'learning_rate': 0.0003, 'epoch': 0.87}\n",
      "{'loss': 1.6007, 'grad_norm': 0.51171875, 'learning_rate': 0.0003, 'epoch': 0.87}\n",
      "{'loss': 1.6703, 'grad_norm': 0.5859375, 'learning_rate': 0.0003, 'epoch': 0.87}\n",
      "{'loss': 1.591, 'grad_norm': 0.4765625, 'learning_rate': 0.0003, 'epoch': 0.87}\n",
      "{'loss': 1.6851, 'grad_norm': 1.28125, 'learning_rate': 0.0003, 'epoch': 0.87}\n",
      "{'loss': 1.738, 'grad_norm': 0.73046875, 'learning_rate': 0.0003, 'epoch': 0.88}\n",
      "{'loss': 1.5843, 'grad_norm': 0.36328125, 'learning_rate': 0.0003, 'epoch': 0.88}\n",
      "{'loss': 1.7331, 'grad_norm': 0.78125, 'learning_rate': 0.0003, 'epoch': 0.88}\n",
      "{'loss': 1.606, 'grad_norm': 0.439453125, 'learning_rate': 0.0003, 'epoch': 0.88}\n",
      "{'loss': 1.7217, 'grad_norm': 1.3125, 'learning_rate': 0.0003, 'epoch': 0.88}\n",
      "{'loss': 1.7892, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 0.89}\n",
      "{'loss': 1.6001, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 0.89}\n",
      "{'loss': 1.67, 'grad_norm': 0.76171875, 'learning_rate': 0.0003, 'epoch': 0.89}\n",
      "{'loss': 1.6521, 'grad_norm': 0.361328125, 'learning_rate': 0.0003, 'epoch': 0.89}\n",
      "{'loss': 1.7298, 'grad_norm': 1.296875, 'learning_rate': 0.0003, 'epoch': 0.89}\n",
      "{'loss': 1.6987, 'grad_norm': 0.875, 'learning_rate': 0.0003, 'epoch': 0.9}\n",
      "{'loss': 1.5314, 'grad_norm': 0.6640625, 'learning_rate': 0.0003, 'epoch': 0.9}\n",
      "{'loss': 1.7447, 'grad_norm': 1.2890625, 'learning_rate': 0.0003, 'epoch': 0.9}\n",
      "{'loss': 1.732, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 0.9}\n",
      "{'loss': 1.8404, 'grad_norm': 2.09375, 'learning_rate': 0.0003, 'epoch': 0.9}\n",
      "{'loss': 1.6684, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 0.91}\n",
      "{'loss': 1.6405, 'grad_norm': 0.2119140625, 'learning_rate': 0.0003, 'epoch': 0.91}\n",
      "{'loss': 1.7588, 'grad_norm': 0.828125, 'learning_rate': 0.0003, 'epoch': 0.91}\n",
      "{'loss': 1.6959, 'grad_norm': 0.47265625, 'learning_rate': 0.0003, 'epoch': 0.91}\n",
      "{'loss': 1.6424, 'grad_norm': 0.6875, 'learning_rate': 0.0003, 'epoch': 0.91}\n",
      "{'loss': 1.6661, 'grad_norm': 0.47265625, 'learning_rate': 0.0003, 'epoch': 0.92}\n",
      "{'loss': 1.7608, 'grad_norm': 0.35546875, 'learning_rate': 0.0003, 'epoch': 0.92}\n",
      "{'loss': 1.7396, 'grad_norm': 0.82421875, 'learning_rate': 0.0003, 'epoch': 0.92}\n",
      "{'loss': 1.5727, 'grad_norm': 0.4609375, 'learning_rate': 0.0003, 'epoch': 0.92}\n",
      "{'loss': 1.7491, 'grad_norm': 1.765625, 'learning_rate': 0.0003, 'epoch': 0.92}\n",
      "{'loss': 1.6226, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 0.93}\n",
      "{'loss': 1.6439, 'grad_norm': 0.46875, 'learning_rate': 0.0003, 'epoch': 0.93}\n",
      "{'loss': 1.8576, 'grad_norm': 1.328125, 'learning_rate': 0.0003, 'epoch': 0.93}\n",
      "{'loss': 1.7179, 'grad_norm': 0.369140625, 'learning_rate': 0.0003, 'epoch': 0.93}\n",
      "{'loss': 1.7093, 'grad_norm': 1.03125, 'learning_rate': 0.0003, 'epoch': 0.93}\n",
      "{'loss': 1.6534, 'grad_norm': 0.59375, 'learning_rate': 0.0003, 'epoch': 0.94}\n",
      "{'loss': 1.6981, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 0.94}\n",
      "{'loss': 1.7419, 'grad_norm': 0.8125, 'learning_rate': 0.0003, 'epoch': 0.94}\n",
      "{'loss': 1.7374, 'grad_norm': 0.49609375, 'learning_rate': 0.0003, 'epoch': 0.94}\n",
      "{'loss': 1.7107, 'grad_norm': 0.90625, 'learning_rate': 0.0003, 'epoch': 0.94}\n",
      "{'loss': 1.7634, 'grad_norm': 0.8515625, 'learning_rate': 0.0003, 'epoch': 0.95}\n",
      "{'loss': 1.6777, 'grad_norm': 0.71875, 'learning_rate': 0.0003, 'epoch': 0.95}\n",
      "{'loss': 1.6665, 'grad_norm': 0.75, 'learning_rate': 0.0003, 'epoch': 0.95}\n",
      "{'loss': 1.5049, 'grad_norm': 0.58984375, 'learning_rate': 0.0003, 'epoch': 0.95}\n",
      "{'loss': 1.8739, 'grad_norm': 1.515625, 'learning_rate': 0.0003, 'epoch': 0.95}\n",
      "{'loss': 1.6462, 'grad_norm': 0.6875, 'learning_rate': 0.0003, 'epoch': 0.96}\n",
      "{'loss': 1.6356, 'grad_norm': 0.400390625, 'learning_rate': 0.0003, 'epoch': 0.96}\n",
      "{'loss': 1.8114, 'grad_norm': 0.75390625, 'learning_rate': 0.0003, 'epoch': 0.96}\n",
      "{'loss': 1.5693, 'grad_norm': 0.42578125, 'learning_rate': 0.0003, 'epoch': 0.96}\n",
      "{'loss': 1.7059, 'grad_norm': 2.453125, 'learning_rate': 0.0003, 'epoch': 0.96}\n",
      "{'loss': 1.6409, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 0.97}\n",
      "{'loss': 1.521, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 0.97}\n",
      "{'loss': 1.661, 'grad_norm': 1.296875, 'learning_rate': 0.0003, 'epoch': 0.97}\n",
      "{'loss': 1.5682, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 0.97}\n",
      "{'loss': 1.7594, 'grad_norm': 2.375, 'learning_rate': 0.0003, 'epoch': 0.97}\n",
      "{'loss': 1.7344, 'grad_norm': 0.734375, 'learning_rate': 0.0003, 'epoch': 0.97}\n",
      "{'loss': 1.5674, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 0.98}\n",
      "{'loss': 1.7053, 'grad_norm': 0.9921875, 'learning_rate': 0.0003, 'epoch': 0.98}\n",
      "{'loss': 1.6396, 'grad_norm': 0.5234375, 'learning_rate': 0.0003, 'epoch': 0.98}\n",
      "{'loss': 1.6424, 'grad_norm': 1.5625, 'learning_rate': 0.0003, 'epoch': 0.98}\n",
      "{'loss': 1.6001, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 0.98}\n",
      "{'loss': 1.6597, 'grad_norm': 0.466796875, 'learning_rate': 0.0003, 'epoch': 0.99}\n",
      "{'loss': 1.8121, 'grad_norm': 0.94140625, 'learning_rate': 0.0003, 'epoch': 0.99}\n",
      "{'loss': 1.5487, 'grad_norm': 0.384765625, 'learning_rate': 0.0003, 'epoch': 0.99}\n",
      "{'loss': 1.6368, 'grad_norm': 0.6953125, 'learning_rate': 0.0003, 'epoch': 0.99}\n",
      "{'loss': 1.7028, 'grad_norm': 0.8203125, 'learning_rate': 0.0003, 'epoch': 0.99}\n",
      "{'loss': 1.7397, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 1.0}\n",
      "{'loss': 1.6848, 'grad_norm': 1.609375, 'learning_rate': 0.0003, 'epoch': 1.0}\n",
      "{'loss': 1.6852, 'grad_norm': 0.328125, 'learning_rate': 0.0003, 'epoch': 1.0}\n",
      "{'loss': 1.5505, 'grad_norm': 0.59375, 'learning_rate': 0.0003, 'epoch': 1.0}\n",
      "{'loss': 1.3774, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 1.0}\n",
      "{'loss': 1.7536, 'grad_norm': 1.0859375, 'learning_rate': 0.0003, 'epoch': 1.01}\n",
      "{'loss': 1.6928, 'grad_norm': 0.466796875, 'learning_rate': 0.0003, 'epoch': 1.01}\n",
      "{'loss': 1.6221, 'grad_norm': 0.4921875, 'learning_rate': 0.0003, 'epoch': 1.01}\n",
      "{'loss': 1.5218, 'grad_norm': 0.72265625, 'learning_rate': 0.0003, 'epoch': 1.01}\n",
      "{'loss': 1.5675, 'grad_norm': 0.46484375, 'learning_rate': 0.0003, 'epoch': 1.01}\n",
      "{'loss': 1.5948, 'grad_norm': 0.78515625, 'learning_rate': 0.0003, 'epoch': 1.02}\n",
      "{'loss': 1.3783, 'grad_norm': 0.453125, 'learning_rate': 0.0003, 'epoch': 1.02}\n",
      "{'loss': 1.5404, 'grad_norm': 0.369140625, 'learning_rate': 0.0003, 'epoch': 1.02}\n",
      "{'loss': 1.5224, 'grad_norm': 0.8359375, 'learning_rate': 0.0003, 'epoch': 1.02}\n",
      "{'loss': 1.4625, 'grad_norm': 0.85546875, 'learning_rate': 0.0003, 'epoch': 1.02}\n",
      "{'loss': 1.5221, 'grad_norm': 1.4375, 'learning_rate': 0.0003, 'epoch': 1.03}\n",
      "{'loss': 1.5405, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 1.03}\n",
      "{'loss': 1.3807, 'grad_norm': 0.47265625, 'learning_rate': 0.0003, 'epoch': 1.03}\n",
      "{'loss': 1.4981, 'grad_norm': 0.7890625, 'learning_rate': 0.0003, 'epoch': 1.03}\n",
      "{'loss': 1.3585, 'grad_norm': 0.392578125, 'learning_rate': 0.0003, 'epoch': 1.03}\n",
      "{'loss': 1.4507, 'grad_norm': 1.8125, 'learning_rate': 0.0003, 'epoch': 1.04}\n",
      "{'loss': 1.2913, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 1.04}\n",
      "{'loss': 1.3848, 'grad_norm': 0.419921875, 'learning_rate': 0.0003, 'epoch': 1.04}\n",
      "{'loss': 1.5571, 'grad_norm': 0.6875, 'learning_rate': 0.0003, 'epoch': 1.04}\n",
      "{'loss': 1.3674, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 1.04}\n",
      "{'loss': 1.503, 'grad_norm': 0.76953125, 'learning_rate': 0.0003, 'epoch': 1.05}\n",
      "{'loss': 1.6422, 'grad_norm': 0.92578125, 'learning_rate': 0.0003, 'epoch': 1.05}\n",
      "{'loss': 1.5588, 'grad_norm': 0.63671875, 'learning_rate': 0.0003, 'epoch': 1.05}\n",
      "{'loss': 1.6044, 'grad_norm': 0.8671875, 'learning_rate': 0.0003, 'epoch': 1.05}\n",
      "{'loss': 1.4538, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 1.05}\n",
      "{'loss': 1.5397, 'grad_norm': 0.9921875, 'learning_rate': 0.0003, 'epoch': 1.06}\n",
      "{'loss': 1.58, 'grad_norm': 0.6171875, 'learning_rate': 0.0003, 'epoch': 1.06}\n",
      "{'loss': 1.368, 'grad_norm': 0.466796875, 'learning_rate': 0.0003, 'epoch': 1.06}\n",
      "{'loss': 1.5376, 'grad_norm': 0.86328125, 'learning_rate': 0.0003, 'epoch': 1.06}\n",
      "{'loss': 1.3231, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 1.06}\n",
      "{'loss': 1.5048, 'grad_norm': 1.21875, 'learning_rate': 0.0003, 'epoch': 1.07}\n",
      "{'loss': 1.486, 'grad_norm': 0.3828125, 'learning_rate': 0.0003, 'epoch': 1.07}\n",
      "{'loss': 1.3432, 'grad_norm': 0.5625, 'learning_rate': 0.0003, 'epoch': 1.07}\n",
      "{'loss': 1.6439, 'grad_norm': 0.71484375, 'learning_rate': 0.0003, 'epoch': 1.07}\n",
      "{'loss': 1.4324, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 1.07}\n",
      "{'loss': 1.6242, 'grad_norm': 1.2265625, 'learning_rate': 0.0003, 'epoch': 1.08}\n",
      "{'loss': 1.4403, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 1.08}\n",
      "{'loss': 1.5641, 'grad_norm': 0.7109375, 'learning_rate': 0.0003, 'epoch': 1.08}\n",
      "{'loss': 1.4752, 'grad_norm': 0.890625, 'learning_rate': 0.0003, 'epoch': 1.08}\n",
      "{'loss': 1.4419, 'grad_norm': 0.482421875, 'learning_rate': 0.0003, 'epoch': 1.08}\n",
      "{'loss': 1.6313, 'grad_norm': 1.0234375, 'learning_rate': 0.0003, 'epoch': 1.08}\n",
      "{'loss': 1.5998, 'grad_norm': 0.6640625, 'learning_rate': 0.0003, 'epoch': 1.09}\n",
      "{'loss': 1.4956, 'grad_norm': 0.40625, 'learning_rate': 0.0003, 'epoch': 1.09}\n",
      "{'loss': 1.5112, 'grad_norm': 0.7421875, 'learning_rate': 0.0003, 'epoch': 1.09}\n",
      "{'loss': 1.4161, 'grad_norm': 0.68359375, 'learning_rate': 0.0003, 'epoch': 1.09}\n",
      "{'loss': 1.5997, 'grad_norm': 1.4921875, 'learning_rate': 0.0003, 'epoch': 1.09}\n",
      "{'loss': 1.6942, 'grad_norm': 0.6328125, 'learning_rate': 0.0003, 'epoch': 1.1}\n",
      "{'loss': 1.5758, 'grad_norm': 0.384765625, 'learning_rate': 0.0003, 'epoch': 1.1}\n",
      "{'loss': 1.5768, 'grad_norm': 1.0703125, 'learning_rate': 0.0003, 'epoch': 1.1}\n",
      "{'loss': 1.6412, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 1.1}\n",
      "{'loss': 1.6538, 'grad_norm': 1.3125, 'learning_rate': 0.0003, 'epoch': 1.1}\n",
      "{'loss': 1.5001, 'grad_norm': 0.58984375, 'learning_rate': 0.0003, 'epoch': 1.11}\n",
      "{'loss': 1.487, 'grad_norm': 0.59765625, 'learning_rate': 0.0003, 'epoch': 1.11}\n",
      "{'loss': 1.5405, 'grad_norm': 0.97265625, 'learning_rate': 0.0003, 'epoch': 1.11}\n",
      "{'loss': 1.5055, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 1.11}\n",
      "{'loss': 1.4493, 'grad_norm': 1.7734375, 'learning_rate': 0.0003, 'epoch': 1.11}\n",
      "{'loss': 1.526, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 1.12}\n",
      "{'loss': 1.5425, 'grad_norm': 0.5234375, 'learning_rate': 0.0003, 'epoch': 1.12}\n",
      "{'loss': 1.5381, 'grad_norm': 0.68359375, 'learning_rate': 0.0003, 'epoch': 1.12}\n",
      "{'loss': 1.4193, 'grad_norm': 0.4296875, 'learning_rate': 0.0003, 'epoch': 1.12}\n",
      "{'loss': 1.5128, 'grad_norm': 1.546875, 'learning_rate': 0.0003, 'epoch': 1.12}\n",
      "{'loss': 1.587, 'grad_norm': 0.6015625, 'learning_rate': 0.0003, 'epoch': 1.13}\n",
      "{'loss': 1.5826, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 1.13}\n",
      "{'loss': 1.5525, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 1.13}\n",
      "{'loss': 1.5427, 'grad_norm': 0.61328125, 'learning_rate': 0.0003, 'epoch': 1.13}\n",
      "{'loss': 1.6024, 'grad_norm': 1.09375, 'learning_rate': 0.0003, 'epoch': 1.13}\n",
      "{'loss': 1.506, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 1.14}\n",
      "{'loss': 1.5244, 'grad_norm': 0.53515625, 'learning_rate': 0.0003, 'epoch': 1.14}\n",
      "{'loss': 1.6035, 'grad_norm': 0.625, 'learning_rate': 0.0003, 'epoch': 1.14}\n",
      "{'loss': 1.5207, 'grad_norm': 0.57421875, 'learning_rate': 0.0003, 'epoch': 1.14}\n",
      "{'loss': 1.5674, 'grad_norm': 0.96875, 'learning_rate': 0.0003, 'epoch': 1.14}\n",
      "{'loss': 1.4574, 'grad_norm': 0.57421875, 'learning_rate': 0.0003, 'epoch': 1.15}\n",
      "{'loss': 1.4704, 'grad_norm': 0.53515625, 'learning_rate': 0.0003, 'epoch': 1.15}\n",
      "{'loss': 1.6228, 'grad_norm': 0.8984375, 'learning_rate': 0.0003, 'epoch': 1.15}\n",
      "{'loss': 1.4177, 'grad_norm': 0.671875, 'learning_rate': 0.0003, 'epoch': 1.15}\n",
      "{'loss': 1.5301, 'grad_norm': 1.171875, 'learning_rate': 0.0003, 'epoch': 1.15}\n",
      "{'loss': 1.6, 'grad_norm': 0.671875, 'learning_rate': 0.0003, 'epoch': 1.16}\n",
      "{'loss': 1.5865, 'grad_norm': 0.451171875, 'learning_rate': 0.0003, 'epoch': 1.16}\n",
      "{'loss': 1.4478, 'grad_norm': 0.9765625, 'learning_rate': 0.0003, 'epoch': 1.16}\n",
      "{'loss': 1.402, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 1.16}\n",
      "{'loss': 1.5066, 'grad_norm': 1.59375, 'learning_rate': 0.0003, 'epoch': 1.16}\n",
      "{'loss': 1.6663, 'grad_norm': 1.53125, 'learning_rate': 0.0003, 'epoch': 1.17}\n",
      "{'loss': 1.4617, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 1.17}\n",
      "{'loss': 1.5308, 'grad_norm': 0.8671875, 'learning_rate': 0.0003, 'epoch': 1.17}\n",
      "{'loss': 1.553, 'grad_norm': 0.4921875, 'learning_rate': 0.0003, 'epoch': 1.17}\n",
      "{'loss': 1.4632, 'grad_norm': 1.09375, 'learning_rate': 0.0003, 'epoch': 1.17}\n",
      "{'loss': 1.676, 'grad_norm': 0.7265625, 'learning_rate': 0.0003, 'epoch': 1.18}\n",
      "{'loss': 1.3895, 'grad_norm': 0.51171875, 'learning_rate': 0.0003, 'epoch': 1.18}\n",
      "{'loss': 1.5116, 'grad_norm': 0.84765625, 'learning_rate': 0.0003, 'epoch': 1.18}\n",
      "{'loss': 1.5712, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 1.18}\n",
      "{'loss': 1.591, 'grad_norm': 1.9453125, 'learning_rate': 0.0003, 'epoch': 1.18}\n",
      "{'loss': 1.3788, 'grad_norm': 0.7421875, 'learning_rate': 0.0003, 'epoch': 1.19}\n",
      "{'loss': 1.5353, 'grad_norm': 0.78125, 'learning_rate': 0.0003, 'epoch': 1.19}\n",
      "{'loss': 1.5854, 'grad_norm': 0.91015625, 'learning_rate': 0.0003, 'epoch': 1.19}\n",
      "{'loss': 1.4653, 'grad_norm': 0.490234375, 'learning_rate': 0.0003, 'epoch': 1.19}\n",
      "{'loss': 1.5675, 'grad_norm': 1.59375, 'learning_rate': 0.0003, 'epoch': 1.19}\n",
      "{'loss': 1.5445, 'grad_norm': 0.609375, 'learning_rate': 0.0003, 'epoch': 1.2}\n",
      "{'loss': 1.3822, 'grad_norm': 0.46484375, 'learning_rate': 0.0003, 'epoch': 1.2}\n",
      "{'loss': 1.6159, 'grad_norm': 0.75, 'learning_rate': 0.0003, 'epoch': 1.2}\n",
      "{'loss': 1.4952, 'grad_norm': 0.578125, 'learning_rate': 0.0003, 'epoch': 1.2}\n",
      "{'loss': 1.6116, 'grad_norm': 0.6875, 'learning_rate': 0.0003, 'epoch': 1.2}\n",
      "{'loss': 1.5613, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 1.2}\n",
      "{'loss': 1.2891, 'grad_norm': 0.423828125, 'learning_rate': 0.0003, 'epoch': 1.21}\n",
      "{'loss': 1.503, 'grad_norm': 0.63671875, 'learning_rate': 0.0003, 'epoch': 1.21}\n",
      "{'loss': 1.6929, 'grad_norm': 0.69140625, 'learning_rate': 0.0003, 'epoch': 1.21}\n",
      "{'loss': 1.6076, 'grad_norm': 1.375, 'learning_rate': 0.0003, 'epoch': 1.21}\n",
      "{'loss': 1.5546, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 1.21}\n",
      "{'loss': 1.6344, 'grad_norm': 0.40234375, 'learning_rate': 0.0003, 'epoch': 1.22}\n",
      "{'loss': 1.6399, 'grad_norm': 0.76171875, 'learning_rate': 0.0003, 'epoch': 1.22}\n",
      "{'loss': 1.5588, 'grad_norm': 0.65625, 'learning_rate': 0.0003, 'epoch': 1.22}\n",
      "{'loss': 1.6193, 'grad_norm': 1.2890625, 'learning_rate': 0.0003, 'epoch': 1.22}\n",
      "{'loss': 1.6815, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 1.22}\n",
      "{'loss': 1.6165, 'grad_norm': 0.59765625, 'learning_rate': 0.0003, 'epoch': 1.23}\n",
      "{'loss': 1.5737, 'grad_norm': 1.03125, 'learning_rate': 0.0003, 'epoch': 1.23}\n",
      "{'loss': 1.5442, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 1.23}\n",
      "{'loss': 1.5263, 'grad_norm': 1.25, 'learning_rate': 0.0003, 'epoch': 1.23}\n",
      "{'loss': 1.4436, 'grad_norm': 0.81640625, 'learning_rate': 0.0003, 'epoch': 1.23}\n",
      "{'loss': 1.7005, 'grad_norm': 0.703125, 'learning_rate': 0.0003, 'epoch': 1.24}\n",
      "{'loss': 1.6602, 'grad_norm': 0.84375, 'learning_rate': 0.0003, 'epoch': 1.24}\n",
      "{'loss': 1.5568, 'grad_norm': 0.5625, 'learning_rate': 0.0003, 'epoch': 1.24}\n",
      "{'loss': 1.5657, 'grad_norm': 1.421875, 'learning_rate': 0.0003, 'epoch': 1.24}\n",
      "{'loss': 1.4696, 'grad_norm': 0.83203125, 'learning_rate': 0.0003, 'epoch': 1.24}\n",
      "{'loss': 1.3467, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 1.25}\n",
      "{'loss': 1.5475, 'grad_norm': 0.703125, 'learning_rate': 0.0003, 'epoch': 1.25}\n",
      "{'loss': 1.4046, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 1.25}\n",
      "{'loss': 1.6013, 'grad_norm': 1.15625, 'learning_rate': 0.0003, 'epoch': 1.25}\n",
      "{'loss': 1.5328, 'grad_norm': 0.60546875, 'learning_rate': 0.0003, 'epoch': 1.25}\n",
      "{'loss': 1.5855, 'grad_norm': 0.416015625, 'learning_rate': 0.0003, 'epoch': 1.26}\n",
      "{'loss': 1.7355, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 1.26}\n",
      "{'loss': 1.7336, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 1.26}\n",
      "{'loss': 1.5613, 'grad_norm': 1.171875, 'learning_rate': 0.0003, 'epoch': 1.26}\n",
      "{'loss': 1.4045, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 1.26}\n",
      "{'loss': 1.5382, 'grad_norm': 0.46875, 'learning_rate': 0.0003, 'epoch': 1.27}\n",
      "{'loss': 1.6271, 'grad_norm': 0.67578125, 'learning_rate': 0.0003, 'epoch': 1.27}\n",
      "{'loss': 1.6371, 'grad_norm': 0.81640625, 'learning_rate': 0.0003, 'epoch': 1.27}\n",
      "{'loss': 1.549, 'grad_norm': 1.1328125, 'learning_rate': 0.0003, 'epoch': 1.27}\n",
      "{'loss': 1.5324, 'grad_norm': 0.609375, 'learning_rate': 0.0003, 'epoch': 1.27}\n",
      "{'loss': 1.483, 'grad_norm': 0.5625, 'learning_rate': 0.0003, 'epoch': 1.28}\n",
      "{'loss': 1.771, 'grad_norm': 0.86328125, 'learning_rate': 0.0003, 'epoch': 1.28}\n",
      "{'loss': 1.505, 'grad_norm': 0.458984375, 'learning_rate': 0.0003, 'epoch': 1.28}\n",
      "{'loss': 1.5101, 'grad_norm': 1.4765625, 'learning_rate': 0.0003, 'epoch': 1.28}\n",
      "{'loss': 1.542, 'grad_norm': 0.57421875, 'learning_rate': 0.0003, 'epoch': 1.28}\n",
      "{'loss': 1.6662, 'grad_norm': 0.53515625, 'learning_rate': 0.0003, 'epoch': 1.29}\n",
      "{'loss': 1.6366, 'grad_norm': 0.9375, 'learning_rate': 0.0003, 'epoch': 1.29}\n",
      "{'loss': 1.533, 'grad_norm': 0.5234375, 'learning_rate': 0.0003, 'epoch': 1.29}\n",
      "{'loss': 1.4795, 'grad_norm': 1.4609375, 'learning_rate': 0.0003, 'epoch': 1.29}\n",
      "{'loss': 1.6115, 'grad_norm': 0.61328125, 'learning_rate': 0.0003, 'epoch': 1.29}\n",
      "{'loss': 1.596, 'grad_norm': 0.92578125, 'learning_rate': 0.0003, 'epoch': 1.3}\n",
      "{'loss': 1.5758, 'grad_norm': 1.3125, 'learning_rate': 0.0003, 'epoch': 1.3}\n",
      "{'loss': 1.3668, 'grad_norm': 0.59765625, 'learning_rate': 0.0003, 'epoch': 1.3}\n",
      "{'loss': 1.5301, 'grad_norm': 0.828125, 'learning_rate': 0.0003, 'epoch': 1.3}\n",
      "{'loss': 1.6019, 'grad_norm': 0.83984375, 'learning_rate': 0.0003, 'epoch': 1.3}\n",
      "{'loss': 1.5057, 'grad_norm': 0.3515625, 'learning_rate': 0.0003, 'epoch': 1.31}\n",
      "{'loss': 1.6581, 'grad_norm': 0.81640625, 'learning_rate': 0.0003, 'epoch': 1.31}\n",
      "{'loss': 1.3372, 'grad_norm': 0.50390625, 'learning_rate': 0.0003, 'epoch': 1.31}\n",
      "{'loss': 1.5322, 'grad_norm': 0.890625, 'learning_rate': 0.0003, 'epoch': 1.31}\n",
      "{'loss': 1.5967, 'grad_norm': 0.72265625, 'learning_rate': 0.0003, 'epoch': 1.31}\n",
      "{'loss': 1.6198, 'grad_norm': 0.375, 'learning_rate': 0.0003, 'epoch': 1.31}\n",
      "{'loss': 1.6041, 'grad_norm': 1.1484375, 'learning_rate': 0.0003, 'epoch': 1.32}\n",
      "{'loss': 1.4616, 'grad_norm': 0.625, 'learning_rate': 0.0003, 'epoch': 1.32}\n",
      "{'loss': 1.4749, 'grad_norm': 1.171875, 'learning_rate': 0.0003, 'epoch': 1.32}\n",
      "{'loss': 1.4636, 'grad_norm': 0.75, 'learning_rate': 0.0003, 'epoch': 1.32}\n",
      "{'loss': 1.46, 'grad_norm': 0.455078125, 'learning_rate': 0.0003, 'epoch': 1.32}\n",
      "{'loss': 1.4371, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 1.33}\n",
      "{'loss': 1.5823, 'grad_norm': 0.5234375, 'learning_rate': 0.0003, 'epoch': 1.33}\n",
      "{'loss': 1.4727, 'grad_norm': 1.65625, 'learning_rate': 0.0003, 'epoch': 1.33}\n",
      "{'loss': 1.7555, 'grad_norm': 0.72265625, 'learning_rate': 0.0003, 'epoch': 1.33}\n",
      "{'loss': 1.6616, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 1.33}\n",
      "{'loss': 1.6333, 'grad_norm': 0.98046875, 'learning_rate': 0.0003, 'epoch': 1.34}\n",
      "{'loss': 1.5682, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 1.34}\n",
      "{'loss': 1.6019, 'grad_norm': 0.8984375, 'learning_rate': 0.0003, 'epoch': 1.34}\n",
      "{'loss': 1.5043, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 1.34}\n",
      "{'loss': 1.4411, 'grad_norm': 0.44921875, 'learning_rate': 0.0003, 'epoch': 1.34}\n",
      "{'loss': 1.5254, 'grad_norm': 0.9296875, 'learning_rate': 0.0003, 'epoch': 1.35}\n",
      "{'loss': 1.6023, 'grad_norm': 0.66015625, 'learning_rate': 0.0003, 'epoch': 1.35}\n",
      "{'loss': 1.5085, 'grad_norm': 1.75, 'learning_rate': 0.0003, 'epoch': 1.35}\n",
      "{'loss': 1.5585, 'grad_norm': 0.81640625, 'learning_rate': 0.0003, 'epoch': 1.35}\n",
      "{'loss': 1.6194, 'grad_norm': 0.46484375, 'learning_rate': 0.0003, 'epoch': 1.35}\n",
      "{'loss': 1.5622, 'grad_norm': 0.89453125, 'learning_rate': 0.0003, 'epoch': 1.36}\n",
      "{'loss': 1.3194, 'grad_norm': 0.453125, 'learning_rate': 0.0003, 'epoch': 1.36}\n",
      "{'loss': 1.6431, 'grad_norm': 2.671875, 'learning_rate': 0.0003, 'epoch': 1.36}\n",
      "{'loss': 1.5405, 'grad_norm': 0.9921875, 'learning_rate': 0.0003, 'epoch': 1.36}\n",
      "{'loss': 1.4841, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 1.36}\n",
      "{'loss': 1.5092, 'grad_norm': 0.94140625, 'learning_rate': 0.0003, 'epoch': 1.37}\n",
      "{'loss': 1.709, 'grad_norm': 0.66015625, 'learning_rate': 0.0003, 'epoch': 1.37}\n",
      "{'loss': 1.5814, 'grad_norm': 1.5078125, 'learning_rate': 0.0003, 'epoch': 1.37}\n",
      "{'loss': 1.6021, 'grad_norm': 0.7265625, 'learning_rate': 0.0003, 'epoch': 1.37}\n",
      "{'loss': 1.4391, 'grad_norm': 0.451171875, 'learning_rate': 0.0003, 'epoch': 1.37}\n",
      "{'loss': 1.6164, 'grad_norm': 1.3828125, 'learning_rate': 0.0003, 'epoch': 1.38}\n",
      "{'loss': 1.584, 'grad_norm': 0.7421875, 'learning_rate': 0.0003, 'epoch': 1.38}\n",
      "{'loss': 1.6534, 'grad_norm': 1.1171875, 'learning_rate': 0.0003, 'epoch': 1.38}\n",
      "{'loss': 1.6993, 'grad_norm': 0.75, 'learning_rate': 0.0003, 'epoch': 1.38}\n",
      "{'loss': 1.4901, 'grad_norm': 0.57421875, 'learning_rate': 0.0003, 'epoch': 1.38}\n",
      "{'loss': 1.4127, 'grad_norm': 0.8203125, 'learning_rate': 0.0003, 'epoch': 1.39}\n",
      "{'loss': 1.4377, 'grad_norm': 0.66015625, 'learning_rate': 0.0003, 'epoch': 1.39}\n",
      "{'loss': 1.5856, 'grad_norm': 1.359375, 'learning_rate': 0.0003, 'epoch': 1.39}\n",
      "{'loss': 1.3595, 'grad_norm': 1.5, 'learning_rate': 0.0003, 'epoch': 1.39}\n",
      "{'loss': 1.5451, 'grad_norm': 0.70703125, 'learning_rate': 0.0003, 'epoch': 1.39}\n",
      "{'loss': 1.439, 'grad_norm': 1.5625, 'learning_rate': 0.0003, 'epoch': 1.4}\n",
      "{'loss': 1.6541, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 1.4}\n",
      "{'loss': 1.5231, 'grad_norm': 1.390625, 'learning_rate': 0.0003, 'epoch': 1.4}\n",
      "{'loss': 1.5741, 'grad_norm': 0.74609375, 'learning_rate': 0.0003, 'epoch': 1.4}\n",
      "{'loss': 1.6757, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 1.4}\n",
      "{'loss': 1.6357, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 1.41}\n",
      "{'loss': 1.5013, 'grad_norm': 0.609375, 'learning_rate': 0.0003, 'epoch': 1.41}\n",
      "{'loss': 1.5123, 'grad_norm': 1.2578125, 'learning_rate': 0.0003, 'epoch': 1.41}\n",
      "{'loss': 1.6293, 'grad_norm': 0.81640625, 'learning_rate': 0.0003, 'epoch': 1.41}\n",
      "{'loss': 1.6031, 'grad_norm': 0.3671875, 'learning_rate': 0.0003, 'epoch': 1.41}\n",
      "{'loss': 1.6423, 'grad_norm': 1.6328125, 'learning_rate': 0.0003, 'epoch': 1.42}\n",
      "{'loss': 1.6212, 'grad_norm': 0.59375, 'learning_rate': 0.0003, 'epoch': 1.42}\n",
      "{'loss': 1.5728, 'grad_norm': 1.140625, 'learning_rate': 0.0003, 'epoch': 1.42}\n",
      "{'loss': 1.4926, 'grad_norm': 0.55859375, 'learning_rate': 0.0003, 'epoch': 1.42}\n",
      "{'loss': 1.4413, 'grad_norm': 0.45703125, 'learning_rate': 0.0003, 'epoch': 1.42}\n",
      "{'loss': 1.6993, 'grad_norm': 1.2109375, 'learning_rate': 0.0003, 'epoch': 1.43}\n",
      "{'loss': 1.4017, 'grad_norm': 0.63671875, 'learning_rate': 0.0003, 'epoch': 1.43}\n",
      "{'loss': 1.4727, 'grad_norm': 1.1796875, 'learning_rate': 0.0003, 'epoch': 1.43}\n",
      "{'loss': 1.5526, 'grad_norm': 0.6796875, 'learning_rate': 0.0003, 'epoch': 1.43}\n",
      "{'loss': 1.2472, 'grad_norm': 0.4453125, 'learning_rate': 0.0003, 'epoch': 1.43}\n",
      "{'loss': 1.5938, 'grad_norm': 1.0390625, 'learning_rate': 0.0003, 'epoch': 1.43}\n",
      "{'loss': 1.5766, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 1.44}\n",
      "{'loss': 1.5386, 'grad_norm': 1.4765625, 'learning_rate': 0.0003, 'epoch': 1.44}\n",
      "{'loss': 1.5994, 'grad_norm': 0.62890625, 'learning_rate': 0.0003, 'epoch': 1.44}\n",
      "{'loss': 1.458, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 1.44}\n",
      "{'loss': 1.5258, 'grad_norm': 0.87890625, 'learning_rate': 0.0003, 'epoch': 1.44}\n",
      "{'loss': 1.3698, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 1.45}\n",
      "{'loss': 1.6547, 'grad_norm': 1.7578125, 'learning_rate': 0.0003, 'epoch': 1.45}\n",
      "{'loss': 1.5345, 'grad_norm': 0.671875, 'learning_rate': 0.0003, 'epoch': 1.45}\n",
      "{'loss': 1.4285, 'grad_norm': 0.416015625, 'learning_rate': 0.0003, 'epoch': 1.45}\n",
      "{'loss': 1.6617, 'grad_norm': 1.6640625, 'learning_rate': 0.0003, 'epoch': 1.45}\n",
      "{'loss': 1.4284, 'grad_norm': 0.5625, 'learning_rate': 0.0003, 'epoch': 1.46}\n",
      "{'loss': 1.4918, 'grad_norm': 1.359375, 'learning_rate': 0.0003, 'epoch': 1.46}\n",
      "{'loss': 1.6288, 'grad_norm': 0.8359375, 'learning_rate': 0.0003, 'epoch': 1.46}\n",
      "{'loss': 1.5703, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 1.46}\n",
      "{'loss': 1.5774, 'grad_norm': 0.765625, 'learning_rate': 0.0003, 'epoch': 1.46}\n",
      "{'loss': 1.5027, 'grad_norm': 0.439453125, 'learning_rate': 0.0003, 'epoch': 1.47}\n",
      "{'loss': 1.4447, 'grad_norm': 0.94140625, 'learning_rate': 0.0003, 'epoch': 1.47}\n",
      "{'loss': 1.6206, 'grad_norm': 1.0390625, 'learning_rate': 0.0003, 'epoch': 1.47}\n",
      "{'loss': 1.5482, 'grad_norm': 0.56640625, 'learning_rate': 0.0003, 'epoch': 1.47}\n",
      "{'loss': 1.6515, 'grad_norm': 1.015625, 'learning_rate': 0.0003, 'epoch': 1.47}\n",
      "{'loss': 1.4932, 'grad_norm': 0.796875, 'learning_rate': 0.0003, 'epoch': 1.48}\n",
      "{'loss': 1.5216, 'grad_norm': 1.4140625, 'learning_rate': 0.0003, 'epoch': 1.48}\n",
      "{'loss': 1.5325, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 1.48}\n",
      "{'loss': 1.693, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 1.48}\n",
      "{'loss': 1.4914, 'grad_norm': 0.79296875, 'learning_rate': 0.0003, 'epoch': 1.48}\n",
      "{'loss': 1.5695, 'grad_norm': 0.55078125, 'learning_rate': 0.0003, 'epoch': 1.49}\n",
      "{'loss': 1.5913, 'grad_norm': 1.171875, 'learning_rate': 0.0003, 'epoch': 1.49}\n",
      "{'loss': 1.5287, 'grad_norm': 0.640625, 'learning_rate': 0.0003, 'epoch': 1.49}\n",
      "{'loss': 1.7163, 'grad_norm': 0.388671875, 'learning_rate': 0.0003, 'epoch': 1.49}\n",
      "{'loss': 1.5603, 'grad_norm': 1.015625, 'learning_rate': 0.0003, 'epoch': 1.49}\n",
      "{'loss': 1.5969, 'grad_norm': 0.64453125, 'learning_rate': 0.0003, 'epoch': 1.5}\n",
      "{'loss': 1.4197, 'grad_norm': 1.3359375, 'learning_rate': 0.0003, 'epoch': 1.5}\n",
      "{'loss': 1.575, 'grad_norm': 0.88671875, 'learning_rate': 0.0003, 'epoch': 1.5}\n",
      "{'loss': 1.5547, 'grad_norm': 0.5078125, 'learning_rate': 0.0003, 'epoch': 1.5}\n",
      "{'loss': 1.5917, 'grad_norm': 0.60546875, 'learning_rate': 0.0003, 'epoch': 1.5}\n",
      "{'loss': 1.4695, 'grad_norm': 0.55078125, 'learning_rate': 0.0003, 'epoch': 1.51}\n",
      "{'loss': 1.5132, 'grad_norm': 1.2265625, 'learning_rate': 0.0003, 'epoch': 1.51}\n",
      "{'loss': 1.5069, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 1.51}\n",
      "{'loss': 1.5075, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 1.51}\n",
      "{'loss': 1.6027, 'grad_norm': 0.875, 'learning_rate': 0.0003, 'epoch': 1.51}\n",
      "{'loss': 1.601, 'grad_norm': 0.8203125, 'learning_rate': 0.0003, 'epoch': 1.52}\n",
      "{'loss': 1.6396, 'grad_norm': 1.5390625, 'learning_rate': 0.0003, 'epoch': 1.52}\n",
      "{'loss': 1.6989, 'grad_norm': 0.55859375, 'learning_rate': 0.0003, 'epoch': 1.52}\n",
      "{'loss': 1.4923, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 1.52}\n",
      "{'loss': 1.5408, 'grad_norm': 1.0390625, 'learning_rate': 0.0003, 'epoch': 1.52}\n",
      "{'loss': 1.5536, 'grad_norm': 0.80078125, 'learning_rate': 0.0003, 'epoch': 1.53}\n",
      "{'loss': 1.5637, 'grad_norm': 1.2421875, 'learning_rate': 0.0003, 'epoch': 1.53}\n",
      "{'loss': 1.5241, 'grad_norm': 0.65234375, 'learning_rate': 0.0003, 'epoch': 1.53}\n",
      "{'loss': 1.6399, 'grad_norm': 0.5234375, 'learning_rate': 0.0003, 'epoch': 1.53}\n",
      "{'loss': 1.5935, 'grad_norm': 0.63671875, 'learning_rate': 0.0003, 'epoch': 1.53}\n",
      "{'loss': 1.407, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 1.54}\n",
      "{'loss': 1.68, 'grad_norm': 1.2890625, 'learning_rate': 0.0003, 'epoch': 1.54}\n",
      "{'loss': 1.6115, 'grad_norm': 0.83984375, 'learning_rate': 0.0003, 'epoch': 1.54}\n",
      "{'loss': 1.4885, 'grad_norm': 0.427734375, 'learning_rate': 0.0003, 'epoch': 1.54}\n",
      "{'loss': 1.6239, 'grad_norm': 0.859375, 'learning_rate': 0.0003, 'epoch': 1.54}\n",
      "{'loss': 1.4634, 'grad_norm': 0.71484375, 'learning_rate': 0.0003, 'epoch': 1.54}\n",
      "{'loss': 1.6534, 'grad_norm': 1.703125, 'learning_rate': 0.0003, 'epoch': 1.55}\n",
      "{'loss': 1.5489, 'grad_norm': 0.62890625, 'learning_rate': 0.0003, 'epoch': 1.55}\n",
      "{'loss': 1.469, 'grad_norm': 0.455078125, 'learning_rate': 0.0003, 'epoch': 1.55}\n",
      "{'loss': 1.6111, 'grad_norm': 1.1328125, 'learning_rate': 0.0003, 'epoch': 1.55}\n",
      "{'loss': 1.48, 'grad_norm': 0.5078125, 'learning_rate': 0.0003, 'epoch': 1.55}\n",
      "{'loss': 1.6378, 'grad_norm': 1.2109375, 'learning_rate': 0.0003, 'epoch': 1.56}\n",
      "{'loss': 1.5373, 'grad_norm': 1.328125, 'learning_rate': 0.0003, 'epoch': 1.56}\n",
      "{'loss': 1.4697, 'grad_norm': 0.390625, 'learning_rate': 0.0003, 'epoch': 1.56}\n",
      "{'loss': 1.5818, 'grad_norm': 0.87890625, 'learning_rate': 0.0003, 'epoch': 1.56}\n",
      "{'loss': 1.5383, 'grad_norm': 0.447265625, 'learning_rate': 0.0003, 'epoch': 1.56}\n",
      "{'loss': 1.5232, 'grad_norm': 1.2890625, 'learning_rate': 0.0003, 'epoch': 1.57}\n",
      "{'loss': 1.6252, 'grad_norm': 0.75390625, 'learning_rate': 0.0003, 'epoch': 1.57}\n",
      "{'loss': 1.4504, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 1.57}\n",
      "{'loss': 1.6214, 'grad_norm': 0.921875, 'learning_rate': 0.0003, 'epoch': 1.57}\n",
      "{'loss': 1.5658, 'grad_norm': 0.4921875, 'learning_rate': 0.0003, 'epoch': 1.57}\n",
      "{'loss': 1.4963, 'grad_norm': 1.390625, 'learning_rate': 0.0003, 'epoch': 1.58}\n",
      "{'loss': 1.5659, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 1.58}\n",
      "{'loss': 1.5303, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 1.58}\n",
      "{'loss': 1.6473, 'grad_norm': 0.91796875, 'learning_rate': 0.0003, 'epoch': 1.58}\n",
      "{'loss': 1.5754, 'grad_norm': 0.625, 'learning_rate': 0.0003, 'epoch': 1.58}\n",
      "{'loss': 1.7704, 'grad_norm': 1.0625, 'learning_rate': 0.0003, 'epoch': 1.59}\n",
      "{'loss': 1.4496, 'grad_norm': 0.8515625, 'learning_rate': 0.0003, 'epoch': 1.59}\n",
      "{'loss': 1.5347, 'grad_norm': 0.56640625, 'learning_rate': 0.0003, 'epoch': 1.59}\n",
      "{'loss': 1.7321, 'grad_norm': 0.94921875, 'learning_rate': 0.0003, 'epoch': 1.59}\n",
      "{'loss': 1.4697, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 1.59}\n",
      "{'loss': 1.5825, 'grad_norm': 1.0546875, 'learning_rate': 0.0003, 'epoch': 1.6}\n",
      "{'loss': 1.4406, 'grad_norm': 0.6953125, 'learning_rate': 0.0003, 'epoch': 1.6}\n",
      "{'loss': 1.5106, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 1.6}\n",
      "{'loss': 1.6694, 'grad_norm': 0.8203125, 'learning_rate': 0.0003, 'epoch': 1.6}\n",
      "{'loss': 1.5168, 'grad_norm': 0.6640625, 'learning_rate': 0.0003, 'epoch': 1.6}\n",
      "{'loss': 1.4202, 'grad_norm': 1.46875, 'learning_rate': 0.0003, 'epoch': 1.61}\n",
      "{'loss': 1.4317, 'grad_norm': 0.8828125, 'learning_rate': 0.0003, 'epoch': 1.61}\n",
      "{'loss': 1.4764, 'grad_norm': 0.40234375, 'learning_rate': 0.0003, 'epoch': 1.61}\n",
      "{'loss': 1.7066, 'grad_norm': 0.78515625, 'learning_rate': 0.0003, 'epoch': 1.61}\n",
      "{'loss': 1.6721, 'grad_norm': 0.6875, 'learning_rate': 0.0003, 'epoch': 1.61}\n",
      "{'loss': 1.6218, 'grad_norm': 1.0390625, 'learning_rate': 0.0003, 'epoch': 1.62}\n",
      "{'loss': 1.4381, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 1.62}\n",
      "{'loss': 1.6222, 'grad_norm': 0.48046875, 'learning_rate': 0.0003, 'epoch': 1.62}\n",
      "{'loss': 1.6104, 'grad_norm': 1.0546875, 'learning_rate': 0.0003, 'epoch': 1.62}\n",
      "{'loss': 1.6077, 'grad_norm': 0.7890625, 'learning_rate': 0.0003, 'epoch': 1.62}\n",
      "{'loss': 1.6031, 'grad_norm': 1.328125, 'learning_rate': 0.0003, 'epoch': 1.63}\n",
      "{'loss': 1.5956, 'grad_norm': 0.8203125, 'learning_rate': 0.0003, 'epoch': 1.63}\n",
      "{'loss': 1.4257, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 1.63}\n",
      "{'loss': 1.6402, 'grad_norm': 0.9453125, 'learning_rate': 0.0003, 'epoch': 1.63}\n",
      "{'loss': 1.5399, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 1.63}\n",
      "{'loss': 1.4781, 'grad_norm': 1.2890625, 'learning_rate': 0.0003, 'epoch': 1.64}\n",
      "{'loss': 1.4826, 'grad_norm': 0.5078125, 'learning_rate': 0.0003, 'epoch': 1.64}\n",
      "{'loss': 1.6626, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 1.64}\n",
      "{'loss': 1.4526, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 1.64}\n",
      "{'loss': 1.6092, 'grad_norm': 0.6015625, 'learning_rate': 0.0003, 'epoch': 1.64}\n",
      "{'loss': 1.5566, 'grad_norm': 2.671875, 'learning_rate': 0.0003, 'epoch': 1.65}\n",
      "{'loss': 1.519, 'grad_norm': 0.90625, 'learning_rate': 0.0003, 'epoch': 1.65}\n",
      "{'loss': 1.6657, 'grad_norm': 0.40234375, 'learning_rate': 0.0003, 'epoch': 1.65}\n",
      "{'loss': 1.6721, 'grad_norm': 0.95703125, 'learning_rate': 0.0003, 'epoch': 1.65}\n",
      "{'loss': 1.31, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 1.65}\n",
      "{'loss': 1.485, 'grad_norm': 0.95703125, 'learning_rate': 0.0003, 'epoch': 1.65}\n",
      "{'loss': 1.5887, 'grad_norm': 0.49609375, 'learning_rate': 0.0003, 'epoch': 1.66}\n",
      "{'loss': 1.5705, 'grad_norm': 0.474609375, 'learning_rate': 0.0003, 'epoch': 1.66}\n",
      "{'loss': 1.4784, 'grad_norm': 0.96484375, 'learning_rate': 0.0003, 'epoch': 1.66}\n",
      "{'loss': 1.4328, 'grad_norm': 0.70703125, 'learning_rate': 0.0003, 'epoch': 1.66}\n",
      "{'loss': 1.6409, 'grad_norm': 1.4609375, 'learning_rate': 0.0003, 'epoch': 1.66}\n",
      "{'loss': 1.4726, 'grad_norm': 0.76171875, 'learning_rate': 0.0003, 'epoch': 1.67}\n",
      "{'loss': 1.7228, 'grad_norm': 0.431640625, 'learning_rate': 0.0003, 'epoch': 1.67}\n",
      "{'loss': 1.594, 'grad_norm': 1.140625, 'learning_rate': 0.0003, 'epoch': 1.67}\n",
      "{'loss': 1.4514, 'grad_norm': 0.55859375, 'learning_rate': 0.0003, 'epoch': 1.67}\n",
      "{'loss': 1.5979, 'grad_norm': 1.8984375, 'learning_rate': 0.0003, 'epoch': 1.67}\n",
      "{'loss': 1.5874, 'grad_norm': 0.59375, 'learning_rate': 0.0003, 'epoch': 1.68}\n",
      "{'loss': 1.4667, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 1.68}\n",
      "{'loss': 1.5813, 'grad_norm': 0.78125, 'learning_rate': 0.0003, 'epoch': 1.68}\n",
      "{'loss': 1.5642, 'grad_norm': 0.6015625, 'learning_rate': 0.0003, 'epoch': 1.68}\n",
      "{'loss': 1.6783, 'grad_norm': 1.4296875, 'learning_rate': 0.0003, 'epoch': 1.68}\n",
      "{'loss': 1.5343, 'grad_norm': 0.58984375, 'learning_rate': 0.0003, 'epoch': 1.69}\n",
      "{'loss': 1.513, 'grad_norm': 0.66015625, 'learning_rate': 0.0003, 'epoch': 1.69}\n",
      "{'loss': 1.6456, 'grad_norm': 0.78515625, 'learning_rate': 0.0003, 'epoch': 1.69}\n",
      "{'loss': 1.38, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 1.69}\n",
      "{'loss': 1.567, 'grad_norm': 1.7734375, 'learning_rate': 0.0003, 'epoch': 1.69}\n",
      "{'loss': 1.5138, 'grad_norm': 0.87109375, 'learning_rate': 0.0003, 'epoch': 1.7}\n",
      "{'loss': 1.4818, 'grad_norm': 0.3359375, 'learning_rate': 0.0003, 'epoch': 1.7}\n",
      "{'loss': 1.5047, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 1.7}\n",
      "{'loss': 1.6193, 'grad_norm': 0.48828125, 'learning_rate': 0.0003, 'epoch': 1.7}\n",
      "{'loss': 1.6428, 'grad_norm': 2.0, 'learning_rate': 0.0003, 'epoch': 1.7}\n",
      "{'loss': 1.5871, 'grad_norm': 1.15625, 'learning_rate': 0.0003, 'epoch': 1.71}\n",
      "{'loss': 1.4913, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 1.71}\n",
      "{'loss': 1.4789, 'grad_norm': 0.7109375, 'learning_rate': 0.0003, 'epoch': 1.71}\n",
      "{'loss': 1.5572, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 1.71}\n",
      "{'loss': 1.6144, 'grad_norm': 1.984375, 'learning_rate': 0.0003, 'epoch': 1.71}\n",
      "{'loss': 1.5677, 'grad_norm': 0.64453125, 'learning_rate': 0.0003, 'epoch': 1.72}\n",
      "{'loss': 1.545, 'grad_norm': 0.5, 'learning_rate': 0.0003, 'epoch': 1.72}\n",
      "{'loss': 1.5198, 'grad_norm': 1.59375, 'learning_rate': 0.0003, 'epoch': 1.72}\n",
      "{'loss': 1.6535, 'grad_norm': 0.50390625, 'learning_rate': 0.0003, 'epoch': 1.72}\n",
      "{'loss': 1.6079, 'grad_norm': 1.3203125, 'learning_rate': 0.0003, 'epoch': 1.72}\n",
      "{'loss': 1.4449, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 1.73}\n",
      "{'loss': 1.7525, 'grad_norm': 0.470703125, 'learning_rate': 0.0003, 'epoch': 1.73}\n",
      "{'loss': 1.6633, 'grad_norm': 0.8046875, 'learning_rate': 0.0003, 'epoch': 1.73}\n",
      "{'loss': 1.6062, 'grad_norm': 0.7734375, 'learning_rate': 0.0003, 'epoch': 1.73}\n",
      "{'loss': 1.5084, 'grad_norm': 1.2890625, 'learning_rate': 0.0003, 'epoch': 1.73}\n",
      "{'loss': 1.401, 'grad_norm': 0.7109375, 'learning_rate': 0.0003, 'epoch': 1.74}\n",
      "{'loss': 1.5137, 'grad_norm': 0.80859375, 'learning_rate': 0.0003, 'epoch': 1.74}\n",
      "{'loss': 1.4685, 'grad_norm': 1.1328125, 'learning_rate': 0.0003, 'epoch': 1.74}\n",
      "{'loss': 1.4376, 'grad_norm': 0.5703125, 'learning_rate': 0.0003, 'epoch': 1.74}\n",
      "{'loss': 1.6984, 'grad_norm': 3.453125, 'learning_rate': 0.0003, 'epoch': 1.74}\n",
      "{'loss': 1.5525, 'grad_norm': 0.6875, 'learning_rate': 0.0003, 'epoch': 1.75}\n",
      "{'loss': 1.5808, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 1.75}\n",
      "{'loss': 1.6043, 'grad_norm': 1.0625, 'learning_rate': 0.0003, 'epoch': 1.75}\n",
      "{'loss': 1.5201, 'grad_norm': 0.79296875, 'learning_rate': 0.0003, 'epoch': 1.75}\n",
      "{'loss': 1.3983, 'grad_norm': 1.546875, 'learning_rate': 0.0003, 'epoch': 1.75}\n",
      "{'loss': 1.5388, 'grad_norm': 0.734375, 'learning_rate': 0.0003, 'epoch': 1.76}\n",
      "{'loss': 1.6112, 'grad_norm': 0.765625, 'learning_rate': 0.0003, 'epoch': 1.76}\n",
      "{'loss': 1.5802, 'grad_norm': 0.90234375, 'learning_rate': 0.0003, 'epoch': 1.76}\n",
      "{'loss': 1.5042, 'grad_norm': 0.49609375, 'learning_rate': 0.0003, 'epoch': 1.76}\n",
      "{'loss': 1.4842, 'grad_norm': 0.796875, 'learning_rate': 0.0003, 'epoch': 1.76}\n",
      "{'loss': 1.5493, 'grad_norm': 0.796875, 'learning_rate': 0.0003, 'epoch': 1.77}\n",
      "{'loss': 1.4821, 'grad_norm': 0.50390625, 'learning_rate': 0.0003, 'epoch': 1.77}\n",
      "{'loss': 1.6107, 'grad_norm': 1.171875, 'learning_rate': 0.0003, 'epoch': 1.77}\n",
      "{'loss': 1.4925, 'grad_norm': 0.73046875, 'learning_rate': 0.0003, 'epoch': 1.77}\n",
      "{'loss': 1.6223, 'grad_norm': 1.1328125, 'learning_rate': 0.0003, 'epoch': 1.77}\n",
      "{'loss': 1.596, 'grad_norm': 0.66796875, 'learning_rate': 0.0003, 'epoch': 1.77}\n",
      "{'loss': 1.535, 'grad_norm': 0.40625, 'learning_rate': 0.0003, 'epoch': 1.78}\n",
      "{'loss': 1.5815, 'grad_norm': 0.828125, 'learning_rate': 0.0003, 'epoch': 1.78}\n",
      "{'loss': 1.4597, 'grad_norm': 0.54296875, 'learning_rate': 0.0003, 'epoch': 1.78}\n",
      "{'loss': 1.6806, 'grad_norm': 1.6875, 'learning_rate': 0.0003, 'epoch': 1.78}\n",
      "{'loss': 1.6253, 'grad_norm': 0.76171875, 'learning_rate': 0.0003, 'epoch': 1.78}\n",
      "{'loss': 1.497, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 1.79}\n",
      "{'loss': 1.6078, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 1.79}\n",
      "{'loss': 1.5143, 'grad_norm': 0.52734375, 'learning_rate': 0.0003, 'epoch': 1.79}\n",
      "{'loss': 1.5836, 'grad_norm': 0.8828125, 'learning_rate': 0.0003, 'epoch': 1.79}\n",
      "{'loss': 1.6186, 'grad_norm': 1.0234375, 'learning_rate': 0.0003, 'epoch': 1.79}\n",
      "{'loss': 1.6302, 'grad_norm': 0.431640625, 'learning_rate': 0.0003, 'epoch': 1.8}\n",
      "{'loss': 1.5608, 'grad_norm': 1.078125, 'learning_rate': 0.0003, 'epoch': 1.8}\n",
      "{'loss': 1.5492, 'grad_norm': 0.7578125, 'learning_rate': 0.0003, 'epoch': 1.8}\n",
      "{'loss': 1.5413, 'grad_norm': 1.296875, 'learning_rate': 0.0003, 'epoch': 1.8}\n",
      "{'loss': 1.369, 'grad_norm': 0.63671875, 'learning_rate': 0.0003, 'epoch': 1.8}\n",
      "{'loss': 1.5939, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 1.81}\n",
      "{'loss': 1.5586, 'grad_norm': 1.1875, 'learning_rate': 0.0003, 'epoch': 1.81}\n",
      "{'loss': 1.4319, 'grad_norm': 0.6953125, 'learning_rate': 0.0003, 'epoch': 1.81}\n",
      "{'loss': 1.5596, 'grad_norm': 1.5234375, 'learning_rate': 0.0003, 'epoch': 1.81}\n",
      "{'loss': 1.519, 'grad_norm': 0.62109375, 'learning_rate': 0.0003, 'epoch': 1.81}\n",
      "{'loss': 1.5349, 'grad_norm': 0.416015625, 'learning_rate': 0.0003, 'epoch': 1.82}\n",
      "{'loss': 1.5655, 'grad_norm': 0.90234375, 'learning_rate': 0.0003, 'epoch': 1.82}\n",
      "{'loss': 1.3695, 'grad_norm': 0.51953125, 'learning_rate': 0.0003, 'epoch': 1.82}\n",
      "{'loss': 1.5362, 'grad_norm': 1.3046875, 'learning_rate': 0.0003, 'epoch': 1.82}\n",
      "{'loss': 1.608, 'grad_norm': 0.765625, 'learning_rate': 0.0003, 'epoch': 1.82}\n",
      "{'loss': 1.5991, 'grad_norm': 0.443359375, 'learning_rate': 0.0003, 'epoch': 1.83}\n",
      "{'loss': 1.6731, 'grad_norm': 2.203125, 'learning_rate': 0.0003, 'epoch': 1.83}\n",
      "{'loss': 1.4902, 'grad_norm': 0.515625, 'learning_rate': 0.0003, 'epoch': 1.83}\n",
      "{'loss': 1.6072, 'grad_norm': 2.5625, 'learning_rate': 0.0003, 'epoch': 1.83}\n",
      "{'loss': 1.6367, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 1.83}\n",
      "{'loss': 1.4598, 'grad_norm': 0.83984375, 'learning_rate': 0.0003, 'epoch': 1.84}\n",
      "{'loss': 1.6741, 'grad_norm': 0.796875, 'learning_rate': 0.0003, 'epoch': 1.84}\n",
      "{'loss': 1.4368, 'grad_norm': 0.65625, 'learning_rate': 0.0003, 'epoch': 1.84}\n",
      "{'loss': 1.6937, 'grad_norm': 1.125, 'learning_rate': 0.0003, 'epoch': 1.84}\n",
      "{'loss': 1.5456, 'grad_norm': 0.640625, 'learning_rate': 0.0003, 'epoch': 1.84}\n",
      "{'loss': 1.5228, 'grad_norm': 0.51171875, 'learning_rate': 0.0003, 'epoch': 1.85}\n",
      "{'loss': 1.6778, 'grad_norm': 1.0078125, 'learning_rate': 0.0003, 'epoch': 1.85}\n",
      "{'loss': 1.5481, 'grad_norm': 0.796875, 'learning_rate': 0.0003, 'epoch': 1.85}\n",
      "{'loss': 1.509, 'grad_norm': 1.1640625, 'learning_rate': 0.0003, 'epoch': 1.85}\n",
      "{'loss': 1.575, 'grad_norm': 0.56640625, 'learning_rate': 0.0003, 'epoch': 1.85}\n",
      "{'loss': 1.4976, 'grad_norm': 0.53515625, 'learning_rate': 0.0003, 'epoch': 1.86}\n",
      "{'loss': 1.5523, 'grad_norm': 0.87890625, 'learning_rate': 0.0003, 'epoch': 1.86}\n",
      "{'loss': 1.5028, 'grad_norm': 0.66015625, 'learning_rate': 0.0003, 'epoch': 1.86}\n",
      "{'loss': 1.6525, 'grad_norm': 1.3671875, 'learning_rate': 0.0003, 'epoch': 1.86}\n",
      "{'loss': 1.503, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 1.86}\n",
      "{'loss': 1.5192, 'grad_norm': 0.87109375, 'learning_rate': 0.0003, 'epoch': 1.87}\n",
      "{'loss': 1.603, 'grad_norm': 1.0234375, 'learning_rate': 0.0003, 'epoch': 1.87}\n",
      "{'loss': 1.3753, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 1.87}\n",
      "{'loss': 1.5574, 'grad_norm': 0.91796875, 'learning_rate': 0.0003, 'epoch': 1.87}\n",
      "{'loss': 1.4615, 'grad_norm': 0.79296875, 'learning_rate': 0.0003, 'epoch': 1.87}\n",
      "{'loss': 1.4582, 'grad_norm': 0.5859375, 'learning_rate': 0.0003, 'epoch': 1.88}\n",
      "{'loss': 1.6024, 'grad_norm': 1.2109375, 'learning_rate': 0.0003, 'epoch': 1.88}\n",
      "{'loss': 1.567, 'grad_norm': 0.78125, 'learning_rate': 0.0003, 'epoch': 1.88}\n",
      "{'loss': 1.5818, 'grad_norm': 1.78125, 'learning_rate': 0.0003, 'epoch': 1.88}\n",
      "{'loss': 1.5509, 'grad_norm': 0.7265625, 'learning_rate': 0.0003, 'epoch': 1.88}\n",
      "{'loss': 1.5589, 'grad_norm': 0.625, 'learning_rate': 0.0003, 'epoch': 1.88}\n",
      "{'loss': 1.5615, 'grad_norm': 1.328125, 'learning_rate': 0.0003, 'epoch': 1.89}\n",
      "{'loss': 1.6075, 'grad_norm': 0.5390625, 'learning_rate': 0.0003, 'epoch': 1.89}\n",
      "{'loss': 1.4143, 'grad_norm': 1.4453125, 'learning_rate': 0.0003, 'epoch': 1.89}\n",
      "{'loss': 1.4722, 'grad_norm': 1.140625, 'learning_rate': 0.0003, 'epoch': 1.89}\n",
      "{'loss': 1.6673, 'grad_norm': 0.48046875, 'learning_rate': 0.0003, 'epoch': 1.89}\n",
      "{'loss': 1.5798, 'grad_norm': 0.90625, 'learning_rate': 0.0003, 'epoch': 1.9}\n",
      "{'loss': 1.3966, 'grad_norm': 0.625, 'learning_rate': 0.0003, 'epoch': 1.9}\n",
      "{'loss': 1.46, 'grad_norm': 0.96875, 'learning_rate': 0.0003, 'epoch': 1.9}\n",
      "{'loss': 1.6158, 'grad_norm': 0.578125, 'learning_rate': 0.0003, 'epoch': 1.9}\n",
      "{'loss': 1.5823, 'grad_norm': 0.59765625, 'learning_rate': 0.0003, 'epoch': 1.9}\n",
      "{'loss': 1.7005, 'grad_norm': 1.0234375, 'learning_rate': 0.0003, 'epoch': 1.91}\n",
      "{'loss': 1.5639, 'grad_norm': 0.8125, 'learning_rate': 0.0003, 'epoch': 1.91}\n",
      "{'loss': 1.4327, 'grad_norm': 1.4765625, 'learning_rate': 0.0003, 'epoch': 1.91}\n",
      "{'loss': 1.5683, 'grad_norm': 0.83984375, 'learning_rate': 0.0003, 'epoch': 1.91}\n",
      "{'loss': 1.6319, 'grad_norm': 0.6875, 'learning_rate': 0.0003, 'epoch': 1.91}\n",
      "{'loss': 1.4642, 'grad_norm': 0.875, 'learning_rate': 0.0003, 'epoch': 1.92}\n",
      "{'loss': 1.3991, 'grad_norm': 0.53125, 'learning_rate': 0.0003, 'epoch': 1.92}\n",
      "{'loss': 1.5572, 'grad_norm': 1.671875, 'learning_rate': 0.0003, 'epoch': 1.92}\n",
      "{'loss': 1.5578, 'grad_norm': 0.76953125, 'learning_rate': 0.0003, 'epoch': 1.92}\n",
      "{'loss': 1.4599, 'grad_norm': 0.5546875, 'learning_rate': 0.0003, 'epoch': 1.92}\n",
      "{'loss': 1.6532, 'grad_norm': 0.76953125, 'learning_rate': 0.0003, 'epoch': 1.93}\n",
      "{'loss': 1.5669, 'grad_norm': 0.7265625, 'learning_rate': 0.0003, 'epoch': 1.93}\n",
      "{'loss': 1.6675, 'grad_norm': 0.8984375, 'learning_rate': 0.0003, 'epoch': 1.93}\n",
      "{'loss': 1.6037, 'grad_norm': 0.57421875, 'learning_rate': 0.0003, 'epoch': 1.93}\n",
      "{'loss': 1.6034, 'grad_norm': 0.55078125, 'learning_rate': 0.0003, 'epoch': 1.93}\n",
      "{'loss': 1.6179, 'grad_norm': 1.1328125, 'learning_rate': 0.0003, 'epoch': 1.94}\n",
      "{'loss': 1.5619, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 1.94}\n",
      "{'loss': 1.5807, 'grad_norm': 1.03125, 'learning_rate': 0.0003, 'epoch': 1.94}\n",
      "{'loss': 1.5161, 'grad_norm': 0.88671875, 'learning_rate': 0.0003, 'epoch': 1.94}\n",
      "{'loss': 1.4361, 'grad_norm': 0.455078125, 'learning_rate': 0.0003, 'epoch': 1.94}\n",
      "{'loss': 1.5315, 'grad_norm': 1.1640625, 'learning_rate': 0.0003, 'epoch': 1.95}\n",
      "{'loss': 1.5231, 'grad_norm': 0.546875, 'learning_rate': 0.0003, 'epoch': 1.95}\n",
      "{'loss': 1.6251, 'grad_norm': 1.8359375, 'learning_rate': 0.0003, 'epoch': 1.95}\n",
      "{'loss': 1.6105, 'grad_norm': 1.40625, 'learning_rate': 0.0003, 'epoch': 1.95}\n",
      "{'loss': 1.3461, 'grad_norm': 0.88671875, 'learning_rate': 0.0003, 'epoch': 1.95}\n",
      "{'loss': 1.6394, 'grad_norm': 1.046875, 'learning_rate': 0.0003, 'epoch': 1.96}\n",
      "{'loss': 1.6402, 'grad_norm': 0.6015625, 'learning_rate': 0.0003, 'epoch': 1.96}\n",
      "{'loss': 1.4549, 'grad_norm': 1.2734375, 'learning_rate': 0.0003, 'epoch': 1.96}\n",
      "{'loss': 1.655, 'grad_norm': 0.703125, 'learning_rate': 0.0003, 'epoch': 1.96}\n",
      "{'loss': 1.6208, 'grad_norm': 0.57421875, 'learning_rate': 0.0003, 'epoch': 1.96}\n",
      "{'loss': 1.5637, 'grad_norm': 1.359375, 'learning_rate': 0.0003, 'epoch': 1.97}\n",
      "{'loss': 1.5169, 'grad_norm': 0.78515625, 'learning_rate': 0.0003, 'epoch': 1.97}\n",
      "{'loss': 1.5319, 'grad_norm': 1.4609375, 'learning_rate': 0.0003, 'epoch': 1.97}\n",
      "{'loss': 1.5447, 'grad_norm': 0.87109375, 'learning_rate': 0.0003, 'epoch': 1.97}\n",
      "{'loss': 1.6324, 'grad_norm': 0.43359375, 'learning_rate': 0.0003, 'epoch': 1.97}\n",
      "{'loss': 1.5812, 'grad_norm': 1.0703125, 'learning_rate': 0.0003, 'epoch': 1.98}\n",
      "{'loss': 1.5882, 'grad_norm': 0.58203125, 'learning_rate': 0.0003, 'epoch': 1.98}\n",
      "{'loss': 1.5865, 'grad_norm': 1.6328125, 'learning_rate': 0.0003, 'epoch': 1.98}\n",
      "{'loss': 1.5294, 'grad_norm': 0.6484375, 'learning_rate': 0.0003, 'epoch': 1.98}\n",
      "{'loss': 1.4866, 'grad_norm': 0.40625, 'learning_rate': 0.0003, 'epoch': 1.98}\n",
      "{'loss': 1.5356, 'grad_norm': 1.0859375, 'learning_rate': 0.0003, 'epoch': 1.99}\n",
      "{'loss': 1.4306, 'grad_norm': 0.55078125, 'learning_rate': 0.0003, 'epoch': 1.99}\n",
      "{'loss': 1.5617, 'grad_norm': 1.4609375, 'learning_rate': 0.0003, 'epoch': 1.99}\n",
      "{'loss': 1.6401, 'grad_norm': 0.77734375, 'learning_rate': 0.0003, 'epoch': 1.99}\n",
      "{'loss': 1.5507, 'grad_norm': 0.482421875, 'learning_rate': 0.0003, 'epoch': 1.99}\n",
      "{'loss': 1.5433, 'grad_norm': 1.34375, 'learning_rate': 0.0003, 'epoch': 2.0}\n",
      "{'loss': 1.3289, 'grad_norm': 0.625, 'learning_rate': 0.0003, 'epoch': 2.0}\n",
      "{'loss': 1.4229, 'grad_norm': 1.390625, 'learning_rate': 0.0003, 'epoch': 2.0}\n",
      "{'train_runtime': 15595.4896, 'train_samples_per_second': 1.957, 'train_steps_per_second': 1.957, 'train_loss': 1.6338039096526609, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30526, training_loss=1.6338039096526609, metrics={'train_runtime': 15595.4896, 'train_samples_per_second': 1.957, 'train_steps_per_second': 1.957, 'train_loss': 1.6338039096526609, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 허깅페이스에서 훈련 도중 러닝레이트가 수정 안되는 경우가 많다 한다. \n",
    "- 스케쥴러 변경을 시도해 보기로 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/experi_03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
