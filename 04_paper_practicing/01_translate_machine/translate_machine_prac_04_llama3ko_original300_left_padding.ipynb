{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: utf-8 -*- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight and Bias Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"translate_machine_llama3ko_with_orgin_data_300\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/train_data_300.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "len(train_data)\n",
    "\n",
    "with open('./data/validation_data_28.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# DatasetDict로 \"train\"과 \"test\" 데이터셋 묶기\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Explainable AI is becoming increasingly important as AI systems are integrated into various industries. With the rise of cloud computing, massive datasets can be processed and analyzed more efficiently, but this often comes at the cost of transparency. By combining explainable AI with cloud computing, organizations can ensure that their AI models are both powerful and understandable. Meanwhile, edge computing allows for data processing closer to the source, which can enhance real-time decision-making capabilities. Integrating explainable AI with edge computing can further improve the trustworthiness and reliability of these real-time systems.',\n",
       " 'korean': '설명 가능한 AI(explainable AI)는 AI 시스템이 다양한 산업에 통합됨에 따라 점점 더 중요해지고 있습니다. 클라우드 컴퓨팅(cloud computing)의 발전으로 대규모 데이터셋을 더 효율적으로 처리하고 분석할 수 있지만, 이는 종종 투명성의 대가로 이루어집니다. 설명 가능한 AI(explainable AI)와 클라우드 컴퓨팅(cloud computing)을 결합하면 조직은 강력하면서도 이해할 수 있는 AI 모델을 보장할 수 있습니다. 한편, 엣지 컴퓨팅(edge computing)은 데이터 처리를 소스에 더 가깝게 하여 실시간 의사 결정 능력을 향상시킬 수 있습니다. 설명 가능한 AI(explainable AI)와 엣지 컴퓨팅(edge computing)을 통합하면 이러한 실시간 시스템의 신뢰성과 신뢰성을 더욱 향상시킬 수 있습니다.',\n",
       " 'terms': 'explainable AI, cloud computing, edge computing'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/Llama-3-KoEn-8B\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for 4-bit QLoRA Training(4bit QLoRA 학습을 위한 설정)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Nvidia의 Ampere 아키텍처 이후 가속기는 bf16으로 속도 향상을 꾀할수 있다. \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# bnb_4bit_quant_type=\"nf4\" 설정상 기본값은 bnb_4bit_quant_type=\"fp4\"이나 허깅페이스 저자들에 의하면\n",
    "# 경험적 결과로 \"nf4\"가 결과가 더 좋았다고 한다. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# bnb_4bit_use_double_quant=True로 하면 매개변수단 0.4bit을 추가로 절약 할 수 있다고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34dc4a5c7714a7482d723addb31135f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# model.config.pretraining_tp = 1\n",
    "# 종종 QLoRA 코드에 이 코드가 보이는데 병렬 학습에 쓰이는 코드로 보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_model_dir)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) # pad_token이 추가되었으므로 embedding과 language modeling head를 resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english', 'korean', 'terms'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting function\n",
    "def formatting_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"english\"])):\n",
    "        text = f\"Translate input sentence to Korean \\n### Input: {example['english'][i]} \\n### Translated: {example['korean'][i]}\" + tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "response_template = \" \\n### Translated:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# https://huggingface.co/docs/trl/sft_trainer#using-tokenids-directly-for-responsetemplate 참고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_template = \" \\n### Translated:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "# response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "# # data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Argument Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = checkpoint_dir\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 20\n",
    "save_total_limit=5\n",
    "num_train_epochs = 2\n",
    "logging_steps = 20\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cc5496fdc24bc29c7c55916588e47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be94812a6a1494c888599182238113b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114492888898693, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/04_paper_practicing/01_translate_machine/wandb/run-20240628_210159-i8glg0xt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/i8glg0xt' target=\"_blank\">ancient-pond-8</a></strong> to <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/i8glg0xt' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/i8glg0xt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.19696848551432292, metrics={'train_runtime': 499.2441, 'train_samples_per_second': 1.202, 'train_steps_per_second': 0.601, 'total_flos': 9466790000295936.0, 'train_loss': 0.19696848551432292, 'epoch': 2.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "lora_model_save_dir = \"./results/translate_machine_llama3ko_origindata300_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "trainer.save_model(lora_model_save_dir)\n",
    "\n",
    "# model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(lora_model_save_dir, save_embedding_layers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/Llama-3-KoEn-8B\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f940e671c74cc2bfeb6f057f1bab8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained(lora_model_save_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=lora_model_save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[128000,  28573,   1988,  11914,    311,  16526,    720,  14711,   5688,\n",
       "              25,  18185,    872,   6205,   4367,     11,   1057,   4211,    656,\n",
       "             539,    617,  15022,   1515,  29736,     82,   7863,    311,   1023,\n",
       "           29736,   6108,   4211,     13,    720,  14711,   4149,  22851,     25,\n",
       "             220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[128000,  28573,   1988,  11914,    311,  16526,    720,  14711,   5688,\n",
       "              25,   5751,   4211,    656,     11,   4869,     11,    617,   1515,\n",
       "           29736,     82,   2731,   1109,    279,   3544,  17989,  86997,   5962,\n",
       "           12939,  25936,    706,   1027,   5068,    311,   8356,    369,   4907,\n",
       "            3196,   4211,    323,   5573,  12864,     13,    720,  14711,   4149,\n",
       "           22851,     25,    220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[128000,  28573,   1988,  11914,    311,  16526,    720,  14711,   5688,\n",
       "              25,   1226,   5357,    389,  10128,    306,  29469,   7713,  27972,\n",
       "            2533,    814,    649,   2804,    264,   7029,   2134,    315,   1803,\n",
       "            1413,   9256,     13,   1115,    990,   5039,    430,   5042,   7060,\n",
       "            2442,  38302,    264,   2678,    961,    315,    279,   1803,   1413,\n",
       "            1646,     13,    720,  14711,   4149,  22851,     25,    220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]])}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting function\n",
    "def formatting_func_inference(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"Input\"])):\n",
    "        text = f\"Translate input sentence to Korean \\n### Input: {example['Input'][i]} \\n### Translated: \"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        output_texts.append(inputs)\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "example = {\"Input\": [\n",
    "    \"Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models.\",\n",
    "    \"Our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching.\",\n",
    "    \"We focus on Latent Diffusion Models since they can perform a wide range of generative tasks. This work shows that simply fine-tuning a small part of the generative model.\",\n",
    "]}\n",
    "\n",
    "tokenized_example = formatting_func_inference(example)\n",
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate input sentence to Korean \n",
      "### Input: Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models. \n",
      "### Translated:  우리 모델은 샘플 품질에도 불구하고, 다른 가능성 기반 모델에 비해 경쟁력 있는 로그 가능성을 갖지 않습니다.- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "\n",
      "####################################################################################################\n",
      "Translate input sentence to Korean \n",
      "### Input: Our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching. \n",
      "### Translated:  우리의 모델은 그러나 에너지 기반 모델과 스코어 매칭에 대해 보고된 것보다 더 큰 추정치로 중요도 샘플링을 수행하여 로그 가능성(log likelihoods)을 더 잘 갖습니다.- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "-\n",
      "####################################################################################################\n",
      "Translate input sentence to Korean \n",
      "### Input: We focus on Latent Diffusion Models since they can perform a wide range of generative tasks. This work shows that simply fine-tuning a small part of the generative model. \n",
      "### Translated:  우리는 잠재 확산 모델(latent diffusion models)을 중점적으로 연구합니다. 이는 생성 모델의 작은 부분을 미세 조정하는 것만으로도 다양한 생성 작업을 수행할 수 있음을 보여줍니다.- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "with torch.cuda.amp.autocast():\n",
    "    for ex in tokenized_example:\n",
    "        ex = ex.to(model.device)\n",
    "        pred = model.generate(\n",
    "            **ex,\n",
    "            max_new_tokens=1024,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        outputs.append(pred)\n",
    "\n",
    "outputs = [tokenizer.batch_decode(t, skip_special_tokens=True)[0] for t in outputs]\n",
    "for o in outputs:\n",
    "    print(o)\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
