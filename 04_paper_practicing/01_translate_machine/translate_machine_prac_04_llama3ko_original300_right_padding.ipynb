{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: utf-8 -*- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight and Bias Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"translate_machine_llama3ko_with_orgin_data_300\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/train_data_300.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "len(train_data)\n",
    "\n",
    "with open('./data/validation_data_28.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# DatasetDict로 \"train\"과 \"test\" 데이터셋 묶기\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Explainable AI is becoming increasingly important as AI systems are integrated into various industries. With the rise of cloud computing, massive datasets can be processed and analyzed more efficiently, but this often comes at the cost of transparency. By combining explainable AI with cloud computing, organizations can ensure that their AI models are both powerful and understandable. Meanwhile, edge computing allows for data processing closer to the source, which can enhance real-time decision-making capabilities. Integrating explainable AI with edge computing can further improve the trustworthiness and reliability of these real-time systems.',\n",
       " 'korean': '설명 가능한 AI(explainable AI)는 AI 시스템이 다양한 산업에 통합됨에 따라 점점 더 중요해지고 있습니다. 클라우드 컴퓨팅(cloud computing)의 발전으로 대규모 데이터셋을 더 효율적으로 처리하고 분석할 수 있지만, 이는 종종 투명성의 대가로 이루어집니다. 설명 가능한 AI(explainable AI)와 클라우드 컴퓨팅(cloud computing)을 결합하면 조직은 강력하면서도 이해할 수 있는 AI 모델을 보장할 수 있습니다. 한편, 엣지 컴퓨팅(edge computing)은 데이터 처리를 소스에 더 가깝게 하여 실시간 의사 결정 능력을 향상시킬 수 있습니다. 설명 가능한 AI(explainable AI)와 엣지 컴퓨팅(edge computing)을 통합하면 이러한 실시간 시스템의 신뢰성과 신뢰성을 더욱 향상시킬 수 있습니다.',\n",
       " 'terms': 'explainable AI, cloud computing, edge computing'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/Llama-3-KoEn-8B\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for 4-bit QLoRA Training(4bit QLoRA 학습을 위한 설정)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Nvidia의 Ampere 아키텍처 이후 가속기는 bf16으로 속도 향상을 꾀할수 있다. \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# bnb_4bit_quant_type=\"nf4\" 설정상 기본값은 bnb_4bit_quant_type=\"fp4\"이나 허깅페이스 저자들에 의하면\n",
    "# 경험적 결과로 \"nf4\"가 결과가 더 좋았다고 한다. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# bnb_4bit_use_double_quant=True로 하면 매개변수단 0.4bit을 추가로 절약 할 수 있다고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f45a4053eb74c249fb7a52375f5b6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# model.config.pretraining_tp = 1\n",
    "# 종종 QLoRA 코드에 이 코드가 보이는데 병렬 학습에 쓰이는 코드로 보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_model_dir)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) # pad_token이 추가되었으므로 embedding과 language modeling head를 resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english', 'korean', 'terms'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting function\n",
    "def formatting_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"english\"])):\n",
    "        text = f\"Translate input sentence to Korean \\n### Input: {example['english'][i]} \\n### Translated: {example['korean'][i]}\" + tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "response_template = \" \\n### Translated:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# https://huggingface.co/docs/trl/sft_trainer#using-tokenids-directly-for-responsetemplate 참고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_template = \" \\n### Translated:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "# response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "# # data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Argument Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = checkpoint_dir\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 20\n",
    "save_total_limit=5\n",
    "num_train_epochs = 2\n",
    "logging_steps = 20\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1869b2be994430a86e39a7d5612c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/04_paper_practicing/01_translate_machine/wandb/run-20240628_170511-z1edsqwk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/z1edsqwk' target=\"_blank\">kind-grass-5</a></strong> to <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/z1edsqwk' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/z1edsqwk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-220 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-240 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-260 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-280 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-300 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.1971247903505961, metrics={'train_runtime': 490.1479, 'train_samples_per_second': 1.224, 'train_steps_per_second': 0.612, 'total_flos': 9466790000295936.0, 'train_loss': 0.1971247903505961, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "lora_model_save_dir = \"./translate_machine_llama3ko_nonintsuct_origindata300_01_right_padding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(lora_model_save_dir, save_embedding_layers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig.from_pretrained(lora_model_save_dir)\n",
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aeolian83/llama_ko_sft_gugugo_experi_01/commit/19dd71bb9c3aebf4c5be4ad2c4a15d34a7a999d6', commit_message='Upload tokenizer', commit_description='', oid='19dd71bb9c3aebf4c5be4ad2c4a15d34a7a999d6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.push_to_hub('aeolian83/llama_ko_sft_gugugo_experi_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=lora_model_save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Group sparsity is a concept in multilinear algebra that promotes sparsity patterns within groups of variables. This technique is particularly useful in applications involving high-dimensional data, where it helps to identify relevant groups of features. In the context of factor graphs, group sparsity can enhance the efficiency of inference algorithms by reducing the complexity of the graph structure. Multilinear algebra provides the mathematical foundation for understanding and manipulating the interactions between these groups. By leveraging group sparsity and multilinear algebra, factor graphs can be optimized to handle large-scale problems more effectively.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test']['english'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    f'''\n",
    "Translate input sentence to Korean\n",
    "### Input: {dataset_dict['test']['english'][0]}\n",
    "''',\n",
    "    f'''\n",
    "Translate input sentence to Korean\n",
    "### Input: {dataset_dict['test']['english'][1]}\n",
    "''',\n",
    " f'''\n",
    "Translate input sentence to Korean\n",
    "### Input: {dataset_dict['test']['english'][2]}\n",
    "''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = tokenizer(examples, return_tensors=\"pt\", padding=True)['input_ids'].to(loaded_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = loaded_model.generate(example_batch, max_new_tokens = 1024, pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate input sentence to Korean\n",
      "### Input: Group sparsity is a concept in multilinear algebra that promotes sparsity patterns within groups of variables. This technique is particularly useful in applications involving high-dimensional data, where it helps to identify relevant groups of features. In the context of factor graphs, group sparsity can enhance the efficiency of inference algorithms by reducing the complexity of the graph structure. Multilinear algebra provides the mathematical foundation for understanding and manipulating the interactions between these groups. By leveraging group sparsity and multilinear algebra, factor graphs can be optimized to handle large-scale problems more effectively.\n",
      "제안: 그룹 희소성(group sparsity)은 변수 집합 내에서 희소성 패턴을 촉진하는 다변수 대수(multilinear algebra)의 개념입니다. 이 기술은 고차원 데이터를 다루는 응용 분야에서 특히 유용하며, 관련된 특징 집합을 식별하는 데 도움을 줍니다. 인자 그래프(factor graphs)에서 그룹 희소성(group sparsity)은 그래프 구조의 복잡성을 줄여 추론 알고리즘의 효율성을 향상시킬 수 있습니다. 다변수 대수(multilinear algebra)는 이러한 그룹 간의 상호작용을 이해하고 조작하는 수학적 기반을 제공합니다. 그룹 희소성(group sparsity)과 다변수 대수(multilinear algebra)를 활용함으로써 인자 그래프(factor graphs)는 대규모 문제를 더 효과적으로 처리할 수 있습니다.- 1st\n",
      "- 11:00 pm\n",
      "- 2 comments\n",
      "I'm not sure if I've mentioned this before, but I'm a big fan of the TV show \"The Office.\" I love the characters, the humor, the drama, the awkwardness, the romance, the office politics, the office pranks, the office parties, the office gossip, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties,\n",
      "####################################################################################################\n",
      "\n",
      "Translate input sentence to Korean\n",
      "### Input: Latent space models are powerful tools in machine learning that represent data in a lower-dimensional space. These models rely on latent variable inference to uncover hidden structures within the data. However, exact latent variable inference can be computationally intensive, which is why approximate inference methods are often used. Approximate inference techniques provide a practical way to estimate the latent variables without the need for exhaustive computation. By leveraging approximate inference, latent space models can efficiently handle large datasets and complex data structures.\n",
      "제안된 잠재 공간 모델(latent space models)은 데이터를 저차원 공간에 표현하는 강력한 도구입니다. 이러한 모델은 데이터 내의 숨겨진 구조를 발견하기 위해 잠재 변수 추론(latent variable inference)을 사용합니다. 그러나 정확한 잠재 변수 추론(latent variable inference)은 계산적으로 집중적일 수 있기 때문에 근사 추론(approximate inference) 방법이 종종 사용됩니다. 근사 추론(approximate inference) 기술은 계산 집중적인 작업 없이 잠재 변수를 추정하는 실용적인 방법을 제공합니다. 근사 추론(approximate inference)을 활용하면 잠재 공간 모델(latent space models)은 대규모 데이터셋과 복잡한 데이터 구조를 효율적으로 처리할 수 있습니다.- 1st\n",
      "- 11:00 pm\n",
      "- 2 comments\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm\n",
      "- 31st\n",
      "- 11:00 pm- 1st\n",
      "- 12:00 pm\n",
      "- 2 comments\n",
      "- 2nd\n",
      "- 12:00 pm\n",
      "- 3rd\n",
      "- 12:00 pm\n",
      "- 4th\n",
      "- 12:00 pm\n",
      "- 5th\n",
      "- 12:00 pm\n",
      "- 6th\n",
      "- 12:00 pm\n",
      "- 7th\n",
      "- 12:00 pm\n",
      "- 8th\n",
      "- 12:00 pm\n",
      "- 9th\n",
      "- 12:00 pm\n",
      "- 10th\n",
      "- 12:00 pm\n",
      "- 11th\n",
      "- 12:00 pm\n",
      "- 12th\n",
      "- 12:00 pm\n",
      "- 13th\n",
      "- 12:00 pm\n",
      "- 14th\n",
      "- 12:00 pm\n",
      "- 15th\n",
      "- 12:00 pm\n",
      "- 16th\n",
      "- 12:00 pm\n",
      "- 17th\n",
      "- 12:00 pm\n",
      "- 18th\n",
      "- 12:00 pm\n",
      "- 19th\n",
      "- 12:00 pm\n",
      "- 20th\n",
      "- 12:00 pm\n",
      "- 21st\n",
      "- 12:00 pm\n",
      "- 22nd\n",
      "- 12:00 pm\n",
      "- 23rd\n",
      "- 12:00 pm\n",
      "- 24th\n",
      "- 12:00 pm\n",
      "- 25th\n",
      "- 12:00 pm\n",
      "- 26th\n",
      "- 12:00 pm\n",
      "- 27th\n",
      "- 12:00 pm\n",
      "- 28th\n",
      "- 12:00 pm\n",
      "- 29th\n",
      "- 12:00 pm\n",
      "- 30th\n",
      "- 12:00 pm\n",
      "- 31st\n",
      "- 12:00 pm- 1st\n",
      "- 12:00 pm\n",
      "- 2nd\n",
      "- 12:00 pm\n",
      "- 3rd\n",
      "- 12:00 pm\n",
      "- 4th\n",
      "- 12:00 pm\n",
      "- 5th\n",
      "- 12:00 pm\n",
      "- 6th\n",
      "- 12:00 pm\n",
      "- 7th\n",
      "- 12:00 pm\n",
      "- 8th\n",
      "- 12:00 pm\n",
      "- 9th\n",
      "####################################################################################################\n",
      "\n",
      "Translate input sentence to Korean\n",
      "### Input: Domain adaptation is a technique used to adapt a machine learning model trained on one domain to perform well on a different but related domain. This is crucial for applications where labeled data in the target domain is scarce. On the other hand, domain generalization aims to train a model that performs well across multiple domains without needing retraining. Style transfer is often used in these contexts to help models learn invariant features by transferring style from one domain to another. Combining domain adaptation and style transfer can significantly enhance the model's ability to generalize across domains, thus achieving better domain generalization.\n",
      "### 출력: 도메인 적응(domain adaptation)은 한 도메인에서 훈련된 기계 학습 모델을 다른 관련 도메인에서 잘 수행하도록 적응시키는 기술입니다. 이는 목표 도메인에서 레이블이 있는 데이터가 부족한 응용 분야에서 중요합니다. 반면에 도메인 일반화(domain generalization)는 모델이 여러 도메인에서 재훈련 없이 잘 수행하도록 훈련하는 것을 목표로 합니다. 스타일 전이(style transfer)는 한 도메인에서 다른 도메인으로 스타일을 전이하여 모델이 불변 특징을 학습하도록 돕는 데 자주 사용됩니다. 도메인 적응(domain adaptation)과 스타일 전이(style transfer)를 결합하면 모델이 도메인 간 일반화(domain generalization)를 크게 향상시킬 수 있어 더 나은 도메인 일반화(domain generalization)를 달성할 수 있습니다.- 1st\n",
      "- 11:00 pm\n",
      "- 2 comments\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm\n",
      "- 31st\n",
      "- 11:00 pm- 1st\n",
      "- 12:00 pm\n",
      "- 2 comments\n",
      "I'm not sure if I've mentioned this before, but I'm a big fan of the show \"The Office.\" I love the characters, the humor, the drama, the awkwardness, the awkwardness, the awkwardness. I love it all. I'm a little sad that the show is coming to an end, but I'm looking forward to the last season. I'm also looking forward to the new show that's coming out in the fall, \"The New Girl.\" I'm a big fan of Zooey Deschanel, and I'm excited to see what she does in this show. I'm also excited to see what she wears. I'm a big fan of her style. I'm also a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "outputs = [tokenizer.decode(t, skip_special_tokens=True) for t in output_tokens]\n",
    "for o in outputs:\n",
    "    print(o)\n",
    "    print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
