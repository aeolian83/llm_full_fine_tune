{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/blog/finetune-llms/\n",
    "\n",
    "https://colab.research.google.com/drive/1vIjBtePIZwUaHWfjfNHzBjwuXOyU_ugD?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Environment & Huggingface, W&B login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"QLoRA_Instruction_finetune_04\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Load\n",
    "\n",
    "- 이 시스템에서는 작업을 외장하드에서 하고 있기 때문에 캐쉬폴더를 별도로 지정(In this system, tasks are being performed on an external hard drive, so a separate cache folder is specified.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = load_dataset(\"nlpai-lab/kullm-v2\", cache_dir=\"/mnt/t7/.cache/huggingface/datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.shuffle(seed=2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'instruction', 'input', 'output'],\n",
       "    num_rows: 152630\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.train_test_split(test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'instruction', 'input', 'output'],\n",
       "        num_rows: 76315\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'instruction', 'input', 'output'],\n",
       "        num_rows: 76315\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ko_instruction_01 = ko_instruction_01['train'].train_test_split(test_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Load\n",
    "\n",
    "- QLoRA로 finetune을 하기위해 일반 7B 모델을 BitsAndBytes를 통해 양자화(4bit quantization)하여 로드, 훈련속도 향상을 위해 데이터 타입을 fp32가 아니라 fp16으로 로드(차후에는 bf16으로 로드해서 훈련예정)\n",
    "- To finetune with QLoRA, a standard 7B model is quantized (4-bit quantization) through BitsAndBytes for loading, and to improve training speed, the data type is loaded as fp16 instead of fp32 (with plans to load and train with bf16 in the future).\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    ),\n",
    "```\n",
    "\n",
    "### *Reference\n",
    "\n",
    "- https://github.com/huggingface/peft/blob/main/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py\n",
    "- https://colab.research.google.com/drive/19AFEOrCI6-bc7h9RTso_NndRwXJRaJ25?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/experi_04\"\n",
    "model_id = \"beomi/llama-2-ko-7b\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit QLoRA 학습을 위한 설정\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c87ae5cb3304d9f9c6fdce430da7fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 학습 진행 중 loss가 치솟다가 0.0으로 떨어지는 문제 해결을 위해 사용\n",
    "# tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    system_prompt = f\"### instruction: {sample['instruction']}\"\n",
    "    input = f\"### input: {sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    output = f\"### output: {sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [system_prompt, input, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_instruction(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88bce0becfa4693a3ee88f98d97bb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/76315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ko_instruction_01['train'].map(template_dataset, remove_columns=list(ko_instruction_01['train'].features), num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 76315\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 동료나 선배가 회의에서 발언하려고 할 때마다 계속 방해합니다. 이런 상황에서는 어떻게 대처하시겠습니까?\\n\\n### output: 회의에서 발언하려고 할 때마다 동료나 선배가 계속 저를 방해한다면, 저는 다음 단계를 통해 상황을 처리할 것입니다:1. 침착하고 평정심을 유지합니다: 침착하고 평정심을 유지하는 것이 중요하며, 방해가 제 감정에 영향을 미치거나 회의의 목적에 방해가 되지 않도록 하는 것이 중요합니다.2. 정중하게 내 요점 주장하기: 정중하지만 단호하게 내 요점을 주장하고 상대방이 끼어들기 전에 내 생각을 마칠 수 있게 해달라고 요청합니다. \"실례합니다, 제 말을 마무리해도 될까요?\" 또는 \"응답하기 전에 제 말을 끝내 주시면 감사하겠습니다.\"와 같은 문구를 사용합니다.3. 적극적으로 경청하는 모습을 보여주세요: 상대방의 요점을 인정하고 사려 깊은 답변을 제공함으로써 적극적으로 경청하는 모습을 보이는 것이 중요합니다. 이렇게 함으로써 내가 대화에 참여하고 있다는 것을 보여줄 수 있고 상대방도 나의 예의에 화답하도록 유도할 수 있습니다.4. 방해에 대처하기: 상대방이 계속해서 제 말을 끊는다면 \"다른 관점을 가지고 있다는 것은 이해하지만, 제 생각을 마칠 수 있게 해 주시면 답변하는 데 도움이 될 것 같습니다.\"와 같이 직접 문제를 해결합니다.5. 그 사람과 개인적으로 이야기합니다: 위의 단계를 시도한 후에도 방해가 계속되는 경우, 회의가 끝난 후 상대방과 개인적으로 이야기하여 우려 사항을 공유합니다. 건설적인 방식으로 대화를 나누고 우리 모두에게 도움이 되는 해결책을 찾으려고 노력합니다.요약하자면, 회의 중에 동료나 선배가 계속 제 말을 끊는다면 침착함을 유지하고, 정중하게 내 주장을 주장하며, 적극적으로 경청하는 태도를 보이고, 방해 행위를 직접 해결하고, 필요한 경우 그 사람과 개인적으로 이야기합니다. 이러한 조치를 취함으로써 저는 제 아이디어가 전문적이고 건설적인 방식으로 경청되고 가치를 인정받을 수 있습니다.</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 뉴욕에서 5일간 여행할 수 있는 프로그램을 계획하는 데 도움을 주실 수 있나요? 아이 3명, 부모님\\n\\n### output: 물론이죠! 가족을 위한 5일간의 뉴욕 여행을 계획하는 데 기꺼이 도와드리겠습니다. 어린이를 동반한 가족에게 적합한 여행 일정을 소개합니다:1일차:* 상징적인 자유의 여신상과 엘리스 아일랜드 방문으로 하루를 시작하세요. 사전에 티켓을 예약하세요.* 그 후 배터리 공원을 산책하며 해안가와 뉴욕 항구를 감상하세요. 근처에 있는 많은 포장마차나 레스토랑에서 점심을 드실 수도 있습니다.* 오후에는 국립 9/11 기념관 및 박물관으로 향하세요. 아이들에게 교육적인 경험이 될 수 있는 감동적인 체험입니다.* 유서 깊은 사우스 스트리트 항구를 산책하고 거리 공연자와 음악가들을 구경하며 하루를 마무리하세요.2일차:* 센트럴 파크 방문으로 하루를 시작하세요. 자전거를 빌리거나 공원을 여유롭게 산책할 수 있습니다. 센트럴 파크 동물원과 센트럴 파크 컨서버토리 가든도 꼭 들러보세요.* 오후에는 미국 자연사 박물관으로 향하세요. 이 박물관에는 공룡 화석, 대왕고래 등 3,000만 개가 넘는 표본이 전시되어 있습니다.* 타임스퀘어 방문으로 하루를 마무리하세요. 아이들은 밝은 조명과 거리 공연자, 그리고 이 지역의 전반적인 에너지를 즐길 수 있습니다.셋째 날:* 엠파이어 스테이트 빌딩 방문으로 하루를 시작하세요. 인파를 피하고 전망대에서 멋진 경치를 즐기려면 일찍 도착하세요.* 다음으로 인트레피드 바다, 항공 및 우주 박물관으로 향하세요. 이 박물관에는 역사적인 항공기, 우주선 및 해군 선박 컬렉션이 전시되어 있습니다.* 오후에는 브루클린 다리를 건너 도시 스카이라인의 장관을 감상하며 산책을 즐겨 보세요.* 브루클린의 윌리엄스버그 또는 덤보의 트렌디한 동네를 둘러보며 하루를 마무리하세요. 두 지역 모두 훌륭한 레스토랑과 상점이 많이 있습니다.4일차:* 메트로폴리탄 미술관 방문으로 하루를 시작하세요. 이 박물관은 전 세계의 방대한 예술품을 소장하고 있으며 아이들에게 훌륭한 교육적 경험이 될 것입니다.* 그 후 독특한 나선형 디자인과 현대 미술 전시회로 유명한 구겐하임 미술관으로 향합니다.* 오후에는 옛 철로 위에 지어진 고가 공원인 하이라인을 산책해 보세요. 이 공원은 도시의 멋진 전망을 제공하며 아이들에게 독특한 경험을 선사합니다.* 훌륭한 상인과 상점이 많은 푸드 홀인 첼시 마켓을 방문하며 하루를 마무리하세요.5일차:* 원 월드 트레이드 센터 102층에서 도시의 멋진 전망을 감상할 수 있는 원 월드 전망대 방문으로 하루를 시작하세요.* 다음으로 뉴욕현대미술관(MoMA)으로 향하세요. 이 박물관에는 반 고흐, 워홀, 피카소 같은 유명 예술가들의 작품을 포함한 인상적인 근현대 미술 컬렉션이 있습니다.* 오후에는 뉴욕에서 가장 매력적인 동네 중 하나인 그리니치 빌리지를 산책해 보세요. 이 지역의 많은 훌륭한 레스토랑 중 한 곳에서 식사를 할 수 있습니다.* 브로드웨이 쇼로 하루를 마무리하세요. 선택할 수 있는 훌륭한 쇼가 많으며 아이들도 좋아할 거예요.물론 가족의 관심사와 선호도에 따라 이 일정을 맞춤 설정할 수 있습니다. 재미있고 기억에 남는 뉴욕 여행을 계획하는 데 도움이 되셨기를 바랍니다!</s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 도커와 VM 머신의 차이점을 이해하지 못했나요?\\n\\n### output: Docker와 가상 머신(VM)은 모두 격리된 환경에서 애플리케이션을 실행하는 데 사용할 수 있는 도구입니다. 그러나 둘 사이에는 몇 가지 주요 차이점이 있습니다.Docker와 VM의 주요 차이점 중 하나는 VM은 하이퍼바이저에서 실행되는 반면, Docker 컨테이너는 호스트 운영 체제에서 실행된다는 점입니다. 즉, VM은 실행에 메모리 및 스토리지와 같은 더 많은 리소스가 필요한 반면, Docker 컨테이너는 더 가볍고 효율적입니다.또 다른 주요 차이점은 VM은 각 애플리케이션에 대해 완전한 독립형 운영 체제를 제공하는 반면, Docker 컨테이너는 호스트 운영 체제를 공유하며 애플리케이션을 실행하는 데 필요한 라이브러리와 종속성만 포함한다는 것입니다. 즉, Docker 컨테이너는 이식성이 뛰어나고 다른 환경 간에 쉽게 이동할 수 있는 반면, VM은 더 격리되어 있으며 특정 운영 체제나 구성이 필요한 애플리케이션에 더 적합합니다.전반적으로 Docker와 VM 중 어떤 것을 사용할지는 애플리케이션과 환경의 특정 요구사항에 따라 달라집니다. 휴대성과 가벼운 실행이 필요한 애플리케이션에는 Docker가 적합하며, 보다 격리되고 사용자 지정 가능한 환경이 필요한 애플리케이션에는 VM이 더 적합합니다.</s>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][3230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 미국 빈곤의 원인을 조사하는 논문에 대한 연구 개요를 작성합니다.\\n\\n### output: I. 서론\\n    A. 빈곤의 정의\\n    B. 미국의 빈곤에 대한 간략한 개요\\n    C. 백서의 목적\\n\\nII. 미국의 빈곤에 대한 배경 정보\\n    A. 빈곤에 관한 통계 데이터\\n    B. 경제 역사 개요\\n    C. 빈곤 문제를 해결하려는 정부 정책\\n\\nIII. 빈곤의 원인\\n    A. 경제적 요인\\n        1. 실업 및 불완전 고용\\n        2. 소득 분배의 불평등\\n        3. 교육 및 취업 기회에 대한 접근성 부족\\n    B. 사회적 요인\\n        1. 차별과 편견\\n        2. 가족 구조\\n        3. 건강 격차\\n    C. 정부 정책\\n        1. 부적절한 사회복지 프로그램\\n        2. 세금 정책\\n        3. 주택 정책\\n    \\nIV. 결론\\n    가. 주요 내용 요약\\n    나. 연구의 시사점\\n    C. 향후 연구를 위한 제언.</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][12312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = ko_instruction_01['test'].map(template_dataset, remove_columns=list(ko_instruction_01['test'].features), num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "output_dir = \"./checkpoint/experi_04\"\n",
    "per_device_train_batch_size = 3\n",
    "gradient_accumulation_steps = 1\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 20\n",
    "save_total_limit=10\n",
    "logging_steps = 5\n",
    "learning_rate = 2e-4\n",
    "num_train_epochs=2\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 1000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e364a0318f2b4fa1bb6e881f22f82f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172990e7f88340318dd590431865b990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113756277740725, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/00_QLoRa_fine_tune/wandb/run-20240425_212233-9pvbgavi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_04/runs/9pvbgavi' target=\"_blank\">drawn-pond-6</a></strong> to <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_04' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_04' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_04</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_04/runs/9pvbgavi' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_04/runs/9pvbgavi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367d0759ca6a499bb8f4282e96318ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8438, 'grad_norm': 0.8151676058769226, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.568, 'grad_norm': 0.5109564661979675, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3867, 'grad_norm': 1.1732099056243896, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1035, 'grad_norm': 0.33722248673439026, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9351, 'grad_norm': 0.306474506855011, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.9709, 'grad_norm': 0.24044185876846313, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.9751, 'grad_norm': 0.26627326011657715, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.8948, 'grad_norm': 0.3147087097167969, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1399, 'grad_norm': 0.36505916714668274, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.9744, 'grad_norm': 0.4032672941684723, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8462, 'grad_norm': 0.30198007822036743, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8538, 'grad_norm': 0.4558054506778717, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7897, 'grad_norm': 0.34025025367736816, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8472, 'grad_norm': 0.40893444418907166, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0564, 'grad_norm': 0.3367737829685211, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9384, 'grad_norm': 0.3827155530452728, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9463, 'grad_norm': 0.4080278277397156, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8186, 'grad_norm': 0.3134373724460602, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8993, 'grad_norm': 0.38993674516677856, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8732, 'grad_norm': 0.33301541209220886, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9366, 'grad_norm': 0.3722999691963196, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8441, 'grad_norm': 0.43778136372566223, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9074, 'grad_norm': 0.4043634831905365, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8586, 'grad_norm': 0.32572707533836365, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8463, 'grad_norm': 0.3685901165008545, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9585, 'grad_norm': 0.39792928099632263, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8249, 'grad_norm': 0.4035135805606842, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8108, 'grad_norm': 0.3798590302467346, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9313, 'grad_norm': 0.5969040393829346, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8913, 'grad_norm': 0.5150415897369385, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8328, 'grad_norm': 0.39661872386932373, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8109, 'grad_norm': 0.5594218969345093, 'learning_rate': 0.0002, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7033, 'grad_norm': 0.415237158536911, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8708, 'grad_norm': 0.5164604783058167, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8827, 'grad_norm': 0.4539189338684082, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9688, 'grad_norm': 0.44970518350601196, 'learning_rate': 0.0002, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8961, 'grad_norm': 0.47629648447036743, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0511, 'grad_norm': 0.4200817346572876, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.7233, 'grad_norm': 0.5094456672668457, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9245, 'grad_norm': 0.55778968334198, 'learning_rate': 0.0002, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9115, 'grad_norm': 0.8700292706489563, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.791, 'grad_norm': 0.4858096241950989, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9416, 'grad_norm': 0.5518664121627808, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9326, 'grad_norm': 0.7881174087524414, 'learning_rate': 0.0002, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9572, 'grad_norm': 0.41201847791671753, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8544, 'grad_norm': 0.5311712026596069, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.937, 'grad_norm': 0.4582647383213043, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9392, 'grad_norm': 0.4270186722278595, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8078, 'grad_norm': 0.3908226191997528, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9191, 'grad_norm': 0.38801509141921997, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9117, 'grad_norm': 0.4320867955684662, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.7808, 'grad_norm': 0.516696035861969, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7889, 'grad_norm': 0.4847352206707001, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9645, 'grad_norm': 0.48175740242004395, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.7785, 'grad_norm': 0.4427056312561035, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9025, 'grad_norm': 0.4763866662979126, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7975, 'grad_norm': 0.512148380279541, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.804, 'grad_norm': 0.41782137751579285, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9413, 'grad_norm': 0.5582929253578186, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.925, 'grad_norm': 0.4915519654750824, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8568, 'grad_norm': 0.500060498714447, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8445, 'grad_norm': 0.5539302229881287, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0211, 'grad_norm': 0.601305365562439, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.7263, 'grad_norm': 0.39714375138282776, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9552, 'grad_norm': 0.6298215389251709, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9701, 'grad_norm': 0.4341321885585785, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8377, 'grad_norm': 0.5479922890663147, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.7629, 'grad_norm': 0.4446237087249756, 'learning_rate': 0.0002, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8226, 'grad_norm': 0.43582475185394287, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.7017, 'grad_norm': 0.5314765572547913, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.7868, 'grad_norm': 0.6836410164833069, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.986, 'grad_norm': 0.47349828481674194, 'learning_rate': 0.0002, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9504, 'grad_norm': 0.49403882026672363, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9978, 'grad_norm': 1.04531991481781, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9755, 'grad_norm': 0.4066174328327179, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.7601, 'grad_norm': 0.4807087779045105, 'learning_rate': 0.0002, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7742, 'grad_norm': 0.48535242676734924, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8858, 'grad_norm': 0.4662754535675049, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8886, 'grad_norm': 0.7733725905418396, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9763, 'grad_norm': 0.5598810911178589, 'learning_rate': 0.0002, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7986, 'grad_norm': 0.5585771203041077, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8162, 'grad_norm': 0.515970766544342, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9247, 'grad_norm': 0.45261287689208984, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9691, 'grad_norm': 0.49826768040657043, 'learning_rate': 0.0002, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7927, 'grad_norm': 0.5330497622489929, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9026, 'grad_norm': 0.4904668927192688, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.7927, 'grad_norm': 0.504016637802124, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9238, 'grad_norm': 0.5446527004241943, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9101, 'grad_norm': 0.4311606287956238, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8057, 'grad_norm': 0.454147607088089, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.7452, 'grad_norm': 0.4750872552394867, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8389, 'grad_norm': 0.45679759979248047, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9269, 'grad_norm': 0.5630127191543579, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.7842, 'grad_norm': 0.4675162136554718, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8678, 'grad_norm': 0.5406028032302856, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0263, 'grad_norm': 0.5559172630310059, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.916, 'grad_norm': 0.5068910121917725, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.7867, 'grad_norm': 0.6899601221084595, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.7218, 'grad_norm': 0.6161937117576599, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8428, 'grad_norm': 0.5178557634353638, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8542, 'grad_norm': 0.515967071056366, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8056, 'grad_norm': 0.5255308747291565, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8419, 'grad_norm': 0.4430941939353943, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8522, 'grad_norm': 0.5634081959724426, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8008, 'grad_norm': 0.4711591601371765, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.7446, 'grad_norm': 0.533434271812439, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9943, 'grad_norm': 0.5148230195045471, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.788, 'grad_norm': 0.4638107419013977, 'learning_rate': 0.0002, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7587, 'grad_norm': 0.505171000957489, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0146, 'grad_norm': 0.4619484841823578, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.938, 'grad_norm': 0.4720548093318939, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.6518, 'grad_norm': 0.7279320955276489, 'learning_rate': 0.0002, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8882, 'grad_norm': 0.6208555102348328, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9408, 'grad_norm': 0.5806399583816528, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.7492, 'grad_norm': 0.7753583192825317, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8211, 'grad_norm': 0.5208086371421814, 'learning_rate': 0.0002, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7966, 'grad_norm': 0.5538206696510315, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8548, 'grad_norm': 0.5811254978179932, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8381, 'grad_norm': 0.4040652811527252, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8966, 'grad_norm': 0.5584394931793213, 'learning_rate': 0.0002, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6174, 'grad_norm': 0.5729421377182007, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9071, 'grad_norm': 0.5408519506454468, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.004, 'grad_norm': 0.4370625615119934, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.733, 'grad_norm': 0.6436948776245117, 'learning_rate': 0.0002, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0391, 'grad_norm': 0.5857195854187012, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.7737, 'grad_norm': 0.5405424237251282, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9035, 'grad_norm': 0.6161229014396667, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8357, 'grad_norm': 0.5005295276641846, 'learning_rate': 0.0002, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6843, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:360\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 360\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/trainer.py:2423\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2423\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/trainer.py:2503\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(staging_output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 2503\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[1;32m   2505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(staging_output_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/trainer.py:2628\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   2623\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   2624\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   2625\u001b[0m     )\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2628\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   2631\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   2633\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/torch/serialization.py:863\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    862\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 863\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 허깅페이스에서 훈련 도중 러닝레이트가 수정 안되는 경우가 많다 한다. \n",
    "- 스케쥴러 변경을 시도해 보기로 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/experi_04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
