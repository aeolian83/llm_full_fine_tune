{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight and Bias Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"QLoRA_Instruction_finetune_06\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = load_dataset(\"nlpai-lab/kullm-v2\", cache_dir=\"/mnt/t7/.cache/huggingface/datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.shuffle(seed=2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'instruction', 'input', 'output'],\n",
       "    num_rows: 152630\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.train_test_split(test_size=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'instruction', 'input', 'output'],\n",
       "        num_rows: 12210\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'instruction', 'input', 'output'],\n",
       "        num_rows: 140420\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'alpaca_{idx}',\n",
       " 'instruction': '다음 단락의 흐름에 대해 1~10점 척도로 평가하세요.',\n",
       " 'input': '산타바바라시는 남부 캘리포니아의 태평양 연안에 위치한 아름답고 활기찬 지역 사회입니다. 아름다운 해변, 온화한 날씨, 스페인 건축 양식으로 유명합니다.',\n",
       " 'output': '문단의 흐름은 9점으로 평가하고 싶습니다. 문장이 일관성 있고 매끄럽게 이어지며 산타바바라라는 도시를 잘 설명하는 정보와 함께 잘 어우러져 있습니다.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/llama-2-ko-7b\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit QLoRA 학습을 위한 설정\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7105cd91c61840cbb8360a16a8a51998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    system_prompt = f\"### instruction: {sample['instruction']}\"\n",
    "    input = f\"### input: {sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    output = f\"### output: {sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [system_prompt, input, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_instruction(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bc816d68a4469b9e46976494227c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/12210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ko_instruction_01['train'].map(template_dataset, remove_columns=list(ko_instruction_01['train'].features), num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### instruction: 다음 단락의 흐름에 대해 1~10점 척도로 평가하세요.\\n\\n### input: 산타바바라시는 남부 캘리포니아의 태평양 연안에 위치한 아름답고 활기찬 지역 사회입니다. 아름다운 해변, 온화한 날씨, 스페인 건축 양식으로 유명합니다.\\n\\n### output: 문단의 흐름은 9점으로 평가하고 싶습니다. 문장이 일관성 있고 매끄럽게 이어지며 산타바바라라는 도시를 잘 설명하는 정보와 함께 잘 어우러져 있습니다.</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Argument Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./checkpoint/experi_05\"\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 20\n",
    "save_total_limit=5\n",
    "num_train_epochs = 2\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf191a993da47d4b00dc8d297f0a37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/00_QLoRa_fine_tune/wandb/run-20240429_141533-4c80enrm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_06/runs/4c80enrm' target=\"_blank\">legendary-terrain-2</a></strong> to <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_06' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_06' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_06</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_06/runs/4c80enrm' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_06/runs/4c80enrm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5823ab48c2de49a2ae930da55e58b5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0978, 'grad_norm': 0.1142578125, 'learning_rate': 5.449591280653951e-06, 'epoch': 0.0}\n",
      "{'loss': 2.408, 'grad_norm': 0.171875, 'learning_rate': 1.0899182561307902e-05, 'epoch': 0.0}\n",
      "{'loss': 2.7818, 'grad_norm': 0.1611328125, 'learning_rate': 1.6348773841961854e-05, 'epoch': 0.0}\n",
      "{'loss': 3.1363, 'grad_norm': 0.263671875, 'learning_rate': 2.1798365122615804e-05, 'epoch': 0.01}\n",
      "{'loss': 3.9882, 'grad_norm': 1.40625, 'learning_rate': 2.7247956403269757e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4287, 'grad_norm': 0.193359375, 'learning_rate': 3.269754768392371e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2763, 'grad_norm': 0.1953125, 'learning_rate': 3.8147138964577664e-05, 'epoch': 0.01}\n",
      "{'loss': 2.385, 'grad_norm': 0.380859375, 'learning_rate': 4.359673024523161e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5278, 'grad_norm': 0.83984375, 'learning_rate': 4.9046321525885565e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5841, 'grad_norm': 6.65625, 'learning_rate': 5.4495912806539515e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8774, 'grad_norm': 0.498046875, 'learning_rate': 5.994550408719346e-05, 'epoch': 0.02}\n",
      "{'loss': 1.9486, 'grad_norm': 0.470703125, 'learning_rate': 6.539509536784741e-05, 'epoch': 0.02}\n",
      "{'loss': 2.2239, 'grad_norm': 0.4140625, 'learning_rate': 7.084468664850136e-05, 'epoch': 0.02}\n",
      "{'loss': 2.0892, 'grad_norm': 0.9296875, 'learning_rate': 7.629427792915533e-05, 'epoch': 0.02}\n",
      "{'loss': 1.9443, 'grad_norm': 3.09375, 'learning_rate': 8.174386920980927e-05, 'epoch': 0.02}\n",
      "{'loss': 2.0929, 'grad_norm': 0.27734375, 'learning_rate': 8.719346049046322e-05, 'epoch': 0.03}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(\"./results/experi_07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(\"./results/experi_07\")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"### instruction: AI의 정의에 대해 설명해줘.\\n\\n### output:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### instruction: AI의 정의에 대해 설명해줘.\n",
      "\n",
      "### output: AI는 인공지능을 말합니다.​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### instruction: AI의 정의에 대해 설명해줘.\n",
      "\n",
      "### output: AI는 인공지능을 말합니다.​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   835, 15278, 29901,   319, 29902, 30708, 32984, 31054, 32550,\n",
       "        32750, 31435, 45189, 29889,    13,    13,  2277, 29937,  1962, 29901,\n",
       "          319, 29902, 31081, 37651, 31286, 32047, 32111, 29889, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166,\n",
       "        30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166, 30166],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
