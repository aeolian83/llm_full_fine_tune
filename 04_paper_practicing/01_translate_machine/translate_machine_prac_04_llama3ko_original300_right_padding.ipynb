{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: utf-8 -*- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight and Bias Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"translate_machine_llama3ko_with_orgin_data_300\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/train_data_300.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "len(train_data)\n",
    "\n",
    "with open('./data/validation_data_28.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# DatasetDictë¡œ \"train\"ê³¼ \"test\" ë°ì´í„°ì…‹ ë¬¶ê¸°\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Explainable AI is becoming increasingly important as AI systems are integrated into various industries. With the rise of cloud computing, massive datasets can be processed and analyzed more efficiently, but this often comes at the cost of transparency. By combining explainable AI with cloud computing, organizations can ensure that their AI models are both powerful and understandable. Meanwhile, edge computing allows for data processing closer to the source, which can enhance real-time decision-making capabilities. Integrating explainable AI with edge computing can further improve the trustworthiness and reliability of these real-time systems.',\n",
       " 'korean': 'ì„¤ëª… ê°€ëŠ¥í•œ AI(explainable AI)ëŠ” AI ì‹œìŠ¤í…œì´ ë‹¤ì–‘í•œ ì‚°ì—…ì— í†µí•©ë¨ì— ë”°ë¼ ì ì  ë” ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤. í´ë¼ìš°ë“œ ì»´í“¨íŒ…(cloud computing)ì˜ ë°œì „ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” ì¢…ì¢… íˆ¬ëª…ì„±ì˜ ëŒ€ê°€ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì„¤ëª… ê°€ëŠ¥í•œ AI(explainable AI)ì™€ í´ë¼ìš°ë“œ ì»´í“¨íŒ…(cloud computing)ì„ ê²°í•©í•˜ë©´ ì¡°ì§ì€ ê°•ë ¥í•˜ë©´ì„œë„ ì´í•´í•  ìˆ˜ ìˆëŠ” AI ëª¨ë¸ì„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œí¸, ì—£ì§€ ì»´í“¨íŒ…(edge computing)ì€ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì†ŒìŠ¤ì— ë” ê°€ê¹ê²Œ í•˜ì—¬ ì‹¤ì‹œê°„ ì˜ì‚¬ ê²°ì • ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„¤ëª… ê°€ëŠ¥í•œ AI(explainable AI)ì™€ ì—£ì§€ ì»´í“¨íŒ…(edge computing)ì„ í†µí•©í•˜ë©´ ì´ëŸ¬í•œ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œì˜ ì‹ ë¢°ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',\n",
       " 'terms': 'explainable AI, cloud computing, edge computing'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/Llama-3-KoEn-8B\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for 4-bit QLoRA Training(4bit QLoRA í•™ìŠµì„ ìœ„í•œ ì„¤ì •)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Nvidiaì˜ Ampere ì•„í‚¤í…ì²˜ ì´í›„ ê°€ì†ê¸°ëŠ” bf16ìœ¼ë¡œ ì†ë„ í–¥ìƒì„ ê¾€í• ìˆ˜ ìˆë‹¤. \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# bnb_4bit_quant_type=\"nf4\" ì„¤ì •ìƒ ê¸°ë³¸ê°’ì€ bnb_4bit_quant_type=\"fp4\"ì´ë‚˜ í—ˆê¹…í˜ì´ìŠ¤ ì €ìë“¤ì— ì˜í•˜ë©´\n",
    "# ê²½í—˜ì  ê²°ê³¼ë¡œ \"nf4\"ê°€ ê²°ê³¼ê°€ ë” ì¢‹ì•˜ë‹¤ê³  í•œë‹¤. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# bnb_4bit_use_double_quant=Trueë¡œ í•˜ë©´ ë§¤ê°œë³€ìˆ˜ë‹¨ 0.4bitì„ ì¶”ê°€ë¡œ ì ˆì•½ í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f45a4053eb74c249fb7a52375f5b6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# model.config.pretraining_tp = 1\n",
    "# ì¢…ì¢… QLoRA ì½”ë“œì— ì´ ì½”ë“œê°€ ë³´ì´ëŠ”ë° ë³‘ë ¬ í•™ìŠµì— ì“°ì´ëŠ” ì½”ë“œë¡œ ë³´ì¸ë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_model_dir)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) # pad_tokenì´ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ embeddingê³¼ language modeling headë¥¼ resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english', 'korean', 'terms'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting function\n",
    "def formatting_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"english\"])):\n",
    "        text = f\"Translate input sentence to Korean \\n### Input: {example['english'][i]} \\n### Translated: {example['korean'][i]}\" + tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "response_template = \" \\n### Translated:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# https://huggingface.co/docs/trl/sft_trainer#using-tokenids-directly-for-responsetemplate ì°¸ê³ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_template = \" \\n### Translated:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "# response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "# # data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Argument Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = checkpoint_dir\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 20\n",
    "save_total_limit=5\n",
    "num_train_epochs = 2\n",
    "logging_steps = 20\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1869b2be994430a86e39a7d5612c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/04_paper_practicing/01_translate_machine/wandb/run-20240628_170511-z1edsqwk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/z1edsqwk' target=\"_blank\">kind-grass-5</a></strong> to <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/z1edsqwk' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/z1edsqwk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-220 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-240 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-260 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-280 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01/checkpoint-300 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.1971247903505961, metrics={'train_runtime': 490.1479, 'train_samples_per_second': 1.224, 'train_steps_per_second': 0.612, 'total_flos': 9466790000295936.0, 'train_loss': 0.1971247903505961, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "lora_model_save_dir = \"./translate_machine_llama3ko_nonintsuct_origindata300_01_right_padding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(lora_model_save_dir, save_embedding_layers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig.from_pretrained(lora_model_save_dir)\n",
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aeolian83/llama_ko_sft_gugugo_experi_01/commit/19dd71bb9c3aebf4c5be4ad2c4a15d34a7a999d6', commit_message='Upload tokenizer', commit_description='', oid='19dd71bb9c3aebf4c5be4ad2c4a15d34a7a999d6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.push_to_hub('aeolian83/llama_ko_sft_gugugo_experi_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=lora_model_save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Group sparsity is a concept in multilinear algebra that promotes sparsity patterns within groups of variables. This technique is particularly useful in applications involving high-dimensional data, where it helps to identify relevant groups of features. In the context of factor graphs, group sparsity can enhance the efficiency of inference algorithms by reducing the complexity of the graph structure. Multilinear algebra provides the mathematical foundation for understanding and manipulating the interactions between these groups. By leveraging group sparsity and multilinear algebra, factor graphs can be optimized to handle large-scale problems more effectively.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test']['english'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    f'''\n",
    "Translate input sentence to Korean\n",
    "### Input: {dataset_dict['test']['english'][0]}\n",
    "''',\n",
    "    f'''\n",
    "Translate input sentence to Korean\n",
    "### Input: {dataset_dict['test']['english'][1]}\n",
    "''',\n",
    " f'''\n",
    "Translate input sentence to Korean\n",
    "### Input: {dataset_dict['test']['english'][2]}\n",
    "''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = tokenizer(examples, return_tensors=\"pt\", padding=True)['input_ids'].to(loaded_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = loaded_model.generate(example_batch, max_new_tokens = 1024, pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate input sentence to Korean\n",
      "### Input: Group sparsity is a concept in multilinear algebra that promotes sparsity patterns within groups of variables. This technique is particularly useful in applications involving high-dimensional data, where it helps to identify relevant groups of features. In the context of factor graphs, group sparsity can enhance the efficiency of inference algorithms by reducing the complexity of the graph structure. Multilinear algebra provides the mathematical foundation for understanding and manipulating the interactions between these groups. By leveraging group sparsity and multilinear algebra, factor graphs can be optimized to handle large-scale problems more effectively.\n",
      "ì œì•ˆ: ê·¸ë£¹ í¬ì†Œì„±(group sparsity)ì€ ë³€ìˆ˜ ì§‘í•© ë‚´ì—ì„œ í¬ì†Œì„± íŒ¨í„´ì„ ì´‰ì§„í•˜ëŠ” ë‹¤ë³€ìˆ˜ ëŒ€ìˆ˜(multilinear algebra)ì˜ ê°œë…ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì‘ìš© ë¶„ì•¼ì—ì„œ íŠ¹íˆ ìœ ìš©í•˜ë©°, ê´€ë ¨ëœ íŠ¹ì§• ì§‘í•©ì„ ì‹ë³„í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì¸ì ê·¸ë˜í”„(factor graphs)ì—ì„œ ê·¸ë£¹ í¬ì†Œì„±(group sparsity)ì€ ê·¸ë˜í”„ êµ¬ì¡°ì˜ ë³µì¡ì„±ì„ ì¤„ì—¬ ì¶”ë¡  ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë³€ìˆ˜ ëŒ€ìˆ˜(multilinear algebra)ëŠ” ì´ëŸ¬í•œ ê·¸ë£¹ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ì´í•´í•˜ê³  ì¡°ì‘í•˜ëŠ” ìˆ˜í•™ì  ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ë£¹ í¬ì†Œì„±(group sparsity)ê³¼ ë‹¤ë³€ìˆ˜ ëŒ€ìˆ˜(multilinear algebra)ë¥¼ í™œìš©í•¨ìœ¼ë¡œì¨ ì¸ì ê·¸ë˜í”„(factor graphs)ëŠ” ëŒ€ê·œëª¨ ë¬¸ì œë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.- 1st\n",
      "- 11:00 pm\n",
      "- 2 comments\n",
      "I'm not sure if I've mentioned this before, but I'm a big fan of the TV show \"The Office.\" I love the characters, the humor, the drama, the awkwardness, the romance, the office politics, the office pranks, the office parties, the office gossip, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties, the office parties,\n",
      "####################################################################################################\n",
      "\n",
      "Translate input sentence to Korean\n",
      "### Input: Latent space models are powerful tools in machine learning that represent data in a lower-dimensional space. These models rely on latent variable inference to uncover hidden structures within the data. However, exact latent variable inference can be computationally intensive, which is why approximate inference methods are often used. Approximate inference techniques provide a practical way to estimate the latent variables without the need for exhaustive computation. By leveraging approximate inference, latent space models can efficiently handle large datasets and complex data structures.\n",
      "ì œì•ˆëœ ì ì¬ ê³µê°„ ëª¨ë¸(latent space models)ì€ ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ì— í‘œí˜„í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ë°ì´í„° ë‚´ì˜ ìˆ¨ê²¨ì§„ êµ¬ì¡°ë¥¼ ë°œê²¬í•˜ê¸° ìœ„í•´ ì ì¬ ë³€ìˆ˜ ì¶”ë¡ (latent variable inference)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì •í™•í•œ ì ì¬ ë³€ìˆ˜ ì¶”ë¡ (latent variable inference)ì€ ê³„ì‚°ì ìœ¼ë¡œ ì§‘ì¤‘ì ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê·¼ì‚¬ ì¶”ë¡ (approximate inference) ë°©ë²•ì´ ì¢…ì¢… ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¼ì‚¬ ì¶”ë¡ (approximate inference) ê¸°ìˆ ì€ ê³„ì‚° ì§‘ì¤‘ì ì¸ ì‘ì—… ì—†ì´ ì ì¬ ë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ì‹¤ìš©ì ì¸ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¼ì‚¬ ì¶”ë¡ (approximate inference)ì„ í™œìš©í•˜ë©´ ì ì¬ ê³µê°„ ëª¨ë¸(latent space models)ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ê³¼ ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.- 1st\n",
      "- 11:00 pm\n",
      "- 2 comments\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm\n",
      "- 31st\n",
      "- 11:00 pm- 1st\n",
      "- 12:00 pm\n",
      "- 2 comments\n",
      "- 2nd\n",
      "- 12:00 pm\n",
      "- 3rd\n",
      "- 12:00 pm\n",
      "- 4th\n",
      "- 12:00 pm\n",
      "- 5th\n",
      "- 12:00 pm\n",
      "- 6th\n",
      "- 12:00 pm\n",
      "- 7th\n",
      "- 12:00 pm\n",
      "- 8th\n",
      "- 12:00 pm\n",
      "- 9th\n",
      "- 12:00 pm\n",
      "- 10th\n",
      "- 12:00 pm\n",
      "- 11th\n",
      "- 12:00 pm\n",
      "- 12th\n",
      "- 12:00 pm\n",
      "- 13th\n",
      "- 12:00 pm\n",
      "- 14th\n",
      "- 12:00 pm\n",
      "- 15th\n",
      "- 12:00 pm\n",
      "- 16th\n",
      "- 12:00 pm\n",
      "- 17th\n",
      "- 12:00 pm\n",
      "- 18th\n",
      "- 12:00 pm\n",
      "- 19th\n",
      "- 12:00 pm\n",
      "- 20th\n",
      "- 12:00 pm\n",
      "- 21st\n",
      "- 12:00 pm\n",
      "- 22nd\n",
      "- 12:00 pm\n",
      "- 23rd\n",
      "- 12:00 pm\n",
      "- 24th\n",
      "- 12:00 pm\n",
      "- 25th\n",
      "- 12:00 pm\n",
      "- 26th\n",
      "- 12:00 pm\n",
      "- 27th\n",
      "- 12:00 pm\n",
      "- 28th\n",
      "- 12:00 pm\n",
      "- 29th\n",
      "- 12:00 pm\n",
      "- 30th\n",
      "- 12:00 pm\n",
      "- 31st\n",
      "- 12:00 pm- 1st\n",
      "- 12:00 pm\n",
      "- 2nd\n",
      "- 12:00 pm\n",
      "- 3rd\n",
      "- 12:00 pm\n",
      "- 4th\n",
      "- 12:00 pm\n",
      "- 5th\n",
      "- 12:00 pm\n",
      "- 6th\n",
      "- 12:00 pm\n",
      "- 7th\n",
      "- 12:00 pm\n",
      "- 8th\n",
      "- 12:00 pm\n",
      "- 9th\n",
      "####################################################################################################\n",
      "\n",
      "Translate input sentence to Korean\n",
      "### Input: Domain adaptation is a technique used to adapt a machine learning model trained on one domain to perform well on a different but related domain. This is crucial for applications where labeled data in the target domain is scarce. On the other hand, domain generalization aims to train a model that performs well across multiple domains without needing retraining. Style transfer is often used in these contexts to help models learn invariant features by transferring style from one domain to another. Combining domain adaptation and style transfer can significantly enhance the model's ability to generalize across domains, thus achieving better domain generalization.\n",
      "### ì¶œë ¥: ë„ë©”ì¸ ì ì‘(domain adaptation)ì€ í•œ ë„ë©”ì¸ì—ì„œ í›ˆë ¨ëœ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ë‹¤ë¥¸ ê´€ë ¨ ë„ë©”ì¸ì—ì„œ ì˜ ìˆ˜í–‰í•˜ë„ë¡ ì ì‘ì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ëŠ” ëª©í‘œ ë„ë©”ì¸ì—ì„œ ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤. ë°˜ë©´ì— ë„ë©”ì¸ ì¼ë°˜í™”(domain generalization)ëŠ” ëª¨ë¸ì´ ì—¬ëŸ¬ ë„ë©”ì¸ì—ì„œ ì¬í›ˆë ¨ ì—†ì´ ì˜ ìˆ˜í–‰í•˜ë„ë¡ í›ˆë ¨í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ìŠ¤íƒ€ì¼ ì „ì´(style transfer)ëŠ” í•œ ë„ë©”ì¸ì—ì„œ ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ìŠ¤íƒ€ì¼ì„ ì „ì´í•˜ì—¬ ëª¨ë¸ì´ ë¶ˆë³€ íŠ¹ì§•ì„ í•™ìŠµí•˜ë„ë¡ ë•ëŠ” ë° ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. ë„ë©”ì¸ ì ì‘(domain adaptation)ê³¼ ìŠ¤íƒ€ì¼ ì „ì´(style transfer)ë¥¼ ê²°í•©í•˜ë©´ ëª¨ë¸ì´ ë„ë©”ì¸ ê°„ ì¼ë°˜í™”(domain generalization)ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì–´ ë” ë‚˜ì€ ë„ë©”ì¸ ì¼ë°˜í™”(domain generalization)ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.- 1st\n",
      "- 11:00 pm\n",
      "- 2 comments\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm\n",
      "- 31st\n",
      "- 11:00 pm- 1st\n",
      "- 12:00 pm\n",
      "- 2 comments\n",
      "I'm not sure if I've mentioned this before, but I'm a big fan of the show \"The Office.\" I love the characters, the humor, the drama, the awkwardness, the awkwardness, the awkwardness. I love it all. I'm a little sad that the show is coming to an end, but I'm looking forward to the last season. I'm also looking forward to the new show that's coming out in the fall, \"The New Girl.\" I'm a big fan of Zooey Deschanel, and I'm excited to see what she does in this show. I'm also excited to see what she wears. I'm a big fan of her style. I'm also a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big fan of her hair. I'm a big\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "outputs = [tokenizer.decode(t, skip_special_tokens=True) for t in output_tokens]\n",
    "for o in outputs:\n",
    "    print(o)\n",
    "    print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
