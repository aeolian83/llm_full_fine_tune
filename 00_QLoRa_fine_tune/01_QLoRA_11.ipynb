{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "1. https://github.com/ashishpatel26/LLM-Finetuning/blob/main/7.FineTune_LLAMA2_with_QLORA.ipynb  \n",
    "2. https://huggingface.co/blog/4bit-transformers-bitsandbytes  \n",
    "3. https://pytorch.org/blog/finetune-llms/  \n",
    "4. https://colab.research.google.com/drive/1vIjBtePIZwUaHWfjfNHzBjwuXOyU_ugD?usp=sharing#scrollTo=6k_nL6xJMZW2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight and Bias Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"QLoRA_Instruction_finetune_08\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = load_dataset(\"aeolian83/OpenOrca-gugugo-ko_test\", cache_dir=\"/mnt/t7/.cache/huggingface/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.shuffle(seed=2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 1008132\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 1232162\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'system_prompt', 'response', 'id'],\n",
       "    num_rows: 1008132\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_instruction_01 = ko_instruction_01.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 806505\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'system_prompt', 'response', 'id'],\n",
       "        num_rows: 201627\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"과제 정의와 입력이 주어졌을 때 출력으로 답하세요. 이 과제에서는 자연어로 작성된 5개의 문장으로 구성된 짧은 이야기가 주어집니다. 그러나 주어진 이야기의 순서가 올바르지 않습니다. 주어진 5개의 문장을 올바른 순서로 반환하여 올바른 순서로 새로운 흐름이 있는 일관된 짧은 이야기를 만드는 것이 여러분의 임무입니다. 올바른 순서로 5개의 문장이 주어졌을 때 '23415'와 같이 올바른 순서로 답을 생성하세요.\\n\\n문장1: 그는 가까운 병원으로 비틀거리며 걸어갔습니다. 문장2: 조는 어느 날 밤 집으로 걸어가고 있었습니다. 문장3: 갑자기 한 남자가 다가와 그를 찌르고 말았습니다. 문장4: 그는 회복하는 데 2주 동안 병원에서 입원했습니다. 문장5: 조는 심하게 다쳤어요!\\n\",\n",
       " 'system_prompt': '정의에 어떻게 사용했는지 설명해 주세요.',\n",
       " 'response': \"문장이 일관되게 전개되도록 문장의 올바른 순서는 '25314'입니다. 이 순서는 논리적 순서를 제공합니다: 먼저 조는 어느 날 밤 집으로 걸어가고 있었습니다(문장2), 그 다음에는 한 남자가 다가와 그를 찌르고(문장3), 그 결과 그는 심하게 다쳤습니다(문장5). 그 후 그는 가까운 병원으로 비틀거리며 걸어갔고(문장1), 회복을 위해 2주 동안 그곳에 있었습니다(문장4).\",\n",
       " 'id': 'niv.379769'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_instruction_01[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"aeolian83/OpenOrca-gugugo-ko_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/llama-2-ko-7b\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for 4-bit QLoRA Training(4bit QLoRA 학습을 위한 설정)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Nvidia의 Ampere 아키텍처 이후 가속기는 bf16으로 속도 향상을 꾀할수 있다. \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# bnb_4bit_quant_type=\"nf4\" 설정상 기본값은 bnb_4bit_quant_type=\"fp4\"이나 허깅페이스 저자들에 의하면\n",
    "# 경험적 결과로 \"nf4\"가 결과가 더 좋았다고 한다. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# bnb_4bit_use_double_quant=True로 하면 매개변수단 0.4bit을 추가로 절약 할 수 있다고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4faf9a401544ce3ad14f6d9e2d5d8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# model.config.pretraining_tp = 1\n",
    "# 종종 QLoRA 코드에 이 코드가 보이는데 병렬 학습에 쓰이는 코드로 보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_model_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드를 쓰지 않는 경우(물론 패딩 토큰을 별도로 사용하는 경우에 해당됨) loss가 0으로 떨어지는 경우가 있다함\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(46337, 4096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) # pad_token이 추가되었으므로 embedding과 language modeling head를 resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    system_prompt = f\"### system prompt: {sample['system_prompt']}\"\n",
    "    input = f\"### question: {sample['question']}\" if len(sample[\"question\"]) > 0 else None\n",
    "    output = f\"### response: {sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [system_prompt, input, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_instruction(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0345c40ebb483c8fc7a97c1daed841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/806505 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ko_instruction_01['train'].map(template_dataset, remove_columns=list(ko_instruction_01['train'].features), num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### system prompt: 정의에 어떻게 사용했는지 설명해 주세요.\\n\\n### question: 과제 정의와 입력이 주어졌을 때 출력으로 답하세요. 이 과제에서는 자연어로 작성된 5개의 문장으로 구성된 짧은 이야기가 주어집니다. 그러나 주어진 이야기의 순서가 올바르지 않습니다. 주어진 5개의 문장을 올바른 순서로 반환하여 올바른 순서로 새로운 흐름이 있는 일관된 짧은 이야기를 만드는 것이 여러분의 임무입니다. 올바른 순서로 5개의 문장이 주어졌을 때 '23415'와 같이 올바른 순서로 답을 생성하세요.\\n\\n문장1: 그는 가까운 병원으로 비틀거리며 걸어갔습니다. 문장2: 조는 어느 날 밤 집으로 걸어가고 있었습니다. 문장3: 갑자기 한 남자가 다가와 그를 찌르고 말았습니다. 문장4: 그는 회복하는 데 2주 동안 병원에서 입원했습니다. 문장5: 조는 심하게 다쳤어요!\\n\\n\\n### response: 문장이 일관되게 전개되도록 문장의 올바른 순서는 '25314'입니다. 이 순서는 논리적 순서를 제공합니다: 먼저 조는 어느 날 밤 집으로 걸어가고 있었습니다(문장2), 그 다음에는 한 남자가 다가와 그를 찌르고(문장3), 그 결과 그는 심하게 다쳤습니다(문장5). 그 후 그는 가까운 병원으로 비틀거리며 걸어갔고(문장1), 회복을 위해 2주 동안 그곳에 있었습니다(문장4).</s>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Argument Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/gugugo-experi_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = checkpoint_dir\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 500\n",
    "save_total_limit=5\n",
    "num_train_epochs = 0.9\n",
    "logging_steps = 20\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd6145fbc7548c59bb2c4dfc1132b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/806505 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/00_QLoRa_fine_tune/wandb/run-20240430_201555-qp0ro9yf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08/runs/qp0ro9yf' target=\"_blank\">celestial-cloud-2</a></strong> to <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08/runs/qp0ro9yf' target=\"_blank\">https://wandb.ai/aeolian83/QLoRA_Instruction_finetune_08/runs/qp0ro9yf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69479fd4f8fb4ef19d965f94195bcb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/362927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9643, 'grad_norm': 0.11669921875, 'learning_rate': 3.6737692872887585e-07, 'epoch': 0.0}\n",
      "{'loss': 3.5213, 'grad_norm': 0.1728515625, 'learning_rate': 7.347538574577517e-07, 'epoch': 0.0}\n",
      "{'loss': 3.5268, 'grad_norm': 0.107421875, 'learning_rate': 1.1021307861866277e-06, 'epoch': 0.0}\n",
      "{'loss': 3.2377, 'grad_norm': 0.1513671875, 'learning_rate': 1.4695077149155034e-06, 'epoch': 0.0}\n",
      "{'loss': 3.9255, 'grad_norm': 0.2294921875, 'learning_rate': 1.8368846436443793e-06, 'epoch': 0.0}\n",
      "{'loss': 3.0301, 'grad_norm': 0.1513671875, 'learning_rate': 2.2042615723732554e-06, 'epoch': 0.0}\n",
      "{'loss': 3.4576, 'grad_norm': 0.185546875, 'learning_rate': 2.571638501102131e-06, 'epoch': 0.0}\n",
      "{'loss': 3.6152, 'grad_norm': 0.126953125, 'learning_rate': 2.9390154298310068e-06, 'epoch': 0.0}\n",
      "{'loss': 3.1777, 'grad_norm': 0.1455078125, 'learning_rate': 3.3063923585598822e-06, 'epoch': 0.0}\n",
      "{'loss': 4.0159, 'grad_norm': 0.361328125, 'learning_rate': 3.6737692872887586e-06, 'epoch': 0.0}\n",
      "{'loss': 2.9886, 'grad_norm': 0.12890625, 'learning_rate': 4.0411462160176345e-06, 'epoch': 0.0}\n",
      "{'loss': 3.5289, 'grad_norm': 0.2041015625, 'learning_rate': 4.408523144746511e-06, 'epoch': 0.0}\n",
      "{'loss': 3.5071, 'grad_norm': 0.1376953125, 'learning_rate': 4.775900073475386e-06, 'epoch': 0.0}\n",
      "{'loss': 3.0493, 'grad_norm': 0.2177734375, 'learning_rate': 5.143277002204262e-06, 'epoch': 0.0}\n",
      "{'loss': 4.0445, 'grad_norm': 0.57421875, 'learning_rate': 5.510653930933138e-06, 'epoch': 0.0}\n",
      "{'loss': 2.9231, 'grad_norm': 0.1884765625, 'learning_rate': 5.8780308596620135e-06, 'epoch': 0.0}\n",
      "{'loss': 3.238, 'grad_norm': 0.3125, 'learning_rate': 6.245407788390889e-06, 'epoch': 0.0}\n",
      "{'loss': 3.5907, 'grad_norm': 0.1962890625, 'learning_rate': 6.6127847171197645e-06, 'epoch': 0.0}\n",
      "{'loss': 3.1111, 'grad_norm': 0.27734375, 'learning_rate': 6.980161645848642e-06, 'epoch': 0.0}\n",
      "{'loss': 3.7172, 'grad_norm': 0.78515625, 'learning_rate': 7.347538574577517e-06, 'epoch': 0.0}\n",
      "{'loss': 2.8735, 'grad_norm': 0.302734375, 'learning_rate': 7.714915503306393e-06, 'epoch': 0.0}\n",
      "{'loss': 3.2847, 'grad_norm': 0.5078125, 'learning_rate': 8.082292432035269e-06, 'epoch': 0.0}\n",
      "{'loss': 3.3523, 'grad_norm': 0.31640625, 'learning_rate': 8.449669360764144e-06, 'epoch': 0.0}\n",
      "{'loss': 2.8977, 'grad_norm': 0.3359375, 'learning_rate': 8.817046289493022e-06, 'epoch': 0.0}\n",
      "{'loss': 3.2181, 'grad_norm': 1.515625, 'learning_rate': 9.184423218221897e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4219, 'grad_norm': 0.51171875, 'learning_rate': 9.551800146950773e-06, 'epoch': 0.0}\n",
      "{'loss': 2.6931, 'grad_norm': 0.88671875, 'learning_rate': 9.919177075679648e-06, 'epoch': 0.0}\n",
      "{'loss': 2.5441, 'grad_norm': 0.369140625, 'learning_rate': 1.0286554004408523e-05, 'epoch': 0.0}\n",
      "{'loss': 2.3462, 'grad_norm': 0.66796875, 'learning_rate': 1.06539309331374e-05, 'epoch': 0.0}\n",
      "{'loss': 2.4817, 'grad_norm': 1.0859375, 'learning_rate': 1.1021307861866276e-05, 'epoch': 0.0}\n",
      "{'loss': 2.1198, 'grad_norm': 0.400390625, 'learning_rate': 1.1388684790595152e-05, 'epoch': 0.0}\n",
      "{'loss': 2.2072, 'grad_norm': 0.81640625, 'learning_rate': 1.1756061719324027e-05, 'epoch': 0.0}\n",
      "{'loss': 2.1549, 'grad_norm': 0.515625, 'learning_rate': 1.2123438648052903e-05, 'epoch': 0.0}\n",
      "{'loss': 2.0403, 'grad_norm': 0.482421875, 'learning_rate': 1.2490815576781778e-05, 'epoch': 0.0}\n",
      "{'loss': 2.1956, 'grad_norm': 1.6015625, 'learning_rate': 1.2858192505510655e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9375, 'grad_norm': 0.54296875, 'learning_rate': 1.3225569434239529e-05, 'epoch': 0.0}\n",
      "{'loss': 2.1279, 'grad_norm': 0.6796875, 'learning_rate': 1.3592946362968406e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9533, 'grad_norm': 0.283203125, 'learning_rate': 1.3960323291697283e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9048, 'grad_norm': 0.494140625, 'learning_rate': 1.4327700220426157e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8866, 'grad_norm': 1.359375, 'learning_rate': 1.4695077149155034e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9117, 'grad_norm': 0.5078125, 'learning_rate': 1.5062454077883908e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9176, 'grad_norm': 0.984375, 'learning_rate': 1.5429831006612787e-05, 'epoch': 0.0}\n",
      "{'loss': 2.0101, 'grad_norm': 0.345703125, 'learning_rate': 1.5797207935341662e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9372, 'grad_norm': 0.486328125, 'learning_rate': 1.6164584864070538e-05, 'epoch': 0.0}\n",
      "{'loss': 2.1319, 'grad_norm': 1.578125, 'learning_rate': 1.6531961792799413e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9472, 'grad_norm': 0.359375, 'learning_rate': 1.689933872152829e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9037, 'grad_norm': 1.0546875, 'learning_rate': 1.7266715650257164e-05, 'epoch': 0.0}\n",
      "{'loss': 2.0317, 'grad_norm': 0.451171875, 'learning_rate': 1.7634092578986043e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8814, 'grad_norm': 0.58984375, 'learning_rate': 1.8001469507714915e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8037, 'grad_norm': 1.25, 'learning_rate': 1.8368846436443794e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8854, 'grad_norm': 0.640625, 'learning_rate': 1.8736223365172666e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8857, 'grad_norm': 0.68359375, 'learning_rate': 1.9103600293901545e-05, 'epoch': 0.0}\n",
      "{'loss': 1.872, 'grad_norm': 0.333984375, 'learning_rate': 1.947097722263042e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9026, 'grad_norm': 0.470703125, 'learning_rate': 1.9838354151359296e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8504, 'grad_norm': 1.2890625, 'learning_rate': 2.020573108008817e-05, 'epoch': 0.0}\n",
      "{'loss': 1.871, 'grad_norm': 0.4609375, 'learning_rate': 2.0573108008817047e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8168, 'grad_norm': 0.59765625, 'learning_rate': 2.0940484937545922e-05, 'epoch': 0.0}\n",
      "{'loss': 1.934, 'grad_norm': 0.5390625, 'learning_rate': 2.13078618662748e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7372, 'grad_norm': 0.578125, 'learning_rate': 2.1675238795003673e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7659, 'grad_norm': 2.3125, 'learning_rate': 2.2042615723732552e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7805, 'grad_norm': 0.6640625, 'learning_rate': 2.2409992652461424e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6788, 'grad_norm': 0.515625, 'learning_rate': 2.2777369581190303e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6981, 'grad_norm': 0.318359375, 'learning_rate': 2.3144746509919175e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8411, 'grad_norm': 0.5625, 'learning_rate': 2.3512123438648054e-05, 'epoch': 0.0}\n",
      "{'loss': 1.9678, 'grad_norm': 1.5859375, 'learning_rate': 2.387950036737693e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7465, 'grad_norm': 0.51953125, 'learning_rate': 2.4246877296105805e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7666, 'grad_norm': 0.73828125, 'learning_rate': 2.461425422483468e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6848, 'grad_norm': 0.31640625, 'learning_rate': 2.4981631153563556e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7508, 'grad_norm': 0.59375, 'learning_rate': 2.5349008082292435e-05, 'epoch': 0.0}\n",
      "{'loss': 1.883, 'grad_norm': 1.40625, 'learning_rate': 2.571638501102131e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8585, 'grad_norm': 0.546875, 'learning_rate': 2.6083761939750186e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7344, 'grad_norm': 0.9609375, 'learning_rate': 2.6451138868479058e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5613, 'grad_norm': 0.296875, 'learning_rate': 2.6818515797207937e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7541, 'grad_norm': 0.67578125, 'learning_rate': 2.7185892725936812e-05, 'epoch': 0.0}\n",
      "{'loss': 1.76, 'grad_norm': 1.4375, 'learning_rate': 2.7553269654665688e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7352, 'grad_norm': 0.455078125, 'learning_rate': 2.7920646583394567e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8085, 'grad_norm': 0.78515625, 'learning_rate': 2.8288023512123442e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8156, 'grad_norm': 0.37890625, 'learning_rate': 2.8655400440852314e-05, 'epoch': 0.0}\n",
      "{'loss': 1.784, 'grad_norm': 0.62109375, 'learning_rate': 2.9022777369581193e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7057, 'grad_norm': 2.046875, 'learning_rate': 2.939015429831007e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7688, 'grad_norm': 0.5859375, 'learning_rate': 2.9757531227038944e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8208, 'grad_norm': 0.8984375, 'learning_rate': 3.0124908155767816e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8159, 'grad_norm': 0.58203125, 'learning_rate': 3.04922850844967e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8248, 'grad_norm': 0.5546875, 'learning_rate': 3.0859662013225574e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7353, 'grad_norm': 1.625, 'learning_rate': 3.122703894195444e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7967, 'grad_norm': 0.56640625, 'learning_rate': 3.1594415870683325e-05, 'epoch': 0.0}\n",
      "{'loss': 1.8385, 'grad_norm': 0.765625, 'learning_rate': 3.19617927994122e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6699, 'grad_norm': 0.435546875, 'learning_rate': 3.2329169728141076e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7401, 'grad_norm': 0.76171875, 'learning_rate': 3.2696546656869944e-05, 'epoch': 0.0}\n",
      "{'loss': 1.735, 'grad_norm': 1.0546875, 'learning_rate': 3.306392358559883e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7843, 'grad_norm': 0.67578125, 'learning_rate': 3.34313005143277e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7545, 'grad_norm': 0.65625, 'learning_rate': 3.379867744305658e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7644, 'grad_norm': 0.376953125, 'learning_rate': 3.416605437178545e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7106, 'grad_norm': 0.54296875, 'learning_rate': 3.453343130051433e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7871, 'grad_norm': 1.4609375, 'learning_rate': 3.4900808229243204e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7361, 'grad_norm': 0.5390625, 'learning_rate': 3.5268185157972086e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7263, 'grad_norm': 0.5859375, 'learning_rate': 3.5635562086700955e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7629, 'grad_norm': 0.423828125, 'learning_rate': 3.600293901542983e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7939, 'grad_norm': 0.490234375, 'learning_rate': 3.6370315944158706e-05, 'epoch': 0.0}\n",
      "{'loss': 1.674, 'grad_norm': 1.28125, 'learning_rate': 3.673769287288759e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8515, 'grad_norm': 0.45703125, 'learning_rate': 3.710506980161646e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8024, 'grad_norm': 0.89453125, 'learning_rate': 3.747244673034533e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6921, 'grad_norm': 0.419921875, 'learning_rate': 3.7839823659074215e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7351, 'grad_norm': 0.58203125, 'learning_rate': 3.820720058780309e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5749, 'grad_norm': 1.234375, 'learning_rate': 3.8574577516531966e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7235, 'grad_norm': 0.40234375, 'learning_rate': 3.894195444526084e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6393, 'grad_norm': 0.97265625, 'learning_rate': 3.9309331373989717e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6461, 'grad_norm': 0.490234375, 'learning_rate': 3.967670830271859e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7391, 'grad_norm': 0.5859375, 'learning_rate': 4.004408523144747e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5823, 'grad_norm': 1.3046875, 'learning_rate': 4.041146216017634e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7615, 'grad_norm': 0.625, 'learning_rate': 4.077883908890522e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7331, 'grad_norm': 0.80078125, 'learning_rate': 4.1146216017634094e-05, 'epoch': 0.01}\n",
      "{'loss': 1.671, 'grad_norm': 0.3515625, 'learning_rate': 4.151359294636297e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7184, 'grad_norm': 0.61328125, 'learning_rate': 4.1880969875091845e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7359, 'grad_norm': 1.53125, 'learning_rate': 4.224834680382072e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7101, 'grad_norm': 0.408203125, 'learning_rate': 4.26157237325496e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7068, 'grad_norm': 0.7734375, 'learning_rate': 4.298310066127848e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6505, 'grad_norm': 0.412109375, 'learning_rate': 4.335047759000735e-05, 'epoch': 0.01}\n",
      "{'loss': 1.594, 'grad_norm': 0.55078125, 'learning_rate': 4.371785451873622e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6683, 'grad_norm': 1.15625, 'learning_rate': 4.4085231447465105e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7307, 'grad_norm': 0.578125, 'learning_rate': 4.445260837619398e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7164, 'grad_norm': 1.0234375, 'learning_rate': 4.481998530492285e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6868, 'grad_norm': 0.4453125, 'learning_rate': 4.518736223365173e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6966, 'grad_norm': 0.5546875, 'learning_rate': 4.5554739162380606e-05, 'epoch': 0.01}\n",
      "{'loss': 1.636, 'grad_norm': 1.3515625, 'learning_rate': 4.592211609110948e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7087, 'grad_norm': 0.5390625, 'learning_rate': 4.628949301983835e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6815, 'grad_norm': 0.75390625, 'learning_rate': 4.665686994856723e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6942, 'grad_norm': 0.451171875, 'learning_rate': 4.702424687729611e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6611, 'grad_norm': 0.71875, 'learning_rate': 4.7391623806024984e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6035, 'grad_norm': 1.5, 'learning_rate': 4.775900073475386e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6575, 'grad_norm': 0.458984375, 'learning_rate': 4.8126377663482735e-05, 'epoch': 0.01}\n",
      "{'loss': 1.772, 'grad_norm': 0.83984375, 'learning_rate': 4.849375459221161e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6794, 'grad_norm': 0.5, 'learning_rate': 4.886113152094049e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7404, 'grad_norm': 0.5859375, 'learning_rate': 4.922850844966936e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5972, 'grad_norm': 1.7109375, 'learning_rate': 4.959588537839824e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7994, 'grad_norm': 0.53515625, 'learning_rate': 4.996326230712711e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6582, 'grad_norm': 0.6484375, 'learning_rate': 5.0330639235855994e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7234, 'grad_norm': 0.51171875, 'learning_rate': 5.069801616458487e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8481, 'grad_norm': 0.671875, 'learning_rate': 5.106539309331374e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5991, 'grad_norm': 1.5078125, 'learning_rate': 5.143277002204262e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7552, 'grad_norm': 0.4296875, 'learning_rate': 5.180014695077149e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7287, 'grad_norm': 0.80859375, 'learning_rate': 5.216752387950037e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6852, 'grad_norm': 0.4140625, 'learning_rate': 5.253490080822925e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6524, 'grad_norm': 0.85546875, 'learning_rate': 5.2902277736958116e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6435, 'grad_norm': 1.453125, 'learning_rate': 5.3269654665687e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7219, 'grad_norm': 0.486328125, 'learning_rate': 5.3637031594415874e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6294, 'grad_norm': 0.87109375, 'learning_rate': 5.400440852314474e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6902, 'grad_norm': 0.5703125, 'learning_rate': 5.4371785451873625e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6757, 'grad_norm': 0.453125, 'learning_rate': 5.473916238060251e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5409, 'grad_norm': 1.0234375, 'learning_rate': 5.5106539309331376e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7749, 'grad_norm': 0.4765625, 'learning_rate': 5.547391623806025e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6471, 'grad_norm': 0.7265625, 'learning_rate': 5.584129316678913e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6566, 'grad_norm': 0.46484375, 'learning_rate': 5.6208670095518e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6909, 'grad_norm': 0.765625, 'learning_rate': 5.6576047024246884e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5659, 'grad_norm': 2.125, 'learning_rate': 5.694342395297576e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6413, 'grad_norm': 0.6796875, 'learning_rate': 5.731080088170463e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6662, 'grad_norm': 1.234375, 'learning_rate': 5.767817781043351e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5013, 'grad_norm': 0.404296875, 'learning_rate': 5.8045554739162386e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7278, 'grad_norm': 0.56640625, 'learning_rate': 5.8412931667891255e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6139, 'grad_norm': 1.3515625, 'learning_rate': 5.878030859662014e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6731, 'grad_norm': 0.56640625, 'learning_rate': 5.9147685525349006e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6514, 'grad_norm': 0.796875, 'learning_rate': 5.951506245407789e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4839, 'grad_norm': 0.55078125, 'learning_rate': 5.9882439382806764e-05, 'epoch': 0.01}\n",
      "{'loss': 1.687, 'grad_norm': 0.68359375, 'learning_rate': 6.024981631153563e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5126, 'grad_norm': 2.15625, 'learning_rate': 6.0617193240264514e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7155, 'grad_norm': 0.353515625, 'learning_rate': 6.09845701689934e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6198, 'grad_norm': 0.94140625, 'learning_rate': 6.135194709772226e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5987, 'grad_norm': 0.51171875, 'learning_rate': 6.171932402645115e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6265, 'grad_norm': 0.74609375, 'learning_rate': 6.208670095518002e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5373, 'grad_norm': 1.4140625, 'learning_rate': 6.245407788390889e-05, 'epoch': 0.01}\n",
      "{'loss': 1.671, 'grad_norm': 0.4765625, 'learning_rate': 6.282145481263777e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5997, 'grad_norm': 0.87890625, 'learning_rate': 6.318883174136665e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6461, 'grad_norm': 0.4296875, 'learning_rate': 6.355620867009553e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5361, 'grad_norm': 0.51953125, 'learning_rate': 6.39235855988244e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6335, 'grad_norm': 1.8828125, 'learning_rate': 6.429096252755328e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7274, 'grad_norm': 0.5625, 'learning_rate': 6.465833945628215e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6169, 'grad_norm': 0.82421875, 'learning_rate': 6.502571638501103e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6289, 'grad_norm': 0.419921875, 'learning_rate': 6.539309331373989e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5871, 'grad_norm': 0.61328125, 'learning_rate': 6.576047024246878e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5834, 'grad_norm': 1.390625, 'learning_rate': 6.612784717119765e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8325, 'grad_norm': 0.46875, 'learning_rate': 6.649522409992653e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6316, 'grad_norm': 0.83203125, 'learning_rate': 6.68626010286554e-05, 'epoch': 0.01}\n",
      "{'loss': 1.569, 'grad_norm': 0.51953125, 'learning_rate': 6.722997795738428e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7475, 'grad_norm': 0.6015625, 'learning_rate': 6.759735488611316e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4786, 'grad_norm': 1.53125, 'learning_rate': 6.796473181484203e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7496, 'grad_norm': 0.51171875, 'learning_rate': 6.83321087435709e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4645, 'grad_norm': 0.59375, 'learning_rate': 6.869948567229978e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5762, 'grad_norm': 0.421875, 'learning_rate': 6.906686260102866e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6615, 'grad_norm': 0.69921875, 'learning_rate': 6.943423952975753e-05, 'epoch': 0.01}\n",
      "{'loss': 1.56, 'grad_norm': 1.7421875, 'learning_rate': 6.980161645848641e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6544, 'grad_norm': 0.486328125, 'learning_rate': 7.016899338721528e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5751, 'grad_norm': 1.0078125, 'learning_rate': 7.053637031594417e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5325, 'grad_norm': 0.451171875, 'learning_rate': 7.090374724467303e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6935, 'grad_norm': 0.51953125, 'learning_rate': 7.127112417340191e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5005, 'grad_norm': 1.4765625, 'learning_rate': 7.16385011021308e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8148, 'grad_norm': 0.51171875, 'learning_rate': 7.200587803085966e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5179, 'grad_norm': 0.86328125, 'learning_rate': 7.237325495958855e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5473, 'grad_norm': 0.47265625, 'learning_rate': 7.274063188831741e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7527, 'grad_norm': 0.640625, 'learning_rate': 7.310800881704629e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7085, 'grad_norm': 1.7734375, 'learning_rate': 7.347538574577518e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6526, 'grad_norm': 0.55078125, 'learning_rate': 7.384276267450404e-05, 'epoch': 0.01}\n",
      "{'loss': 1.697, 'grad_norm': 0.87109375, 'learning_rate': 7.421013960323291e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6136, 'grad_norm': 0.416015625, 'learning_rate': 7.45775165319618e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7351, 'grad_norm': 0.59375, 'learning_rate': 7.494489346069066e-05, 'epoch': 0.01}\n",
      "{'loss': 1.484, 'grad_norm': 1.5859375, 'learning_rate': 7.531227038941955e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7454, 'grad_norm': 0.396484375, 'learning_rate': 7.567964731814843e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5718, 'grad_norm': 0.921875, 'learning_rate': 7.604702424687729e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5922, 'grad_norm': 0.66015625, 'learning_rate': 7.641440117560618e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6329, 'grad_norm': 0.734375, 'learning_rate': 7.678177810433506e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5405, 'grad_norm': 1.703125, 'learning_rate': 7.714915503306393e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6435, 'grad_norm': 0.390625, 'learning_rate': 7.75165319617928e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6542, 'grad_norm': 0.625, 'learning_rate': 7.788390889052168e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5868, 'grad_norm': 0.51953125, 'learning_rate': 7.825128581925056e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6185, 'grad_norm': 0.578125, 'learning_rate': 7.861866274797943e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5259, 'grad_norm': 1.6875, 'learning_rate': 7.89860396767083e-05, 'epoch': 0.01}\n",
      "{'loss': 1.709, 'grad_norm': 0.5859375, 'learning_rate': 7.935341660543718e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5703, 'grad_norm': 0.8515625, 'learning_rate': 7.972079353416606e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5068, 'grad_norm': 0.40625, 'learning_rate': 8.008817046289494e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6043, 'grad_norm': 0.609375, 'learning_rate': 8.045554739162381e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4738, 'grad_norm': 1.2265625, 'learning_rate': 8.082292432035269e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6918, 'grad_norm': 0.4453125, 'learning_rate': 8.119030124908156e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6129, 'grad_norm': 0.8984375, 'learning_rate': 8.155767817781044e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5867, 'grad_norm': 0.5859375, 'learning_rate': 8.192505510653931e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5651, 'grad_norm': 0.703125, 'learning_rate': 8.229243203526819e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4531, 'grad_norm': 1.3828125, 'learning_rate': 8.265980896399706e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6648, 'grad_norm': 0.55859375, 'learning_rate': 8.302718589272594e-05, 'epoch': 0.01}\n",
      "{'loss': 1.546, 'grad_norm': 0.81640625, 'learning_rate': 8.339456282145481e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5867, 'grad_norm': 0.4609375, 'learning_rate': 8.376193975018369e-05, 'epoch': 0.01}\n",
      "{'loss': 1.557, 'grad_norm': 0.58984375, 'learning_rate': 8.412931667891258e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4922, 'grad_norm': 1.34375, 'learning_rate': 8.449669360764144e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6145, 'grad_norm': 0.412109375, 'learning_rate': 8.486407053637032e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6395, 'grad_norm': 0.890625, 'learning_rate': 8.52314474650992e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5828, 'grad_norm': 0.400390625, 'learning_rate': 8.559882439382807e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5504, 'grad_norm': 0.5859375, 'learning_rate': 8.596620132255696e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5433, 'grad_norm': 1.28125, 'learning_rate': 8.633357825128582e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6983, 'grad_norm': 0.6796875, 'learning_rate': 8.67009551800147e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4481, 'grad_norm': 1.125, 'learning_rate': 8.706833210874358e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6456, 'grad_norm': 0.474609375, 'learning_rate': 8.743570903747244e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5102, 'grad_norm': 0.6875, 'learning_rate': 8.780308596620132e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4125, 'grad_norm': 1.203125, 'learning_rate': 8.817046289493021e-05, 'epoch': 0.01}\n",
      "{'loss': 1.807, 'grad_norm': 0.4453125, 'learning_rate': 8.853783982365907e-05, 'epoch': 0.01}\n",
      "{'loss': 1.523, 'grad_norm': 0.7890625, 'learning_rate': 8.890521675238796e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5821, 'grad_norm': 0.427734375, 'learning_rate': 8.927259368111684e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5416, 'grad_norm': 0.7265625, 'learning_rate': 8.96399706098457e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4602, 'grad_norm': 1.15625, 'learning_rate': 9.000734753857459e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6931, 'grad_norm': 0.671875, 'learning_rate': 9.037472446730346e-05, 'epoch': 0.01}\n",
      "{'loss': 1.472, 'grad_norm': 0.90625, 'learning_rate': 9.074210139603234e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5416, 'grad_norm': 0.447265625, 'learning_rate': 9.110947832476121e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5165, 'grad_norm': 0.80078125, 'learning_rate': 9.147685525349009e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4896, 'grad_norm': 0.85546875, 'learning_rate': 9.184423218221896e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6386, 'grad_norm': 0.515625, 'learning_rate': 9.221160911094784e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4872, 'grad_norm': 1.1015625, 'learning_rate': 9.25789860396767e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5197, 'grad_norm': 0.50390625, 'learning_rate': 9.294636296840559e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5165, 'grad_norm': 0.59375, 'learning_rate': 9.331373989713447e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6111, 'grad_norm': 1.8828125, 'learning_rate': 9.368111682586334e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6338, 'grad_norm': 0.5234375, 'learning_rate': 9.404849375459222e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6055, 'grad_norm': 0.78125, 'learning_rate': 9.441587068332109e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5963, 'grad_norm': 0.5546875, 'learning_rate': 9.478324761204997e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5608, 'grad_norm': 0.5859375, 'learning_rate': 9.515062454077884e-05, 'epoch': 0.01}\n",
      "{'loss': 1.46, 'grad_norm': 1.25, 'learning_rate': 9.551800146950772e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6898, 'grad_norm': 0.474609375, 'learning_rate': 9.58853783982366e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5723, 'grad_norm': 0.8984375, 'learning_rate': 9.625275532696547e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4836, 'grad_norm': 0.380859375, 'learning_rate': 9.662013225569434e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6807, 'grad_norm': 0.75390625, 'learning_rate': 9.698750918442322e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5688, 'grad_norm': 0.84375, 'learning_rate': 9.73548861131521e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6737, 'grad_norm': 0.55078125, 'learning_rate': 9.772226304188098e-05, 'epoch': 0.01}\n",
      "{'loss': 1.4283, 'grad_norm': 0.6640625, 'learning_rate': 9.808963997060985e-05, 'epoch': 0.01}\n",
      "{'loss': 1.572, 'grad_norm': 0.384765625, 'learning_rate': 9.845701689933872e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5401, 'grad_norm': 0.57421875, 'learning_rate': 9.882439382806761e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5147, 'grad_norm': 1.25, 'learning_rate': 9.919177075679647e-05, 'epoch': 0.01}\n",
      "{'loss': 1.629, 'grad_norm': 0.53515625, 'learning_rate': 9.955914768552536e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5072, 'grad_norm': 0.6953125, 'learning_rate': 9.992652461425422e-05, 'epoch': 0.01}\n",
      "{'loss': 1.574, 'grad_norm': 0.41015625, 'learning_rate': 0.0001002939015429831, 'epoch': 0.01}\n",
      "{'loss': 1.6004, 'grad_norm': 0.65234375, 'learning_rate': 0.00010066127847171199, 'epoch': 0.01}\n",
      "{'loss': 1.4425, 'grad_norm': 0.87109375, 'learning_rate': 0.00010102865540044086, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5482, 'grad_norm': 0.474609375, 'learning_rate': 0.00010139603232916974, 'epoch': 0.01}\n",
      "{'loss': 1.5635, 'grad_norm': 0.734375, 'learning_rate': 0.0001017634092578986, 'epoch': 0.01}\n",
      "{'loss': 1.536, 'grad_norm': 0.4609375, 'learning_rate': 0.00010213078618662748, 'epoch': 0.01}\n",
      "{'loss': 1.5047, 'grad_norm': 0.671875, 'learning_rate': 0.00010249816311535637, 'epoch': 0.01}\n",
      "{'loss': 1.5079, 'grad_norm': 1.375, 'learning_rate': 0.00010286554004408524, 'epoch': 0.01}\n",
      "{'loss': 1.6525, 'grad_norm': 0.67578125, 'learning_rate': 0.00010323291697281412, 'epoch': 0.01}\n",
      "{'loss': 1.6795, 'grad_norm': 0.75, 'learning_rate': 0.00010360029390154298, 'epoch': 0.01}\n",
      "{'loss': 1.5907, 'grad_norm': 0.4375, 'learning_rate': 0.00010396767083027185, 'epoch': 0.01}\n",
      "{'loss': 1.5335, 'grad_norm': 0.54296875, 'learning_rate': 0.00010433504775900074, 'epoch': 0.01}\n",
      "{'loss': 1.4349, 'grad_norm': 0.9140625, 'learning_rate': 0.00010470242468772962, 'epoch': 0.01}\n",
      "{'loss': 1.6058, 'grad_norm': 0.470703125, 'learning_rate': 0.0001050698016164585, 'epoch': 0.01}\n",
      "{'loss': 1.5843, 'grad_norm': 0.69140625, 'learning_rate': 0.00010543717854518737, 'epoch': 0.01}\n",
      "{'loss': 1.492, 'grad_norm': 0.38671875, 'learning_rate': 0.00010580455547391623, 'epoch': 0.01}\n",
      "{'loss': 1.5832, 'grad_norm': 0.64453125, 'learning_rate': 0.00010617193240264511, 'epoch': 0.01}\n",
      "{'loss': 1.3862, 'grad_norm': 0.92578125, 'learning_rate': 0.000106539309331374, 'epoch': 0.01}\n",
      "{'loss': 1.7055, 'grad_norm': 0.482421875, 'learning_rate': 0.00010690668626010287, 'epoch': 0.01}\n",
      "{'loss': 1.5037, 'grad_norm': 0.7109375, 'learning_rate': 0.00010727406318883175, 'epoch': 0.01}\n",
      "{'loss': 1.5782, 'grad_norm': 0.474609375, 'learning_rate': 0.00010764144011756064, 'epoch': 0.01}\n",
      "{'loss': 1.4654, 'grad_norm': 0.734375, 'learning_rate': 0.00010800881704628948, 'epoch': 0.01}\n",
      "{'loss': 1.4754, 'grad_norm': 1.4140625, 'learning_rate': 0.00010837619397501837, 'epoch': 0.01}\n",
      "{'loss': 1.6693, 'grad_norm': 0.486328125, 'learning_rate': 0.00010874357090374725, 'epoch': 0.01}\n",
      "{'loss': 1.5972, 'grad_norm': 1.09375, 'learning_rate': 0.00010911094783247612, 'epoch': 0.01}\n",
      "{'loss': 1.4847, 'grad_norm': 0.482421875, 'learning_rate': 0.00010947832476120501, 'epoch': 0.01}\n",
      "{'loss': 1.6462, 'grad_norm': 0.66796875, 'learning_rate': 0.00010984570168993386, 'epoch': 0.01}\n",
      "{'loss': 1.6777, 'grad_norm': 1.09375, 'learning_rate': 0.00011021307861866275, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6902, 'grad_norm': 0.5546875, 'learning_rate': 0.00011058045554739163, 'epoch': 0.01}\n",
      "{'loss': 1.5565, 'grad_norm': 0.82421875, 'learning_rate': 0.0001109478324761205, 'epoch': 0.01}\n",
      "{'loss': 1.5602, 'grad_norm': 0.4375, 'learning_rate': 0.00011131520940484939, 'epoch': 0.02}\n",
      "{'loss': 1.4281, 'grad_norm': 0.58203125, 'learning_rate': 0.00011168258633357827, 'epoch': 0.02}\n",
      "{'loss': 1.427, 'grad_norm': 1.453125, 'learning_rate': 0.00011204996326230713, 'epoch': 0.02}\n",
      "{'loss': 1.6441, 'grad_norm': 0.5546875, 'learning_rate': 0.000112417340191036, 'epoch': 0.02}\n",
      "{'loss': 1.3127, 'grad_norm': 0.75390625, 'learning_rate': 0.00011278471711976488, 'epoch': 0.02}\n",
      "{'loss': 1.5335, 'grad_norm': 0.4296875, 'learning_rate': 0.00011315209404849377, 'epoch': 0.02}\n",
      "{'loss': 1.4675, 'grad_norm': 0.546875, 'learning_rate': 0.00011351947097722264, 'epoch': 0.02}\n",
      "{'loss': 1.2993, 'grad_norm': 1.28125, 'learning_rate': 0.00011388684790595152, 'epoch': 0.02}\n",
      "{'loss': 1.5628, 'grad_norm': 0.640625, 'learning_rate': 0.00011425422483468038, 'epoch': 0.02}\n",
      "{'loss': 1.5741, 'grad_norm': 0.75, 'learning_rate': 0.00011462160176340926, 'epoch': 0.02}\n",
      "{'loss': 1.5431, 'grad_norm': 0.46484375, 'learning_rate': 0.00011498897869213813, 'epoch': 0.02}\n",
      "{'loss': 1.4988, 'grad_norm': 0.515625, 'learning_rate': 0.00011535635562086702, 'epoch': 0.02}\n",
      "{'loss': 1.4208, 'grad_norm': 0.9453125, 'learning_rate': 0.0001157237325495959, 'epoch': 0.02}\n",
      "{'loss': 1.6372, 'grad_norm': 0.408203125, 'learning_rate': 0.00011609110947832477, 'epoch': 0.02}\n",
      "{'loss': 1.4841, 'grad_norm': 0.640625, 'learning_rate': 0.00011645848640705363, 'epoch': 0.02}\n",
      "{'loss': 1.5782, 'grad_norm': 0.36328125, 'learning_rate': 0.00011682586333578251, 'epoch': 0.02}\n",
      "{'loss': 1.5343, 'grad_norm': 0.734375, 'learning_rate': 0.0001171932402645114, 'epoch': 0.02}\n",
      "{'loss': 1.4591, 'grad_norm': 1.1640625, 'learning_rate': 0.00011756061719324027, 'epoch': 0.02}\n",
      "{'loss': 1.5266, 'grad_norm': 0.416015625, 'learning_rate': 0.00011792799412196915, 'epoch': 0.02}\n",
      "{'loss': 1.632, 'grad_norm': 0.84765625, 'learning_rate': 0.00011829537105069801, 'epoch': 0.02}\n",
      "{'loss': 1.5363, 'grad_norm': 0.439453125, 'learning_rate': 0.00011866274797942689, 'epoch': 0.02}\n",
      "{'loss': 1.5217, 'grad_norm': 0.6328125, 'learning_rate': 0.00011903012490815578, 'epoch': 0.02}\n",
      "{'loss': 1.4746, 'grad_norm': 1.2578125, 'learning_rate': 0.00011939750183688465, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.534, 'grad_norm': 0.55078125, 'learning_rate': 0.00011976487876561353, 'epoch': 0.02}\n",
      "{'loss': 1.4839, 'grad_norm': 0.9140625, 'learning_rate': 0.00012013225569434242, 'epoch': 0.02}\n",
      "{'loss': 1.4577, 'grad_norm': 0.345703125, 'learning_rate': 0.00012049963262307126, 'epoch': 0.02}\n",
      "{'loss': 1.5781, 'grad_norm': 0.44140625, 'learning_rate': 0.00012086700955180015, 'epoch': 0.02}\n",
      "{'loss': 1.3562, 'grad_norm': 1.140625, 'learning_rate': 0.00012123438648052903, 'epoch': 0.02}\n",
      "{'loss': 1.686, 'grad_norm': 0.43359375, 'learning_rate': 0.0001216017634092579, 'epoch': 0.02}\n",
      "{'loss': 1.5392, 'grad_norm': 0.8046875, 'learning_rate': 0.0001219691403379868, 'epoch': 0.02}\n",
      "{'loss': 1.4505, 'grad_norm': 0.376953125, 'learning_rate': 0.00012233651726671567, 'epoch': 0.02}\n",
      "{'loss': 1.5848, 'grad_norm': 0.54296875, 'learning_rate': 0.00012270389419544452, 'epoch': 0.02}\n",
      "{'loss': 1.5188, 'grad_norm': 1.328125, 'learning_rate': 0.0001230712711241734, 'epoch': 0.02}\n",
      "{'loss': 1.6225, 'grad_norm': 0.451171875, 'learning_rate': 0.0001234386480529023, 'epoch': 0.02}\n",
      "{'loss': 1.514, 'grad_norm': 0.53515625, 'learning_rate': 0.00012380602498163117, 'epoch': 0.02}\n",
      "{'loss': 1.5015, 'grad_norm': 0.40234375, 'learning_rate': 0.00012417340191036005, 'epoch': 0.02}\n",
      "{'loss': 1.5328, 'grad_norm': 0.55078125, 'learning_rate': 0.0001245407788390889, 'epoch': 0.02}\n",
      "{'loss': 1.4805, 'grad_norm': 0.91015625, 'learning_rate': 0.00012490815576781777, 'epoch': 0.02}\n",
      "{'loss': 1.6668, 'grad_norm': 0.474609375, 'learning_rate': 0.00012527553269654667, 'epoch': 0.02}\n",
      "{'loss': 1.5276, 'grad_norm': 0.70703125, 'learning_rate': 0.00012564290962527555, 'epoch': 0.02}\n",
      "{'loss': 1.6395, 'grad_norm': 0.408203125, 'learning_rate': 0.00012601028655400442, 'epoch': 0.02}\n",
      "{'loss': 1.6521, 'grad_norm': 0.7734375, 'learning_rate': 0.0001263776634827333, 'epoch': 0.02}\n",
      "{'loss': 1.4789, 'grad_norm': 0.92578125, 'learning_rate': 0.00012674504041146215, 'epoch': 0.02}\n",
      "{'loss': 1.6067, 'grad_norm': 0.5234375, 'learning_rate': 0.00012711241734019105, 'epoch': 0.02}\n",
      "{'loss': 1.525, 'grad_norm': 0.76953125, 'learning_rate': 0.00012747979426891993, 'epoch': 0.02}\n",
      "{'loss': 1.4053, 'grad_norm': 0.4453125, 'learning_rate': 0.0001278471711976488, 'epoch': 0.02}\n",
      "{'loss': 1.507, 'grad_norm': 0.5390625, 'learning_rate': 0.00012821454812637768, 'epoch': 0.02}\n",
      "{'loss': 1.3693, 'grad_norm': 1.328125, 'learning_rate': 0.00012858192505510655, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5653, 'grad_norm': 0.4609375, 'learning_rate': 0.0001289493019838354, 'epoch': 0.02}\n",
      "{'loss': 1.5036, 'grad_norm': 0.466796875, 'learning_rate': 0.0001293166789125643, 'epoch': 0.02}\n",
      "{'loss': 1.5995, 'grad_norm': 0.462890625, 'learning_rate': 0.00012968405584129318, 'epoch': 0.02}\n",
      "{'loss': 1.6066, 'grad_norm': 0.57421875, 'learning_rate': 0.00013005143277002205, 'epoch': 0.02}\n",
      "{'loss': 1.38, 'grad_norm': 1.140625, 'learning_rate': 0.00013041880969875093, 'epoch': 0.02}\n",
      "{'loss': 1.6557, 'grad_norm': 0.451171875, 'learning_rate': 0.00013078618662747978, 'epoch': 0.02}\n",
      "{'loss': 1.4926, 'grad_norm': 0.8203125, 'learning_rate': 0.00013115356355620868, 'epoch': 0.02}\n",
      "{'loss': 1.3958, 'grad_norm': 0.4140625, 'learning_rate': 0.00013152094048493756, 'epoch': 0.02}\n",
      "{'loss': 1.5295, 'grad_norm': 0.7265625, 'learning_rate': 0.00013188831741366643, 'epoch': 0.02}\n",
      "{'loss': 1.5282, 'grad_norm': 1.28125, 'learning_rate': 0.0001322556943423953, 'epoch': 0.02}\n",
      "{'loss': 1.5115, 'grad_norm': 0.5234375, 'learning_rate': 0.00013262307127112418, 'epoch': 0.02}\n",
      "{'loss': 1.4871, 'grad_norm': 0.87109375, 'learning_rate': 0.00013299044819985306, 'epoch': 0.02}\n",
      "{'loss': 1.3945, 'grad_norm': 0.421875, 'learning_rate': 0.00013335782512858193, 'epoch': 0.02}\n",
      "{'loss': 1.695, 'grad_norm': 0.5078125, 'learning_rate': 0.0001337252020573108, 'epoch': 0.02}\n",
      "{'loss': 1.4948, 'grad_norm': 0.88671875, 'learning_rate': 0.00013409257898603968, 'epoch': 0.02}\n",
      "{'loss': 1.5366, 'grad_norm': 0.48046875, 'learning_rate': 0.00013445995591476856, 'epoch': 0.02}\n",
      "{'loss': 1.4738, 'grad_norm': 0.8203125, 'learning_rate': 0.00013482733284349744, 'epoch': 0.02}\n",
      "{'loss': 1.5028, 'grad_norm': 0.36328125, 'learning_rate': 0.0001351947097722263, 'epoch': 0.02}\n",
      "{'loss': 1.6653, 'grad_norm': 0.5234375, 'learning_rate': 0.00013556208670095519, 'epoch': 0.02}\n",
      "{'loss': 1.4915, 'grad_norm': 0.9765625, 'learning_rate': 0.00013592946362968406, 'epoch': 0.02}\n",
      "{'loss': 1.5734, 'grad_norm': 0.44921875, 'learning_rate': 0.00013629684055841294, 'epoch': 0.02}\n",
      "{'loss': 1.5294, 'grad_norm': 0.9296875, 'learning_rate': 0.0001366642174871418, 'epoch': 0.02}\n",
      "{'loss': 1.5086, 'grad_norm': 0.42578125, 'learning_rate': 0.0001370315944158707, 'epoch': 0.02}\n",
      "{'loss': 1.4904, 'grad_norm': 0.546875, 'learning_rate': 0.00013739897134459956, 'epoch': 0.02}\n",
      "{'loss': 1.3799, 'grad_norm': 0.8359375, 'learning_rate': 0.00013776634827332844, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.55, 'grad_norm': 0.45703125, 'learning_rate': 0.00013813372520205731, 'epoch': 0.02}\n",
      "{'loss': 1.4852, 'grad_norm': 0.71484375, 'learning_rate': 0.0001385011021307862, 'epoch': 0.02}\n",
      "{'loss': 1.4986, 'grad_norm': 0.41796875, 'learning_rate': 0.00013886847905951507, 'epoch': 0.02}\n",
      "{'loss': 1.4363, 'grad_norm': 0.57421875, 'learning_rate': 0.00013923585598824394, 'epoch': 0.02}\n",
      "{'loss': 1.3618, 'grad_norm': 0.96484375, 'learning_rate': 0.00013960323291697282, 'epoch': 0.02}\n",
      "{'loss': 1.5539, 'grad_norm': 0.44921875, 'learning_rate': 0.0001399706098457017, 'epoch': 0.02}\n",
      "{'loss': 1.4871, 'grad_norm': 0.9296875, 'learning_rate': 0.00014033798677443057, 'epoch': 0.02}\n",
      "{'loss': 1.4629, 'grad_norm': 0.490234375, 'learning_rate': 0.00014070536370315944, 'epoch': 0.02}\n",
      "{'loss': 1.5603, 'grad_norm': 0.3984375, 'learning_rate': 0.00014107274063188835, 'epoch': 0.02}\n",
      "{'loss': 1.4042, 'grad_norm': 1.5703125, 'learning_rate': 0.0001414401175606172, 'epoch': 0.02}\n",
      "{'loss': 1.656, 'grad_norm': 0.431640625, 'learning_rate': 0.00014180749448934607, 'epoch': 0.02}\n",
      "{'loss': 1.4114, 'grad_norm': 0.66796875, 'learning_rate': 0.00014217487141807494, 'epoch': 0.02}\n",
      "{'loss': 1.5018, 'grad_norm': 0.380859375, 'learning_rate': 0.00014254224834680382, 'epoch': 0.02}\n",
      "{'loss': 1.3679, 'grad_norm': 0.56640625, 'learning_rate': 0.00014290962527553272, 'epoch': 0.02}\n",
      "{'loss': 1.4464, 'grad_norm': 1.0546875, 'learning_rate': 0.0001432770022042616, 'epoch': 0.02}\n",
      "{'loss': 1.555, 'grad_norm': 0.421875, 'learning_rate': 0.00014364437913299045, 'epoch': 0.02}\n",
      "{'loss': 1.4241, 'grad_norm': 0.72265625, 'learning_rate': 0.00014401175606171932, 'epoch': 0.02}\n",
      "{'loss': 1.4428, 'grad_norm': 0.4296875, 'learning_rate': 0.0001443791329904482, 'epoch': 0.02}\n",
      "{'loss': 1.437, 'grad_norm': 0.71875, 'learning_rate': 0.0001447465099191771, 'epoch': 0.02}\n",
      "{'loss': 1.4475, 'grad_norm': 1.046875, 'learning_rate': 0.00014511388684790598, 'epoch': 0.02}\n",
      "{'loss': 1.5986, 'grad_norm': 0.490234375, 'learning_rate': 0.00014548126377663482, 'epoch': 0.02}\n",
      "{'loss': 1.5337, 'grad_norm': 0.58984375, 'learning_rate': 0.0001458486407053637, 'epoch': 0.02}\n",
      "{'loss': 1.6135, 'grad_norm': 0.361328125, 'learning_rate': 0.00014621601763409257, 'epoch': 0.02}\n",
      "{'loss': 1.5377, 'grad_norm': 0.58984375, 'learning_rate': 0.00014658339456282145, 'epoch': 0.02}\n",
      "{'loss': 1.4346, 'grad_norm': 1.0625, 'learning_rate': 0.00014695077149155035, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.55, 'grad_norm': 0.55859375, 'learning_rate': 0.00014731814842027923, 'epoch': 0.02}\n",
      "{'loss': 1.5389, 'grad_norm': 0.76171875, 'learning_rate': 0.00014768552534900808, 'epoch': 0.02}\n",
      "{'loss': 1.5074, 'grad_norm': 0.41015625, 'learning_rate': 0.00014805290227773695, 'epoch': 0.02}\n",
      "{'loss': 1.5687, 'grad_norm': 0.49609375, 'learning_rate': 0.00014842027920646583, 'epoch': 0.02}\n",
      "{'loss': 1.4023, 'grad_norm': 1.171875, 'learning_rate': 0.00014878765613519473, 'epoch': 0.02}\n",
      "{'loss': 1.6255, 'grad_norm': 0.45703125, 'learning_rate': 0.0001491550330639236, 'epoch': 0.02}\n",
      "{'loss': 1.3731, 'grad_norm': 0.69921875, 'learning_rate': 0.00014952240999265248, 'epoch': 0.02}\n",
      "{'loss': 1.507, 'grad_norm': 0.40234375, 'learning_rate': 0.00014988978692138133, 'epoch': 0.02}\n",
      "{'loss': 1.4951, 'grad_norm': 0.5390625, 'learning_rate': 0.0001502571638501102, 'epoch': 0.02}\n",
      "{'loss': 1.357, 'grad_norm': 0.83984375, 'learning_rate': 0.0001506245407788391, 'epoch': 0.02}\n",
      "{'loss': 1.6624, 'grad_norm': 0.453125, 'learning_rate': 0.00015099191770756798, 'epoch': 0.02}\n",
      "{'loss': 1.4718, 'grad_norm': 0.76953125, 'learning_rate': 0.00015135929463629686, 'epoch': 0.02}\n",
      "{'loss': 1.3648, 'grad_norm': 0.390625, 'learning_rate': 0.0001517266715650257, 'epoch': 0.02}\n",
      "{'loss': 1.5197, 'grad_norm': 0.462890625, 'learning_rate': 0.00015209404849375458, 'epoch': 0.02}\n",
      "{'loss': 1.344, 'grad_norm': 0.8828125, 'learning_rate': 0.00015246142542248349, 'epoch': 0.02}\n",
      "{'loss': 1.649, 'grad_norm': 0.482421875, 'learning_rate': 0.00015282880235121236, 'epoch': 0.02}\n",
      "{'loss': 1.5083, 'grad_norm': 0.5, 'learning_rate': 0.00015319617927994124, 'epoch': 0.02}\n",
      "{'loss': 1.4231, 'grad_norm': 0.3515625, 'learning_rate': 0.0001535635562086701, 'epoch': 0.02}\n",
      "{'loss': 1.4963, 'grad_norm': 0.478515625, 'learning_rate': 0.00015393093313739896, 'epoch': 0.02}\n",
      "{'loss': 1.3794, 'grad_norm': 1.03125, 'learning_rate': 0.00015429831006612786, 'epoch': 0.02}\n",
      "{'loss': 1.452, 'grad_norm': 0.439453125, 'learning_rate': 0.00015466568699485674, 'epoch': 0.02}\n",
      "{'loss': 1.4373, 'grad_norm': 0.5859375, 'learning_rate': 0.0001550330639235856, 'epoch': 0.02}\n",
      "{'loss': 1.3803, 'grad_norm': 0.34765625, 'learning_rate': 0.0001554004408523145, 'epoch': 0.02}\n",
      "{'loss': 1.4578, 'grad_norm': 0.61328125, 'learning_rate': 0.00015576781778104336, 'epoch': 0.02}\n",
      "{'loss': 1.3908, 'grad_norm': 0.78515625, 'learning_rate': 0.0001561351947097722, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6105, 'grad_norm': 0.435546875, 'learning_rate': 0.00015650257163850112, 'epoch': 0.02}\n",
      "{'loss': 1.3696, 'grad_norm': 0.46484375, 'learning_rate': 0.00015686994856723, 'epoch': 0.02}\n",
      "{'loss': 1.4257, 'grad_norm': 0.3984375, 'learning_rate': 0.00015723732549595887, 'epoch': 0.02}\n",
      "{'loss': 1.5419, 'grad_norm': 0.53515625, 'learning_rate': 0.00015760470242468774, 'epoch': 0.02}\n",
      "{'loss': 1.3389, 'grad_norm': 1.796875, 'learning_rate': 0.0001579720793534166, 'epoch': 0.02}\n",
      "{'loss': 1.5226, 'grad_norm': 0.462890625, 'learning_rate': 0.0001583394562821455, 'epoch': 0.02}\n",
      "{'loss': 1.4611, 'grad_norm': 0.65234375, 'learning_rate': 0.00015870683321087437, 'epoch': 0.02}\n",
      "{'loss': 1.4665, 'grad_norm': 0.35546875, 'learning_rate': 0.00015907421013960324, 'epoch': 0.02}\n",
      "{'loss': 1.482, 'grad_norm': 0.52734375, 'learning_rate': 0.00015944158706833212, 'epoch': 0.02}\n",
      "{'loss': 1.4562, 'grad_norm': 0.734375, 'learning_rate': 0.000159808963997061, 'epoch': 0.02}\n",
      "{'loss': 1.5894, 'grad_norm': 0.44140625, 'learning_rate': 0.00016017634092578987, 'epoch': 0.02}\n",
      "{'loss': 1.4266, 'grad_norm': 0.8046875, 'learning_rate': 0.00016054371785451875, 'epoch': 0.02}\n",
      "{'loss': 1.5996, 'grad_norm': 0.404296875, 'learning_rate': 0.00016091109478324762, 'epoch': 0.02}\n",
      "{'loss': 1.4745, 'grad_norm': 0.546875, 'learning_rate': 0.0001612784717119765, 'epoch': 0.02}\n",
      "{'loss': 1.3784, 'grad_norm': 0.65625, 'learning_rate': 0.00016164584864070537, 'epoch': 0.02}\n",
      "{'loss': 1.6069, 'grad_norm': 0.50390625, 'learning_rate': 0.00016201322556943425, 'epoch': 0.02}\n",
      "{'loss': 1.4087, 'grad_norm': 0.81640625, 'learning_rate': 0.00016238060249816312, 'epoch': 0.02}\n",
      "{'loss': 1.4579, 'grad_norm': 0.447265625, 'learning_rate': 0.000162747979426892, 'epoch': 0.02}\n",
      "{'loss': 1.4535, 'grad_norm': 0.82421875, 'learning_rate': 0.00016311535635562087, 'epoch': 0.02}\n",
      "{'loss': 1.2463, 'grad_norm': 0.64453125, 'learning_rate': 0.00016348273328434975, 'epoch': 0.02}\n",
      "{'loss': 1.6138, 'grad_norm': 0.419921875, 'learning_rate': 0.00016385011021307862, 'epoch': 0.02}\n",
      "{'loss': 1.5677, 'grad_norm': 0.58203125, 'learning_rate': 0.0001642174871418075, 'epoch': 0.02}\n",
      "{'loss': 1.4931, 'grad_norm': 0.408203125, 'learning_rate': 0.00016458486407053638, 'epoch': 0.02}\n",
      "{'loss': 1.7364, 'grad_norm': 0.4609375, 'learning_rate': 0.00016495224099926525, 'epoch': 0.02}\n",
      "{'loss': 1.3302, 'grad_norm': 1.5703125, 'learning_rate': 0.00016531961792799413, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6878, 'grad_norm': 0.37890625, 'learning_rate': 0.000165686994856723, 'epoch': 0.02}\n",
      "{'loss': 1.4233, 'grad_norm': 0.6484375, 'learning_rate': 0.00016605437178545188, 'epoch': 0.02}\n",
      "{'loss': 1.4807, 'grad_norm': 0.3828125, 'learning_rate': 0.00016642174871418075, 'epoch': 0.02}\n",
      "{'loss': 1.4015, 'grad_norm': 0.498046875, 'learning_rate': 0.00016678912564290963, 'epoch': 0.02}\n",
      "{'loss': 1.3194, 'grad_norm': 1.09375, 'learning_rate': 0.0001671565025716385, 'epoch': 0.02}\n",
      "{'loss': 1.5582, 'grad_norm': 0.5390625, 'learning_rate': 0.00016752387950036738, 'epoch': 0.02}\n",
      "{'loss': 1.4363, 'grad_norm': 0.62109375, 'learning_rate': 0.00016789125642909626, 'epoch': 0.02}\n",
      "{'loss': 1.5102, 'grad_norm': 0.38671875, 'learning_rate': 0.00016825863335782516, 'epoch': 0.02}\n",
      "{'loss': 1.5714, 'grad_norm': 0.47265625, 'learning_rate': 0.000168626010286554, 'epoch': 0.02}\n",
      "{'loss': 1.3245, 'grad_norm': 0.8125, 'learning_rate': 0.00016899338721528288, 'epoch': 0.02}\n",
      "{'loss': 1.5239, 'grad_norm': 0.466796875, 'learning_rate': 0.00016936076414401176, 'epoch': 0.02}\n",
      "{'loss': 1.4402, 'grad_norm': 0.67578125, 'learning_rate': 0.00016972814107274063, 'epoch': 0.02}\n",
      "{'loss': 1.3202, 'grad_norm': 0.3671875, 'learning_rate': 0.00017009551800146954, 'epoch': 0.02}\n",
      "{'loss': 1.4362, 'grad_norm': 0.6015625, 'learning_rate': 0.0001704628949301984, 'epoch': 0.02}\n",
      "{'loss': 1.2477, 'grad_norm': 0.875, 'learning_rate': 0.00017083027185892726, 'epoch': 0.02}\n",
      "{'loss': 1.5812, 'grad_norm': 0.4921875, 'learning_rate': 0.00017119764878765613, 'epoch': 0.02}\n",
      "{'loss': 1.4387, 'grad_norm': 0.6796875, 'learning_rate': 0.000171565025716385, 'epoch': 0.02}\n",
      "{'loss': 1.4434, 'grad_norm': 0.388671875, 'learning_rate': 0.0001719324026451139, 'epoch': 0.02}\n",
      "{'loss': 1.4167, 'grad_norm': 0.431640625, 'learning_rate': 0.0001722997795738428, 'epoch': 0.02}\n",
      "{'loss': 1.4896, 'grad_norm': 0.9609375, 'learning_rate': 0.00017266715650257164, 'epoch': 0.02}\n",
      "{'loss': 1.3919, 'grad_norm': 0.447265625, 'learning_rate': 0.0001730345334313005, 'epoch': 0.02}\n",
      "{'loss': 1.4704, 'grad_norm': 0.51171875, 'learning_rate': 0.0001734019103600294, 'epoch': 0.02}\n",
      "{'loss': 1.3945, 'grad_norm': 0.419921875, 'learning_rate': 0.00017376928728875826, 'epoch': 0.02}\n",
      "{'loss': 1.4747, 'grad_norm': 0.5078125, 'learning_rate': 0.00017413666421748717, 'epoch': 0.02}\n",
      "{'loss': 1.3371, 'grad_norm': 0.61328125, 'learning_rate': 0.00017450404114621604, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.449, 'grad_norm': 0.447265625, 'learning_rate': 0.0001748714180749449, 'epoch': 0.02}\n",
      "{'loss': 1.4704, 'grad_norm': 0.58203125, 'learning_rate': 0.00017523879500367376, 'epoch': 0.02}\n",
      "{'loss': 1.5236, 'grad_norm': 0.375, 'learning_rate': 0.00017560617193240264, 'epoch': 0.02}\n",
      "{'loss': 1.4977, 'grad_norm': 0.58203125, 'learning_rate': 0.00017597354886113154, 'epoch': 0.02}\n",
      "{'loss': 1.371, 'grad_norm': 1.4765625, 'learning_rate': 0.00017634092578986042, 'epoch': 0.02}\n",
      "{'loss': 1.5638, 'grad_norm': 0.45703125, 'learning_rate': 0.0001767083027185893, 'epoch': 0.02}\n",
      "{'loss': 1.3612, 'grad_norm': 0.6640625, 'learning_rate': 0.00017707567964731814, 'epoch': 0.02}\n",
      "{'loss': 1.3901, 'grad_norm': 0.3125, 'learning_rate': 0.00017744305657604702, 'epoch': 0.02}\n",
      "{'loss': 1.4795, 'grad_norm': 0.5234375, 'learning_rate': 0.00017781043350477592, 'epoch': 0.02}\n",
      "{'loss': 1.332, 'grad_norm': 0.53125, 'learning_rate': 0.0001781778104335048, 'epoch': 0.02}\n",
      "{'loss': 1.5336, 'grad_norm': 0.392578125, 'learning_rate': 0.00017854518736223367, 'epoch': 0.02}\n",
      "{'loss': 1.3281, 'grad_norm': 0.6484375, 'learning_rate': 0.00017891256429096252, 'epoch': 0.02}\n",
      "{'loss': 1.443, 'grad_norm': 0.384765625, 'learning_rate': 0.0001792799412196914, 'epoch': 0.02}\n",
      "{'loss': 1.5767, 'grad_norm': 0.5390625, 'learning_rate': 0.0001796473181484203, 'epoch': 0.02}\n",
      "{'loss': 1.3838, 'grad_norm': 0.98046875, 'learning_rate': 0.00018001469507714917, 'epoch': 0.02}\n",
      "{'loss': 1.4764, 'grad_norm': 0.396484375, 'learning_rate': 0.00018038207200587805, 'epoch': 0.02}\n",
      "{'loss': 1.4109, 'grad_norm': 0.52734375, 'learning_rate': 0.00018074944893460692, 'epoch': 0.02}\n",
      "{'loss': 1.4539, 'grad_norm': 0.36328125, 'learning_rate': 0.00018111682586333577, 'epoch': 0.02}\n",
      "{'loss': 1.43, 'grad_norm': 0.5078125, 'learning_rate': 0.00018148420279206467, 'epoch': 0.02}\n",
      "{'loss': 1.3564, 'grad_norm': 0.451171875, 'learning_rate': 0.00018185157972079355, 'epoch': 0.02}\n",
      "{'loss': 1.502, 'grad_norm': 0.39453125, 'learning_rate': 0.00018221895664952243, 'epoch': 0.02}\n",
      "{'loss': 1.4729, 'grad_norm': 0.69140625, 'learning_rate': 0.0001825863335782513, 'epoch': 0.02}\n",
      "{'loss': 1.4481, 'grad_norm': 0.3515625, 'learning_rate': 0.00018295371050698018, 'epoch': 0.02}\n",
      "{'loss': 1.5288, 'grad_norm': 0.443359375, 'learning_rate': 0.00018332108743570903, 'epoch': 0.02}\n",
      "{'loss': 1.3487, 'grad_norm': 0.54296875, 'learning_rate': 0.00018368846436443793, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6298, 'grad_norm': 0.359375, 'learning_rate': 0.0001840558412931668, 'epoch': 0.02}\n",
      "{'loss': 1.3906, 'grad_norm': 0.6015625, 'learning_rate': 0.00018442321822189568, 'epoch': 0.02}\n",
      "{'loss': 1.4109, 'grad_norm': 0.35546875, 'learning_rate': 0.00018479059515062455, 'epoch': 0.02}\n",
      "{'loss': 1.4233, 'grad_norm': 0.490234375, 'learning_rate': 0.0001851579720793534, 'epoch': 0.02}\n",
      "{'loss': 1.3589, 'grad_norm': 0.87890625, 'learning_rate': 0.0001855253490080823, 'epoch': 0.03}\n",
      "{'loss': 1.4421, 'grad_norm': 0.396484375, 'learning_rate': 0.00018589272593681118, 'epoch': 0.03}\n",
      "{'loss': 1.3791, 'grad_norm': 0.546875, 'learning_rate': 0.00018626010286554006, 'epoch': 0.03}\n",
      "{'loss': 1.3988, 'grad_norm': 0.279296875, 'learning_rate': 0.00018662747979426893, 'epoch': 0.03}\n",
      "{'loss': 1.5662, 'grad_norm': 0.671875, 'learning_rate': 0.0001869948567229978, 'epoch': 0.03}\n",
      "{'loss': 1.2653, 'grad_norm': 1.4609375, 'learning_rate': 0.00018736223365172668, 'epoch': 0.03}\n",
      "{'loss': 1.6625, 'grad_norm': 0.43359375, 'learning_rate': 0.00018772961058045556, 'epoch': 0.03}\n",
      "{'loss': 1.3361, 'grad_norm': 0.62109375, 'learning_rate': 0.00018809698750918443, 'epoch': 0.03}\n",
      "{'loss': 1.4764, 'grad_norm': 0.322265625, 'learning_rate': 0.0001884643644379133, 'epoch': 0.03}\n",
      "{'loss': 1.464, 'grad_norm': 0.484375, 'learning_rate': 0.00018883174136664218, 'epoch': 0.03}\n",
      "{'loss': 1.4084, 'grad_norm': 0.78125, 'learning_rate': 0.00018919911829537106, 'epoch': 0.03}\n",
      "{'loss': 1.6654, 'grad_norm': 0.484375, 'learning_rate': 0.00018956649522409994, 'epoch': 0.03}\n",
      "{'loss': 1.3661, 'grad_norm': 0.6015625, 'learning_rate': 0.0001899338721528288, 'epoch': 0.03}\n",
      "{'loss': 1.3554, 'grad_norm': 0.400390625, 'learning_rate': 0.00019030124908155769, 'epoch': 0.03}\n",
      "{'loss': 1.5346, 'grad_norm': 0.384765625, 'learning_rate': 0.00019066862601028656, 'epoch': 0.03}\n",
      "{'loss': 1.4039, 'grad_norm': 0.62109375, 'learning_rate': 0.00019103600293901544, 'epoch': 0.03}\n",
      "{'loss': 1.6514, 'grad_norm': 0.4453125, 'learning_rate': 0.0001914033798677443, 'epoch': 0.03}\n",
      "{'loss': 1.5151, 'grad_norm': 0.6328125, 'learning_rate': 0.0001917707567964732, 'epoch': 0.03}\n",
      "{'loss': 1.4892, 'grad_norm': 0.29296875, 'learning_rate': 0.00019213813372520206, 'epoch': 0.03}\n",
      "{'loss': 1.5193, 'grad_norm': 0.51953125, 'learning_rate': 0.00019250551065393094, 'epoch': 0.03}\n",
      "{'loss': 1.3351, 'grad_norm': 0.59375, 'learning_rate': 0.00019287288758265981, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4949, 'grad_norm': 0.455078125, 'learning_rate': 0.0001932402645113887, 'epoch': 0.03}\n",
      "{'loss': 1.452, 'grad_norm': 0.66015625, 'learning_rate': 0.00019360764144011757, 'epoch': 0.03}\n",
      "{'loss': 1.6242, 'grad_norm': 0.376953125, 'learning_rate': 0.00019397501836884644, 'epoch': 0.03}\n",
      "{'loss': 1.5251, 'grad_norm': 0.439453125, 'learning_rate': 0.00019434239529757532, 'epoch': 0.03}\n",
      "{'loss': 1.3971, 'grad_norm': 0.73828125, 'learning_rate': 0.0001947097722263042, 'epoch': 0.03}\n",
      "{'loss': 1.4373, 'grad_norm': 0.455078125, 'learning_rate': 0.00019507714915503307, 'epoch': 0.03}\n",
      "{'loss': 1.4068, 'grad_norm': 0.5078125, 'learning_rate': 0.00019544452608376197, 'epoch': 0.03}\n",
      "{'loss': 1.3793, 'grad_norm': 0.306640625, 'learning_rate': 0.00019581190301249082, 'epoch': 0.03}\n",
      "{'loss': 1.4675, 'grad_norm': 0.51171875, 'learning_rate': 0.0001961792799412197, 'epoch': 0.03}\n",
      "{'loss': 1.4064, 'grad_norm': 0.7109375, 'learning_rate': 0.00019654665686994857, 'epoch': 0.03}\n",
      "{'loss': 1.5163, 'grad_norm': 0.376953125, 'learning_rate': 0.00019691403379867744, 'epoch': 0.03}\n",
      "{'loss': 1.384, 'grad_norm': 0.498046875, 'learning_rate': 0.00019728141072740635, 'epoch': 0.03}\n",
      "{'loss': 1.4139, 'grad_norm': 0.34375, 'learning_rate': 0.00019764878765613522, 'epoch': 0.03}\n",
      "{'loss': 1.534, 'grad_norm': 0.8125, 'learning_rate': 0.00019801616458486407, 'epoch': 0.03}\n",
      "{'loss': 1.3078, 'grad_norm': 0.703125, 'learning_rate': 0.00019838354151359295, 'epoch': 0.03}\n",
      "{'loss': 1.635, 'grad_norm': 0.4296875, 'learning_rate': 0.00019875091844232182, 'epoch': 0.03}\n",
      "{'loss': 1.3742, 'grad_norm': 0.546875, 'learning_rate': 0.00019911829537105072, 'epoch': 0.03}\n",
      "{'loss': 1.4516, 'grad_norm': 0.32421875, 'learning_rate': 0.0001994856722997796, 'epoch': 0.03}\n",
      "{'loss': 1.4681, 'grad_norm': 0.5234375, 'learning_rate': 0.00019985304922850845, 'epoch': 0.03}\n",
      "{'loss': 1.287, 'grad_norm': 1.0859375, 'learning_rate': 0.00019999318257352168, 'epoch': 0.03}\n",
      "{'loss': 1.5476, 'grad_norm': 0.412109375, 'learning_rate': 0.00019998182019605784, 'epoch': 0.03}\n",
      "{'loss': 1.3926, 'grad_norm': 0.5859375, 'learning_rate': 0.000199970457818594, 'epoch': 0.03}\n",
      "{'loss': 1.3259, 'grad_norm': 0.3125, 'learning_rate': 0.0001999590954411301, 'epoch': 0.03}\n",
      "{'loss': 1.4286, 'grad_norm': 0.5, 'learning_rate': 0.00019994773306366626, 'epoch': 0.03}\n",
      "{'loss': 1.3284, 'grad_norm': 0.875, 'learning_rate': 0.0001999363706862024, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7151, 'grad_norm': 0.5, 'learning_rate': 0.00019992500830873854, 'epoch': 0.03}\n",
      "{'loss': 1.3562, 'grad_norm': 0.61328125, 'learning_rate': 0.00019991364593127466, 'epoch': 0.03}\n",
      "{'loss': 1.5192, 'grad_norm': 0.349609375, 'learning_rate': 0.0001999022835538108, 'epoch': 0.03}\n",
      "{'loss': 1.4471, 'grad_norm': 0.435546875, 'learning_rate': 0.00019989092117634697, 'epoch': 0.03}\n",
      "{'loss': 1.3053, 'grad_norm': 0.7734375, 'learning_rate': 0.0001998795587988831, 'epoch': 0.03}\n",
      "{'loss': 1.6444, 'grad_norm': 0.37890625, 'learning_rate': 0.00019986819642141924, 'epoch': 0.03}\n",
      "{'loss': 1.493, 'grad_norm': 0.7578125, 'learning_rate': 0.00019985683404395537, 'epoch': 0.03}\n",
      "{'loss': 1.3418, 'grad_norm': 0.32421875, 'learning_rate': 0.0001998454716664915, 'epoch': 0.03}\n",
      "{'loss': 1.4951, 'grad_norm': 0.56640625, 'learning_rate': 0.00019983410928902764, 'epoch': 0.03}\n",
      "{'loss': 1.3543, 'grad_norm': 0.84375, 'learning_rate': 0.0001998227469115638, 'epoch': 0.03}\n",
      "{'loss': 1.4359, 'grad_norm': 0.392578125, 'learning_rate': 0.00019981138453409994, 'epoch': 0.03}\n",
      "{'loss': 1.3955, 'grad_norm': 0.48828125, 'learning_rate': 0.00019980002215663607, 'epoch': 0.03}\n",
      "{'loss': 1.5267, 'grad_norm': 0.369140625, 'learning_rate': 0.0001997886597791722, 'epoch': 0.03}\n",
      "{'loss': 1.4116, 'grad_norm': 0.70703125, 'learning_rate': 0.00019977729740170834, 'epoch': 0.03}\n",
      "{'loss': 1.2929, 'grad_norm': 0.87890625, 'learning_rate': 0.00019976593502424447, 'epoch': 0.03}\n",
      "{'loss': 1.4399, 'grad_norm': 0.337890625, 'learning_rate': 0.00019975457264678062, 'epoch': 0.03}\n",
      "{'loss': 1.4181, 'grad_norm': 0.52734375, 'learning_rate': 0.00019974321026931677, 'epoch': 0.03}\n",
      "{'loss': 1.4439, 'grad_norm': 0.33984375, 'learning_rate': 0.0001997318478918529, 'epoch': 0.03}\n",
      "{'loss': 1.3766, 'grad_norm': 0.412109375, 'learning_rate': 0.00019972048551438905, 'epoch': 0.03}\n",
      "{'loss': 1.3716, 'grad_norm': 0.62890625, 'learning_rate': 0.00019970912313692517, 'epoch': 0.03}\n",
      "{'loss': 1.526, 'grad_norm': 0.37890625, 'learning_rate': 0.00019969776075946132, 'epoch': 0.03}\n",
      "{'loss': 1.3659, 'grad_norm': 0.546875, 'learning_rate': 0.00019968639838199745, 'epoch': 0.03}\n",
      "{'loss': 1.4252, 'grad_norm': 0.32421875, 'learning_rate': 0.0001996750360045336, 'epoch': 0.03}\n",
      "{'loss': 1.4346, 'grad_norm': 0.4453125, 'learning_rate': 0.00019966367362706975, 'epoch': 0.03}\n",
      "{'loss': 1.3266, 'grad_norm': 0.59375, 'learning_rate': 0.00019965231124960587, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5006, 'grad_norm': 0.345703125, 'learning_rate': 0.00019964094887214203, 'epoch': 0.03}\n",
      "{'loss': 1.4135, 'grad_norm': 0.5078125, 'learning_rate': 0.00019962958649467815, 'epoch': 0.03}\n",
      "{'loss': 1.5884, 'grad_norm': 0.333984375, 'learning_rate': 0.0001996182241172143, 'epoch': 0.03}\n",
      "{'loss': 1.4273, 'grad_norm': 0.4921875, 'learning_rate': 0.00019960686173975043, 'epoch': 0.03}\n",
      "{'loss': 1.2563, 'grad_norm': 0.5859375, 'learning_rate': 0.00019959549936228658, 'epoch': 0.03}\n",
      "{'loss': 1.4811, 'grad_norm': 0.431640625, 'learning_rate': 0.00019958413698482273, 'epoch': 0.03}\n",
      "{'loss': 1.3883, 'grad_norm': 0.67578125, 'learning_rate': 0.00019957277460735885, 'epoch': 0.03}\n",
      "{'loss': 1.3552, 'grad_norm': 0.333984375, 'learning_rate': 0.000199561412229895, 'epoch': 0.03}\n",
      "{'loss': 1.4603, 'grad_norm': 0.41015625, 'learning_rate': 0.00019955004985243113, 'epoch': 0.03}\n",
      "{'loss': 1.395, 'grad_norm': 0.765625, 'learning_rate': 0.00019953868747496728, 'epoch': 0.03}\n",
      "{'loss': 1.4815, 'grad_norm': 0.5546875, 'learning_rate': 0.0001995273250975034, 'epoch': 0.03}\n",
      "{'loss': 1.2969, 'grad_norm': 0.51171875, 'learning_rate': 0.00019951596272003953, 'epoch': 0.03}\n",
      "{'loss': 1.1799, 'grad_norm': 0.353515625, 'learning_rate': 0.0001995046003425757, 'epoch': 0.03}\n",
      "{'loss': 1.4175, 'grad_norm': 0.404296875, 'learning_rate': 0.00019949323796511183, 'epoch': 0.03}\n",
      "{'loss': 1.3243, 'grad_norm': 0.59765625, 'learning_rate': 0.00019948187558764798, 'epoch': 0.03}\n",
      "{'loss': 1.5315, 'grad_norm': 0.35546875, 'learning_rate': 0.0001994705132101841, 'epoch': 0.03}\n",
      "{'loss': 1.4483, 'grad_norm': 0.6640625, 'learning_rate': 0.00019945915083272023, 'epoch': 0.03}\n",
      "{'loss': 1.3243, 'grad_norm': 0.3984375, 'learning_rate': 0.00019944778845525638, 'epoch': 0.03}\n",
      "{'loss': 1.4083, 'grad_norm': 0.498046875, 'learning_rate': 0.00019943642607779253, 'epoch': 0.03}\n",
      "{'loss': 1.3212, 'grad_norm': 0.5625, 'learning_rate': 0.00019942506370032869, 'epoch': 0.03}\n",
      "{'loss': 1.4096, 'grad_norm': 0.50390625, 'learning_rate': 0.0001994137013228648, 'epoch': 0.03}\n",
      "{'loss': 1.3194, 'grad_norm': 0.66796875, 'learning_rate': 0.00019940233894540093, 'epoch': 0.03}\n",
      "{'loss': 1.5147, 'grad_norm': 0.333984375, 'learning_rate': 0.00019939097656793709, 'epoch': 0.03}\n",
      "{'loss': 1.5364, 'grad_norm': 0.431640625, 'learning_rate': 0.0001993796141904732, 'epoch': 0.03}\n",
      "{'loss': 1.347, 'grad_norm': 0.8828125, 'learning_rate': 0.00019936825181300936, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4699, 'grad_norm': 0.412109375, 'learning_rate': 0.0001993568894355455, 'epoch': 0.03}\n",
      "{'loss': 1.4423, 'grad_norm': 0.546875, 'learning_rate': 0.00019934552705808164, 'epoch': 0.03}\n",
      "{'loss': 1.4563, 'grad_norm': 0.322265625, 'learning_rate': 0.0001993341646806178, 'epoch': 0.03}\n",
      "{'loss': 1.4558, 'grad_norm': 0.353515625, 'learning_rate': 0.0001993228023031539, 'epoch': 0.03}\n",
      "{'loss': 1.2705, 'grad_norm': 1.2265625, 'learning_rate': 0.00019931143992569006, 'epoch': 0.03}\n",
      "{'loss': 1.5716, 'grad_norm': 0.419921875, 'learning_rate': 0.0001993000775482262, 'epoch': 0.03}\n",
      "{'loss': 1.4652, 'grad_norm': 0.625, 'learning_rate': 0.00019928871517076234, 'epoch': 0.03}\n",
      "{'loss': 1.4366, 'grad_norm': 0.373046875, 'learning_rate': 0.0001992773527932985, 'epoch': 0.03}\n",
      "{'loss': 1.4438, 'grad_norm': 0.53515625, 'learning_rate': 0.00019926599041583462, 'epoch': 0.03}\n",
      "{'loss': 1.3157, 'grad_norm': 1.0390625, 'learning_rate': 0.00019925462803837077, 'epoch': 0.03}\n",
      "{'loss': 1.6117, 'grad_norm': 0.400390625, 'learning_rate': 0.0001992432656609069, 'epoch': 0.03}\n",
      "{'loss': 1.3199, 'grad_norm': 0.53515625, 'learning_rate': 0.00019923190328344304, 'epoch': 0.03}\n",
      "{'loss': 1.4808, 'grad_norm': 0.294921875, 'learning_rate': 0.00019922054090597917, 'epoch': 0.03}\n",
      "{'loss': 1.4871, 'grad_norm': 0.46484375, 'learning_rate': 0.00019920917852851532, 'epoch': 0.03}\n",
      "{'loss': 1.2584, 'grad_norm': 0.6953125, 'learning_rate': 0.00019919781615105147, 'epoch': 0.03}\n",
      "{'loss': 1.4001, 'grad_norm': 0.41015625, 'learning_rate': 0.0001991864537735876, 'epoch': 0.03}\n",
      "{'loss': 1.3953, 'grad_norm': 0.796875, 'learning_rate': 0.00019917509139612375, 'epoch': 0.03}\n",
      "{'loss': 1.3846, 'grad_norm': 0.333984375, 'learning_rate': 0.00019916372901865987, 'epoch': 0.03}\n",
      "{'loss': 1.4199, 'grad_norm': 0.380859375, 'learning_rate': 0.00019915236664119602, 'epoch': 0.03}\n",
      "{'loss': 1.3286, 'grad_norm': 0.82421875, 'learning_rate': 0.00019914100426373215, 'epoch': 0.03}\n",
      "{'loss': 1.7357, 'grad_norm': 0.439453125, 'learning_rate': 0.0001991296418862683, 'epoch': 0.03}\n",
      "{'loss': 1.3005, 'grad_norm': 0.4296875, 'learning_rate': 0.00019911827950880445, 'epoch': 0.03}\n",
      "{'loss': 1.4139, 'grad_norm': 0.419921875, 'learning_rate': 0.00019910691713134057, 'epoch': 0.03}\n",
      "{'loss': 1.3796, 'grad_norm': 0.361328125, 'learning_rate': 0.00019909555475387672, 'epoch': 0.03}\n",
      "{'loss': 1.312, 'grad_norm': 0.8359375, 'learning_rate': 0.00019908419237641285, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5396, 'grad_norm': 0.51953125, 'learning_rate': 0.00019907282999894897, 'epoch': 0.03}\n",
      "{'loss': 1.4538, 'grad_norm': 0.58203125, 'learning_rate': 0.00019906146762148512, 'epoch': 0.03}\n",
      "{'loss': 1.3493, 'grad_norm': 0.3671875, 'learning_rate': 0.00019905010524402128, 'epoch': 0.03}\n",
      "{'loss': 1.4911, 'grad_norm': 0.60546875, 'learning_rate': 0.00019903874286655743, 'epoch': 0.03}\n",
      "{'loss': 1.3203, 'grad_norm': 0.67578125, 'learning_rate': 0.00019902738048909355, 'epoch': 0.03}\n",
      "{'loss': 1.4651, 'grad_norm': 0.318359375, 'learning_rate': 0.00019901601811162968, 'epoch': 0.03}\n",
      "{'loss': 1.3667, 'grad_norm': 0.62109375, 'learning_rate': 0.00019900465573416583, 'epoch': 0.03}\n",
      "{'loss': 1.4568, 'grad_norm': 0.33984375, 'learning_rate': 0.00019899329335670195, 'epoch': 0.03}\n",
      "{'loss': 1.4633, 'grad_norm': 0.423828125, 'learning_rate': 0.0001989819309792381, 'epoch': 0.03}\n",
      "{'loss': 1.2692, 'grad_norm': 0.94921875, 'learning_rate': 0.00019897056860177425, 'epoch': 0.03}\n",
      "{'loss': 1.5815, 'grad_norm': 0.431640625, 'learning_rate': 0.00019895920622431038, 'epoch': 0.03}\n",
      "{'loss': 1.4446, 'grad_norm': 0.59765625, 'learning_rate': 0.00019894784384684653, 'epoch': 0.03}\n",
      "{'loss': 1.5202, 'grad_norm': 0.333984375, 'learning_rate': 0.00019893648146938265, 'epoch': 0.03}\n",
      "{'loss': 1.4923, 'grad_norm': 0.44140625, 'learning_rate': 0.0001989251190919188, 'epoch': 0.03}\n",
      "{'loss': 1.3151, 'grad_norm': 0.7421875, 'learning_rate': 0.00019891375671445493, 'epoch': 0.03}\n",
      "{'loss': 1.5524, 'grad_norm': 0.388671875, 'learning_rate': 0.00019890239433699108, 'epoch': 0.03}\n",
      "{'loss': 1.5476, 'grad_norm': 0.5234375, 'learning_rate': 0.00019889103195952723, 'epoch': 0.03}\n",
      "{'loss': 1.5908, 'grad_norm': 0.369140625, 'learning_rate': 0.00019887966958206336, 'epoch': 0.03}\n",
      "{'loss': 1.4969, 'grad_norm': 0.57421875, 'learning_rate': 0.0001988683072045995, 'epoch': 0.03}\n",
      "{'loss': 1.4345, 'grad_norm': 0.671875, 'learning_rate': 0.00019885694482713563, 'epoch': 0.03}\n",
      "{'loss': 1.6121, 'grad_norm': 0.3203125, 'learning_rate': 0.00019884558244967178, 'epoch': 0.03}\n",
      "{'loss': 1.4034, 'grad_norm': 0.6328125, 'learning_rate': 0.0001988342200722079, 'epoch': 0.03}\n",
      "{'loss': 1.2903, 'grad_norm': 0.287109375, 'learning_rate': 0.00019882285769474406, 'epoch': 0.03}\n",
      "{'loss': 1.5554, 'grad_norm': 0.484375, 'learning_rate': 0.0001988114953172802, 'epoch': 0.03}\n",
      "{'loss': 1.2319, 'grad_norm': 0.50390625, 'learning_rate': 0.00019880013293981634, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4148, 'grad_norm': 0.375, 'learning_rate': 0.0001987887705623525, 'epoch': 0.03}\n",
      "{'loss': 1.3681, 'grad_norm': 0.5546875, 'learning_rate': 0.0001987774081848886, 'epoch': 0.03}\n",
      "{'loss': 1.351, 'grad_norm': 0.30859375, 'learning_rate': 0.00019876604580742476, 'epoch': 0.03}\n",
      "{'loss': 1.4042, 'grad_norm': 0.46875, 'learning_rate': 0.0001987546834299609, 'epoch': 0.03}\n",
      "{'loss': 1.289, 'grad_norm': 0.443359375, 'learning_rate': 0.00019874332105249704, 'epoch': 0.03}\n",
      "{'loss': 1.6349, 'grad_norm': 0.3203125, 'learning_rate': 0.0001987319586750332, 'epoch': 0.03}\n",
      "{'loss': 1.33, 'grad_norm': 0.439453125, 'learning_rate': 0.00019872059629756931, 'epoch': 0.03}\n",
      "{'loss': 1.2393, 'grad_norm': 0.359375, 'learning_rate': 0.00019870923392010546, 'epoch': 0.03}\n",
      "{'loss': 1.5247, 'grad_norm': 0.5703125, 'learning_rate': 0.0001986978715426416, 'epoch': 0.03}\n",
      "{'loss': 1.3285, 'grad_norm': 0.83203125, 'learning_rate': 0.00019868650916517771, 'epoch': 0.03}\n",
      "{'loss': 1.5434, 'grad_norm': 0.4140625, 'learning_rate': 0.00019867514678771387, 'epoch': 0.03}\n",
      "{'loss': 1.336, 'grad_norm': 0.5390625, 'learning_rate': 0.00019866378441025002, 'epoch': 0.03}\n",
      "{'loss': 1.4774, 'grad_norm': 0.302734375, 'learning_rate': 0.00019865242203278617, 'epoch': 0.03}\n",
      "{'loss': 1.3759, 'grad_norm': 0.33984375, 'learning_rate': 0.0001986410596553223, 'epoch': 0.03}\n",
      "{'loss': 1.3446, 'grad_norm': 0.65234375, 'learning_rate': 0.00019862969727785842, 'epoch': 0.03}\n",
      "{'loss': 1.5261, 'grad_norm': 0.314453125, 'learning_rate': 0.00019861833490039457, 'epoch': 0.03}\n",
      "{'loss': 1.5128, 'grad_norm': 0.6953125, 'learning_rate': 0.0001986069725229307, 'epoch': 0.03}\n",
      "{'loss': 1.3708, 'grad_norm': 0.341796875, 'learning_rate': 0.00019859561014546684, 'epoch': 0.03}\n",
      "{'loss': 1.5387, 'grad_norm': 0.4921875, 'learning_rate': 0.000198584247768003, 'epoch': 0.03}\n",
      "{'loss': 1.358, 'grad_norm': 0.71484375, 'learning_rate': 0.00019857288539053912, 'epoch': 0.03}\n",
      "{'loss': 1.6071, 'grad_norm': 0.373046875, 'learning_rate': 0.00019856152301307527, 'epoch': 0.03}\n",
      "{'loss': 1.3911, 'grad_norm': 0.546875, 'learning_rate': 0.0001985501606356114, 'epoch': 0.03}\n",
      "{'loss': 1.397, 'grad_norm': 0.451171875, 'learning_rate': 0.00019853879825814755, 'epoch': 0.03}\n",
      "{'loss': 1.4126, 'grad_norm': 0.57421875, 'learning_rate': 0.00019852743588068367, 'epoch': 0.03}\n",
      "{'loss': 1.348, 'grad_norm': 0.6796875, 'learning_rate': 0.00019851607350321982, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5333, 'grad_norm': 0.353515625, 'learning_rate': 0.00019850471112575597, 'epoch': 0.03}\n",
      "{'loss': 1.3668, 'grad_norm': 0.56640625, 'learning_rate': 0.0001984933487482921, 'epoch': 0.03}\n",
      "{'loss': 1.4056, 'grad_norm': 0.291015625, 'learning_rate': 0.00019848198637082825, 'epoch': 0.03}\n",
      "{'loss': 1.4487, 'grad_norm': 0.44140625, 'learning_rate': 0.00019847062399336437, 'epoch': 0.03}\n",
      "{'loss': 1.2323, 'grad_norm': 0.66015625, 'learning_rate': 0.00019845926161590052, 'epoch': 0.03}\n",
      "{'loss': 1.5099, 'grad_norm': 0.4140625, 'learning_rate': 0.00019844789923843665, 'epoch': 0.03}\n",
      "{'loss': 1.356, 'grad_norm': 0.5078125, 'learning_rate': 0.0001984365368609728, 'epoch': 0.03}\n",
      "{'loss': 1.3402, 'grad_norm': 0.423828125, 'learning_rate': 0.00019842517448350895, 'epoch': 0.03}\n",
      "{'loss': 1.405, 'grad_norm': 0.46484375, 'learning_rate': 0.00019841381210604508, 'epoch': 0.03}\n",
      "{'loss': 1.2102, 'grad_norm': 0.58984375, 'learning_rate': 0.00019840244972858123, 'epoch': 0.03}\n",
      "{'loss': 1.6028, 'grad_norm': 0.39453125, 'learning_rate': 0.00019839108735111735, 'epoch': 0.03}\n",
      "{'loss': 1.3828, 'grad_norm': 0.5625, 'learning_rate': 0.0001983797249736535, 'epoch': 0.03}\n",
      "{'loss': 1.4564, 'grad_norm': 0.443359375, 'learning_rate': 0.00019836836259618963, 'epoch': 0.03}\n",
      "{'loss': 1.4369, 'grad_norm': 0.56640625, 'learning_rate': 0.00019835700021872578, 'epoch': 0.03}\n",
      "{'loss': 1.3684, 'grad_norm': 0.55078125, 'learning_rate': 0.00019834563784126193, 'epoch': 0.03}\n",
      "{'loss': 1.4614, 'grad_norm': 0.388671875, 'learning_rate': 0.00019833427546379805, 'epoch': 0.03}\n",
      "{'loss': 1.2675, 'grad_norm': 0.546875, 'learning_rate': 0.0001983229130863342, 'epoch': 0.03}\n",
      "{'loss': 1.3979, 'grad_norm': 0.30859375, 'learning_rate': 0.00019831155070887033, 'epoch': 0.03}\n",
      "{'loss': 1.4156, 'grad_norm': 0.58203125, 'learning_rate': 0.00019830018833140645, 'epoch': 0.03}\n",
      "{'loss': 1.3426, 'grad_norm': 0.78515625, 'learning_rate': 0.0001982888259539426, 'epoch': 0.03}\n",
      "{'loss': 1.4912, 'grad_norm': 0.380859375, 'learning_rate': 0.00019827746357647876, 'epoch': 0.03}\n",
      "{'loss': 1.427, 'grad_norm': 0.5546875, 'learning_rate': 0.0001982661011990149, 'epoch': 0.03}\n",
      "{'loss': 1.4142, 'grad_norm': 0.349609375, 'learning_rate': 0.00019825473882155103, 'epoch': 0.03}\n",
      "{'loss': 1.4619, 'grad_norm': 0.412109375, 'learning_rate': 0.00019824337644408716, 'epoch': 0.03}\n",
      "{'loss': 1.3079, 'grad_norm': 0.73828125, 'learning_rate': 0.0001982320140666233, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5855, 'grad_norm': 0.498046875, 'learning_rate': 0.00019822065168915943, 'epoch': 0.03}\n",
      "{'loss': 1.3986, 'grad_norm': 0.439453125, 'learning_rate': 0.00019820928931169558, 'epoch': 0.03}\n",
      "{'loss': 1.2686, 'grad_norm': 0.357421875, 'learning_rate': 0.00019819792693423174, 'epoch': 0.03}\n",
      "{'loss': 1.504, 'grad_norm': 0.41015625, 'learning_rate': 0.00019818656455676786, 'epoch': 0.03}\n",
      "{'loss': 1.3525, 'grad_norm': 0.98046875, 'learning_rate': 0.000198175202179304, 'epoch': 0.03}\n",
      "{'loss': 1.5887, 'grad_norm': 0.4140625, 'learning_rate': 0.00019816383980184014, 'epoch': 0.04}\n",
      "{'loss': 1.3351, 'grad_norm': 0.64453125, 'learning_rate': 0.0001981524774243763, 'epoch': 0.04}\n",
      "{'loss': 1.5316, 'grad_norm': 0.33203125, 'learning_rate': 0.0001981411150469124, 'epoch': 0.04}\n",
      "{'loss': 1.3955, 'grad_norm': 0.421875, 'learning_rate': 0.00019812975266944856, 'epoch': 0.04}\n",
      "{'loss': 1.2497, 'grad_norm': 0.8203125, 'learning_rate': 0.00019811839029198471, 'epoch': 0.04}\n",
      "{'loss': 1.6473, 'grad_norm': 0.326171875, 'learning_rate': 0.00019810702791452084, 'epoch': 0.04}\n",
      "{'loss': 1.4461, 'grad_norm': 0.4765625, 'learning_rate': 0.000198095665537057, 'epoch': 0.04}\n",
      "{'loss': 1.3968, 'grad_norm': 0.345703125, 'learning_rate': 0.00019808430315959311, 'epoch': 0.04}\n",
      "{'loss': 1.4635, 'grad_norm': 0.44140625, 'learning_rate': 0.00019807294078212927, 'epoch': 0.04}\n",
      "{'loss': 1.2222, 'grad_norm': 0.5390625, 'learning_rate': 0.0001980615784046654, 'epoch': 0.04}\n",
      "{'loss': 1.6098, 'grad_norm': 0.34375, 'learning_rate': 0.00019805021602720154, 'epoch': 0.04}\n",
      "{'loss': 1.3605, 'grad_norm': 0.640625, 'learning_rate': 0.0001980388536497377, 'epoch': 0.04}\n",
      "{'loss': 1.3627, 'grad_norm': 0.56640625, 'learning_rate': 0.00019802749127227382, 'epoch': 0.04}\n",
      "{'loss': 1.4698, 'grad_norm': 0.419921875, 'learning_rate': 0.00019801612889480997, 'epoch': 0.04}\n",
      "{'loss': 1.281, 'grad_norm': 0.59765625, 'learning_rate': 0.0001980047665173461, 'epoch': 0.04}\n",
      "{'loss': 1.4474, 'grad_norm': 0.37109375, 'learning_rate': 0.00019799340413988224, 'epoch': 0.04}\n",
      "{'loss': 1.4383, 'grad_norm': 0.515625, 'learning_rate': 0.00019798204176241837, 'epoch': 0.04}\n",
      "{'loss': 1.3121, 'grad_norm': 0.34765625, 'learning_rate': 0.00019797067938495452, 'epoch': 0.04}\n",
      "{'loss': 1.4811, 'grad_norm': 0.5, 'learning_rate': 0.00019795931700749067, 'epoch': 0.04}\n",
      "{'loss': 1.2637, 'grad_norm': 0.5546875, 'learning_rate': 0.0001979479546300268, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4593, 'grad_norm': 0.3828125, 'learning_rate': 0.00019793659225256295, 'epoch': 0.04}\n",
      "{'loss': 1.3713, 'grad_norm': 0.63671875, 'learning_rate': 0.00019792522987509907, 'epoch': 0.04}\n",
      "{'loss': 1.2593, 'grad_norm': 0.341796875, 'learning_rate': 0.0001979138674976352, 'epoch': 0.04}\n",
      "{'loss': 1.3846, 'grad_norm': 0.4140625, 'learning_rate': 0.00019790250512017135, 'epoch': 0.04}\n",
      "{'loss': 1.2418, 'grad_norm': 0.6171875, 'learning_rate': 0.0001978911427427075, 'epoch': 0.04}\n",
      "{'loss': 1.391, 'grad_norm': 0.322265625, 'learning_rate': 0.00019787978036524365, 'epoch': 0.04}\n",
      "{'loss': 1.4197, 'grad_norm': 0.43359375, 'learning_rate': 0.00019786841798777977, 'epoch': 0.04}\n",
      "{'loss': 1.4299, 'grad_norm': 0.36328125, 'learning_rate': 0.0001978570556103159, 'epoch': 0.04}\n",
      "{'loss': 1.41, 'grad_norm': 0.451171875, 'learning_rate': 0.00019784569323285205, 'epoch': 0.04}\n",
      "{'loss': 1.2851, 'grad_norm': 0.81640625, 'learning_rate': 0.00019783433085538817, 'epoch': 0.04}\n",
      "{'loss': 1.4332, 'grad_norm': 0.3828125, 'learning_rate': 0.00019782296847792433, 'epoch': 0.04}\n",
      "{'loss': 1.4342, 'grad_norm': 0.56640625, 'learning_rate': 0.00019781160610046048, 'epoch': 0.04}\n",
      "{'loss': 1.4223, 'grad_norm': 0.34375, 'learning_rate': 0.0001978002437229966, 'epoch': 0.04}\n",
      "{'loss': 1.4208, 'grad_norm': 0.400390625, 'learning_rate': 0.00019778888134553275, 'epoch': 0.04}\n",
      "{'loss': 1.2747, 'grad_norm': 0.58984375, 'learning_rate': 0.00019777751896806888, 'epoch': 0.04}\n",
      "{'loss': 1.4076, 'grad_norm': 0.53515625, 'learning_rate': 0.00019776615659060503, 'epoch': 0.04}\n",
      "{'loss': 1.4122, 'grad_norm': 0.625, 'learning_rate': 0.00019775479421314115, 'epoch': 0.04}\n",
      "{'loss': 1.4129, 'grad_norm': 0.314453125, 'learning_rate': 0.0001977434318356773, 'epoch': 0.04}\n",
      "{'loss': 1.3351, 'grad_norm': 0.54296875, 'learning_rate': 0.00019773206945821346, 'epoch': 0.04}\n",
      "{'loss': 1.1874, 'grad_norm': 0.87890625, 'learning_rate': 0.00019772070708074958, 'epoch': 0.04}\n",
      "{'loss': 1.5108, 'grad_norm': 0.330078125, 'learning_rate': 0.00019770934470328573, 'epoch': 0.04}\n",
      "{'loss': 1.4944, 'grad_norm': 0.453125, 'learning_rate': 0.00019769798232582186, 'epoch': 0.04}\n",
      "{'loss': 1.3847, 'grad_norm': 0.330078125, 'learning_rate': 0.000197686619948358, 'epoch': 0.04}\n",
      "{'loss': 1.4863, 'grad_norm': 0.474609375, 'learning_rate': 0.00019767525757089413, 'epoch': 0.04}\n",
      "{'loss': 1.2812, 'grad_norm': 0.609375, 'learning_rate': 0.00019766389519343028, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3517, 'grad_norm': 0.439453125, 'learning_rate': 0.00019765253281596643, 'epoch': 0.04}\n",
      "{'loss': 1.4764, 'grad_norm': 0.46484375, 'learning_rate': 0.00019764117043850256, 'epoch': 0.04}\n",
      "{'loss': 1.3906, 'grad_norm': 0.33203125, 'learning_rate': 0.0001976298080610387, 'epoch': 0.04}\n",
      "{'loss': 1.4589, 'grad_norm': 0.419921875, 'learning_rate': 0.00019761844568357483, 'epoch': 0.04}\n",
      "{'loss': 1.2786, 'grad_norm': 1.4609375, 'learning_rate': 0.00019760708330611099, 'epoch': 0.04}\n",
      "{'loss': 1.5267, 'grad_norm': 0.384765625, 'learning_rate': 0.0001975957209286471, 'epoch': 0.04}\n",
      "{'loss': 1.3908, 'grad_norm': 0.482421875, 'learning_rate': 0.00019758435855118326, 'epoch': 0.04}\n",
      "{'loss': 1.3815, 'grad_norm': 0.357421875, 'learning_rate': 0.0001975729961737194, 'epoch': 0.04}\n",
      "{'loss': 1.3151, 'grad_norm': 0.52734375, 'learning_rate': 0.00019756163379625554, 'epoch': 0.04}\n",
      "{'loss': 1.2508, 'grad_norm': 0.58203125, 'learning_rate': 0.0001975502714187917, 'epoch': 0.04}\n",
      "{'loss': 1.4161, 'grad_norm': 0.396484375, 'learning_rate': 0.0001975389090413278, 'epoch': 0.04}\n",
      "{'loss': 1.3758, 'grad_norm': 0.46875, 'learning_rate': 0.00019752754666386394, 'epoch': 0.04}\n",
      "{'loss': 1.3799, 'grad_norm': 0.310546875, 'learning_rate': 0.0001975161842864001, 'epoch': 0.04}\n",
      "{'loss': 1.4572, 'grad_norm': 0.546875, 'learning_rate': 0.00019750482190893624, 'epoch': 0.04}\n",
      "{'loss': 1.2953, 'grad_norm': 0.62109375, 'learning_rate': 0.0001974934595314724, 'epoch': 0.04}\n",
      "{'loss': 1.4192, 'grad_norm': 0.44140625, 'learning_rate': 0.00019748209715400852, 'epoch': 0.04}\n",
      "{'loss': 1.3696, 'grad_norm': 0.73828125, 'learning_rate': 0.00019747073477654464, 'epoch': 0.04}\n",
      "{'loss': 1.4194, 'grad_norm': 0.376953125, 'learning_rate': 0.0001974593723990808, 'epoch': 0.04}\n",
      "{'loss': 1.3544, 'grad_norm': 0.43359375, 'learning_rate': 0.00019744801002161692, 'epoch': 0.04}\n",
      "{'loss': 1.2591, 'grad_norm': 0.96875, 'learning_rate': 0.00019743664764415307, 'epoch': 0.04}\n",
      "{'loss': 1.5316, 'grad_norm': 0.37109375, 'learning_rate': 0.00019742528526668922, 'epoch': 0.04}\n",
      "{'loss': 1.385, 'grad_norm': 0.474609375, 'learning_rate': 0.00019741392288922534, 'epoch': 0.04}\n",
      "{'loss': 1.3706, 'grad_norm': 0.322265625, 'learning_rate': 0.0001974025605117615, 'epoch': 0.04}\n",
      "{'loss': 1.3508, 'grad_norm': 0.390625, 'learning_rate': 0.00019739119813429762, 'epoch': 0.04}\n",
      "{'loss': 1.1676, 'grad_norm': 0.427734375, 'learning_rate': 0.00019737983575683377, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5846, 'grad_norm': 0.447265625, 'learning_rate': 0.0001973684733793699, 'epoch': 0.04}\n",
      "{'loss': 1.29, 'grad_norm': 0.5234375, 'learning_rate': 0.00019735711100190605, 'epoch': 0.04}\n",
      "{'loss': 1.3406, 'grad_norm': 0.353515625, 'learning_rate': 0.0001973457486244422, 'epoch': 0.04}\n",
      "{'loss': 1.4892, 'grad_norm': 0.400390625, 'learning_rate': 0.00019733438624697832, 'epoch': 0.04}\n",
      "{'loss': 1.2971, 'grad_norm': 0.6796875, 'learning_rate': 0.00019732302386951447, 'epoch': 0.04}\n",
      "{'loss': 1.6048, 'grad_norm': 0.3984375, 'learning_rate': 0.0001973116614920506, 'epoch': 0.04}\n",
      "{'loss': 1.4503, 'grad_norm': 0.478515625, 'learning_rate': 0.00019730029911458675, 'epoch': 0.04}\n",
      "{'loss': 1.3054, 'grad_norm': 0.32421875, 'learning_rate': 0.00019728893673712287, 'epoch': 0.04}\n",
      "{'loss': 1.407, 'grad_norm': 0.3984375, 'learning_rate': 0.00019727757435965902, 'epoch': 0.04}\n",
      "{'loss': 1.2904, 'grad_norm': 0.765625, 'learning_rate': 0.00019726621198219518, 'epoch': 0.04}\n",
      "{'loss': 1.5528, 'grad_norm': 0.46484375, 'learning_rate': 0.0001972548496047313, 'epoch': 0.04}\n",
      "{'loss': 1.4143, 'grad_norm': 0.40234375, 'learning_rate': 0.00019724348722726745, 'epoch': 0.04}\n",
      "{'loss': 1.3667, 'grad_norm': 0.33203125, 'learning_rate': 0.00019723212484980358, 'epoch': 0.04}\n",
      "{'loss': 1.4218, 'grad_norm': 0.453125, 'learning_rate': 0.00019722076247233973, 'epoch': 0.04}\n",
      "{'loss': 1.2595, 'grad_norm': 1.25, 'learning_rate': 0.00019720940009487585, 'epoch': 0.04}\n",
      "{'loss': 1.3801, 'grad_norm': 0.38671875, 'learning_rate': 0.000197198037717412, 'epoch': 0.04}\n",
      "{'loss': 1.4808, 'grad_norm': 0.412109375, 'learning_rate': 0.00019718667533994815, 'epoch': 0.04}\n",
      "{'loss': 1.3169, 'grad_norm': 0.353515625, 'learning_rate': 0.00019717531296248428, 'epoch': 0.04}\n",
      "{'loss': 1.4282, 'grad_norm': 0.484375, 'learning_rate': 0.00019716395058502043, 'epoch': 0.04}\n",
      "{'loss': 1.252, 'grad_norm': 0.56640625, 'learning_rate': 0.00019715258820755655, 'epoch': 0.04}\n",
      "{'loss': 1.4976, 'grad_norm': 0.404296875, 'learning_rate': 0.00019714122583009268, 'epoch': 0.04}\n",
      "{'loss': 1.4119, 'grad_norm': 0.51953125, 'learning_rate': 0.00019712986345262883, 'epoch': 0.04}\n",
      "{'loss': 1.363, 'grad_norm': 0.36328125, 'learning_rate': 0.00019711850107516498, 'epoch': 0.04}\n",
      "{'loss': 1.4044, 'grad_norm': 0.4140625, 'learning_rate': 0.00019710713869770113, 'epoch': 0.04}\n",
      "{'loss': 1.275, 'grad_norm': 0.796875, 'learning_rate': 0.00019709577632023726, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4859, 'grad_norm': 0.361328125, 'learning_rate': 0.00019708441394277338, 'epoch': 0.04}\n",
      "{'loss': 1.403, 'grad_norm': 0.56640625, 'learning_rate': 0.00019707305156530953, 'epoch': 0.04}\n",
      "{'loss': 1.2701, 'grad_norm': 0.384765625, 'learning_rate': 0.00019706168918784566, 'epoch': 0.04}\n",
      "{'loss': 1.3584, 'grad_norm': 0.373046875, 'learning_rate': 0.00019705032681038184, 'epoch': 0.04}\n",
      "{'loss': 1.1147, 'grad_norm': 0.73046875, 'learning_rate': 0.00019703896443291796, 'epoch': 0.04}\n",
      "{'loss': 1.5502, 'grad_norm': 0.4609375, 'learning_rate': 0.00019702760205545408, 'epoch': 0.04}\n",
      "{'loss': 1.4373, 'grad_norm': 0.486328125, 'learning_rate': 0.00019701623967799024, 'epoch': 0.04}\n",
      "{'loss': 1.3555, 'grad_norm': 0.37109375, 'learning_rate': 0.00019700487730052636, 'epoch': 0.04}\n",
      "{'loss': 1.4615, 'grad_norm': 0.59765625, 'learning_rate': 0.0001969935149230625, 'epoch': 0.04}\n",
      "{'loss': 1.1639, 'grad_norm': 0.63671875, 'learning_rate': 0.00019698215254559864, 'epoch': 0.04}\n",
      "{'loss': 1.4425, 'grad_norm': 0.296875, 'learning_rate': 0.0001969707901681348, 'epoch': 0.04}\n",
      "{'loss': 1.4719, 'grad_norm': 0.875, 'learning_rate': 0.00019695942779067094, 'epoch': 0.04}\n",
      "{'loss': 1.3219, 'grad_norm': 0.353515625, 'learning_rate': 0.00019694806541320706, 'epoch': 0.04}\n",
      "{'loss': 1.4581, 'grad_norm': 0.546875, 'learning_rate': 0.00019693670303574321, 'epoch': 0.04}\n",
      "{'loss': 1.2228, 'grad_norm': 0.76171875, 'learning_rate': 0.00019692534065827934, 'epoch': 0.04}\n",
      "{'loss': 1.5206, 'grad_norm': 0.48828125, 'learning_rate': 0.0001969139782808155, 'epoch': 0.04}\n",
      "{'loss': 1.3602, 'grad_norm': 0.66796875, 'learning_rate': 0.00019690261590335161, 'epoch': 0.04}\n",
      "{'loss': 1.3324, 'grad_norm': 0.361328125, 'learning_rate': 0.00019689125352588777, 'epoch': 0.04}\n",
      "{'loss': 1.2663, 'grad_norm': 0.42578125, 'learning_rate': 0.00019687989114842392, 'epoch': 0.04}\n",
      "{'loss': 1.1856, 'grad_norm': 0.59765625, 'learning_rate': 0.00019686852877096004, 'epoch': 0.04}\n",
      "{'loss': 1.4715, 'grad_norm': 0.439453125, 'learning_rate': 0.0001968571663934962, 'epoch': 0.04}\n",
      "{'loss': 1.3267, 'grad_norm': 0.51171875, 'learning_rate': 0.00019684580401603232, 'epoch': 0.04}\n",
      "{'loss': 1.4505, 'grad_norm': 0.326171875, 'learning_rate': 0.00019683444163856847, 'epoch': 0.04}\n",
      "{'loss': 1.417, 'grad_norm': 0.54296875, 'learning_rate': 0.0001968230792611046, 'epoch': 0.04}\n",
      "{'loss': 1.2443, 'grad_norm': 0.65234375, 'learning_rate': 0.00019681171688364074, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4266, 'grad_norm': 0.384765625, 'learning_rate': 0.0001968003545061769, 'epoch': 0.04}\n",
      "{'loss': 1.2717, 'grad_norm': 0.68359375, 'learning_rate': 0.00019678899212871302, 'epoch': 0.04}\n",
      "{'loss': 1.3519, 'grad_norm': 0.310546875, 'learning_rate': 0.00019677762975124917, 'epoch': 0.04}\n",
      "{'loss': 1.3955, 'grad_norm': 0.53515625, 'learning_rate': 0.0001967662673737853, 'epoch': 0.04}\n",
      "{'loss': 1.2594, 'grad_norm': 0.6640625, 'learning_rate': 0.00019675490499632142, 'epoch': 0.04}\n",
      "{'loss': 1.6632, 'grad_norm': 0.333984375, 'learning_rate': 0.00019674354261885757, 'epoch': 0.04}\n",
      "{'loss': 1.2164, 'grad_norm': 0.609375, 'learning_rate': 0.00019673218024139372, 'epoch': 0.04}\n",
      "{'loss': 1.4057, 'grad_norm': 0.41015625, 'learning_rate': 0.00019672081786392987, 'epoch': 0.04}\n",
      "{'loss': 1.3642, 'grad_norm': 0.57421875, 'learning_rate': 0.000196709455486466, 'epoch': 0.04}\n",
      "{'loss': 1.3726, 'grad_norm': 0.6484375, 'learning_rate': 0.00019669809310900212, 'epoch': 0.04}\n",
      "{'loss': 1.436, 'grad_norm': 0.42578125, 'learning_rate': 0.00019668673073153827, 'epoch': 0.04}\n",
      "{'loss': 1.3636, 'grad_norm': 0.6328125, 'learning_rate': 0.0001966753683540744, 'epoch': 0.04}\n",
      "{'loss': 1.2751, 'grad_norm': 0.345703125, 'learning_rate': 0.00019666400597661058, 'epoch': 0.04}\n",
      "{'loss': 1.4864, 'grad_norm': 0.361328125, 'learning_rate': 0.0001966526435991467, 'epoch': 0.04}\n",
      "{'loss': 1.236, 'grad_norm': 0.671875, 'learning_rate': 0.00019664128122168283, 'epoch': 0.04}\n",
      "{'loss': 1.4797, 'grad_norm': 0.37890625, 'learning_rate': 0.00019662991884421898, 'epoch': 0.04}\n",
      "{'loss': 1.4557, 'grad_norm': 0.53125, 'learning_rate': 0.0001966185564667551, 'epoch': 0.04}\n",
      "{'loss': 1.3356, 'grad_norm': 0.296875, 'learning_rate': 0.00019660719408929125, 'epoch': 0.04}\n",
      "{'loss': 1.3978, 'grad_norm': 0.37109375, 'learning_rate': 0.00019659583171182738, 'epoch': 0.04}\n",
      "{'loss': 1.3221, 'grad_norm': 0.482421875, 'learning_rate': 0.00019658446933436353, 'epoch': 0.04}\n",
      "{'loss': 1.5355, 'grad_norm': 0.451171875, 'learning_rate': 0.00019657310695689968, 'epoch': 0.04}\n",
      "{'loss': 1.2825, 'grad_norm': 0.5234375, 'learning_rate': 0.0001965617445794358, 'epoch': 0.04}\n",
      "{'loss': 1.2984, 'grad_norm': 0.310546875, 'learning_rate': 0.00019655038220197196, 'epoch': 0.04}\n",
      "{'loss': 1.3411, 'grad_norm': 0.400390625, 'learning_rate': 0.00019653901982450808, 'epoch': 0.04}\n",
      "{'loss': 1.18, 'grad_norm': 0.9375, 'learning_rate': 0.00019652765744704423, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4462, 'grad_norm': 0.416015625, 'learning_rate': 0.00019651629506958036, 'epoch': 0.04}\n",
      "{'loss': 1.3626, 'grad_norm': 0.72265625, 'learning_rate': 0.0001965049326921165, 'epoch': 0.04}\n",
      "{'loss': 1.2896, 'grad_norm': 0.4296875, 'learning_rate': 0.00019649357031465266, 'epoch': 0.04}\n",
      "{'loss': 1.4359, 'grad_norm': 0.384765625, 'learning_rate': 0.00019648220793718878, 'epoch': 0.04}\n",
      "{'loss': 1.2309, 'grad_norm': 0.765625, 'learning_rate': 0.00019647084555972493, 'epoch': 0.04}\n",
      "{'loss': 1.3232, 'grad_norm': 0.36328125, 'learning_rate': 0.00019645948318226106, 'epoch': 0.04}\n",
      "{'loss': 1.2915, 'grad_norm': 0.455078125, 'learning_rate': 0.0001964481208047972, 'epoch': 0.04}\n",
      "{'loss': 1.3296, 'grad_norm': 0.353515625, 'learning_rate': 0.00019643675842733333, 'epoch': 0.04}\n",
      "{'loss': 1.4576, 'grad_norm': 0.390625, 'learning_rate': 0.00019642539604986949, 'epoch': 0.04}\n",
      "{'loss': 1.1429, 'grad_norm': 1.2421875, 'learning_rate': 0.00019641403367240564, 'epoch': 0.04}\n",
      "{'loss': 1.5802, 'grad_norm': 0.43359375, 'learning_rate': 0.00019640267129494176, 'epoch': 0.04}\n",
      "{'loss': 1.3768, 'grad_norm': 0.50390625, 'learning_rate': 0.0001963913089174779, 'epoch': 0.04}\n",
      "{'loss': 1.444, 'grad_norm': 0.3828125, 'learning_rate': 0.00019637994654001404, 'epoch': 0.04}\n",
      "{'loss': 1.3347, 'grad_norm': 0.423828125, 'learning_rate': 0.00019636858416255016, 'epoch': 0.04}\n",
      "{'loss': 1.1548, 'grad_norm': 0.43359375, 'learning_rate': 0.00019635722178508634, 'epoch': 0.04}\n",
      "{'loss': 1.4249, 'grad_norm': 0.35546875, 'learning_rate': 0.00019634585940762246, 'epoch': 0.04}\n",
      "{'loss': 1.3126, 'grad_norm': 0.466796875, 'learning_rate': 0.00019633449703015862, 'epoch': 0.04}\n",
      "{'loss': 1.4047, 'grad_norm': 0.333984375, 'learning_rate': 0.00019632313465269474, 'epoch': 0.04}\n",
      "{'loss': 1.4072, 'grad_norm': 0.453125, 'learning_rate': 0.00019631177227523086, 'epoch': 0.04}\n",
      "{'loss': 1.2033, 'grad_norm': 0.29296875, 'learning_rate': 0.00019630040989776702, 'epoch': 0.04}\n",
      "{'loss': 1.4554, 'grad_norm': 0.435546875, 'learning_rate': 0.00019628904752030314, 'epoch': 0.04}\n",
      "{'loss': 1.4237, 'grad_norm': 0.5703125, 'learning_rate': 0.00019627768514283932, 'epoch': 0.04}\n",
      "{'loss': 1.4163, 'grad_norm': 0.328125, 'learning_rate': 0.00019626632276537544, 'epoch': 0.04}\n",
      "{'loss': 1.3433, 'grad_norm': 0.431640625, 'learning_rate': 0.00019625496038791157, 'epoch': 0.04}\n",
      "{'loss': 1.2906, 'grad_norm': 0.98046875, 'learning_rate': 0.00019624359801044772, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4018, 'grad_norm': 0.458984375, 'learning_rate': 0.00019623223563298384, 'epoch': 0.04}\n",
      "{'loss': 1.2725, 'grad_norm': 0.484375, 'learning_rate': 0.00019622087325552, 'epoch': 0.04}\n",
      "{'loss': 1.4552, 'grad_norm': 0.337890625, 'learning_rate': 0.00019620951087805612, 'epoch': 0.04}\n",
      "{'loss': 1.3176, 'grad_norm': 0.4375, 'learning_rate': 0.00019619814850059227, 'epoch': 0.04}\n",
      "{'loss': 1.3849, 'grad_norm': 0.68359375, 'learning_rate': 0.00019618678612312842, 'epoch': 0.04}\n",
      "{'loss': 1.5007, 'grad_norm': 0.365234375, 'learning_rate': 0.00019617542374566455, 'epoch': 0.04}\n",
      "{'loss': 1.348, 'grad_norm': 0.50390625, 'learning_rate': 0.0001961640613682007, 'epoch': 0.04}\n",
      "{'loss': 1.5162, 'grad_norm': 0.314453125, 'learning_rate': 0.00019615269899073682, 'epoch': 0.04}\n",
      "{'loss': 1.3994, 'grad_norm': 0.3984375, 'learning_rate': 0.00019614133661327297, 'epoch': 0.04}\n",
      "{'loss': 1.1821, 'grad_norm': 0.79296875, 'learning_rate': 0.0001961299742358091, 'epoch': 0.04}\n",
      "{'loss': 1.3958, 'grad_norm': 0.392578125, 'learning_rate': 0.00019611861185834525, 'epoch': 0.04}\n",
      "{'loss': 1.4521, 'grad_norm': 0.63671875, 'learning_rate': 0.0001961072494808814, 'epoch': 0.04}\n",
      "{'loss': 1.2646, 'grad_norm': 0.33984375, 'learning_rate': 0.00019609588710341752, 'epoch': 0.04}\n",
      "{'loss': 1.4296, 'grad_norm': 0.4375, 'learning_rate': 0.00019608452472595368, 'epoch': 0.04}\n",
      "{'loss': 1.2148, 'grad_norm': 0.4375, 'learning_rate': 0.0001960731623484898, 'epoch': 0.04}\n",
      "{'loss': 1.5133, 'grad_norm': 0.458984375, 'learning_rate': 0.00019606179997102595, 'epoch': 0.04}\n",
      "{'loss': 1.3569, 'grad_norm': 0.443359375, 'learning_rate': 0.0001960504375935621, 'epoch': 0.04}\n",
      "{'loss': 1.3825, 'grad_norm': 0.3203125, 'learning_rate': 0.00019603907521609823, 'epoch': 0.04}\n",
      "{'loss': 1.3644, 'grad_norm': 0.341796875, 'learning_rate': 0.00019602771283863438, 'epoch': 0.04}\n",
      "{'loss': 1.3089, 'grad_norm': 0.76171875, 'learning_rate': 0.0001960163504611705, 'epoch': 0.04}\n",
      "{'loss': 1.4552, 'grad_norm': 0.37890625, 'learning_rate': 0.00019600498808370665, 'epoch': 0.04}\n",
      "{'loss': 1.3887, 'grad_norm': 0.7109375, 'learning_rate': 0.00019599362570624278, 'epoch': 0.04}\n",
      "{'loss': 1.2394, 'grad_norm': 0.33984375, 'learning_rate': 0.0001959822633287789, 'epoch': 0.04}\n",
      "{'loss': 1.3234, 'grad_norm': 0.451171875, 'learning_rate': 0.00019597090095131508, 'epoch': 0.04}\n",
      "{'loss': 1.2314, 'grad_norm': 0.63671875, 'learning_rate': 0.0001959595385738512, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4851, 'grad_norm': 0.38671875, 'learning_rate': 0.00019594817619638736, 'epoch': 0.04}\n",
      "{'loss': 1.3009, 'grad_norm': 0.3828125, 'learning_rate': 0.00019593681381892348, 'epoch': 0.04}\n",
      "{'loss': 1.3831, 'grad_norm': 0.333984375, 'learning_rate': 0.0001959254514414596, 'epoch': 0.04}\n",
      "{'loss': 1.3156, 'grad_norm': 0.384765625, 'learning_rate': 0.00019591408906399576, 'epoch': 0.04}\n",
      "{'loss': 1.2949, 'grad_norm': 0.51171875, 'learning_rate': 0.00019590272668653188, 'epoch': 0.04}\n",
      "{'loss': 1.4841, 'grad_norm': 0.4140625, 'learning_rate': 0.00019589136430906806, 'epoch': 0.04}\n",
      "{'loss': 1.3711, 'grad_norm': 0.59375, 'learning_rate': 0.00019588000193160418, 'epoch': 0.04}\n",
      "{'loss': 1.3643, 'grad_norm': 0.3046875, 'learning_rate': 0.0001958686395541403, 'epoch': 0.05}\n",
      "{'loss': 1.4005, 'grad_norm': 0.5078125, 'learning_rate': 0.00019585727717667646, 'epoch': 0.05}\n",
      "{'loss': 1.1387, 'grad_norm': 0.87109375, 'learning_rate': 0.00019584591479921258, 'epoch': 0.05}\n",
      "{'loss': 1.5078, 'grad_norm': 0.408203125, 'learning_rate': 0.00019583455242174874, 'epoch': 0.05}\n",
      "{'loss': 1.273, 'grad_norm': 0.53125, 'learning_rate': 0.00019582319004428486, 'epoch': 0.05}\n",
      "{'loss': 1.1962, 'grad_norm': 0.435546875, 'learning_rate': 0.000195811827666821, 'epoch': 0.05}\n",
      "{'loss': 1.5806, 'grad_norm': 0.47265625, 'learning_rate': 0.00019580046528935716, 'epoch': 0.05}\n",
      "{'loss': 1.2502, 'grad_norm': 0.88671875, 'learning_rate': 0.0001957891029118933, 'epoch': 0.05}\n",
      "{'loss': 1.5293, 'grad_norm': 0.482421875, 'learning_rate': 0.00019577774053442944, 'epoch': 0.05}\n",
      "{'loss': 1.2906, 'grad_norm': 0.46484375, 'learning_rate': 0.00019576637815696556, 'epoch': 0.05}\n",
      "{'loss': 1.3013, 'grad_norm': 0.3515625, 'learning_rate': 0.00019575501577950171, 'epoch': 0.05}\n",
      "{'loss': 1.38, 'grad_norm': 0.44140625, 'learning_rate': 0.00019574365340203784, 'epoch': 0.05}\n",
      "{'loss': 1.3004, 'grad_norm': 1.0703125, 'learning_rate': 0.000195732291024574, 'epoch': 0.05}\n",
      "{'loss': 1.4387, 'grad_norm': 0.36328125, 'learning_rate': 0.00019572092864711014, 'epoch': 0.05}\n",
      "{'loss': 1.3189, 'grad_norm': 0.400390625, 'learning_rate': 0.00019570956626964627, 'epoch': 0.05}\n",
      "{'loss': 1.3462, 'grad_norm': 0.54296875, 'learning_rate': 0.00019569820389218242, 'epoch': 0.05}\n",
      "{'loss': 1.4906, 'grad_norm': 0.5078125, 'learning_rate': 0.00019568684151471854, 'epoch': 0.05}\n",
      "{'loss': 1.3078, 'grad_norm': 0.58203125, 'learning_rate': 0.0001956754791372547, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5834, 'grad_norm': 0.384765625, 'learning_rate': 0.00019566411675979084, 'epoch': 0.05}\n",
      "{'loss': 1.2668, 'grad_norm': 0.67578125, 'learning_rate': 0.00019565275438232697, 'epoch': 0.05}\n",
      "{'loss': 1.3686, 'grad_norm': 0.478515625, 'learning_rate': 0.00019564139200486312, 'epoch': 0.05}\n",
      "{'loss': 1.4549, 'grad_norm': 0.427734375, 'learning_rate': 0.00019563002962739924, 'epoch': 0.05}\n",
      "{'loss': 1.3383, 'grad_norm': 0.53515625, 'learning_rate': 0.0001956186672499354, 'epoch': 0.05}\n",
      "{'loss': 1.4703, 'grad_norm': 0.349609375, 'learning_rate': 0.00019560730487247152, 'epoch': 0.05}\n",
      "{'loss': 1.3162, 'grad_norm': 0.46484375, 'learning_rate': 0.00019559594249500764, 'epoch': 0.05}\n",
      "{'loss': 1.3075, 'grad_norm': 0.333984375, 'learning_rate': 0.00019558458011754382, 'epoch': 0.05}\n",
      "{'loss': 1.458, 'grad_norm': 0.404296875, 'learning_rate': 0.00019557321774007995, 'epoch': 0.05}\n",
      "{'loss': 1.2391, 'grad_norm': 0.5234375, 'learning_rate': 0.0001955618553626161, 'epoch': 0.05}\n",
      "{'loss': 1.319, 'grad_norm': 0.54296875, 'learning_rate': 0.00019555049298515222, 'epoch': 0.05}\n",
      "{'loss': 1.2903, 'grad_norm': 1.0078125, 'learning_rate': 0.00019553913060768835, 'epoch': 0.05}\n",
      "{'loss': 1.3235, 'grad_norm': 0.34765625, 'learning_rate': 0.0001955277682302245, 'epoch': 0.05}\n",
      "{'loss': 1.4141, 'grad_norm': 0.375, 'learning_rate': 0.00019551640585276062, 'epoch': 0.05}\n",
      "{'loss': 1.2506, 'grad_norm': 0.76953125, 'learning_rate': 0.0001955050434752968, 'epoch': 0.05}\n",
      "{'loss': 1.5109, 'grad_norm': 0.423828125, 'learning_rate': 0.00019549368109783293, 'epoch': 0.05}\n",
      "{'loss': 1.3517, 'grad_norm': 0.58203125, 'learning_rate': 0.00019548231872036905, 'epoch': 0.05}\n",
      "{'loss': 1.3328, 'grad_norm': 0.34375, 'learning_rate': 0.0001954709563429052, 'epoch': 0.05}\n",
      "{'loss': 1.497, 'grad_norm': 0.54296875, 'learning_rate': 0.00019545959396544133, 'epoch': 0.05}\n",
      "{'loss': 1.2637, 'grad_norm': 0.7265625, 'learning_rate': 0.00019544823158797748, 'epoch': 0.05}\n",
      "{'loss': 1.5537, 'grad_norm': 0.328125, 'learning_rate': 0.0001954368692105136, 'epoch': 0.05}\n",
      "{'loss': 1.2672, 'grad_norm': 0.609375, 'learning_rate': 0.00019542550683304975, 'epoch': 0.05}\n",
      "{'loss': 1.4338, 'grad_norm': 0.337890625, 'learning_rate': 0.0001954141444555859, 'epoch': 0.05}\n",
      "{'loss': 1.3551, 'grad_norm': 0.326171875, 'learning_rate': 0.00019540278207812203, 'epoch': 0.05}\n",
      "{'loss': 1.234, 'grad_norm': 0.64453125, 'learning_rate': 0.00019539141970065818, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4162, 'grad_norm': 0.462890625, 'learning_rate': 0.0001953800573231943, 'epoch': 0.05}\n",
      "{'loss': 1.3904, 'grad_norm': 0.4765625, 'learning_rate': 0.00019536869494573045, 'epoch': 0.05}\n",
      "{'loss': 1.3521, 'grad_norm': 0.33984375, 'learning_rate': 0.0001953573325682666, 'epoch': 0.05}\n",
      "{'loss': 1.4078, 'grad_norm': 0.447265625, 'learning_rate': 0.00019534597019080273, 'epoch': 0.05}\n",
      "{'loss': 1.243, 'grad_norm': 1.0703125, 'learning_rate': 0.00019533460781333888, 'epoch': 0.05}\n",
      "{'loss': 1.4502, 'grad_norm': 0.41015625, 'learning_rate': 0.000195323245435875, 'epoch': 0.05}\n",
      "{'loss': 1.3578, 'grad_norm': 0.55859375, 'learning_rate': 0.00019531188305841116, 'epoch': 0.05}\n",
      "{'loss': 1.3223, 'grad_norm': 0.359375, 'learning_rate': 0.00019530052068094728, 'epoch': 0.05}\n",
      "{'loss': 1.4114, 'grad_norm': 0.50390625, 'learning_rate': 0.00019528915830348343, 'epoch': 0.05}\n",
      "{'loss': 1.1517, 'grad_norm': 0.5546875, 'learning_rate': 0.00019527779592601958, 'epoch': 0.05}\n",
      "{'loss': 1.4807, 'grad_norm': 0.39453125, 'learning_rate': 0.0001952664335485557, 'epoch': 0.05}\n",
      "{'loss': 1.4707, 'grad_norm': 0.58203125, 'learning_rate': 0.00019525507117109186, 'epoch': 0.05}\n",
      "{'loss': 1.367, 'grad_norm': 0.341796875, 'learning_rate': 0.00019524370879362798, 'epoch': 0.05}\n",
      "{'loss': 1.3942, 'grad_norm': 0.396484375, 'learning_rate': 0.00019523234641616414, 'epoch': 0.05}\n",
      "{'loss': 1.246, 'grad_norm': 0.62890625, 'learning_rate': 0.00019522098403870026, 'epoch': 0.05}\n",
      "{'loss': 1.4681, 'grad_norm': 0.478515625, 'learning_rate': 0.00019520962166123638, 'epoch': 0.05}\n",
      "{'loss': 1.4538, 'grad_norm': 0.6015625, 'learning_rate': 0.00019519825928377256, 'epoch': 0.05}\n",
      "{'loss': 1.2886, 'grad_norm': 0.34375, 'learning_rate': 0.0001951868969063087, 'epoch': 0.05}\n",
      "{'loss': 1.4427, 'grad_norm': 0.54296875, 'learning_rate': 0.00019517553452884484, 'epoch': 0.05}\n",
      "{'loss': 1.1841, 'grad_norm': 0.60546875, 'learning_rate': 0.00019516417215138096, 'epoch': 0.05}\n",
      "{'loss': 1.5349, 'grad_norm': 0.37890625, 'learning_rate': 0.0001951528097739171, 'epoch': 0.05}\n",
      "{'loss': 1.2736, 'grad_norm': 0.51171875, 'learning_rate': 0.00019514144739645324, 'epoch': 0.05}\n",
      "{'loss': 1.5056, 'grad_norm': 0.37109375, 'learning_rate': 0.00019513008501898936, 'epoch': 0.05}\n",
      "{'loss': 1.3743, 'grad_norm': 0.51953125, 'learning_rate': 0.00019511872264152554, 'epoch': 0.05}\n",
      "{'loss': 1.1937, 'grad_norm': 0.7421875, 'learning_rate': 0.00019510736026406167, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4645, 'grad_norm': 0.4609375, 'learning_rate': 0.0001950959978865978, 'epoch': 0.05}\n",
      "{'loss': 1.3362, 'grad_norm': 0.60546875, 'learning_rate': 0.00019508463550913394, 'epoch': 0.05}\n",
      "{'loss': 1.1783, 'grad_norm': 0.28125, 'learning_rate': 0.00019507327313167007, 'epoch': 0.05}\n",
      "{'loss': 1.2998, 'grad_norm': 0.451171875, 'learning_rate': 0.00019506191075420622, 'epoch': 0.05}\n",
      "{'loss': 1.2498, 'grad_norm': 0.376953125, 'learning_rate': 0.00019505054837674234, 'epoch': 0.05}\n",
      "{'loss': 1.6009, 'grad_norm': 0.4296875, 'learning_rate': 0.0001950391859992785, 'epoch': 0.05}\n",
      "{'loss': 1.1968, 'grad_norm': 0.59765625, 'learning_rate': 0.00019502782362181464, 'epoch': 0.05}\n",
      "{'loss': 1.3352, 'grad_norm': 0.37109375, 'learning_rate': 0.00019501646124435077, 'epoch': 0.05}\n",
      "{'loss': 1.4253, 'grad_norm': 0.515625, 'learning_rate': 0.00019500509886688692, 'epoch': 0.05}\n",
      "{'loss': 1.2769, 'grad_norm': 0.490234375, 'learning_rate': 0.00019499373648942304, 'epoch': 0.05}\n",
      "{'loss': 1.4335, 'grad_norm': 0.396484375, 'learning_rate': 0.0001949823741119592, 'epoch': 0.05}\n",
      "{'loss': 1.3485, 'grad_norm': 0.63671875, 'learning_rate': 0.00019497101173449535, 'epoch': 0.05}\n",
      "{'loss': 1.3367, 'grad_norm': 0.396484375, 'learning_rate': 0.00019495964935703147, 'epoch': 0.05}\n",
      "{'loss': 1.41, 'grad_norm': 0.4609375, 'learning_rate': 0.00019494828697956762, 'epoch': 0.05}\n",
      "{'loss': 1.2052, 'grad_norm': 0.58984375, 'learning_rate': 0.00019493692460210375, 'epoch': 0.05}\n",
      "{'loss': 1.4992, 'grad_norm': 0.47265625, 'learning_rate': 0.0001949255622246399, 'epoch': 0.05}\n",
      "{'loss': 1.4331, 'grad_norm': 0.625, 'learning_rate': 0.00019491419984717602, 'epoch': 0.05}\n",
      "{'loss': 1.3583, 'grad_norm': 0.3046875, 'learning_rate': 0.00019490283746971217, 'epoch': 0.05}\n",
      "{'loss': 1.4721, 'grad_norm': 0.4609375, 'learning_rate': 0.00019489147509224833, 'epoch': 0.05}\n",
      "{'loss': 1.3534, 'grad_norm': 0.57421875, 'learning_rate': 0.00019488011271478445, 'epoch': 0.05}\n",
      "{'loss': 1.4843, 'grad_norm': 0.4765625, 'learning_rate': 0.0001948687503373206, 'epoch': 0.05}\n",
      "{'loss': 1.2983, 'grad_norm': 0.4453125, 'learning_rate': 0.00019485738795985673, 'epoch': 0.05}\n",
      "{'loss': 1.447, 'grad_norm': 0.34375, 'learning_rate': 0.00019484602558239288, 'epoch': 0.05}\n",
      "{'loss': 1.4635, 'grad_norm': 0.439453125, 'learning_rate': 0.000194834663204929, 'epoch': 0.05}\n",
      "{'loss': 1.2348, 'grad_norm': 0.88671875, 'learning_rate': 0.00019482330082746513, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4227, 'grad_norm': 0.353515625, 'learning_rate': 0.0001948119384500013, 'epoch': 0.05}\n",
      "{'loss': 1.365, 'grad_norm': 0.609375, 'learning_rate': 0.00019480057607253743, 'epoch': 0.05}\n",
      "{'loss': 1.3187, 'grad_norm': 0.365234375, 'learning_rate': 0.00019478921369507358, 'epoch': 0.05}\n",
      "{'loss': 1.4362, 'grad_norm': 0.4609375, 'learning_rate': 0.0001947778513176097, 'epoch': 0.05}\n",
      "{'loss': 1.2711, 'grad_norm': 1.0234375, 'learning_rate': 0.00019476648894014583, 'epoch': 0.05}\n",
      "{'loss': 1.4497, 'grad_norm': 0.318359375, 'learning_rate': 0.00019475512656268198, 'epoch': 0.05}\n",
      "{'loss': 1.2486, 'grad_norm': 0.6875, 'learning_rate': 0.0001947437641852181, 'epoch': 0.05}\n",
      "{'loss': 1.2849, 'grad_norm': 0.3671875, 'learning_rate': 0.00019473240180775428, 'epoch': 0.05}\n",
      "{'loss': 1.3625, 'grad_norm': 0.453125, 'learning_rate': 0.0001947210394302904, 'epoch': 0.05}\n",
      "{'loss': 1.2248, 'grad_norm': 0.52734375, 'learning_rate': 0.00019470967705282653, 'epoch': 0.05}\n",
      "{'loss': 1.4377, 'grad_norm': 0.388671875, 'learning_rate': 0.00019469831467536268, 'epoch': 0.05}\n",
      "{'loss': 1.3501, 'grad_norm': 0.5, 'learning_rate': 0.0001946869522978988, 'epoch': 0.05}\n",
      "{'loss': 1.3164, 'grad_norm': 0.34375, 'learning_rate': 0.00019467558992043496, 'epoch': 0.05}\n",
      "{'loss': 1.3802, 'grad_norm': 0.5, 'learning_rate': 0.0001946642275429711, 'epoch': 0.05}\n",
      "{'loss': 1.1808, 'grad_norm': 0.84375, 'learning_rate': 0.00019465286516550723, 'epoch': 0.05}\n",
      "{'loss': 1.4736, 'grad_norm': 0.34765625, 'learning_rate': 0.00019464150278804339, 'epoch': 0.05}\n",
      "{'loss': 1.3711, 'grad_norm': 0.640625, 'learning_rate': 0.0001946301404105795, 'epoch': 0.05}\n",
      "{'loss': 1.4672, 'grad_norm': 0.380859375, 'learning_rate': 0.00019461877803311566, 'epoch': 0.05}\n",
      "{'loss': 1.4221, 'grad_norm': 0.4140625, 'learning_rate': 0.00019460741565565179, 'epoch': 0.05}\n",
      "{'loss': 1.2241, 'grad_norm': 0.498046875, 'learning_rate': 0.00019459605327818794, 'epoch': 0.05}\n",
      "{'loss': 1.5919, 'grad_norm': 0.44140625, 'learning_rate': 0.0001945846909007241, 'epoch': 0.05}\n",
      "{'loss': 1.3778, 'grad_norm': 0.6953125, 'learning_rate': 0.0001945733285232602, 'epoch': 0.05}\n",
      "{'loss': 1.3478, 'grad_norm': 0.318359375, 'learning_rate': 0.00019456196614579636, 'epoch': 0.05}\n",
      "{'loss': 1.3645, 'grad_norm': 0.451171875, 'learning_rate': 0.0001945506037683325, 'epoch': 0.05}\n",
      "{'loss': 1.2346, 'grad_norm': 0.68359375, 'learning_rate': 0.00019453924139086864, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6033, 'grad_norm': 0.40234375, 'learning_rate': 0.00019452787901340476, 'epoch': 0.05}\n",
      "{'loss': 1.3642, 'grad_norm': 0.53125, 'learning_rate': 0.00019451651663594092, 'epoch': 0.05}\n",
      "{'loss': 1.2797, 'grad_norm': 0.33203125, 'learning_rate': 0.00019450515425847707, 'epoch': 0.05}\n",
      "{'loss': 1.4025, 'grad_norm': 0.478515625, 'learning_rate': 0.0001944937918810132, 'epoch': 0.05}\n",
      "{'loss': 1.3168, 'grad_norm': 0.55859375, 'learning_rate': 0.00019448242950354934, 'epoch': 0.05}\n",
      "{'loss': 1.5567, 'grad_norm': 0.35546875, 'learning_rate': 0.00019447106712608547, 'epoch': 0.05}\n",
      "{'loss': 1.2968, 'grad_norm': 0.5546875, 'learning_rate': 0.00019445970474862162, 'epoch': 0.05}\n",
      "{'loss': 1.3217, 'grad_norm': 0.302734375, 'learning_rate': 0.00019444834237115774, 'epoch': 0.05}\n",
      "{'loss': 1.4308, 'grad_norm': 0.427734375, 'learning_rate': 0.00019443697999369387, 'epoch': 0.05}\n",
      "{'loss': 1.1525, 'grad_norm': 0.7421875, 'learning_rate': 0.00019442561761623005, 'epoch': 0.05}\n",
      "{'loss': 1.406, 'grad_norm': 0.427734375, 'learning_rate': 0.00019441425523876617, 'epoch': 0.05}\n",
      "{'loss': 1.3827, 'grad_norm': 0.59765625, 'learning_rate': 0.00019440289286130232, 'epoch': 0.05}\n",
      "{'loss': 1.3074, 'grad_norm': 0.416015625, 'learning_rate': 0.00019439153048383845, 'epoch': 0.05}\n",
      "{'loss': 1.3583, 'grad_norm': 0.43359375, 'learning_rate': 0.00019438016810637457, 'epoch': 0.05}\n",
      "{'loss': 1.2546, 'grad_norm': 0.9921875, 'learning_rate': 0.00019436880572891072, 'epoch': 0.05}\n",
      "{'loss': 1.6029, 'grad_norm': 0.40234375, 'learning_rate': 0.00019435744335144685, 'epoch': 0.05}\n",
      "{'loss': 1.4375, 'grad_norm': 0.71484375, 'learning_rate': 0.00019434608097398302, 'epoch': 0.05}\n",
      "{'loss': 1.3113, 'grad_norm': 0.384765625, 'learning_rate': 0.00019433471859651915, 'epoch': 0.05}\n",
      "{'loss': 1.3758, 'grad_norm': 0.51171875, 'learning_rate': 0.00019432335621905527, 'epoch': 0.05}\n",
      "{'loss': 1.3487, 'grad_norm': 0.6796875, 'learning_rate': 0.00019431199384159142, 'epoch': 0.05}\n",
      "{'loss': 1.4311, 'grad_norm': 0.4609375, 'learning_rate': 0.00019430063146412755, 'epoch': 0.05}\n",
      "{'loss': 1.3652, 'grad_norm': 0.578125, 'learning_rate': 0.0001942892690866637, 'epoch': 0.05}\n",
      "{'loss': 1.4414, 'grad_norm': 0.328125, 'learning_rate': 0.00019427790670919985, 'epoch': 0.05}\n",
      "{'loss': 1.4583, 'grad_norm': 0.4140625, 'learning_rate': 0.00019426654433173598, 'epoch': 0.05}\n",
      "{'loss': 1.1961, 'grad_norm': 0.5078125, 'learning_rate': 0.00019425518195427213, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4418, 'grad_norm': 0.35546875, 'learning_rate': 0.00019424381957680825, 'epoch': 0.05}\n",
      "{'loss': 1.2316, 'grad_norm': 0.6484375, 'learning_rate': 0.0001942324571993444, 'epoch': 0.05}\n",
      "{'loss': 1.4433, 'grad_norm': 0.31640625, 'learning_rate': 0.00019422109482188053, 'epoch': 0.05}\n",
      "{'loss': 1.4267, 'grad_norm': 0.4140625, 'learning_rate': 0.00019420973244441668, 'epoch': 0.05}\n",
      "{'loss': 1.186, 'grad_norm': 0.63671875, 'learning_rate': 0.00019419837006695283, 'epoch': 0.05}\n",
      "{'loss': 1.5375, 'grad_norm': 0.50390625, 'learning_rate': 0.00019418700768948895, 'epoch': 0.05}\n",
      "{'loss': 1.2646, 'grad_norm': 0.69140625, 'learning_rate': 0.0001941756453120251, 'epoch': 0.05}\n",
      "{'loss': 1.3581, 'grad_norm': 0.30859375, 'learning_rate': 0.00019416428293456123, 'epoch': 0.05}\n",
      "{'loss': 1.4178, 'grad_norm': 0.474609375, 'learning_rate': 0.00019415292055709738, 'epoch': 0.05}\n",
      "{'loss': 1.1761, 'grad_norm': 0.7734375, 'learning_rate': 0.0001941415581796335, 'epoch': 0.05}\n",
      "{'loss': 1.5123, 'grad_norm': 0.455078125, 'learning_rate': 0.00019413019580216966, 'epoch': 0.05}\n",
      "{'loss': 1.2777, 'grad_norm': 0.5703125, 'learning_rate': 0.0001941188334247058, 'epoch': 0.05}\n",
      "{'loss': 1.2079, 'grad_norm': 0.392578125, 'learning_rate': 0.00019410747104724193, 'epoch': 0.05}\n",
      "{'loss': 1.4249, 'grad_norm': 0.466796875, 'learning_rate': 0.00019409610866977808, 'epoch': 0.05}\n",
      "{'loss': 1.2267, 'grad_norm': 0.921875, 'learning_rate': 0.0001940847462923142, 'epoch': 0.05}\n",
      "{'loss': 1.5287, 'grad_norm': 0.384765625, 'learning_rate': 0.00019407338391485036, 'epoch': 0.05}\n",
      "{'loss': 1.3328, 'grad_norm': 0.59375, 'learning_rate': 0.00019406202153738648, 'epoch': 0.05}\n",
      "{'loss': 1.2991, 'grad_norm': 0.333984375, 'learning_rate': 0.0001940506591599226, 'epoch': 0.05}\n",
      "{'loss': 1.3885, 'grad_norm': 0.435546875, 'learning_rate': 0.0001940392967824588, 'epoch': 0.05}\n",
      "{'loss': 1.1095, 'grad_norm': 0.302734375, 'learning_rate': 0.0001940279344049949, 'epoch': 0.05}\n",
      "{'loss': 1.5404, 'grad_norm': 0.396484375, 'learning_rate': 0.00019401657202753106, 'epoch': 0.05}\n",
      "{'loss': 1.348, 'grad_norm': 0.421875, 'learning_rate': 0.0001940052096500672, 'epoch': 0.05}\n",
      "{'loss': 1.304, 'grad_norm': 0.326171875, 'learning_rate': 0.0001939938472726033, 'epoch': 0.05}\n",
      "{'loss': 1.36, 'grad_norm': 0.435546875, 'learning_rate': 0.00019398248489513946, 'epoch': 0.05}\n",
      "{'loss': 1.2778, 'grad_norm': 0.5859375, 'learning_rate': 0.00019397112251767561, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4363, 'grad_norm': 0.3359375, 'learning_rate': 0.00019395976014021177, 'epoch': 0.05}\n",
      "{'loss': 1.2358, 'grad_norm': 0.609375, 'learning_rate': 0.0001939483977627479, 'epoch': 0.05}\n",
      "{'loss': 1.3288, 'grad_norm': 0.33984375, 'learning_rate': 0.00019393703538528401, 'epoch': 0.05}\n",
      "{'loss': 1.4801, 'grad_norm': 0.455078125, 'learning_rate': 0.00019392567300782017, 'epoch': 0.05}\n",
      "{'loss': 1.1363, 'grad_norm': 0.94921875, 'learning_rate': 0.0001939143106303563, 'epoch': 0.05}\n",
      "{'loss': 1.531, 'grad_norm': 0.37109375, 'learning_rate': 0.00019390294825289244, 'epoch': 0.05}\n",
      "{'loss': 1.2766, 'grad_norm': 0.412109375, 'learning_rate': 0.0001938915858754286, 'epoch': 0.05}\n",
      "{'loss': 1.3649, 'grad_norm': 0.328125, 'learning_rate': 0.00019388022349796472, 'epoch': 0.05}\n",
      "{'loss': 1.5306, 'grad_norm': 0.462890625, 'learning_rate': 0.00019386886112050087, 'epoch': 0.05}\n",
      "{'loss': 1.2956, 'grad_norm': 0.72265625, 'learning_rate': 0.000193857498743037, 'epoch': 0.05}\n",
      "{'loss': 1.4934, 'grad_norm': 0.453125, 'learning_rate': 0.00019384613636557314, 'epoch': 0.05}\n",
      "{'loss': 1.2939, 'grad_norm': 0.54296875, 'learning_rate': 0.00019383477398810927, 'epoch': 0.05}\n",
      "{'loss': 1.1987, 'grad_norm': 0.27734375, 'learning_rate': 0.00019382341161064542, 'epoch': 0.05}\n",
      "{'loss': 1.3904, 'grad_norm': 0.470703125, 'learning_rate': 0.00019381204923318157, 'epoch': 0.05}\n",
      "{'loss': 1.314, 'grad_norm': 0.33984375, 'learning_rate': 0.0001938006868557177, 'epoch': 0.05}\n",
      "{'loss': 1.458, 'grad_norm': 0.427734375, 'learning_rate': 0.00019378932447825385, 'epoch': 0.05}\n",
      "{'loss': 1.3466, 'grad_norm': 0.4609375, 'learning_rate': 0.00019377796210078997, 'epoch': 0.05}\n",
      "{'loss': 1.3856, 'grad_norm': 0.365234375, 'learning_rate': 0.00019376659972332612, 'epoch': 0.05}\n",
      "{'loss': 1.4903, 'grad_norm': 0.466796875, 'learning_rate': 0.00019375523734586225, 'epoch': 0.05}\n",
      "{'loss': 1.1703, 'grad_norm': 0.640625, 'learning_rate': 0.0001937438749683984, 'epoch': 0.05}\n",
      "{'loss': 1.446, 'grad_norm': 0.44921875, 'learning_rate': 0.00019373251259093455, 'epoch': 0.05}\n",
      "{'loss': 1.3708, 'grad_norm': 0.609375, 'learning_rate': 0.00019372115021347067, 'epoch': 0.05}\n",
      "{'loss': 1.305, 'grad_norm': 0.3671875, 'learning_rate': 0.00019370978783600683, 'epoch': 0.05}\n",
      "{'loss': 1.3511, 'grad_norm': 0.5078125, 'learning_rate': 0.00019369842545854295, 'epoch': 0.05}\n",
      "{'loss': 1.3782, 'grad_norm': 0.640625, 'learning_rate': 0.0001936870630810791, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4357, 'grad_norm': 0.51171875, 'learning_rate': 0.00019367570070361523, 'epoch': 0.05}\n",
      "{'loss': 1.2642, 'grad_norm': 0.69140625, 'learning_rate': 0.00019366433832615135, 'epoch': 0.05}\n",
      "{'loss': 1.2346, 'grad_norm': 0.345703125, 'learning_rate': 0.00019365297594868753, 'epoch': 0.05}\n",
      "{'loss': 1.458, 'grad_norm': 0.447265625, 'learning_rate': 0.00019364161357122365, 'epoch': 0.05}\n",
      "{'loss': 1.3129, 'grad_norm': 0.89453125, 'learning_rate': 0.0001936302511937598, 'epoch': 0.05}\n",
      "{'loss': 1.5765, 'grad_norm': 0.408203125, 'learning_rate': 0.00019361888881629593, 'epoch': 0.05}\n",
      "{'loss': 1.3264, 'grad_norm': 0.58984375, 'learning_rate': 0.00019360752643883205, 'epoch': 0.05}\n",
      "{'loss': 1.4462, 'grad_norm': 0.455078125, 'learning_rate': 0.0001935961640613682, 'epoch': 0.05}\n",
      "{'loss': 1.3879, 'grad_norm': 0.44140625, 'learning_rate': 0.00019358480168390436, 'epoch': 0.06}\n",
      "{'loss': 1.2201, 'grad_norm': 0.76953125, 'learning_rate': 0.0001935734393064405, 'epoch': 0.06}\n",
      "{'loss': 1.584, 'grad_norm': 0.41796875, 'learning_rate': 0.00019356207692897663, 'epoch': 0.06}\n",
      "{'loss': 1.1963, 'grad_norm': 0.58203125, 'learning_rate': 0.00019355071455151276, 'epoch': 0.06}\n",
      "{'loss': 1.3137, 'grad_norm': 0.361328125, 'learning_rate': 0.0001935393521740489, 'epoch': 0.06}\n",
      "{'loss': 1.4067, 'grad_norm': 0.45703125, 'learning_rate': 0.00019352798979658503, 'epoch': 0.06}\n",
      "{'loss': 1.0968, 'grad_norm': 0.5078125, 'learning_rate': 0.00019351662741912118, 'epoch': 0.06}\n",
      "{'loss': 1.3902, 'grad_norm': 0.365234375, 'learning_rate': 0.00019350526504165733, 'epoch': 0.06}\n",
      "{'loss': 1.3851, 'grad_norm': 0.5078125, 'learning_rate': 0.00019349390266419346, 'epoch': 0.06}\n",
      "{'loss': 1.5156, 'grad_norm': 0.296875, 'learning_rate': 0.0001934825402867296, 'epoch': 0.06}\n",
      "{'loss': 1.3776, 'grad_norm': 0.458984375, 'learning_rate': 0.00019347117790926573, 'epoch': 0.06}\n",
      "{'loss': 1.1297, 'grad_norm': 0.51171875, 'learning_rate': 0.00019345981553180189, 'epoch': 0.06}\n",
      "{'loss': 1.4573, 'grad_norm': 0.33203125, 'learning_rate': 0.000193448453154338, 'epoch': 0.06}\n",
      "{'loss': 1.463, 'grad_norm': 0.625, 'learning_rate': 0.00019343709077687416, 'epoch': 0.06}\n",
      "{'loss': 1.4269, 'grad_norm': 0.3203125, 'learning_rate': 0.0001934257283994103, 'epoch': 0.06}\n",
      "{'loss': 1.5025, 'grad_norm': 0.4296875, 'learning_rate': 0.00019341436602194644, 'epoch': 0.06}\n",
      "{'loss': 1.1939, 'grad_norm': 0.93359375, 'learning_rate': 0.0001934030036444826, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6069, 'grad_norm': 0.361328125, 'learning_rate': 0.0001933916412670187, 'epoch': 0.06}\n",
      "{'loss': 1.3709, 'grad_norm': 0.7421875, 'learning_rate': 0.00019338027888955486, 'epoch': 0.06}\n",
      "{'loss': 1.4052, 'grad_norm': 0.2890625, 'learning_rate': 0.000193368916512091, 'epoch': 0.06}\n",
      "{'loss': 1.4592, 'grad_norm': 0.455078125, 'learning_rate': 0.00019335755413462714, 'epoch': 0.06}\n",
      "{'loss': 1.2163, 'grad_norm': 0.55078125, 'learning_rate': 0.0001933461917571633, 'epoch': 0.06}\n",
      "{'loss': 1.3872, 'grad_norm': 0.421875, 'learning_rate': 0.00019333482937969942, 'epoch': 0.06}\n",
      "{'loss': 1.3615, 'grad_norm': 0.5078125, 'learning_rate': 0.00019332346700223557, 'epoch': 0.06}\n",
      "{'loss': 1.3316, 'grad_norm': 0.369140625, 'learning_rate': 0.0001933121046247717, 'epoch': 0.06}\n",
      "{'loss': 1.3017, 'grad_norm': 0.51171875, 'learning_rate': 0.00019330074224730784, 'epoch': 0.06}\n",
      "{'loss': 1.18, 'grad_norm': 0.66015625, 'learning_rate': 0.00019328937986984397, 'epoch': 0.06}\n",
      "{'loss': 1.4439, 'grad_norm': 0.4140625, 'learning_rate': 0.00019327801749238012, 'epoch': 0.06}\n",
      "{'loss': 1.4303, 'grad_norm': 0.58984375, 'learning_rate': 0.00019326665511491627, 'epoch': 0.06}\n",
      "{'loss': 1.2978, 'grad_norm': 0.341796875, 'learning_rate': 0.0001932552927374524, 'epoch': 0.06}\n",
      "{'loss': 1.4137, 'grad_norm': 0.515625, 'learning_rate': 0.00019324393035998855, 'epoch': 0.06}\n",
      "{'loss': 1.0738, 'grad_norm': 0.625, 'learning_rate': 0.00019323256798252467, 'epoch': 0.06}\n",
      "{'loss': 1.5354, 'grad_norm': 0.400390625, 'learning_rate': 0.0001932212056050608, 'epoch': 0.06}\n",
      "{'loss': 1.2603, 'grad_norm': 0.59765625, 'learning_rate': 0.00019320984322759695, 'epoch': 0.06}\n",
      "{'loss': 1.3012, 'grad_norm': 0.376953125, 'learning_rate': 0.0001931984808501331, 'epoch': 0.06}\n",
      "{'loss': 1.4284, 'grad_norm': 0.67578125, 'learning_rate': 0.00019318711847266925, 'epoch': 0.06}\n",
      "{'loss': 1.2677, 'grad_norm': 0.86328125, 'learning_rate': 0.00019317575609520537, 'epoch': 0.06}\n",
      "{'loss': 1.4886, 'grad_norm': 0.3984375, 'learning_rate': 0.0001931643937177415, 'epoch': 0.06}\n",
      "{'loss': 1.335, 'grad_norm': 0.49609375, 'learning_rate': 0.00019315303134027765, 'epoch': 0.06}\n",
      "{'loss': 1.2202, 'grad_norm': 0.455078125, 'learning_rate': 0.00019314166896281377, 'epoch': 0.06}\n",
      "{'loss': 1.3393, 'grad_norm': 0.470703125, 'learning_rate': 0.00019313030658534992, 'epoch': 0.06}\n",
      "{'loss': 1.2257, 'grad_norm': 0.7109375, 'learning_rate': 0.00019311894420788608, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3999, 'grad_norm': 0.41796875, 'learning_rate': 0.0001931075818304222, 'epoch': 0.06}\n",
      "{'loss': 1.3498, 'grad_norm': 0.4453125, 'learning_rate': 0.00019309621945295835, 'epoch': 0.06}\n",
      "{'loss': 1.3394, 'grad_norm': 0.353515625, 'learning_rate': 0.00019308485707549448, 'epoch': 0.06}\n",
      "{'loss': 1.5841, 'grad_norm': 0.5078125, 'learning_rate': 0.00019307349469803063, 'epoch': 0.06}\n",
      "{'loss': 1.1924, 'grad_norm': 0.412109375, 'learning_rate': 0.00019306213232056675, 'epoch': 0.06}\n",
      "{'loss': 1.4191, 'grad_norm': 0.380859375, 'learning_rate': 0.0001930507699431029, 'epoch': 0.06}\n",
      "{'loss': 1.3756, 'grad_norm': 0.56640625, 'learning_rate': 0.00019303940756563905, 'epoch': 0.06}\n",
      "{'loss': 1.3663, 'grad_norm': 0.43359375, 'learning_rate': 0.00019302804518817518, 'epoch': 0.06}\n",
      "{'loss': 1.3481, 'grad_norm': 0.46875, 'learning_rate': 0.00019301668281071133, 'epoch': 0.06}\n",
      "{'loss': 1.2458, 'grad_norm': 0.83203125, 'learning_rate': 0.00019300532043324745, 'epoch': 0.06}\n",
      "{'loss': 1.4319, 'grad_norm': 0.384765625, 'learning_rate': 0.0001929939580557836, 'epoch': 0.06}\n",
      "{'loss': 1.3035, 'grad_norm': 0.55078125, 'learning_rate': 0.00019298259567831973, 'epoch': 0.06}\n",
      "{'loss': 1.339, 'grad_norm': 0.3046875, 'learning_rate': 0.00019297123330085588, 'epoch': 0.06}\n",
      "{'loss': 1.3497, 'grad_norm': 0.56640625, 'learning_rate': 0.00019295987092339203, 'epoch': 0.06}\n",
      "{'loss': 1.2755, 'grad_norm': 0.58984375, 'learning_rate': 0.00019294850854592816, 'epoch': 0.06}\n",
      "{'loss': 1.4998, 'grad_norm': 0.39453125, 'learning_rate': 0.0001929371461684643, 'epoch': 0.06}\n",
      "{'loss': 1.3227, 'grad_norm': 0.384765625, 'learning_rate': 0.00019292578379100043, 'epoch': 0.06}\n",
      "{'loss': 1.3655, 'grad_norm': 0.3203125, 'learning_rate': 0.00019291442141353658, 'epoch': 0.06}\n",
      "{'loss': 1.3144, 'grad_norm': 0.5859375, 'learning_rate': 0.0001929030590360727, 'epoch': 0.06}\n",
      "{'loss': 1.1845, 'grad_norm': 0.953125, 'learning_rate': 0.00019289169665860886, 'epoch': 0.06}\n",
      "{'loss': 1.3588, 'grad_norm': 0.37109375, 'learning_rate': 0.000192880334281145, 'epoch': 0.06}\n",
      "{'loss': 1.3372, 'grad_norm': 0.8671875, 'learning_rate': 0.00019286897190368114, 'epoch': 0.06}\n",
      "{'loss': 1.2953, 'grad_norm': 0.361328125, 'learning_rate': 0.0001928576095262173, 'epoch': 0.06}\n",
      "{'loss': 1.4134, 'grad_norm': 0.46484375, 'learning_rate': 0.0001928462471487534, 'epoch': 0.06}\n",
      "{'loss': 1.2542, 'grad_norm': 0.703125, 'learning_rate': 0.00019283488477128954, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4715, 'grad_norm': 0.380859375, 'learning_rate': 0.0001928235223938257, 'epoch': 0.06}\n",
      "{'loss': 1.3997, 'grad_norm': 0.59375, 'learning_rate': 0.00019281216001636184, 'epoch': 0.06}\n",
      "{'loss': 1.3282, 'grad_norm': 0.333984375, 'learning_rate': 0.000192800797638898, 'epoch': 0.06}\n",
      "{'loss': 1.4961, 'grad_norm': 0.453125, 'learning_rate': 0.00019278943526143411, 'epoch': 0.06}\n",
      "{'loss': 1.1619, 'grad_norm': 0.8828125, 'learning_rate': 0.00019277807288397024, 'epoch': 0.06}\n",
      "{'loss': 1.5268, 'grad_norm': 0.4140625, 'learning_rate': 0.0001927667105065064, 'epoch': 0.06}\n",
      "{'loss': 1.3452, 'grad_norm': 0.51953125, 'learning_rate': 0.00019275534812904251, 'epoch': 0.06}\n",
      "{'loss': 1.3709, 'grad_norm': 0.3359375, 'learning_rate': 0.00019274398575157867, 'epoch': 0.06}\n",
      "{'loss': 1.3796, 'grad_norm': 0.427734375, 'learning_rate': 0.00019273262337411482, 'epoch': 0.06}\n",
      "{'loss': 1.2868, 'grad_norm': 1.0078125, 'learning_rate': 0.00019272126099665094, 'epoch': 0.06}\n",
      "{'loss': 1.4498, 'grad_norm': 0.5546875, 'learning_rate': 0.0001927098986191871, 'epoch': 0.06}\n",
      "{'loss': 1.3054, 'grad_norm': 0.5390625, 'learning_rate': 0.00019269853624172322, 'epoch': 0.06}\n",
      "{'loss': 1.347, 'grad_norm': 0.396484375, 'learning_rate': 0.00019268717386425937, 'epoch': 0.06}\n",
      "{'loss': 1.3524, 'grad_norm': 0.46484375, 'learning_rate': 0.0001926758114867955, 'epoch': 0.06}\n",
      "{'loss': 1.2612, 'grad_norm': 0.703125, 'learning_rate': 0.00019266444910933164, 'epoch': 0.06}\n",
      "{'loss': 1.4779, 'grad_norm': 0.435546875, 'learning_rate': 0.0001926530867318678, 'epoch': 0.06}\n",
      "{'loss': 1.2311, 'grad_norm': 0.61328125, 'learning_rate': 0.00019264172435440392, 'epoch': 0.06}\n",
      "{'loss': 1.2673, 'grad_norm': 0.3125, 'learning_rate': 0.00019263036197694007, 'epoch': 0.06}\n",
      "{'loss': 1.3493, 'grad_norm': 0.4140625, 'learning_rate': 0.0001926189995994762, 'epoch': 0.06}\n",
      "{'loss': 1.1893, 'grad_norm': 1.0859375, 'learning_rate': 0.00019260763722201235, 'epoch': 0.06}\n",
      "{'loss': 1.5165, 'grad_norm': 0.376953125, 'learning_rate': 0.00019259627484454847, 'epoch': 0.06}\n",
      "{'loss': 1.2673, 'grad_norm': 0.51953125, 'learning_rate': 0.00019258491246708462, 'epoch': 0.06}\n",
      "{'loss': 1.4228, 'grad_norm': 0.359375, 'learning_rate': 0.00019257355008962077, 'epoch': 0.06}\n",
      "{'loss': 1.3553, 'grad_norm': 0.390625, 'learning_rate': 0.0001925621877121569, 'epoch': 0.06}\n",
      "{'loss': 1.1849, 'grad_norm': 0.2412109375, 'learning_rate': 0.00019255082533469305, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4842, 'grad_norm': 0.451171875, 'learning_rate': 0.00019253946295722917, 'epoch': 0.06}\n",
      "{'loss': 1.2018, 'grad_norm': 0.55078125, 'learning_rate': 0.00019252810057976533, 'epoch': 0.06}\n",
      "{'loss': 1.4215, 'grad_norm': 0.41015625, 'learning_rate': 0.00019251673820230145, 'epoch': 0.06}\n",
      "{'loss': 1.3445, 'grad_norm': 0.53125, 'learning_rate': 0.0001925053758248376, 'epoch': 0.06}\n",
      "{'loss': 1.1354, 'grad_norm': 0.91015625, 'learning_rate': 0.00019249401344737375, 'epoch': 0.06}\n",
      "{'loss': 1.47, 'grad_norm': 0.375, 'learning_rate': 0.00019248265106990988, 'epoch': 0.06}\n",
      "{'loss': 1.3811, 'grad_norm': 0.5546875, 'learning_rate': 0.00019247128869244603, 'epoch': 0.06}\n",
      "{'loss': 1.3217, 'grad_norm': 0.34375, 'learning_rate': 0.00019245992631498215, 'epoch': 0.06}\n",
      "{'loss': 1.3041, 'grad_norm': 0.390625, 'learning_rate': 0.00019244856393751828, 'epoch': 0.06}\n",
      "{'loss': 1.2214, 'grad_norm': 0.64453125, 'learning_rate': 0.00019243720156005443, 'epoch': 0.06}\n",
      "{'loss': 1.5567, 'grad_norm': 0.44921875, 'learning_rate': 0.00019242583918259058, 'epoch': 0.06}\n",
      "{'loss': 1.2086, 'grad_norm': 0.75, 'learning_rate': 0.00019241447680512673, 'epoch': 0.06}\n",
      "{'loss': 1.1978, 'grad_norm': 0.443359375, 'learning_rate': 0.00019240311442766286, 'epoch': 0.06}\n",
      "{'loss': 1.247, 'grad_norm': 0.54296875, 'learning_rate': 0.00019239175205019898, 'epoch': 0.06}\n",
      "{'loss': 1.1837, 'grad_norm': 0.458984375, 'learning_rate': 0.00019238038967273513, 'epoch': 0.06}\n",
      "{'loss': 1.3909, 'grad_norm': 0.412109375, 'learning_rate': 0.00019236902729527126, 'epoch': 0.06}\n",
      "{'loss': 1.3274, 'grad_norm': 0.5625, 'learning_rate': 0.0001923576649178074, 'epoch': 0.06}\n",
      "{'loss': 1.5201, 'grad_norm': 0.3984375, 'learning_rate': 0.00019234630254034356, 'epoch': 0.06}\n",
      "{'loss': 1.3708, 'grad_norm': 0.6328125, 'learning_rate': 0.00019233494016287968, 'epoch': 0.06}\n",
      "{'loss': 1.2441, 'grad_norm': 0.9296875, 'learning_rate': 0.00019232357778541583, 'epoch': 0.06}\n",
      "{'loss': 1.4624, 'grad_norm': 0.41015625, 'learning_rate': 0.00019231221540795196, 'epoch': 0.06}\n",
      "{'loss': 1.3275, 'grad_norm': 0.65625, 'learning_rate': 0.0001923008530304881, 'epoch': 0.06}\n",
      "{'loss': 1.34, 'grad_norm': 0.38671875, 'learning_rate': 0.00019228949065302423, 'epoch': 0.06}\n",
      "{'loss': 1.2956, 'grad_norm': 0.474609375, 'learning_rate': 0.00019227812827556039, 'epoch': 0.06}\n",
      "{'loss': 1.2984, 'grad_norm': 0.88671875, 'learning_rate': 0.00019226676589809654, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4847, 'grad_norm': 0.361328125, 'learning_rate': 0.00019225540352063266, 'epoch': 0.06}\n",
      "{'loss': 1.3311, 'grad_norm': 0.6171875, 'learning_rate': 0.0001922440411431688, 'epoch': 0.06}\n",
      "{'loss': 1.3761, 'grad_norm': 0.353515625, 'learning_rate': 0.00019223267876570494, 'epoch': 0.06}\n",
      "{'loss': 1.4157, 'grad_norm': 0.498046875, 'learning_rate': 0.0001922213163882411, 'epoch': 0.06}\n",
      "{'loss': 1.178, 'grad_norm': 0.484375, 'learning_rate': 0.0001922099540107772, 'epoch': 0.06}\n",
      "{'loss': 1.3822, 'grad_norm': 0.4453125, 'learning_rate': 0.00019219859163331336, 'epoch': 0.06}\n",
      "{'loss': 1.2657, 'grad_norm': 0.515625, 'learning_rate': 0.00019218722925584951, 'epoch': 0.06}\n",
      "{'loss': 1.2588, 'grad_norm': 0.388671875, 'learning_rate': 0.00019217586687838564, 'epoch': 0.06}\n",
      "{'loss': 1.304, 'grad_norm': 0.453125, 'learning_rate': 0.0001921645045009218, 'epoch': 0.06}\n",
      "{'loss': 1.2055, 'grad_norm': 0.4765625, 'learning_rate': 0.00019215314212345791, 'epoch': 0.06}\n",
      "{'loss': 1.4694, 'grad_norm': 0.396484375, 'learning_rate': 0.00019214177974599407, 'epoch': 0.06}\n",
      "{'loss': 1.2402, 'grad_norm': 0.7734375, 'learning_rate': 0.0001921304173685302, 'epoch': 0.06}\n",
      "{'loss': 1.3623, 'grad_norm': 0.37109375, 'learning_rate': 0.00019211905499106634, 'epoch': 0.06}\n",
      "{'loss': 1.393, 'grad_norm': 0.4765625, 'learning_rate': 0.0001921076926136025, 'epoch': 0.06}\n",
      "{'loss': 1.1541, 'grad_norm': 0.79296875, 'learning_rate': 0.00019209633023613862, 'epoch': 0.06}\n",
      "{'loss': 1.5115, 'grad_norm': 0.41015625, 'learning_rate': 0.00019208496785867477, 'epoch': 0.06}\n",
      "{'loss': 1.2794, 'grad_norm': 0.546875, 'learning_rate': 0.0001920736054812109, 'epoch': 0.06}\n",
      "{'loss': 1.3014, 'grad_norm': 0.408203125, 'learning_rate': 0.00019206224310374702, 'epoch': 0.06}\n",
      "{'loss': 1.3494, 'grad_norm': 0.859375, 'learning_rate': 0.00019205088072628317, 'epoch': 0.06}\n",
      "{'loss': 1.1408, 'grad_norm': 0.86328125, 'learning_rate': 0.00019203951834881932, 'epoch': 0.06}\n",
      "{'loss': 1.4337, 'grad_norm': 0.388671875, 'learning_rate': 0.00019202815597135547, 'epoch': 0.06}\n",
      "{'loss': 1.2367, 'grad_norm': 0.69140625, 'learning_rate': 0.0001920167935938916, 'epoch': 0.06}\n",
      "{'loss': 1.3498, 'grad_norm': 0.369140625, 'learning_rate': 0.00019200543121642772, 'epoch': 0.06}\n",
      "{'loss': 1.2907, 'grad_norm': 0.41015625, 'learning_rate': 0.00019199406883896387, 'epoch': 0.06}\n",
      "{'loss': 1.2483, 'grad_norm': 0.5546875, 'learning_rate': 0.0001919827064615, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5585, 'grad_norm': 0.421875, 'learning_rate': 0.00019197134408403615, 'epoch': 0.06}\n",
      "{'loss': 1.3391, 'grad_norm': 0.6484375, 'learning_rate': 0.0001919599817065723, 'epoch': 0.06}\n",
      "{'loss': 1.3822, 'grad_norm': 0.359375, 'learning_rate': 0.00019194861932910842, 'epoch': 0.06}\n",
      "{'loss': 1.427, 'grad_norm': 0.53515625, 'learning_rate': 0.00019193725695164457, 'epoch': 0.06}\n",
      "{'loss': 1.1129, 'grad_norm': 0.5390625, 'learning_rate': 0.0001919258945741807, 'epoch': 0.06}\n",
      "{'loss': 1.5694, 'grad_norm': 0.38671875, 'learning_rate': 0.00019191453219671685, 'epoch': 0.06}\n",
      "{'loss': 1.341, 'grad_norm': 0.53515625, 'learning_rate': 0.00019190316981925297, 'epoch': 0.06}\n",
      "{'loss': 1.2459, 'grad_norm': 0.32421875, 'learning_rate': 0.00019189180744178913, 'epoch': 0.06}\n",
      "{'loss': 1.2911, 'grad_norm': 0.61328125, 'learning_rate': 0.00019188044506432528, 'epoch': 0.06}\n",
      "{'loss': 1.1896, 'grad_norm': 0.8046875, 'learning_rate': 0.0001918690826868614, 'epoch': 0.06}\n",
      "{'loss': 1.4103, 'grad_norm': 0.3828125, 'learning_rate': 0.00019185772030939755, 'epoch': 0.06}\n",
      "{'loss': 1.2442, 'grad_norm': 0.6875, 'learning_rate': 0.00019184635793193368, 'epoch': 0.06}\n",
      "{'loss': 1.2819, 'grad_norm': 0.36328125, 'learning_rate': 0.00019183499555446983, 'epoch': 0.06}\n",
      "{'loss': 1.4213, 'grad_norm': 0.6171875, 'learning_rate': 0.00019182363317700595, 'epoch': 0.06}\n",
      "{'loss': 1.2318, 'grad_norm': 0.78125, 'learning_rate': 0.0001918122707995421, 'epoch': 0.06}\n",
      "{'loss': 1.401, 'grad_norm': 0.43359375, 'learning_rate': 0.00019180090842207826, 'epoch': 0.06}\n",
      "{'loss': 1.1953, 'grad_norm': 0.73828125, 'learning_rate': 0.00019178954604461438, 'epoch': 0.06}\n",
      "{'loss': 1.3922, 'grad_norm': 0.39453125, 'learning_rate': 0.00019177818366715053, 'epoch': 0.06}\n",
      "{'loss': 1.4082, 'grad_norm': 0.51953125, 'learning_rate': 0.00019176682128968666, 'epoch': 0.06}\n",
      "{'loss': 1.2277, 'grad_norm': 0.53515625, 'learning_rate': 0.0001917554589122228, 'epoch': 0.06}\n",
      "{'loss': 1.4633, 'grad_norm': 0.365234375, 'learning_rate': 0.00019174409653475893, 'epoch': 0.06}\n",
      "{'loss': 1.2243, 'grad_norm': 0.578125, 'learning_rate': 0.00019173273415729508, 'epoch': 0.06}\n",
      "{'loss': 1.2919, 'grad_norm': 0.400390625, 'learning_rate': 0.00019172137177983123, 'epoch': 0.06}\n",
      "{'loss': 1.3264, 'grad_norm': 0.482421875, 'learning_rate': 0.00019171000940236736, 'epoch': 0.06}\n",
      "{'loss': 1.2193, 'grad_norm': 0.9921875, 'learning_rate': 0.0001916986470249035, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4129, 'grad_norm': 0.33984375, 'learning_rate': 0.00019168728464743963, 'epoch': 0.06}\n",
      "{'loss': 1.3644, 'grad_norm': 0.59375, 'learning_rate': 0.00019167592226997576, 'epoch': 0.06}\n",
      "{'loss': 1.3783, 'grad_norm': 0.380859375, 'learning_rate': 0.0001916645598925119, 'epoch': 0.06}\n",
      "{'loss': 1.3848, 'grad_norm': 0.5078125, 'learning_rate': 0.00019165319751504806, 'epoch': 0.06}\n",
      "{'loss': 1.1834, 'grad_norm': 0.396484375, 'learning_rate': 0.0001916418351375842, 'epoch': 0.06}\n",
      "{'loss': 1.4153, 'grad_norm': 0.375, 'learning_rate': 0.00019163047276012034, 'epoch': 0.06}\n",
      "{'loss': 1.3035, 'grad_norm': 0.470703125, 'learning_rate': 0.00019161911038265646, 'epoch': 0.06}\n",
      "{'loss': 1.3439, 'grad_norm': 0.33203125, 'learning_rate': 0.0001916077480051926, 'epoch': 0.06}\n",
      "{'loss': 1.3536, 'grad_norm': 0.546875, 'learning_rate': 0.00019159638562772874, 'epoch': 0.06}\n",
      "{'loss': 1.1813, 'grad_norm': 0.6875, 'learning_rate': 0.00019158502325026492, 'epoch': 0.06}\n",
      "{'loss': 1.5411, 'grad_norm': 0.373046875, 'learning_rate': 0.00019157366087280104, 'epoch': 0.06}\n",
      "{'loss': 1.3425, 'grad_norm': 0.5234375, 'learning_rate': 0.00019156229849533716, 'epoch': 0.06}\n",
      "{'loss': 1.3712, 'grad_norm': 0.380859375, 'learning_rate': 0.00019155093611787332, 'epoch': 0.06}\n",
      "{'loss': 1.3531, 'grad_norm': 0.462890625, 'learning_rate': 0.00019153957374040944, 'epoch': 0.06}\n",
      "{'loss': 1.212, 'grad_norm': 0.85546875, 'learning_rate': 0.0001915282113629456, 'epoch': 0.06}\n",
      "{'loss': 1.3953, 'grad_norm': 0.369140625, 'learning_rate': 0.00019151684898548172, 'epoch': 0.06}\n",
      "{'loss': 1.3238, 'grad_norm': 0.73828125, 'learning_rate': 0.0001915054866080179, 'epoch': 0.06}\n",
      "{'loss': 1.3694, 'grad_norm': 0.341796875, 'learning_rate': 0.00019149412423055402, 'epoch': 0.06}\n",
      "{'loss': 1.2506, 'grad_norm': 0.416015625, 'learning_rate': 0.00019148276185309014, 'epoch': 0.06}\n",
      "{'loss': 1.1407, 'grad_norm': 0.734375, 'learning_rate': 0.0001914713994756263, 'epoch': 0.06}\n",
      "{'loss': 1.4216, 'grad_norm': 0.435546875, 'learning_rate': 0.00019146003709816242, 'epoch': 0.06}\n",
      "{'loss': 1.2536, 'grad_norm': 0.62890625, 'learning_rate': 0.00019144867472069857, 'epoch': 0.06}\n",
      "{'loss': 1.2001, 'grad_norm': 0.341796875, 'learning_rate': 0.0001914373123432347, 'epoch': 0.06}\n",
      "{'loss': 1.4282, 'grad_norm': 0.453125, 'learning_rate': 0.00019142594996577085, 'epoch': 0.06}\n",
      "{'loss': 1.1173, 'grad_norm': 0.58203125, 'learning_rate': 0.000191414587588307, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5082, 'grad_norm': 0.369140625, 'learning_rate': 0.00019140322521084312, 'epoch': 0.06}\n",
      "{'loss': 1.3675, 'grad_norm': 0.49609375, 'learning_rate': 0.00019139186283337927, 'epoch': 0.06}\n",
      "{'loss': 1.2234, 'grad_norm': 0.373046875, 'learning_rate': 0.0001913805004559154, 'epoch': 0.06}\n",
      "{'loss': 1.3151, 'grad_norm': 0.419921875, 'learning_rate': 0.00019136913807845155, 'epoch': 0.06}\n",
      "{'loss': 1.1672, 'grad_norm': 0.6328125, 'learning_rate': 0.00019135777570098767, 'epoch': 0.06}\n",
      "{'loss': 1.536, 'grad_norm': 0.41796875, 'learning_rate': 0.00019134641332352382, 'epoch': 0.06}\n",
      "{'loss': 1.3802, 'grad_norm': 0.578125, 'learning_rate': 0.00019133505094605998, 'epoch': 0.06}\n",
      "{'loss': 1.3567, 'grad_norm': 0.306640625, 'learning_rate': 0.0001913236885685961, 'epoch': 0.06}\n",
      "{'loss': 1.4555, 'grad_norm': 0.333984375, 'learning_rate': 0.00019131232619113225, 'epoch': 0.06}\n",
      "{'loss': 1.3361, 'grad_norm': 0.796875, 'learning_rate': 0.00019130096381366838, 'epoch': 0.06}\n",
      "{'loss': 1.5003, 'grad_norm': 0.3984375, 'learning_rate': 0.0001912896014362045, 'epoch': 0.07}\n",
      "{'loss': 1.2191, 'grad_norm': 0.640625, 'learning_rate': 0.00019127823905874065, 'epoch': 0.07}\n",
      "{'loss': 1.3653, 'grad_norm': 0.34765625, 'learning_rate': 0.0001912668766812768, 'epoch': 0.07}\n",
      "{'loss': 1.4069, 'grad_norm': 0.46484375, 'learning_rate': 0.00019125551430381295, 'epoch': 0.07}\n",
      "{'loss': 1.1309, 'grad_norm': 0.6953125, 'learning_rate': 0.00019124415192634908, 'epoch': 0.07}\n",
      "{'loss': 1.4581, 'grad_norm': 0.5390625, 'learning_rate': 0.0001912327895488852, 'epoch': 0.07}\n",
      "{'loss': 1.2656, 'grad_norm': 0.376953125, 'learning_rate': 0.00019122142717142135, 'epoch': 0.07}\n",
      "{'loss': 1.2476, 'grad_norm': 0.359375, 'learning_rate': 0.00019121006479395748, 'epoch': 0.07}\n",
      "{'loss': 1.4432, 'grad_norm': 0.6484375, 'learning_rate': 0.00019119870241649366, 'epoch': 0.07}\n",
      "{'loss': 1.1683, 'grad_norm': 0.734375, 'learning_rate': 0.00019118734003902978, 'epoch': 0.07}\n",
      "{'loss': 1.4137, 'grad_norm': 0.423828125, 'learning_rate': 0.0001911759776615659, 'epoch': 0.07}\n",
      "{'loss': 1.2753, 'grad_norm': 0.55078125, 'learning_rate': 0.00019116461528410206, 'epoch': 0.07}\n",
      "{'loss': 1.376, 'grad_norm': 0.287109375, 'learning_rate': 0.00019115325290663818, 'epoch': 0.07}\n",
      "{'loss': 1.4036, 'grad_norm': 0.4296875, 'learning_rate': 0.00019114189052917433, 'epoch': 0.07}\n",
      "{'loss': 1.1649, 'grad_norm': 0.3828125, 'learning_rate': 0.00019113052815171046, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3822, 'grad_norm': 0.384765625, 'learning_rate': 0.00019111916577424664, 'epoch': 0.07}\n",
      "{'loss': 1.2709, 'grad_norm': 0.67578125, 'learning_rate': 0.00019110780339678276, 'epoch': 0.07}\n",
      "{'loss': 1.3498, 'grad_norm': 0.29296875, 'learning_rate': 0.00019109644101931888, 'epoch': 0.07}\n",
      "{'loss': 1.3511, 'grad_norm': 0.5234375, 'learning_rate': 0.00019108507864185504, 'epoch': 0.07}\n",
      "{'loss': 1.299, 'grad_norm': 0.83203125, 'learning_rate': 0.00019107371626439116, 'epoch': 0.07}\n",
      "{'loss': 1.3856, 'grad_norm': 0.478515625, 'learning_rate': 0.0001910623538869273, 'epoch': 0.07}\n",
      "{'loss': 1.3401, 'grad_norm': 0.53125, 'learning_rate': 0.00019105099150946344, 'epoch': 0.07}\n",
      "{'loss': 1.266, 'grad_norm': 0.37890625, 'learning_rate': 0.0001910396291319996, 'epoch': 0.07}\n",
      "{'loss': 1.4249, 'grad_norm': 0.435546875, 'learning_rate': 0.00019102826675453574, 'epoch': 0.07}\n",
      "{'loss': 1.1649, 'grad_norm': 0.5390625, 'learning_rate': 0.00019101690437707186, 'epoch': 0.07}\n",
      "{'loss': 1.4052, 'grad_norm': 0.45703125, 'learning_rate': 0.00019100554199960801, 'epoch': 0.07}\n",
      "{'loss': 1.3578, 'grad_norm': 0.54296875, 'learning_rate': 0.00019099417962214414, 'epoch': 0.07}\n",
      "{'loss': 1.3539, 'grad_norm': 0.3671875, 'learning_rate': 0.0001909828172446803, 'epoch': 0.07}\n",
      "{'loss': 1.2576, 'grad_norm': 0.60546875, 'learning_rate': 0.00019097145486721641, 'epoch': 0.07}\n",
      "{'loss': 1.3055, 'grad_norm': 1.6640625, 'learning_rate': 0.00019096009248975257, 'epoch': 0.07}\n",
      "{'loss': 1.3371, 'grad_norm': 0.458984375, 'learning_rate': 0.00019094873011228872, 'epoch': 0.07}\n",
      "{'loss': 1.2798, 'grad_norm': 0.625, 'learning_rate': 0.00019093736773482484, 'epoch': 0.07}\n",
      "{'loss': 1.4012, 'grad_norm': 0.357421875, 'learning_rate': 0.000190926005357361, 'epoch': 0.07}\n",
      "{'loss': 1.4557, 'grad_norm': 0.5, 'learning_rate': 0.00019091464297989712, 'epoch': 0.07}\n",
      "{'loss': 1.2854, 'grad_norm': 0.75390625, 'learning_rate': 0.00019090328060243324, 'epoch': 0.07}\n",
      "{'loss': 1.5396, 'grad_norm': 0.412109375, 'learning_rate': 0.00019089191822496942, 'epoch': 0.07}\n",
      "{'loss': 1.2569, 'grad_norm': 0.5625, 'learning_rate': 0.00019088055584750554, 'epoch': 0.07}\n",
      "{'loss': 1.2016, 'grad_norm': 0.384765625, 'learning_rate': 0.0001908691934700417, 'epoch': 0.07}\n",
      "{'loss': 1.483, 'grad_norm': 0.55078125, 'learning_rate': 0.00019085783109257782, 'epoch': 0.07}\n",
      "{'loss': 1.2396, 'grad_norm': 0.75, 'learning_rate': 0.00019084646871511394, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4755, 'grad_norm': 0.451171875, 'learning_rate': 0.0001908351063376501, 'epoch': 0.07}\n",
      "{'loss': 1.3143, 'grad_norm': 0.609375, 'learning_rate': 0.00019082374396018622, 'epoch': 0.07}\n",
      "{'loss': 1.2527, 'grad_norm': 0.34765625, 'learning_rate': 0.0001908123815827224, 'epoch': 0.07}\n",
      "{'loss': 1.2575, 'grad_norm': 0.482421875, 'learning_rate': 0.00019080101920525852, 'epoch': 0.07}\n",
      "{'loss': 1.1835, 'grad_norm': 0.56640625, 'learning_rate': 0.00019078965682779465, 'epoch': 0.07}\n",
      "{'loss': 1.5343, 'grad_norm': 0.384765625, 'learning_rate': 0.0001907782944503308, 'epoch': 0.07}\n",
      "{'loss': 1.2613, 'grad_norm': 0.70703125, 'learning_rate': 0.00019076693207286692, 'epoch': 0.07}\n",
      "{'loss': 1.3509, 'grad_norm': 0.330078125, 'learning_rate': 0.00019075556969540307, 'epoch': 0.07}\n",
      "{'loss': 1.3702, 'grad_norm': 0.5390625, 'learning_rate': 0.0001907442073179392, 'epoch': 0.07}\n",
      "{'loss': 1.3227, 'grad_norm': 0.6328125, 'learning_rate': 0.00019073284494047538, 'epoch': 0.07}\n",
      "{'loss': 1.4548, 'grad_norm': 0.46875, 'learning_rate': 0.0001907214825630115, 'epoch': 0.07}\n",
      "{'loss': 1.1901, 'grad_norm': 0.421875, 'learning_rate': 0.00019071012018554763, 'epoch': 0.07}\n",
      "{'loss': 1.3925, 'grad_norm': 0.455078125, 'learning_rate': 0.00019069875780808378, 'epoch': 0.07}\n",
      "{'loss': 1.4326, 'grad_norm': 0.36328125, 'learning_rate': 0.0001906873954306199, 'epoch': 0.07}\n",
      "{'loss': 1.096, 'grad_norm': 0.7734375, 'learning_rate': 0.00019067603305315605, 'epoch': 0.07}\n",
      "{'loss': 1.4899, 'grad_norm': 0.5234375, 'learning_rate': 0.00019066467067569218, 'epoch': 0.07}\n",
      "{'loss': 1.2944, 'grad_norm': 0.53125, 'learning_rate': 0.00019065330829822833, 'epoch': 0.07}\n",
      "{'loss': 1.4003, 'grad_norm': 0.291015625, 'learning_rate': 0.00019064194592076448, 'epoch': 0.07}\n",
      "{'loss': 1.3261, 'grad_norm': 0.396484375, 'learning_rate': 0.0001906305835433006, 'epoch': 0.07}\n",
      "{'loss': 1.2181, 'grad_norm': 0.53125, 'learning_rate': 0.00019061922116583676, 'epoch': 0.07}\n",
      "{'loss': 1.467, 'grad_norm': 0.3828125, 'learning_rate': 0.00019060785878837288, 'epoch': 0.07}\n",
      "{'loss': 1.252, 'grad_norm': 0.5078125, 'learning_rate': 0.00019059649641090903, 'epoch': 0.07}\n",
      "{'loss': 1.2199, 'grad_norm': 0.333984375, 'learning_rate': 0.00019058513403344516, 'epoch': 0.07}\n",
      "{'loss': 1.3801, 'grad_norm': 0.5390625, 'learning_rate': 0.0001905737716559813, 'epoch': 0.07}\n",
      "{'loss': 1.2132, 'grad_norm': 0.59765625, 'learning_rate': 0.00019056240927851746, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4723, 'grad_norm': 0.443359375, 'learning_rate': 0.00019055104690105358, 'epoch': 0.07}\n",
      "{'loss': 1.3388, 'grad_norm': 0.67578125, 'learning_rate': 0.00019053968452358973, 'epoch': 0.07}\n",
      "{'loss': 1.2848, 'grad_norm': 0.3984375, 'learning_rate': 0.00019052832214612586, 'epoch': 0.07}\n",
      "{'loss': 1.4583, 'grad_norm': 0.45703125, 'learning_rate': 0.00019051695976866198, 'epoch': 0.07}\n",
      "{'loss': 1.1407, 'grad_norm': 0.5546875, 'learning_rate': 0.00019050559739119816, 'epoch': 0.07}\n",
      "{'loss': 1.4988, 'grad_norm': 0.33984375, 'learning_rate': 0.00019049423501373429, 'epoch': 0.07}\n",
      "{'loss': 1.3384, 'grad_norm': 0.734375, 'learning_rate': 0.00019048287263627044, 'epoch': 0.07}\n",
      "{'loss': 1.4214, 'grad_norm': 0.376953125, 'learning_rate': 0.00019047151025880656, 'epoch': 0.07}\n",
      "{'loss': 1.4437, 'grad_norm': 0.50390625, 'learning_rate': 0.00019046014788134269, 'epoch': 0.07}\n",
      "{'loss': 1.2164, 'grad_norm': 0.62109375, 'learning_rate': 0.00019044878550387884, 'epoch': 0.07}\n",
      "{'loss': 1.4971, 'grad_norm': 0.4453125, 'learning_rate': 0.00019043742312641496, 'epoch': 0.07}\n",
      "{'loss': 1.3202, 'grad_norm': 0.62890625, 'learning_rate': 0.00019042606074895114, 'epoch': 0.07}\n",
      "{'loss': 1.2895, 'grad_norm': 0.3984375, 'learning_rate': 0.00019041469837148726, 'epoch': 0.07}\n",
      "{'loss': 1.35, 'grad_norm': 0.51953125, 'learning_rate': 0.0001904033359940234, 'epoch': 0.07}\n",
      "{'loss': 1.0841, 'grad_norm': 0.416015625, 'learning_rate': 0.00019039197361655954, 'epoch': 0.07}\n",
      "{'loss': 1.5051, 'grad_norm': 0.421875, 'learning_rate': 0.00019038061123909566, 'epoch': 0.07}\n",
      "{'loss': 1.3321, 'grad_norm': 0.57421875, 'learning_rate': 0.00019036924886163182, 'epoch': 0.07}\n",
      "{'loss': 1.3429, 'grad_norm': 0.38671875, 'learning_rate': 0.00019035788648416794, 'epoch': 0.07}\n",
      "{'loss': 1.2845, 'grad_norm': 0.4453125, 'learning_rate': 0.00019034652410670412, 'epoch': 0.07}\n",
      "{'loss': 1.3036, 'grad_norm': 0.80859375, 'learning_rate': 0.00019033516172924024, 'epoch': 0.07}\n",
      "{'loss': 1.3561, 'grad_norm': 0.474609375, 'learning_rate': 0.00019032379935177637, 'epoch': 0.07}\n",
      "{'loss': 1.2631, 'grad_norm': 0.51171875, 'learning_rate': 0.00019031243697431252, 'epoch': 0.07}\n",
      "{'loss': 1.3407, 'grad_norm': 0.349609375, 'learning_rate': 0.00019030107459684864, 'epoch': 0.07}\n",
      "{'loss': 1.4478, 'grad_norm': 0.4765625, 'learning_rate': 0.0001902897122193848, 'epoch': 0.07}\n",
      "{'loss': 1.202, 'grad_norm': 0.6640625, 'learning_rate': 0.00019027834984192092, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4738, 'grad_norm': 0.451171875, 'learning_rate': 0.00019026698746445707, 'epoch': 0.07}\n",
      "{'loss': 1.3056, 'grad_norm': 0.62890625, 'learning_rate': 0.00019025562508699322, 'epoch': 0.07}\n",
      "{'loss': 1.2779, 'grad_norm': 0.359375, 'learning_rate': 0.00019024426270952935, 'epoch': 0.07}\n",
      "{'loss': 1.1979, 'grad_norm': 0.46875, 'learning_rate': 0.0001902329003320655, 'epoch': 0.07}\n",
      "{'loss': 1.1835, 'grad_norm': 1.1953125, 'learning_rate': 0.00019022153795460162, 'epoch': 0.07}\n",
      "{'loss': 1.5132, 'grad_norm': 0.443359375, 'learning_rate': 0.00019021017557713777, 'epoch': 0.07}\n",
      "{'loss': 1.3148, 'grad_norm': 0.6875, 'learning_rate': 0.00019019881319967392, 'epoch': 0.07}\n",
      "{'loss': 1.3525, 'grad_norm': 0.3515625, 'learning_rate': 0.00019018745082221005, 'epoch': 0.07}\n",
      "{'loss': 1.3236, 'grad_norm': 0.4453125, 'learning_rate': 0.0001901760884447462, 'epoch': 0.07}\n",
      "{'loss': 1.289, 'grad_norm': 0.55078125, 'learning_rate': 0.00019016472606728232, 'epoch': 0.07}\n",
      "{'loss': 1.5, 'grad_norm': 0.392578125, 'learning_rate': 0.00019015336368981848, 'epoch': 0.07}\n",
      "{'loss': 1.2419, 'grad_norm': 0.52734375, 'learning_rate': 0.0001901420013123546, 'epoch': 0.07}\n",
      "{'loss': 1.4508, 'grad_norm': 0.373046875, 'learning_rate': 0.00019013063893489072, 'epoch': 0.07}\n",
      "{'loss': 1.3597, 'grad_norm': 0.56640625, 'learning_rate': 0.0001901192765574269, 'epoch': 0.07}\n",
      "{'loss': 1.1786, 'grad_norm': 0.99609375, 'learning_rate': 0.00019010791417996303, 'epoch': 0.07}\n",
      "{'loss': 1.5462, 'grad_norm': 0.546875, 'learning_rate': 0.00019009655180249918, 'epoch': 0.07}\n",
      "{'loss': 1.2785, 'grad_norm': 0.466796875, 'learning_rate': 0.0001900851894250353, 'epoch': 0.07}\n",
      "{'loss': 1.2873, 'grad_norm': 0.361328125, 'learning_rate': 0.00019007382704757143, 'epoch': 0.07}\n",
      "{'loss': 1.3949, 'grad_norm': 0.515625, 'learning_rate': 0.00019006246467010758, 'epoch': 0.07}\n",
      "{'loss': 1.2716, 'grad_norm': 0.69921875, 'learning_rate': 0.0001900511022926437, 'epoch': 0.07}\n",
      "{'loss': 1.341, 'grad_norm': 0.357421875, 'learning_rate': 0.00019003973991517988, 'epoch': 0.07}\n",
      "{'loss': 1.3617, 'grad_norm': 0.5, 'learning_rate': 0.000190028377537716, 'epoch': 0.07}\n",
      "{'loss': 1.4135, 'grad_norm': 0.326171875, 'learning_rate': 0.00019001701516025213, 'epoch': 0.07}\n",
      "{'loss': 1.4531, 'grad_norm': 0.55078125, 'learning_rate': 0.00019000565278278828, 'epoch': 0.07}\n",
      "{'loss': 1.3624, 'grad_norm': 0.79296875, 'learning_rate': 0.0001899942904053244, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4426, 'grad_norm': 0.470703125, 'learning_rate': 0.00018998292802786056, 'epoch': 0.07}\n",
      "{'loss': 1.4239, 'grad_norm': 0.609375, 'learning_rate': 0.00018997156565039668, 'epoch': 0.07}\n",
      "{'loss': 1.3893, 'grad_norm': 0.345703125, 'learning_rate': 0.00018996020327293286, 'epoch': 0.07}\n",
      "{'loss': 1.2875, 'grad_norm': 0.52734375, 'learning_rate': 0.00018994884089546898, 'epoch': 0.07}\n",
      "{'loss': 1.1316, 'grad_norm': 0.703125, 'learning_rate': 0.0001899374785180051, 'epoch': 0.07}\n",
      "{'loss': 1.4498, 'grad_norm': 0.32421875, 'learning_rate': 0.00018992611614054126, 'epoch': 0.07}\n",
      "{'loss': 1.3339, 'grad_norm': 0.75390625, 'learning_rate': 0.00018991475376307738, 'epoch': 0.07}\n",
      "{'loss': 1.2397, 'grad_norm': 0.359375, 'learning_rate': 0.00018990339138561354, 'epoch': 0.07}\n",
      "{'loss': 1.2903, 'grad_norm': 0.47265625, 'learning_rate': 0.00018989202900814966, 'epoch': 0.07}\n",
      "{'loss': 1.224, 'grad_norm': 0.59765625, 'learning_rate': 0.0001898806666306858, 'epoch': 0.07}\n",
      "{'loss': 1.3736, 'grad_norm': 0.421875, 'learning_rate': 0.00018986930425322196, 'epoch': 0.07}\n",
      "{'loss': 1.2844, 'grad_norm': 0.68359375, 'learning_rate': 0.0001898579418757581, 'epoch': 0.07}\n",
      "{'loss': 1.2373, 'grad_norm': 0.34375, 'learning_rate': 0.00018984657949829424, 'epoch': 0.07}\n",
      "{'loss': 1.3312, 'grad_norm': 0.640625, 'learning_rate': 0.00018983521712083036, 'epoch': 0.07}\n",
      "{'loss': 1.2397, 'grad_norm': 0.494140625, 'learning_rate': 0.00018982385474336651, 'epoch': 0.07}\n",
      "{'loss': 1.3912, 'grad_norm': 0.380859375, 'learning_rate': 0.00018981249236590267, 'epoch': 0.07}\n",
      "{'loss': 1.39, 'grad_norm': 0.546875, 'learning_rate': 0.0001898011299884388, 'epoch': 0.07}\n",
      "{'loss': 1.5438, 'grad_norm': 0.365234375, 'learning_rate': 0.00018978976761097494, 'epoch': 0.07}\n",
      "{'loss': 1.3887, 'grad_norm': 0.56640625, 'learning_rate': 0.00018977840523351107, 'epoch': 0.07}\n",
      "{'loss': 1.2216, 'grad_norm': 0.5390625, 'learning_rate': 0.00018976704285604722, 'epoch': 0.07}\n",
      "{'loss': 1.4556, 'grad_norm': 0.404296875, 'learning_rate': 0.00018975568047858334, 'epoch': 0.07}\n",
      "{'loss': 1.4037, 'grad_norm': 0.498046875, 'learning_rate': 0.00018974431810111947, 'epoch': 0.07}\n",
      "{'loss': 1.3446, 'grad_norm': 0.37890625, 'learning_rate': 0.00018973295572365564, 'epoch': 0.07}\n",
      "{'loss': 1.2425, 'grad_norm': 0.408203125, 'learning_rate': 0.00018972159334619177, 'epoch': 0.07}\n",
      "{'loss': 1.2122, 'grad_norm': 0.8046875, 'learning_rate': 0.00018971023096872792, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3539, 'grad_norm': 0.43359375, 'learning_rate': 0.00018969886859126404, 'epoch': 0.07}\n",
      "{'loss': 1.2925, 'grad_norm': 0.578125, 'learning_rate': 0.00018968750621380017, 'epoch': 0.07}\n",
      "{'loss': 1.3957, 'grad_norm': 0.333984375, 'learning_rate': 0.00018967614383633632, 'epoch': 0.07}\n",
      "{'loss': 1.3125, 'grad_norm': 0.3828125, 'learning_rate': 0.00018966478145887244, 'epoch': 0.07}\n",
      "{'loss': 1.2018, 'grad_norm': 1.0390625, 'learning_rate': 0.00018965341908140862, 'epoch': 0.07}\n",
      "{'loss': 1.4904, 'grad_norm': 0.447265625, 'learning_rate': 0.00018964205670394475, 'epoch': 0.07}\n",
      "{'loss': 1.3022, 'grad_norm': 0.5234375, 'learning_rate': 0.00018963069432648087, 'epoch': 0.07}\n",
      "{'loss': 1.4076, 'grad_norm': 0.3125, 'learning_rate': 0.00018961933194901702, 'epoch': 0.07}\n",
      "{'loss': 1.418, 'grad_norm': 0.48828125, 'learning_rate': 0.00018960796957155315, 'epoch': 0.07}\n",
      "{'loss': 1.1223, 'grad_norm': 1.0390625, 'learning_rate': 0.0001895966071940893, 'epoch': 0.07}\n",
      "{'loss': 1.3485, 'grad_norm': 0.421875, 'learning_rate': 0.00018958524481662542, 'epoch': 0.07}\n",
      "{'loss': 1.3191, 'grad_norm': 0.330078125, 'learning_rate': 0.0001895738824391616, 'epoch': 0.07}\n",
      "{'loss': 1.343, 'grad_norm': 0.40625, 'learning_rate': 0.00018956252006169773, 'epoch': 0.07}\n",
      "{'loss': 1.3174, 'grad_norm': 0.455078125, 'learning_rate': 0.00018955115768423385, 'epoch': 0.07}\n",
      "{'loss': 1.2478, 'grad_norm': 0.7890625, 'learning_rate': 0.00018953979530677, 'epoch': 0.07}\n",
      "{'loss': 1.4646, 'grad_norm': 0.3984375, 'learning_rate': 0.00018952843292930613, 'epoch': 0.07}\n",
      "{'loss': 1.2389, 'grad_norm': 0.58203125, 'learning_rate': 0.00018951707055184228, 'epoch': 0.07}\n",
      "{'loss': 1.3098, 'grad_norm': 0.376953125, 'learning_rate': 0.00018950570817437843, 'epoch': 0.07}\n",
      "{'loss': 1.2165, 'grad_norm': 0.49609375, 'learning_rate': 0.00018949434579691455, 'epoch': 0.07}\n",
      "{'loss': 1.1942, 'grad_norm': 0.93359375, 'learning_rate': 0.0001894829834194507, 'epoch': 0.07}\n",
      "{'loss': 1.5082, 'grad_norm': 0.4921875, 'learning_rate': 0.00018947162104198683, 'epoch': 0.07}\n",
      "{'loss': 1.2384, 'grad_norm': 0.5, 'learning_rate': 0.00018946025866452298, 'epoch': 0.07}\n",
      "{'loss': 1.4059, 'grad_norm': 0.3828125, 'learning_rate': 0.0001894488962870591, 'epoch': 0.07}\n",
      "{'loss': 1.4278, 'grad_norm': 0.5078125, 'learning_rate': 0.00018943753390959526, 'epoch': 0.07}\n",
      "{'loss': 1.1521, 'grad_norm': 0.298828125, 'learning_rate': 0.0001894261715321314, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.383, 'grad_norm': 0.55078125, 'learning_rate': 0.00018941480915466753, 'epoch': 0.07}\n",
      "{'loss': 1.3374, 'grad_norm': 0.69140625, 'learning_rate': 0.00018940344677720368, 'epoch': 0.07}\n",
      "{'loss': 1.3714, 'grad_norm': 0.3203125, 'learning_rate': 0.0001893920843997398, 'epoch': 0.07}\n",
      "{'loss': 1.3434, 'grad_norm': 0.46875, 'learning_rate': 0.00018938072202227596, 'epoch': 0.07}\n",
      "{'loss': 1.1374, 'grad_norm': 0.310546875, 'learning_rate': 0.00018936935964481208, 'epoch': 0.07}\n",
      "{'loss': 1.3951, 'grad_norm': 0.384765625, 'learning_rate': 0.0001893579972673482, 'epoch': 0.07}\n",
      "{'loss': 1.3574, 'grad_norm': 0.435546875, 'learning_rate': 0.00018934663488988439, 'epoch': 0.07}\n",
      "{'loss': 1.2508, 'grad_norm': 0.353515625, 'learning_rate': 0.0001893352725124205, 'epoch': 0.07}\n",
      "{'loss': 1.3413, 'grad_norm': 0.53515625, 'learning_rate': 0.00018932391013495666, 'epoch': 0.07}\n",
      "{'loss': 1.1438, 'grad_norm': 0.59375, 'learning_rate': 0.00018931254775749279, 'epoch': 0.07}\n",
      "{'loss': 1.5064, 'grad_norm': 0.4296875, 'learning_rate': 0.0001893011853800289, 'epoch': 0.07}\n",
      "{'loss': 1.1993, 'grad_norm': 0.58984375, 'learning_rate': 0.00018928982300256506, 'epoch': 0.07}\n",
      "{'loss': 1.2135, 'grad_norm': 0.447265625, 'learning_rate': 0.00018927846062510119, 'epoch': 0.07}\n",
      "{'loss': 1.3643, 'grad_norm': 0.373046875, 'learning_rate': 0.00018926709824763736, 'epoch': 0.07}\n",
      "{'loss': 1.1977, 'grad_norm': 0.6640625, 'learning_rate': 0.0001892557358701735, 'epoch': 0.07}\n",
      "{'loss': 1.4747, 'grad_norm': 0.3984375, 'learning_rate': 0.0001892443734927096, 'epoch': 0.07}\n",
      "{'loss': 1.3903, 'grad_norm': 0.486328125, 'learning_rate': 0.00018923301111524576, 'epoch': 0.07}\n",
      "{'loss': 1.3492, 'grad_norm': 0.328125, 'learning_rate': 0.0001892216487377819, 'epoch': 0.07}\n",
      "{'loss': 1.3543, 'grad_norm': 0.376953125, 'learning_rate': 0.00018921028636031804, 'epoch': 0.07}\n",
      "{'loss': 1.1566, 'grad_norm': 0.376953125, 'learning_rate': 0.00018919892398285416, 'epoch': 0.07}\n",
      "{'loss': 1.3666, 'grad_norm': 0.498046875, 'learning_rate': 0.00018918756160539034, 'epoch': 0.07}\n",
      "{'loss': 1.2519, 'grad_norm': 0.5234375, 'learning_rate': 0.00018917619922792647, 'epoch': 0.07}\n",
      "{'loss': 1.2554, 'grad_norm': 0.38671875, 'learning_rate': 0.0001891648368504626, 'epoch': 0.07}\n",
      "{'loss': 1.3634, 'grad_norm': 0.5859375, 'learning_rate': 0.00018915347447299874, 'epoch': 0.07}\n",
      "{'loss': 1.1027, 'grad_norm': 0.412109375, 'learning_rate': 0.00018914211209553487, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4223, 'grad_norm': 0.412109375, 'learning_rate': 0.00018913074971807102, 'epoch': 0.07}\n",
      "{'loss': 1.3578, 'grad_norm': 0.53515625, 'learning_rate': 0.00018911938734060717, 'epoch': 0.07}\n",
      "{'loss': 1.2725, 'grad_norm': 0.349609375, 'learning_rate': 0.0001891080249631433, 'epoch': 0.07}\n",
      "{'loss': 1.3489, 'grad_norm': 0.44921875, 'learning_rate': 0.00018909666258567945, 'epoch': 0.07}\n",
      "{'loss': 1.2247, 'grad_norm': 0.443359375, 'learning_rate': 0.00018908530020821557, 'epoch': 0.07}\n",
      "{'loss': 1.343, 'grad_norm': 0.353515625, 'learning_rate': 0.00018907393783075172, 'epoch': 0.07}\n",
      "{'loss': 1.3918, 'grad_norm': 0.5234375, 'learning_rate': 0.00018906257545328785, 'epoch': 0.07}\n",
      "{'loss': 1.2495, 'grad_norm': 0.322265625, 'learning_rate': 0.000189051213075824, 'epoch': 0.07}\n",
      "{'loss': 1.5646, 'grad_norm': 0.53125, 'learning_rate': 0.00018903985069836015, 'epoch': 0.07}\n",
      "{'loss': 1.1988, 'grad_norm': 0.80078125, 'learning_rate': 0.00018902848832089627, 'epoch': 0.07}\n",
      "{'loss': 1.4649, 'grad_norm': 0.53515625, 'learning_rate': 0.00018901712594343242, 'epoch': 0.07}\n",
      "{'loss': 1.2219, 'grad_norm': 0.498046875, 'learning_rate': 0.00018900576356596855, 'epoch': 0.07}\n",
      "{'loss': 1.2118, 'grad_norm': 0.38671875, 'learning_rate': 0.0001889944011885047, 'epoch': 0.08}\n",
      "{'loss': 1.3518, 'grad_norm': 0.458984375, 'learning_rate': 0.00018898303881104082, 'epoch': 0.08}\n",
      "{'loss': 1.126, 'grad_norm': 0.8125, 'learning_rate': 0.00018897167643357695, 'epoch': 0.08}\n",
      "{'loss': 1.5811, 'grad_norm': 0.3515625, 'learning_rate': 0.00018896031405611313, 'epoch': 0.08}\n",
      "{'loss': 1.1656, 'grad_norm': 0.5390625, 'learning_rate': 0.00018894895167864925, 'epoch': 0.08}\n",
      "{'loss': 1.3088, 'grad_norm': 0.35546875, 'learning_rate': 0.0001889375893011854, 'epoch': 0.08}\n",
      "{'loss': 1.5063, 'grad_norm': 0.419921875, 'learning_rate': 0.00018892622692372153, 'epoch': 0.08}\n",
      "{'loss': 1.2001, 'grad_norm': 0.53515625, 'learning_rate': 0.00018891486454625765, 'epoch': 0.08}\n",
      "{'loss': 1.3944, 'grad_norm': 0.3203125, 'learning_rate': 0.0001889035021687938, 'epoch': 0.08}\n",
      "{'loss': 1.3812, 'grad_norm': 0.451171875, 'learning_rate': 0.00018889213979132993, 'epoch': 0.08}\n",
      "{'loss': 1.321, 'grad_norm': 0.423828125, 'learning_rate': 0.0001888807774138661, 'epoch': 0.08}\n",
      "{'loss': 1.2785, 'grad_norm': 0.5625, 'learning_rate': 0.00018886941503640223, 'epoch': 0.08}\n",
      "{'loss': 1.0626, 'grad_norm': 0.404296875, 'learning_rate': 0.00018885805265893835, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4791, 'grad_norm': 0.5, 'learning_rate': 0.0001888466902814745, 'epoch': 0.08}\n",
      "{'loss': 1.2612, 'grad_norm': 0.474609375, 'learning_rate': 0.00018883532790401063, 'epoch': 0.08}\n",
      "{'loss': 1.2984, 'grad_norm': 0.330078125, 'learning_rate': 0.00018882396552654678, 'epoch': 0.08}\n",
      "{'loss': 1.4619, 'grad_norm': 0.5234375, 'learning_rate': 0.00018881260314908293, 'epoch': 0.08}\n",
      "{'loss': 1.1468, 'grad_norm': 0.8984375, 'learning_rate': 0.00018880124077161908, 'epoch': 0.08}\n",
      "{'loss': 1.4439, 'grad_norm': 0.349609375, 'learning_rate': 0.0001887898783941552, 'epoch': 0.08}\n",
      "{'loss': 1.2734, 'grad_norm': 0.4765625, 'learning_rate': 0.00018877851601669133, 'epoch': 0.08}\n",
      "{'loss': 1.3551, 'grad_norm': 0.30859375, 'learning_rate': 0.00018876715363922748, 'epoch': 0.08}\n",
      "{'loss': 1.2853, 'grad_norm': 0.4453125, 'learning_rate': 0.0001887557912617636, 'epoch': 0.08}\n",
      "{'loss': 1.3232, 'grad_norm': 0.474609375, 'learning_rate': 0.00018874442888429976, 'epoch': 0.08}\n",
      "{'loss': 1.3605, 'grad_norm': 0.341796875, 'learning_rate': 0.0001887330665068359, 'epoch': 0.08}\n",
      "{'loss': 1.3331, 'grad_norm': 0.4765625, 'learning_rate': 0.00018872170412937203, 'epoch': 0.08}\n",
      "{'loss': 1.3273, 'grad_norm': 0.365234375, 'learning_rate': 0.00018871034175190819, 'epoch': 0.08}\n",
      "{'loss': 1.3235, 'grad_norm': 0.52734375, 'learning_rate': 0.0001886989793744443, 'epoch': 0.08}\n",
      "{'loss': 1.2298, 'grad_norm': 0.53125, 'learning_rate': 0.00018868761699698046, 'epoch': 0.08}\n",
      "{'loss': 1.5988, 'grad_norm': 0.44921875, 'learning_rate': 0.00018867625461951659, 'epoch': 0.08}\n",
      "{'loss': 1.3221, 'grad_norm': 0.51953125, 'learning_rate': 0.00018866489224205274, 'epoch': 0.08}\n",
      "{'loss': 1.41, 'grad_norm': 0.388671875, 'learning_rate': 0.0001886535298645889, 'epoch': 0.08}\n",
      "{'loss': 1.2813, 'grad_norm': 0.5234375, 'learning_rate': 0.000188642167487125, 'epoch': 0.08}\n",
      "{'loss': 1.0405, 'grad_norm': 0.97265625, 'learning_rate': 0.00018863080510966116, 'epoch': 0.08}\n",
      "{'loss': 1.5708, 'grad_norm': 0.41015625, 'learning_rate': 0.0001886194427321973, 'epoch': 0.08}\n",
      "{'loss': 1.2982, 'grad_norm': 0.427734375, 'learning_rate': 0.00018860808035473344, 'epoch': 0.08}\n",
      "{'loss': 1.2802, 'grad_norm': 0.404296875, 'learning_rate': 0.00018859671797726956, 'epoch': 0.08}\n",
      "{'loss': 1.4686, 'grad_norm': 0.8046875, 'learning_rate': 0.0001885853555998057, 'epoch': 0.08}\n",
      "{'loss': 1.2261, 'grad_norm': 0.75, 'learning_rate': 0.00018857399322234187, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5473, 'grad_norm': 0.412109375, 'learning_rate': 0.000188562630844878, 'epoch': 0.08}\n",
      "{'loss': 1.4594, 'grad_norm': 0.609375, 'learning_rate': 0.00018855126846741414, 'epoch': 0.08}\n",
      "{'loss': 1.2023, 'grad_norm': 0.326171875, 'learning_rate': 0.00018853990608995027, 'epoch': 0.08}\n",
      "{'loss': 1.3512, 'grad_norm': 0.5859375, 'learning_rate': 0.0001885285437124864, 'epoch': 0.08}\n",
      "{'loss': 1.2969, 'grad_norm': 0.5390625, 'learning_rate': 0.00018851718133502254, 'epoch': 0.08}\n",
      "{'loss': 1.297, 'grad_norm': 0.42578125, 'learning_rate': 0.0001885058189575587, 'epoch': 0.08}\n",
      "{'loss': 1.3863, 'grad_norm': 0.5703125, 'learning_rate': 0.00018849445658009485, 'epoch': 0.08}\n",
      "{'loss': 1.1969, 'grad_norm': 0.3515625, 'learning_rate': 0.00018848309420263097, 'epoch': 0.08}\n",
      "{'loss': 1.3725, 'grad_norm': 0.5625, 'learning_rate': 0.0001884717318251671, 'epoch': 0.08}\n",
      "{'loss': 1.1739, 'grad_norm': 0.482421875, 'learning_rate': 0.00018846036944770325, 'epoch': 0.08}\n",
      "{'loss': 1.3597, 'grad_norm': 0.4453125, 'learning_rate': 0.00018844900707023937, 'epoch': 0.08}\n",
      "{'loss': 1.2696, 'grad_norm': 0.5625, 'learning_rate': 0.00018843764469277552, 'epoch': 0.08}\n",
      "{'loss': 1.4035, 'grad_norm': 0.42578125, 'learning_rate': 0.00018842628231531167, 'epoch': 0.08}\n",
      "{'loss': 1.3751, 'grad_norm': 0.412109375, 'learning_rate': 0.00018841491993784782, 'epoch': 0.08}\n",
      "{'loss': 1.1244, 'grad_norm': 0.61328125, 'learning_rate': 0.00018840355756038395, 'epoch': 0.08}\n",
      "{'loss': 1.4094, 'grad_norm': 0.54296875, 'learning_rate': 0.00018839219518292007, 'epoch': 0.08}\n",
      "{'loss': 1.2539, 'grad_norm': 0.7265625, 'learning_rate': 0.00018838083280545622, 'epoch': 0.08}\n",
      "{'loss': 1.4679, 'grad_norm': 0.322265625, 'learning_rate': 0.00018836947042799235, 'epoch': 0.08}\n",
      "{'loss': 1.4258, 'grad_norm': 0.56640625, 'learning_rate': 0.0001883581080505285, 'epoch': 0.08}\n",
      "{'loss': 1.2681, 'grad_norm': 0.345703125, 'learning_rate': 0.00018834674567306465, 'epoch': 0.08}\n",
      "{'loss': 1.4923, 'grad_norm': 0.439453125, 'learning_rate': 0.00018833538329560078, 'epoch': 0.08}\n",
      "{'loss': 1.3212, 'grad_norm': 0.453125, 'learning_rate': 0.00018832402091813693, 'epoch': 0.08}\n",
      "{'loss': 1.2617, 'grad_norm': 0.359375, 'learning_rate': 0.00018831265854067305, 'epoch': 0.08}\n",
      "{'loss': 1.2338, 'grad_norm': 0.5390625, 'learning_rate': 0.0001883012961632092, 'epoch': 0.08}\n",
      "{'loss': 1.2223, 'grad_norm': 0.421875, 'learning_rate': 0.00018828993378574533, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3993, 'grad_norm': 0.384765625, 'learning_rate': 0.00018827857140828148, 'epoch': 0.08}\n",
      "{'loss': 1.3373, 'grad_norm': 1.4609375, 'learning_rate': 0.00018826720903081763, 'epoch': 0.08}\n",
      "{'loss': 1.272, 'grad_norm': 0.34765625, 'learning_rate': 0.00018825584665335375, 'epoch': 0.08}\n",
      "{'loss': 1.3613, 'grad_norm': 0.443359375, 'learning_rate': 0.0001882444842758899, 'epoch': 0.08}\n",
      "{'loss': 1.2012, 'grad_norm': 0.7578125, 'learning_rate': 0.00018823312189842603, 'epoch': 0.08}\n",
      "{'loss': 1.412, 'grad_norm': 0.46875, 'learning_rate': 0.00018822175952096218, 'epoch': 0.08}\n",
      "{'loss': 1.3429, 'grad_norm': 0.65234375, 'learning_rate': 0.0001882103971434983, 'epoch': 0.08}\n",
      "{'loss': 1.2491, 'grad_norm': 0.42578125, 'learning_rate': 0.00018819903476603443, 'epoch': 0.08}\n",
      "{'loss': 1.3053, 'grad_norm': 0.443359375, 'learning_rate': 0.0001881876723885706, 'epoch': 0.08}\n",
      "{'loss': 1.1449, 'grad_norm': 0.490234375, 'learning_rate': 0.00018817631001110673, 'epoch': 0.08}\n",
      "{'loss': 1.657, 'grad_norm': 0.4140625, 'learning_rate': 0.00018816494763364288, 'epoch': 0.08}\n",
      "{'loss': 1.2697, 'grad_norm': 0.7578125, 'learning_rate': 0.000188153585256179, 'epoch': 0.08}\n",
      "{'loss': 1.2855, 'grad_norm': 0.41796875, 'learning_rate': 0.00018814222287871513, 'epoch': 0.08}\n",
      "{'loss': 1.3437, 'grad_norm': 0.44140625, 'learning_rate': 0.00018813086050125128, 'epoch': 0.08}\n",
      "{'loss': 1.2399, 'grad_norm': 0.53515625, 'learning_rate': 0.00018811949812378744, 'epoch': 0.08}\n",
      "{'loss': 1.4679, 'grad_norm': 0.44921875, 'learning_rate': 0.0001881081357463236, 'epoch': 0.08}\n",
      "{'loss': 1.3532, 'grad_norm': 0.58984375, 'learning_rate': 0.0001880967733688597, 'epoch': 0.08}\n",
      "{'loss': 1.4098, 'grad_norm': 0.330078125, 'learning_rate': 0.00018808541099139584, 'epoch': 0.08}\n",
      "{'loss': 1.2486, 'grad_norm': 0.458984375, 'learning_rate': 0.000188074048613932, 'epoch': 0.08}\n",
      "{'loss': 1.2097, 'grad_norm': 0.83203125, 'learning_rate': 0.0001880626862364681, 'epoch': 0.08}\n",
      "{'loss': 1.3615, 'grad_norm': 0.4765625, 'learning_rate': 0.00018805132385900426, 'epoch': 0.08}\n",
      "{'loss': 1.2525, 'grad_norm': 0.609375, 'learning_rate': 0.00018803996148154041, 'epoch': 0.08}\n",
      "{'loss': 1.2939, 'grad_norm': 0.34375, 'learning_rate': 0.00018802859910407657, 'epoch': 0.08}\n",
      "{'loss': 1.3687, 'grad_norm': 0.435546875, 'learning_rate': 0.0001880172367266127, 'epoch': 0.08}\n",
      "{'loss': 1.1454, 'grad_norm': 0.4453125, 'learning_rate': 0.00018800587434914881, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4366, 'grad_norm': 0.34375, 'learning_rate': 0.00018799451197168497, 'epoch': 0.08}\n",
      "{'loss': 1.3764, 'grad_norm': 0.7109375, 'learning_rate': 0.0001879831495942211, 'epoch': 0.08}\n",
      "{'loss': 1.3356, 'grad_norm': 0.50390625, 'learning_rate': 0.00018797178721675724, 'epoch': 0.08}\n",
      "{'loss': 1.445, 'grad_norm': 0.453125, 'learning_rate': 0.0001879604248392934, 'epoch': 0.08}\n",
      "{'loss': 1.1651, 'grad_norm': 0.6640625, 'learning_rate': 0.00018794906246182952, 'epoch': 0.08}\n",
      "{'loss': 1.3584, 'grad_norm': 0.392578125, 'learning_rate': 0.00018793770008436567, 'epoch': 0.08}\n",
      "{'loss': 1.319, 'grad_norm': 0.5703125, 'learning_rate': 0.0001879263377069018, 'epoch': 0.08}\n",
      "{'loss': 1.2455, 'grad_norm': 0.40625, 'learning_rate': 0.00018791497532943794, 'epoch': 0.08}\n",
      "{'loss': 1.2928, 'grad_norm': 0.62109375, 'learning_rate': 0.00018790361295197407, 'epoch': 0.08}\n",
      "{'loss': 1.2046, 'grad_norm': 0.671875, 'learning_rate': 0.00018789225057451022, 'epoch': 0.08}\n",
      "{'loss': 1.4621, 'grad_norm': 0.55859375, 'learning_rate': 0.00018788088819704637, 'epoch': 0.08}\n",
      "{'loss': 1.2955, 'grad_norm': 0.5546875, 'learning_rate': 0.0001878695258195825, 'epoch': 0.08}\n",
      "{'loss': 1.4163, 'grad_norm': 0.359375, 'learning_rate': 0.00018785816344211865, 'epoch': 0.08}\n",
      "{'loss': 1.2878, 'grad_norm': 0.453125, 'learning_rate': 0.00018784680106465477, 'epoch': 0.08}\n",
      "{'loss': 1.0998, 'grad_norm': 0.91015625, 'learning_rate': 0.00018783543868719092, 'epoch': 0.08}\n",
      "{'loss': 1.6517, 'grad_norm': 0.5234375, 'learning_rate': 0.00018782407630972705, 'epoch': 0.08}\n",
      "{'loss': 1.3334, 'grad_norm': 0.55859375, 'learning_rate': 0.0001878127139322632, 'epoch': 0.08}\n",
      "{'loss': 1.2558, 'grad_norm': 0.451171875, 'learning_rate': 0.00018780135155479935, 'epoch': 0.08}\n",
      "{'loss': 1.3993, 'grad_norm': 0.58203125, 'learning_rate': 0.00018778998917733547, 'epoch': 0.08}\n",
      "{'loss': 1.1785, 'grad_norm': 0.333984375, 'learning_rate': 0.00018777862679987163, 'epoch': 0.08}\n",
      "{'loss': 1.531, 'grad_norm': 0.486328125, 'learning_rate': 0.00018776726442240775, 'epoch': 0.08}\n",
      "{'loss': 1.2268, 'grad_norm': 0.52734375, 'learning_rate': 0.00018775590204494387, 'epoch': 0.08}\n",
      "{'loss': 1.2526, 'grad_norm': 0.3828125, 'learning_rate': 0.00018774453966748003, 'epoch': 0.08}\n",
      "{'loss': 1.462, 'grad_norm': 0.5078125, 'learning_rate': 0.00018773317729001618, 'epoch': 0.08}\n",
      "{'loss': 1.1108, 'grad_norm': 0.640625, 'learning_rate': 0.00018772181491255233, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4468, 'grad_norm': 0.404296875, 'learning_rate': 0.00018771045253508845, 'epoch': 0.08}\n",
      "{'loss': 1.2513, 'grad_norm': 0.5078125, 'learning_rate': 0.00018769909015762458, 'epoch': 0.08}\n",
      "{'loss': 1.2476, 'grad_norm': 0.37109375, 'learning_rate': 0.00018768772778016073, 'epoch': 0.08}\n",
      "{'loss': 1.452, 'grad_norm': 0.42578125, 'learning_rate': 0.00018767636540269685, 'epoch': 0.08}\n",
      "{'loss': 1.2553, 'grad_norm': 0.41796875, 'learning_rate': 0.000187665003025233, 'epoch': 0.08}\n",
      "{'loss': 1.3483, 'grad_norm': 0.357421875, 'learning_rate': 0.00018765364064776916, 'epoch': 0.08}\n",
      "{'loss': 1.352, 'grad_norm': 0.83203125, 'learning_rate': 0.0001876422782703053, 'epoch': 0.08}\n",
      "{'loss': 1.3058, 'grad_norm': 0.4296875, 'learning_rate': 0.00018763091589284143, 'epoch': 0.08}\n",
      "{'loss': 1.221, 'grad_norm': 0.474609375, 'learning_rate': 0.00018761955351537756, 'epoch': 0.08}\n",
      "{'loss': 1.1859, 'grad_norm': 0.51171875, 'learning_rate': 0.0001876081911379137, 'epoch': 0.08}\n",
      "{'loss': 1.4494, 'grad_norm': 0.38671875, 'learning_rate': 0.00018759682876044983, 'epoch': 0.08}\n",
      "{'loss': 1.264, 'grad_norm': 0.447265625, 'learning_rate': 0.00018758546638298598, 'epoch': 0.08}\n",
      "{'loss': 1.2698, 'grad_norm': 0.4375, 'learning_rate': 0.00018757410400552213, 'epoch': 0.08}\n",
      "{'loss': 1.2785, 'grad_norm': 0.453125, 'learning_rate': 0.00018756274162805826, 'epoch': 0.08}\n",
      "{'loss': 1.2671, 'grad_norm': 0.88671875, 'learning_rate': 0.0001875513792505944, 'epoch': 0.08}\n",
      "{'loss': 1.5563, 'grad_norm': 0.44921875, 'learning_rate': 0.00018754001687313053, 'epoch': 0.08}\n",
      "{'loss': 1.2845, 'grad_norm': 0.703125, 'learning_rate': 0.00018752865449566669, 'epoch': 0.08}\n",
      "{'loss': 1.3509, 'grad_norm': 0.330078125, 'learning_rate': 0.0001875172921182028, 'epoch': 0.08}\n",
      "{'loss': 1.3626, 'grad_norm': 0.55859375, 'learning_rate': 0.00018750592974073896, 'epoch': 0.08}\n",
      "{'loss': 1.105, 'grad_norm': 0.85546875, 'learning_rate': 0.0001874945673632751, 'epoch': 0.08}\n",
      "{'loss': 1.4351, 'grad_norm': 0.42578125, 'learning_rate': 0.00018748320498581124, 'epoch': 0.08}\n",
      "{'loss': 1.2614, 'grad_norm': 0.6484375, 'learning_rate': 0.0001874718426083474, 'epoch': 0.08}\n",
      "{'loss': 1.3815, 'grad_norm': 0.3515625, 'learning_rate': 0.0001874604802308835, 'epoch': 0.08}\n",
      "{'loss': 1.4017, 'grad_norm': 0.478515625, 'learning_rate': 0.00018744911785341966, 'epoch': 0.08}\n",
      "{'loss': 1.2387, 'grad_norm': 0.76171875, 'learning_rate': 0.0001874377554759558, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4746, 'grad_norm': 0.419921875, 'learning_rate': 0.00018742639309849194, 'epoch': 0.08}\n",
      "{'loss': 1.2231, 'grad_norm': 0.4609375, 'learning_rate': 0.0001874150307210281, 'epoch': 0.08}\n",
      "{'loss': 1.2069, 'grad_norm': 0.32421875, 'learning_rate': 0.00018740366834356422, 'epoch': 0.08}\n",
      "{'loss': 1.3622, 'grad_norm': 0.62109375, 'learning_rate': 0.00018739230596610037, 'epoch': 0.08}\n",
      "{'loss': 1.1945, 'grad_norm': 0.625, 'learning_rate': 0.0001873809435886365, 'epoch': 0.08}\n",
      "{'loss': 1.4361, 'grad_norm': 0.376953125, 'learning_rate': 0.00018736958121117262, 'epoch': 0.08}\n",
      "{'loss': 1.2511, 'grad_norm': 0.52734375, 'learning_rate': 0.00018735821883370877, 'epoch': 0.08}\n",
      "{'loss': 1.2171, 'grad_norm': 0.390625, 'learning_rate': 0.00018734685645624492, 'epoch': 0.08}\n",
      "{'loss': 1.4371, 'grad_norm': 0.57421875, 'learning_rate': 0.00018733549407878107, 'epoch': 0.08}\n",
      "{'loss': 1.2679, 'grad_norm': 1.0546875, 'learning_rate': 0.0001873241317013172, 'epoch': 0.08}\n",
      "{'loss': 1.4341, 'grad_norm': 0.44921875, 'learning_rate': 0.00018731276932385332, 'epoch': 0.08}\n",
      "{'loss': 1.38, 'grad_norm': 0.89453125, 'learning_rate': 0.00018730140694638947, 'epoch': 0.08}\n",
      "{'loss': 1.2001, 'grad_norm': 0.3359375, 'learning_rate': 0.0001872900445689256, 'epoch': 0.08}\n",
      "{'loss': 1.3047, 'grad_norm': 0.494140625, 'learning_rate': 0.00018727868219146175, 'epoch': 0.08}\n",
      "{'loss': 1.2371, 'grad_norm': 0.671875, 'learning_rate': 0.0001872673198139979, 'epoch': 0.08}\n",
      "{'loss': 1.5514, 'grad_norm': 0.388671875, 'learning_rate': 0.00018725595743653405, 'epoch': 0.08}\n",
      "{'loss': 1.3986, 'grad_norm': 0.62109375, 'learning_rate': 0.00018724459505907017, 'epoch': 0.08}\n",
      "{'loss': 1.2382, 'grad_norm': 0.45703125, 'learning_rate': 0.0001872332326816063, 'epoch': 0.08}\n",
      "{'loss': 1.3209, 'grad_norm': 0.4921875, 'learning_rate': 0.00018722187030414245, 'epoch': 0.08}\n",
      "{'loss': 1.1759, 'grad_norm': 0.83984375, 'learning_rate': 0.00018721050792667857, 'epoch': 0.08}\n",
      "{'loss': 1.4663, 'grad_norm': 0.3515625, 'learning_rate': 0.00018719914554921472, 'epoch': 0.08}\n",
      "{'loss': 1.2989, 'grad_norm': 0.482421875, 'learning_rate': 0.00018718778317175088, 'epoch': 0.08}\n",
      "{'loss': 1.3188, 'grad_norm': 0.423828125, 'learning_rate': 0.000187176420794287, 'epoch': 0.08}\n",
      "{'loss': 1.2982, 'grad_norm': 0.416015625, 'learning_rate': 0.00018716505841682315, 'epoch': 0.08}\n",
      "{'loss': 1.2525, 'grad_norm': 0.828125, 'learning_rate': 0.00018715369603935928, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4753, 'grad_norm': 0.33984375, 'learning_rate': 0.00018714233366189543, 'epoch': 0.08}\n",
      "{'loss': 1.2641, 'grad_norm': 0.5546875, 'learning_rate': 0.00018713097128443155, 'epoch': 0.08}\n",
      "{'loss': 1.1859, 'grad_norm': 0.38671875, 'learning_rate': 0.0001871196089069677, 'epoch': 0.08}\n",
      "{'loss': 1.2619, 'grad_norm': 0.53125, 'learning_rate': 0.00018710824652950385, 'epoch': 0.08}\n",
      "{'loss': 1.235, 'grad_norm': 0.6953125, 'learning_rate': 0.00018709688415203998, 'epoch': 0.08}\n",
      "{'loss': 1.3819, 'grad_norm': 0.357421875, 'learning_rate': 0.00018708552177457613, 'epoch': 0.08}\n",
      "{'loss': 1.2655, 'grad_norm': 0.54296875, 'learning_rate': 0.00018707415939711225, 'epoch': 0.08}\n",
      "{'loss': 1.3604, 'grad_norm': 0.408203125, 'learning_rate': 0.0001870627970196484, 'epoch': 0.08}\n",
      "{'loss': 1.2148, 'grad_norm': 0.46484375, 'learning_rate': 0.00018705143464218453, 'epoch': 0.08}\n",
      "{'loss': 1.2029, 'grad_norm': 1.0703125, 'learning_rate': 0.00018704007226472068, 'epoch': 0.08}\n",
      "{'loss': 1.4858, 'grad_norm': 0.3359375, 'learning_rate': 0.00018702870988725683, 'epoch': 0.08}\n",
      "{'loss': 1.2345, 'grad_norm': 0.5859375, 'learning_rate': 0.00018701734750979296, 'epoch': 0.08}\n",
      "{'loss': 1.3565, 'grad_norm': 0.3515625, 'learning_rate': 0.0001870059851323291, 'epoch': 0.08}\n",
      "{'loss': 1.3748, 'grad_norm': 0.38671875, 'learning_rate': 0.00018699462275486523, 'epoch': 0.08}\n",
      "{'loss': 1.2306, 'grad_norm': 0.4609375, 'learning_rate': 0.00018698326037740136, 'epoch': 0.08}\n",
      "{'loss': 1.446, 'grad_norm': 0.44921875, 'learning_rate': 0.0001869718979999375, 'epoch': 0.08}\n",
      "{'loss': 1.2306, 'grad_norm': 0.51953125, 'learning_rate': 0.00018696053562247366, 'epoch': 0.08}\n",
      "{'loss': 1.1555, 'grad_norm': 0.390625, 'learning_rate': 0.0001869491732450098, 'epoch': 0.08}\n",
      "{'loss': 1.3721, 'grad_norm': 0.412109375, 'learning_rate': 0.00018693781086754594, 'epoch': 0.08}\n",
      "{'loss': 1.1619, 'grad_norm': 0.6328125, 'learning_rate': 0.0001869264484900821, 'epoch': 0.08}\n",
      "{'loss': 1.4532, 'grad_norm': 0.453125, 'learning_rate': 0.0001869150861126182, 'epoch': 0.08}\n",
      "{'loss': 1.374, 'grad_norm': 0.5078125, 'learning_rate': 0.00018690372373515434, 'epoch': 0.08}\n",
      "{'loss': 1.3065, 'grad_norm': 0.34375, 'learning_rate': 0.0001868923613576905, 'epoch': 0.08}\n",
      "{'loss': 1.422, 'grad_norm': 0.470703125, 'learning_rate': 0.00018688099898022664, 'epoch': 0.08}\n",
      "{'loss': 1.1833, 'grad_norm': 0.95703125, 'learning_rate': 0.0001868696366027628, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3307, 'grad_norm': 0.3671875, 'learning_rate': 0.00018685827422529891, 'epoch': 0.08}\n",
      "{'loss': 1.3369, 'grad_norm': 0.484375, 'learning_rate': 0.00018684691184783504, 'epoch': 0.08}\n",
      "{'loss': 1.2284, 'grad_norm': 0.41015625, 'learning_rate': 0.0001868355494703712, 'epoch': 0.08}\n",
      "{'loss': 1.3249, 'grad_norm': 0.50390625, 'learning_rate': 0.00018682418709290731, 'epoch': 0.08}\n",
      "{'loss': 1.0992, 'grad_norm': 0.83203125, 'learning_rate': 0.00018681282471544347, 'epoch': 0.08}\n",
      "{'loss': 1.4822, 'grad_norm': 0.369140625, 'learning_rate': 0.00018680146233797962, 'epoch': 0.08}\n",
      "{'loss': 1.2843, 'grad_norm': 0.474609375, 'learning_rate': 0.00018679009996051574, 'epoch': 0.08}\n",
      "{'loss': 1.415, 'grad_norm': 0.435546875, 'learning_rate': 0.0001867787375830519, 'epoch': 0.08}\n",
      "{'loss': 1.3093, 'grad_norm': 0.416015625, 'learning_rate': 0.00018676737520558802, 'epoch': 0.08}\n",
      "{'loss': 1.2318, 'grad_norm': 0.71875, 'learning_rate': 0.00018675601282812417, 'epoch': 0.08}\n",
      "{'loss': 1.4096, 'grad_norm': 0.3515625, 'learning_rate': 0.0001867446504506603, 'epoch': 0.08}\n",
      "{'loss': 1.2381, 'grad_norm': 0.515625, 'learning_rate': 0.00018673328807319644, 'epoch': 0.08}\n",
      "{'loss': 1.2242, 'grad_norm': 0.37890625, 'learning_rate': 0.0001867219256957326, 'epoch': 0.08}\n",
      "{'loss': 1.3717, 'grad_norm': 0.37890625, 'learning_rate': 0.00018671056331826872, 'epoch': 0.09}\n",
      "{'loss': 1.296, 'grad_norm': 0.671875, 'learning_rate': 0.00018669920094080487, 'epoch': 0.09}\n",
      "{'loss': 1.4402, 'grad_norm': 0.484375, 'learning_rate': 0.000186687838563341, 'epoch': 0.09}\n",
      "{'loss': 1.2876, 'grad_norm': 0.4765625, 'learning_rate': 0.00018667647618587715, 'epoch': 0.09}\n",
      "{'loss': 1.2076, 'grad_norm': 0.3203125, 'learning_rate': 0.00018666511380841327, 'epoch': 0.09}\n",
      "{'loss': 1.2959, 'grad_norm': 0.53515625, 'learning_rate': 0.00018665375143094942, 'epoch': 0.09}\n",
      "{'loss': 1.1113, 'grad_norm': 0.8359375, 'learning_rate': 0.00018664238905348557, 'epoch': 0.09}\n",
      "{'loss': 1.4759, 'grad_norm': 0.408203125, 'learning_rate': 0.0001866310266760217, 'epoch': 0.09}\n",
      "{'loss': 1.4021, 'grad_norm': 0.63671875, 'learning_rate': 0.00018661966429855785, 'epoch': 0.09}\n",
      "{'loss': 1.3726, 'grad_norm': 0.287109375, 'learning_rate': 0.00018660830192109397, 'epoch': 0.09}\n",
      "{'loss': 1.3792, 'grad_norm': 0.423828125, 'learning_rate': 0.0001865969395436301, 'epoch': 0.09}\n",
      "{'loss': 1.2426, 'grad_norm': 0.6875, 'learning_rate': 0.00018658557716616625, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4081, 'grad_norm': 0.5078125, 'learning_rate': 0.0001865742147887024, 'epoch': 0.09}\n",
      "{'loss': 1.3163, 'grad_norm': 0.58984375, 'learning_rate': 0.00018656285241123855, 'epoch': 0.09}\n",
      "{'loss': 1.2132, 'grad_norm': 0.361328125, 'learning_rate': 0.00018655149003377468, 'epoch': 0.09}\n",
      "{'loss': 1.3514, 'grad_norm': 0.4921875, 'learning_rate': 0.00018654012765631083, 'epoch': 0.09}\n",
      "{'loss': 1.1708, 'grad_norm': 0.41796875, 'learning_rate': 0.00018652876527884695, 'epoch': 0.09}\n",
      "{'loss': 1.4447, 'grad_norm': 0.388671875, 'learning_rate': 0.00018651740290138308, 'epoch': 0.09}\n",
      "{'loss': 1.2789, 'grad_norm': 0.640625, 'learning_rate': 0.00018650604052391923, 'epoch': 0.09}\n",
      "{'loss': 1.3242, 'grad_norm': 0.39453125, 'learning_rate': 0.00018649467814645538, 'epoch': 0.09}\n",
      "{'loss': 1.3467, 'grad_norm': 0.4296875, 'learning_rate': 0.00018648331576899153, 'epoch': 0.09}\n",
      "{'loss': 1.1834, 'grad_norm': 0.56640625, 'learning_rate': 0.00018647195339152766, 'epoch': 0.09}\n",
      "{'loss': 1.3643, 'grad_norm': 0.421875, 'learning_rate': 0.00018646059101406378, 'epoch': 0.09}\n",
      "{'loss': 1.2367, 'grad_norm': 0.5078125, 'learning_rate': 0.00018644922863659993, 'epoch': 0.09}\n",
      "{'loss': 1.2344, 'grad_norm': 0.361328125, 'learning_rate': 0.00018643786625913606, 'epoch': 0.09}\n",
      "{'loss': 1.3152, 'grad_norm': 0.48828125, 'learning_rate': 0.00018642650388167223, 'epoch': 0.09}\n",
      "{'loss': 1.3124, 'grad_norm': 0.71484375, 'learning_rate': 0.00018641514150420836, 'epoch': 0.09}\n",
      "{'loss': 1.4917, 'grad_norm': 0.478515625, 'learning_rate': 0.00018640377912674448, 'epoch': 0.09}\n",
      "{'loss': 1.2131, 'grad_norm': 0.57421875, 'learning_rate': 0.00018639241674928063, 'epoch': 0.09}\n",
      "{'loss': 1.326, 'grad_norm': 0.375, 'learning_rate': 0.00018638105437181676, 'epoch': 0.09}\n",
      "{'loss': 1.3361, 'grad_norm': 0.462890625, 'learning_rate': 0.0001863696919943529, 'epoch': 0.09}\n",
      "{'loss': 1.3185, 'grad_norm': 0.765625, 'learning_rate': 0.00018635832961688903, 'epoch': 0.09}\n",
      "{'loss': 1.4856, 'grad_norm': 0.478515625, 'learning_rate': 0.00018634696723942519, 'epoch': 0.09}\n",
      "{'loss': 1.2498, 'grad_norm': 0.45703125, 'learning_rate': 0.00018633560486196134, 'epoch': 0.09}\n",
      "{'loss': 1.3483, 'grad_norm': 0.271484375, 'learning_rate': 0.00018632424248449746, 'epoch': 0.09}\n",
      "{'loss': 1.4147, 'grad_norm': 0.41796875, 'learning_rate': 0.0001863128801070336, 'epoch': 0.09}\n",
      "{'loss': 1.1515, 'grad_norm': 0.609375, 'learning_rate': 0.00018630151772956974, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4697, 'grad_norm': 0.439453125, 'learning_rate': 0.0001862901553521059, 'epoch': 0.09}\n",
      "{'loss': 1.2623, 'grad_norm': 0.64453125, 'learning_rate': 0.000186278792974642, 'epoch': 0.09}\n",
      "{'loss': 1.428, 'grad_norm': 0.33984375, 'learning_rate': 0.00018626743059717816, 'epoch': 0.09}\n",
      "{'loss': 1.3683, 'grad_norm': 0.43359375, 'learning_rate': 0.00018625606821971432, 'epoch': 0.09}\n",
      "{'loss': 1.1203, 'grad_norm': 0.75390625, 'learning_rate': 0.00018624470584225044, 'epoch': 0.09}\n",
      "{'loss': 1.4122, 'grad_norm': 0.392578125, 'learning_rate': 0.0001862333434647866, 'epoch': 0.09}\n",
      "{'loss': 1.2735, 'grad_norm': 0.58203125, 'learning_rate': 0.00018622198108732272, 'epoch': 0.09}\n",
      "{'loss': 1.2578, 'grad_norm': 0.4296875, 'learning_rate': 0.00018621061870985884, 'epoch': 0.09}\n",
      "{'loss': 1.2367, 'grad_norm': 0.52734375, 'learning_rate': 0.000186199256332395, 'epoch': 0.09}\n",
      "{'loss': 1.1995, 'grad_norm': 0.8671875, 'learning_rate': 0.00018618789395493114, 'epoch': 0.09}\n",
      "{'loss': 1.4205, 'grad_norm': 0.6171875, 'learning_rate': 0.0001861765315774673, 'epoch': 0.09}\n",
      "{'loss': 1.2316, 'grad_norm': 0.578125, 'learning_rate': 0.00018616516920000342, 'epoch': 0.09}\n",
      "{'loss': 1.2676, 'grad_norm': 0.359375, 'learning_rate': 0.00018615380682253957, 'epoch': 0.09}\n",
      "{'loss': 1.3989, 'grad_norm': 0.380859375, 'learning_rate': 0.0001861424444450757, 'epoch': 0.09}\n",
      "{'loss': 1.359, 'grad_norm': 0.65625, 'learning_rate': 0.00018613108206761182, 'epoch': 0.09}\n",
      "{'loss': 1.5051, 'grad_norm': 0.4921875, 'learning_rate': 0.00018611971969014797, 'epoch': 0.09}\n",
      "{'loss': 1.2388, 'grad_norm': 0.4921875, 'learning_rate': 0.00018610835731268412, 'epoch': 0.09}\n",
      "{'loss': 1.1597, 'grad_norm': 0.337890625, 'learning_rate': 0.00018609699493522027, 'epoch': 0.09}\n",
      "{'loss': 1.3954, 'grad_norm': 0.435546875, 'learning_rate': 0.0001860856325577564, 'epoch': 0.09}\n",
      "{'loss': 1.2357, 'grad_norm': 0.86328125, 'learning_rate': 0.00018607427018029252, 'epoch': 0.09}\n",
      "{'loss': 1.3886, 'grad_norm': 0.380859375, 'learning_rate': 0.00018606290780282867, 'epoch': 0.09}\n",
      "{'loss': 1.2932, 'grad_norm': 0.609375, 'learning_rate': 0.0001860515454253648, 'epoch': 0.09}\n",
      "{'loss': 1.3699, 'grad_norm': 0.43359375, 'learning_rate': 0.00018604018304790098, 'epoch': 0.09}\n",
      "{'loss': 1.3854, 'grad_norm': 0.44921875, 'learning_rate': 0.0001860288206704371, 'epoch': 0.09}\n",
      "{'loss': 1.2958, 'grad_norm': 0.470703125, 'learning_rate': 0.00018601745829297322, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.425, 'grad_norm': 0.4296875, 'learning_rate': 0.00018600609591550938, 'epoch': 0.09}\n",
      "{'loss': 1.1943, 'grad_norm': 0.59375, 'learning_rate': 0.0001859947335380455, 'epoch': 0.09}\n",
      "{'loss': 1.2653, 'grad_norm': 0.365234375, 'learning_rate': 0.00018598337116058165, 'epoch': 0.09}\n",
      "{'loss': 1.2613, 'grad_norm': 0.453125, 'learning_rate': 0.00018597200878311778, 'epoch': 0.09}\n",
      "{'loss': 1.0635, 'grad_norm': 0.48046875, 'learning_rate': 0.00018596064640565393, 'epoch': 0.09}\n",
      "{'loss': 1.4262, 'grad_norm': 0.4140625, 'learning_rate': 0.00018594928402819008, 'epoch': 0.09}\n",
      "{'loss': 1.2958, 'grad_norm': 0.51171875, 'learning_rate': 0.0001859379216507262, 'epoch': 0.09}\n",
      "{'loss': 1.4099, 'grad_norm': 0.416015625, 'learning_rate': 0.00018592655927326235, 'epoch': 0.09}\n",
      "{'loss': 1.3693, 'grad_norm': 0.44921875, 'learning_rate': 0.00018591519689579848, 'epoch': 0.09}\n",
      "{'loss': 1.0542, 'grad_norm': 0.60546875, 'learning_rate': 0.00018590383451833463, 'epoch': 0.09}\n",
      "{'loss': 1.5357, 'grad_norm': 0.38671875, 'learning_rate': 0.00018589247214087075, 'epoch': 0.09}\n",
      "{'loss': 1.3794, 'grad_norm': 0.66015625, 'learning_rate': 0.0001858811097634069, 'epoch': 0.09}\n",
      "{'loss': 1.3371, 'grad_norm': 0.353515625, 'learning_rate': 0.00018586974738594306, 'epoch': 0.09}\n",
      "{'loss': 1.4153, 'grad_norm': 0.384765625, 'learning_rate': 0.00018585838500847918, 'epoch': 0.09}\n",
      "{'loss': 1.1018, 'grad_norm': 0.640625, 'learning_rate': 0.00018584702263101533, 'epoch': 0.09}\n",
      "{'loss': 1.3294, 'grad_norm': 0.392578125, 'learning_rate': 0.00018583566025355146, 'epoch': 0.09}\n",
      "{'loss': 1.2505, 'grad_norm': 0.470703125, 'learning_rate': 0.00018582429787608758, 'epoch': 0.09}\n",
      "{'loss': 1.2686, 'grad_norm': 0.375, 'learning_rate': 0.00018581293549862373, 'epoch': 0.09}\n",
      "{'loss': 1.4063, 'grad_norm': 0.5859375, 'learning_rate': 0.00018580157312115988, 'epoch': 0.09}\n",
      "{'loss': 1.1233, 'grad_norm': 0.609375, 'learning_rate': 0.00018579021074369603, 'epoch': 0.09}\n",
      "{'loss': 1.2449, 'grad_norm': 0.490234375, 'learning_rate': 0.00018577884836623216, 'epoch': 0.09}\n",
      "{'loss': 1.2161, 'grad_norm': 0.443359375, 'learning_rate': 0.0001857674859887683, 'epoch': 0.09}\n",
      "{'loss': 1.4511, 'grad_norm': 0.4140625, 'learning_rate': 0.00018575612361130444, 'epoch': 0.09}\n",
      "{'loss': 1.3123, 'grad_norm': 0.46875, 'learning_rate': 0.00018574476123384056, 'epoch': 0.09}\n",
      "{'loss': 1.0884, 'grad_norm': 0.69140625, 'learning_rate': 0.00018573339885637674, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3897, 'grad_norm': 0.56640625, 'learning_rate': 0.00018572203647891286, 'epoch': 0.09}\n",
      "{'loss': 1.1534, 'grad_norm': 0.4609375, 'learning_rate': 0.000185710674101449, 'epoch': 0.09}\n",
      "{'loss': 1.2545, 'grad_norm': 0.330078125, 'learning_rate': 0.00018569931172398514, 'epoch': 0.09}\n",
      "{'loss': 1.3367, 'grad_norm': 0.51953125, 'learning_rate': 0.00018568794934652126, 'epoch': 0.09}\n",
      "{'loss': 1.1908, 'grad_norm': 0.84375, 'learning_rate': 0.0001856765869690574, 'epoch': 0.09}\n",
      "{'loss': 1.4473, 'grad_norm': 0.36328125, 'learning_rate': 0.00018566522459159354, 'epoch': 0.09}\n",
      "{'loss': 1.306, 'grad_norm': 0.76171875, 'learning_rate': 0.00018565386221412972, 'epoch': 0.09}\n",
      "{'loss': 1.2971, 'grad_norm': 0.390625, 'learning_rate': 0.00018564249983666584, 'epoch': 0.09}\n",
      "{'loss': 1.3174, 'grad_norm': 0.45703125, 'learning_rate': 0.00018563113745920196, 'epoch': 0.09}\n",
      "{'loss': 1.173, 'grad_norm': 0.5390625, 'learning_rate': 0.00018561977508173812, 'epoch': 0.09}\n",
      "{'loss': 1.5138, 'grad_norm': 0.41796875, 'learning_rate': 0.00018560841270427424, 'epoch': 0.09}\n",
      "{'loss': 1.2905, 'grad_norm': 0.63671875, 'learning_rate': 0.0001855970503268104, 'epoch': 0.09}\n",
      "{'loss': 1.223, 'grad_norm': 0.41796875, 'learning_rate': 0.00018558568794934652, 'epoch': 0.09}\n",
      "{'loss': 1.3473, 'grad_norm': 0.52734375, 'learning_rate': 0.00018557432557188267, 'epoch': 0.09}\n",
      "{'loss': 1.1553, 'grad_norm': 0.7734375, 'learning_rate': 0.00018556296319441882, 'epoch': 0.09}\n",
      "{'loss': 1.364, 'grad_norm': 0.46875, 'learning_rate': 0.00018555160081695494, 'epoch': 0.09}\n",
      "{'loss': 1.4085, 'grad_norm': 0.625, 'learning_rate': 0.0001855402384394911, 'epoch': 0.09}\n",
      "{'loss': 1.3931, 'grad_norm': 0.349609375, 'learning_rate': 0.00018552887606202722, 'epoch': 0.09}\n",
      "{'loss': 1.3568, 'grad_norm': 0.486328125, 'learning_rate': 0.00018551751368456337, 'epoch': 0.09}\n",
      "{'loss': 1.2943, 'grad_norm': 0.828125, 'learning_rate': 0.0001855061513070995, 'epoch': 0.09}\n",
      "{'loss': 1.4214, 'grad_norm': 0.453125, 'learning_rate': 0.00018549478892963565, 'epoch': 0.09}\n",
      "{'loss': 1.3324, 'grad_norm': 0.6640625, 'learning_rate': 0.0001854834265521718, 'epoch': 0.09}\n",
      "{'loss': 1.272, 'grad_norm': 0.345703125, 'learning_rate': 0.00018547206417470792, 'epoch': 0.09}\n",
      "{'loss': 1.2999, 'grad_norm': 0.54296875, 'learning_rate': 0.00018546070179724407, 'epoch': 0.09}\n",
      "{'loss': 1.1869, 'grad_norm': 1.015625, 'learning_rate': 0.0001854493394197802, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3251, 'grad_norm': 0.3828125, 'learning_rate': 0.00018543797704231632, 'epoch': 0.09}\n",
      "{'loss': 1.3191, 'grad_norm': 0.640625, 'learning_rate': 0.00018542661466485247, 'epoch': 0.09}\n",
      "{'loss': 1.3179, 'grad_norm': 0.400390625, 'learning_rate': 0.00018541525228738862, 'epoch': 0.09}\n",
      "{'loss': 1.4555, 'grad_norm': 0.455078125, 'learning_rate': 0.00018540388990992478, 'epoch': 0.09}\n",
      "{'loss': 1.1935, 'grad_norm': 0.70703125, 'learning_rate': 0.0001853925275324609, 'epoch': 0.09}\n",
      "{'loss': 1.5373, 'grad_norm': 0.49609375, 'learning_rate': 0.00018538116515499705, 'epoch': 0.09}\n",
      "{'loss': 1.2417, 'grad_norm': 0.52734375, 'learning_rate': 0.00018536980277753318, 'epoch': 0.09}\n",
      "{'loss': 1.2363, 'grad_norm': 0.384765625, 'learning_rate': 0.0001853584404000693, 'epoch': 0.09}\n",
      "{'loss': 1.3606, 'grad_norm': 0.427734375, 'learning_rate': 0.00018534707802260548, 'epoch': 0.09}\n",
      "{'loss': 1.1562, 'grad_norm': 0.53515625, 'learning_rate': 0.0001853357156451416, 'epoch': 0.09}\n",
      "{'loss': 1.5171, 'grad_norm': 0.3984375, 'learning_rate': 0.00018532435326767775, 'epoch': 0.09}\n",
      "{'loss': 1.3083, 'grad_norm': 0.5390625, 'learning_rate': 0.00018531299089021388, 'epoch': 0.09}\n",
      "{'loss': 1.3, 'grad_norm': 0.359375, 'learning_rate': 0.00018530162851275, 'epoch': 0.09}\n",
      "{'loss': 1.3651, 'grad_norm': 0.45703125, 'learning_rate': 0.00018529026613528615, 'epoch': 0.09}\n",
      "{'loss': 1.239, 'grad_norm': 0.322265625, 'learning_rate': 0.00018527890375782228, 'epoch': 0.09}\n",
      "{'loss': 1.4669, 'grad_norm': 0.400390625, 'learning_rate': 0.00018526754138035846, 'epoch': 0.09}\n",
      "{'loss': 1.2833, 'grad_norm': 0.5703125, 'learning_rate': 0.00018525617900289458, 'epoch': 0.09}\n",
      "{'loss': 1.4147, 'grad_norm': 0.373046875, 'learning_rate': 0.0001852448166254307, 'epoch': 0.09}\n",
      "{'loss': 1.2689, 'grad_norm': 0.466796875, 'learning_rate': 0.00018523345424796686, 'epoch': 0.09}\n",
      "{'loss': 1.2283, 'grad_norm': 0.92578125, 'learning_rate': 0.00018522209187050298, 'epoch': 0.09}\n",
      "{'loss': 1.2362, 'grad_norm': 0.4375, 'learning_rate': 0.00018521072949303913, 'epoch': 0.09}\n",
      "{'loss': 1.3409, 'grad_norm': 0.7890625, 'learning_rate': 0.00018519936711557526, 'epoch': 0.09}\n",
      "{'loss': 1.3844, 'grad_norm': 0.345703125, 'learning_rate': 0.0001851880047381114, 'epoch': 0.09}\n",
      "{'loss': 1.2574, 'grad_norm': 0.41796875, 'learning_rate': 0.00018517664236064756, 'epoch': 0.09}\n",
      "{'loss': 1.2293, 'grad_norm': 0.7734375, 'learning_rate': 0.00018516527998318368, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5561, 'grad_norm': 0.447265625, 'learning_rate': 0.00018515391760571984, 'epoch': 0.09}\n",
      "{'loss': 1.164, 'grad_norm': 0.58203125, 'learning_rate': 0.00018514255522825596, 'epoch': 0.09}\n",
      "{'loss': 1.2369, 'grad_norm': 0.5234375, 'learning_rate': 0.0001851311928507921, 'epoch': 0.09}\n",
      "{'loss': 1.4062, 'grad_norm': 0.43359375, 'learning_rate': 0.00018511983047332824, 'epoch': 0.09}\n",
      "{'loss': 1.0879, 'grad_norm': 0.51171875, 'learning_rate': 0.0001851084680958644, 'epoch': 0.09}\n",
      "{'loss': 1.4361, 'grad_norm': 0.40625, 'learning_rate': 0.00018509710571840054, 'epoch': 0.09}\n",
      "{'loss': 1.1617, 'grad_norm': 0.56640625, 'learning_rate': 0.00018508574334093666, 'epoch': 0.09}\n",
      "{'loss': 1.2692, 'grad_norm': 0.34375, 'learning_rate': 0.00018507438096347281, 'epoch': 0.09}\n",
      "{'loss': 1.3032, 'grad_norm': 0.455078125, 'learning_rate': 0.00018506301858600894, 'epoch': 0.09}\n",
      "{'loss': 1.1778, 'grad_norm': 0.3828125, 'learning_rate': 0.00018505165620854506, 'epoch': 0.09}\n",
      "{'loss': 1.518, 'grad_norm': 0.44921875, 'learning_rate': 0.00018504029383108124, 'epoch': 0.09}\n",
      "{'loss': 1.3367, 'grad_norm': 0.494140625, 'learning_rate': 0.00018502893145361737, 'epoch': 0.09}\n",
      "{'loss': 1.3037, 'grad_norm': 0.4140625, 'learning_rate': 0.00018501756907615352, 'epoch': 0.09}\n",
      "{'loss': 1.3067, 'grad_norm': 0.4765625, 'learning_rate': 0.00018500620669868964, 'epoch': 0.09}\n",
      "{'loss': 1.2619, 'grad_norm': 0.53125, 'learning_rate': 0.0001849948443212258, 'epoch': 0.09}\n",
      "{'loss': 1.3131, 'grad_norm': 0.474609375, 'learning_rate': 0.00018498348194376192, 'epoch': 0.09}\n",
      "{'loss': 1.3385, 'grad_norm': 0.51953125, 'learning_rate': 0.00018497211956629804, 'epoch': 0.09}\n",
      "{'loss': 1.2924, 'grad_norm': 0.375, 'learning_rate': 0.00018496075718883422, 'epoch': 0.09}\n",
      "{'loss': 1.3109, 'grad_norm': 0.470703125, 'learning_rate': 0.00018494939481137034, 'epoch': 0.09}\n",
      "{'loss': 1.1674, 'grad_norm': 0.54296875, 'learning_rate': 0.0001849380324339065, 'epoch': 0.09}\n",
      "{'loss': 1.4355, 'grad_norm': 0.44140625, 'learning_rate': 0.00018492667005644262, 'epoch': 0.09}\n",
      "{'loss': 1.2589, 'grad_norm': 0.5078125, 'learning_rate': 0.00018491530767897874, 'epoch': 0.09}\n",
      "{'loss': 1.2919, 'grad_norm': 0.41796875, 'learning_rate': 0.0001849039453015149, 'epoch': 0.09}\n",
      "{'loss': 1.4588, 'grad_norm': 0.46484375, 'learning_rate': 0.00018489258292405102, 'epoch': 0.09}\n",
      "{'loss': 1.0722, 'grad_norm': 0.76953125, 'learning_rate': 0.0001848812205465872, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4589, 'grad_norm': 0.52734375, 'learning_rate': 0.00018486985816912332, 'epoch': 0.09}\n",
      "{'loss': 1.3568, 'grad_norm': 0.4609375, 'learning_rate': 0.00018485849579165945, 'epoch': 0.09}\n",
      "{'loss': 1.2142, 'grad_norm': 0.35546875, 'learning_rate': 0.0001848471334141956, 'epoch': 0.09}\n",
      "{'loss': 1.378, 'grad_norm': 0.6484375, 'learning_rate': 0.00018483577103673172, 'epoch': 0.09}\n",
      "{'loss': 1.1816, 'grad_norm': 1.734375, 'learning_rate': 0.00018482440865926787, 'epoch': 0.09}\n",
      "{'loss': 1.4909, 'grad_norm': 0.39453125, 'learning_rate': 0.000184813046281804, 'epoch': 0.09}\n",
      "{'loss': 1.2722, 'grad_norm': 0.6171875, 'learning_rate': 0.00018480168390434015, 'epoch': 0.09}\n",
      "{'loss': 1.3335, 'grad_norm': 0.369140625, 'learning_rate': 0.0001847903215268763, 'epoch': 0.09}\n",
      "{'loss': 1.419, 'grad_norm': 0.52734375, 'learning_rate': 0.00018477895914941243, 'epoch': 0.09}\n",
      "{'loss': 1.2597, 'grad_norm': 0.65234375, 'learning_rate': 0.00018476759677194858, 'epoch': 0.09}\n",
      "{'loss': 1.4453, 'grad_norm': 0.52734375, 'learning_rate': 0.0001847562343944847, 'epoch': 0.09}\n",
      "{'loss': 1.2029, 'grad_norm': 0.5859375, 'learning_rate': 0.00018474487201702085, 'epoch': 0.09}\n",
      "{'loss': 1.269, 'grad_norm': 0.388671875, 'learning_rate': 0.000184733509639557, 'epoch': 0.09}\n",
      "{'loss': 1.3073, 'grad_norm': 0.46875, 'learning_rate': 0.00018472214726209313, 'epoch': 0.09}\n",
      "{'loss': 1.2682, 'grad_norm': 0.7890625, 'learning_rate': 0.00018471078488462928, 'epoch': 0.09}\n",
      "{'loss': 1.3401, 'grad_norm': 0.453125, 'learning_rate': 0.0001846994225071654, 'epoch': 0.09}\n",
      "{'loss': 1.2908, 'grad_norm': 0.5078125, 'learning_rate': 0.00018468806012970156, 'epoch': 0.09}\n",
      "{'loss': 1.2509, 'grad_norm': 0.419921875, 'learning_rate': 0.00018467669775223768, 'epoch': 0.09}\n",
      "{'loss': 1.4158, 'grad_norm': 0.466796875, 'learning_rate': 0.0001846653353747738, 'epoch': 0.09}\n",
      "{'loss': 1.1698, 'grad_norm': 0.7578125, 'learning_rate': 0.00018465397299730998, 'epoch': 0.09}\n",
      "{'loss': 1.3715, 'grad_norm': 0.421875, 'learning_rate': 0.0001846426106198461, 'epoch': 0.09}\n",
      "{'loss': 1.2586, 'grad_norm': 0.6953125, 'learning_rate': 0.00018463124824238226, 'epoch': 0.09}\n",
      "{'loss': 1.3133, 'grad_norm': 0.388671875, 'learning_rate': 0.00018461988586491838, 'epoch': 0.09}\n",
      "{'loss': 1.1217, 'grad_norm': 0.494140625, 'learning_rate': 0.00018460852348745453, 'epoch': 0.09}\n",
      "{'loss': 1.2258, 'grad_norm': 0.75390625, 'learning_rate': 0.00018459716110999066, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4137, 'grad_norm': 0.404296875, 'learning_rate': 0.00018458579873252678, 'epoch': 0.09}\n",
      "{'loss': 1.2568, 'grad_norm': 0.59765625, 'learning_rate': 0.00018457443635506296, 'epoch': 0.09}\n",
      "{'loss': 1.2323, 'grad_norm': 0.3828125, 'learning_rate': 0.00018456307397759909, 'epoch': 0.09}\n",
      "{'loss': 1.3923, 'grad_norm': 0.5, 'learning_rate': 0.00018455171160013524, 'epoch': 0.09}\n",
      "{'loss': 1.1564, 'grad_norm': 0.578125, 'learning_rate': 0.00018454034922267136, 'epoch': 0.09}\n",
      "{'loss': 1.4098, 'grad_norm': 0.53515625, 'learning_rate': 0.00018452898684520749, 'epoch': 0.09}\n",
      "{'loss': 1.3001, 'grad_norm': 0.52734375, 'learning_rate': 0.00018451762446774364, 'epoch': 0.09}\n",
      "{'loss': 1.2568, 'grad_norm': 0.3203125, 'learning_rate': 0.00018450626209027976, 'epoch': 0.09}\n",
      "{'loss': 1.3608, 'grad_norm': 0.478515625, 'learning_rate': 0.00018449489971281594, 'epoch': 0.09}\n",
      "{'loss': 1.2164, 'grad_norm': 0.59765625, 'learning_rate': 0.00018448353733535206, 'epoch': 0.09}\n",
      "{'loss': 1.5122, 'grad_norm': 0.48828125, 'learning_rate': 0.0001844721749578882, 'epoch': 0.09}\n",
      "{'loss': 1.306, 'grad_norm': 0.462890625, 'learning_rate': 0.00018446081258042434, 'epoch': 0.09}\n",
      "{'loss': 1.3466, 'grad_norm': 0.404296875, 'learning_rate': 0.00018444945020296046, 'epoch': 0.09}\n",
      "{'loss': 1.2264, 'grad_norm': 0.51171875, 'learning_rate': 0.00018443808782549662, 'epoch': 0.09}\n",
      "{'loss': 1.2181, 'grad_norm': 0.68359375, 'learning_rate': 0.00018442672544803274, 'epoch': 0.09}\n",
      "{'loss': 1.3423, 'grad_norm': 0.357421875, 'learning_rate': 0.0001844153630705689, 'epoch': 0.1}\n",
      "{'loss': 1.3382, 'grad_norm': 0.48046875, 'learning_rate': 0.00018440400069310504, 'epoch': 0.1}\n",
      "{'loss': 1.4694, 'grad_norm': 0.3828125, 'learning_rate': 0.00018439263831564117, 'epoch': 0.1}\n",
      "{'loss': 1.4397, 'grad_norm': 0.46484375, 'learning_rate': 0.00018438127593817732, 'epoch': 0.1}\n",
      "{'loss': 1.2468, 'grad_norm': 0.65234375, 'learning_rate': 0.00018436991356071344, 'epoch': 0.1}\n",
      "{'loss': 1.4013, 'grad_norm': 0.416015625, 'learning_rate': 0.0001843585511832496, 'epoch': 0.1}\n",
      "{'loss': 1.3449, 'grad_norm': 0.38671875, 'learning_rate': 0.00018434718880578575, 'epoch': 0.1}\n",
      "{'loss': 1.3351, 'grad_norm': 0.396484375, 'learning_rate': 0.00018433582642832187, 'epoch': 0.1}\n",
      "{'loss': 1.3482, 'grad_norm': 0.50390625, 'learning_rate': 0.00018432446405085802, 'epoch': 0.1}\n",
      "{'loss': 1.1317, 'grad_norm': 0.49609375, 'learning_rate': 0.00018431310167339415, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4223, 'grad_norm': 0.439453125, 'learning_rate': 0.0001843017392959303, 'epoch': 0.1}\n",
      "{'loss': 1.3073, 'grad_norm': 0.71875, 'learning_rate': 0.00018429037691846642, 'epoch': 0.1}\n",
      "{'loss': 1.3237, 'grad_norm': 0.41015625, 'learning_rate': 0.00018427901454100255, 'epoch': 0.1}\n",
      "{'loss': 1.302, 'grad_norm': 0.5390625, 'learning_rate': 0.00018426765216353872, 'epoch': 0.1}\n",
      "{'loss': 1.2609, 'grad_norm': 0.96875, 'learning_rate': 0.00018425628978607485, 'epoch': 0.1}\n",
      "{'loss': 1.3378, 'grad_norm': 0.353515625, 'learning_rate': 0.000184244927408611, 'epoch': 0.1}\n",
      "{'loss': 1.2376, 'grad_norm': 0.65625, 'learning_rate': 0.00018423356503114712, 'epoch': 0.1}\n",
      "{'loss': 1.3051, 'grad_norm': 0.33203125, 'learning_rate': 0.00018422220265368328, 'epoch': 0.1}\n",
      "{'loss': 1.4745, 'grad_norm': 0.423828125, 'learning_rate': 0.0001842108402762194, 'epoch': 0.1}\n",
      "{'loss': 1.1491, 'grad_norm': 0.484375, 'learning_rate': 0.00018419947789875552, 'epoch': 0.1}\n",
      "{'loss': 1.3469, 'grad_norm': 0.341796875, 'learning_rate': 0.0001841881155212917, 'epoch': 0.1}\n",
      "{'loss': 1.3697, 'grad_norm': 0.60546875, 'learning_rate': 0.00018417675314382783, 'epoch': 0.1}\n",
      "{'loss': 1.2101, 'grad_norm': 0.412109375, 'learning_rate': 0.00018416539076636398, 'epoch': 0.1}\n",
      "{'loss': 1.4009, 'grad_norm': 0.490234375, 'learning_rate': 0.0001841540283889001, 'epoch': 0.1}\n",
      "{'loss': 1.0954, 'grad_norm': 0.59765625, 'learning_rate': 0.00018414266601143623, 'epoch': 0.1}\n",
      "{'loss': 1.31, 'grad_norm': 0.439453125, 'learning_rate': 0.00018413130363397238, 'epoch': 0.1}\n",
      "{'loss': 1.2426, 'grad_norm': 0.703125, 'learning_rate': 0.0001841199412565085, 'epoch': 0.1}\n",
      "{'loss': 1.3316, 'grad_norm': 0.38671875, 'learning_rate': 0.00018410857887904468, 'epoch': 0.1}\n",
      "{'loss': 1.3436, 'grad_norm': 0.5703125, 'learning_rate': 0.0001840972165015808, 'epoch': 0.1}\n",
      "{'loss': 1.1713, 'grad_norm': 0.37109375, 'learning_rate': 0.00018408585412411693, 'epoch': 0.1}\n",
      "{'loss': 1.4439, 'grad_norm': 0.41015625, 'learning_rate': 0.00018407449174665308, 'epoch': 0.1}\n",
      "{'loss': 1.2685, 'grad_norm': 0.56640625, 'learning_rate': 0.0001840631293691892, 'epoch': 0.1}\n",
      "{'loss': 1.4056, 'grad_norm': 0.37109375, 'learning_rate': 0.00018405176699172536, 'epoch': 0.1}\n",
      "{'loss': 1.19, 'grad_norm': 0.431640625, 'learning_rate': 0.0001840404046142615, 'epoch': 0.1}\n",
      "{'loss': 1.2156, 'grad_norm': 0.64453125, 'learning_rate': 0.00018402904223679763, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5225, 'grad_norm': 0.51171875, 'learning_rate': 0.00018401767985933378, 'epoch': 0.1}\n",
      "{'loss': 1.2764, 'grad_norm': 0.58984375, 'learning_rate': 0.0001840063174818699, 'epoch': 0.1}\n",
      "{'loss': 1.2093, 'grad_norm': 0.51953125, 'learning_rate': 0.00018399495510440606, 'epoch': 0.1}\n",
      "{'loss': 1.2246, 'grad_norm': 0.4609375, 'learning_rate': 0.00018398359272694218, 'epoch': 0.1}\n",
      "{'loss': 1.2443, 'grad_norm': 0.92578125, 'learning_rate': 0.00018397223034947834, 'epoch': 0.1}\n",
      "{'loss': 1.4571, 'grad_norm': 0.427734375, 'learning_rate': 0.0001839608679720145, 'epoch': 0.1}\n",
      "{'loss': 1.3114, 'grad_norm': 0.625, 'learning_rate': 0.0001839495055945506, 'epoch': 0.1}\n",
      "{'loss': 1.3562, 'grad_norm': 0.35546875, 'learning_rate': 0.00018393814321708676, 'epoch': 0.1}\n",
      "{'loss': 1.3465, 'grad_norm': 0.46484375, 'learning_rate': 0.0001839267808396229, 'epoch': 0.1}\n",
      "{'loss': 1.2597, 'grad_norm': 1.078125, 'learning_rate': 0.00018391541846215904, 'epoch': 0.1}\n",
      "{'loss': 1.4988, 'grad_norm': 0.53125, 'learning_rate': 0.00018390405608469516, 'epoch': 0.1}\n",
      "{'loss': 1.4302, 'grad_norm': 0.71484375, 'learning_rate': 0.0001838926937072313, 'epoch': 0.1}\n",
      "{'loss': 1.2061, 'grad_norm': 0.390625, 'learning_rate': 0.00018388133132976747, 'epoch': 0.1}\n",
      "{'loss': 1.3731, 'grad_norm': 0.64453125, 'learning_rate': 0.0001838699689523036, 'epoch': 0.1}\n",
      "{'loss': 1.1944, 'grad_norm': 0.609375, 'learning_rate': 0.00018385860657483974, 'epoch': 0.1}\n",
      "{'loss': 1.5032, 'grad_norm': 0.72265625, 'learning_rate': 0.00018384724419737587, 'epoch': 0.1}\n",
      "{'loss': 1.3039, 'grad_norm': 0.5625, 'learning_rate': 0.00018383588181991202, 'epoch': 0.1}\n",
      "{'loss': 1.2693, 'grad_norm': 0.423828125, 'learning_rate': 0.00018382451944244814, 'epoch': 0.1}\n",
      "{'loss': 1.3498, 'grad_norm': 0.6015625, 'learning_rate': 0.00018381315706498427, 'epoch': 0.1}\n",
      "{'loss': 1.045, 'grad_norm': 0.82421875, 'learning_rate': 0.00018380179468752044, 'epoch': 0.1}\n",
      "{'loss': 1.3528, 'grad_norm': 0.447265625, 'learning_rate': 0.00018379043231005657, 'epoch': 0.1}\n",
      "{'loss': 1.2977, 'grad_norm': 1.0078125, 'learning_rate': 0.00018377906993259272, 'epoch': 0.1}\n",
      "{'loss': 1.3067, 'grad_norm': 0.39453125, 'learning_rate': 0.00018376770755512884, 'epoch': 0.1}\n",
      "{'loss': 1.1905, 'grad_norm': 0.330078125, 'learning_rate': 0.00018375634517766497, 'epoch': 0.1}\n",
      "{'loss': 1.1874, 'grad_norm': 0.73828125, 'learning_rate': 0.00018374498280020112, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3942, 'grad_norm': 0.458984375, 'learning_rate': 0.00018373362042273724, 'epoch': 0.1}\n",
      "{'loss': 1.1844, 'grad_norm': 0.5234375, 'learning_rate': 0.00018372225804527342, 'epoch': 0.1}\n",
      "{'loss': 1.4133, 'grad_norm': 0.419921875, 'learning_rate': 0.00018371089566780955, 'epoch': 0.1}\n",
      "{'loss': 1.3529, 'grad_norm': 0.62890625, 'learning_rate': 0.00018369953329034567, 'epoch': 0.1}\n",
      "{'loss': 1.1899, 'grad_norm': 0.81640625, 'learning_rate': 0.00018368817091288182, 'epoch': 0.1}\n",
      "{'loss': 1.3942, 'grad_norm': 0.498046875, 'learning_rate': 0.00018367680853541795, 'epoch': 0.1}\n",
      "{'loss': 1.1977, 'grad_norm': 0.55078125, 'learning_rate': 0.0001836654461579541, 'epoch': 0.1}\n",
      "{'loss': 1.2857, 'grad_norm': 0.38671875, 'learning_rate': 0.00018365408378049025, 'epoch': 0.1}\n",
      "{'loss': 1.2839, 'grad_norm': 0.56640625, 'learning_rate': 0.00018364272140302637, 'epoch': 0.1}\n",
      "{'loss': 1.1295, 'grad_norm': 0.65234375, 'learning_rate': 0.00018363135902556253, 'epoch': 0.1}\n",
      "{'loss': 1.4672, 'grad_norm': 0.3984375, 'learning_rate': 0.00018361999664809865, 'epoch': 0.1}\n",
      "{'loss': 1.3448, 'grad_norm': 0.65234375, 'learning_rate': 0.0001836086342706348, 'epoch': 0.1}\n",
      "{'loss': 1.3106, 'grad_norm': 0.390625, 'learning_rate': 0.00018359727189317093, 'epoch': 0.1}\n",
      "{'loss': 1.3536, 'grad_norm': 0.466796875, 'learning_rate': 0.00018358590951570708, 'epoch': 0.1}\n",
      "{'loss': 1.2975, 'grad_norm': 0.87109375, 'learning_rate': 0.00018357454713824323, 'epoch': 0.1}\n",
      "{'loss': 1.5153, 'grad_norm': 0.38671875, 'learning_rate': 0.00018356318476077935, 'epoch': 0.1}\n",
      "{'loss': 1.2815, 'grad_norm': 0.66796875, 'learning_rate': 0.0001835518223833155, 'epoch': 0.1}\n",
      "{'loss': 1.3002, 'grad_norm': 0.380859375, 'learning_rate': 0.00018354046000585163, 'epoch': 0.1}\n",
      "{'loss': 1.3581, 'grad_norm': 0.46875, 'learning_rate': 0.00018352909762838778, 'epoch': 0.1}\n",
      "{'loss': 1.1239, 'grad_norm': 0.81640625, 'learning_rate': 0.0001835177352509239, 'epoch': 0.1}\n",
      "{'loss': 1.4023, 'grad_norm': 0.453125, 'learning_rate': 0.00018350637287346003, 'epoch': 0.1}\n",
      "{'loss': 1.3017, 'grad_norm': 0.421875, 'learning_rate': 0.0001834950104959962, 'epoch': 0.1}\n",
      "{'loss': 1.2973, 'grad_norm': 0.40625, 'learning_rate': 0.00018348364811853233, 'epoch': 0.1}\n",
      "{'loss': 1.3691, 'grad_norm': 0.5234375, 'learning_rate': 0.00018347228574106848, 'epoch': 0.1}\n",
      "{'loss': 1.1605, 'grad_norm': 0.75, 'learning_rate': 0.0001834609233636046, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5554, 'grad_norm': 0.380859375, 'learning_rate': 0.00018344956098614076, 'epoch': 0.1}\n",
      "{'loss': 1.1929, 'grad_norm': 0.57421875, 'learning_rate': 0.00018343819860867688, 'epoch': 0.1}\n",
      "{'loss': 1.3089, 'grad_norm': 0.390625, 'learning_rate': 0.000183426836231213, 'epoch': 0.1}\n",
      "{'loss': 1.3308, 'grad_norm': 0.6171875, 'learning_rate': 0.00018341547385374919, 'epoch': 0.1}\n",
      "{'loss': 1.1406, 'grad_norm': 1.1015625, 'learning_rate': 0.0001834041114762853, 'epoch': 0.1}\n",
      "{'loss': 1.4468, 'grad_norm': 0.439453125, 'learning_rate': 0.00018339274909882146, 'epoch': 0.1}\n",
      "{'loss': 1.3425, 'grad_norm': 0.51953125, 'learning_rate': 0.00018338138672135759, 'epoch': 0.1}\n",
      "{'loss': 1.2181, 'grad_norm': 0.375, 'learning_rate': 0.0001833700243438937, 'epoch': 0.1}\n",
      "{'loss': 1.4153, 'grad_norm': 0.52734375, 'learning_rate': 0.00018335866196642986, 'epoch': 0.1}\n",
      "{'loss': 1.1818, 'grad_norm': 1.125, 'learning_rate': 0.000183347299588966, 'epoch': 0.1}\n",
      "{'loss': 1.4549, 'grad_norm': 0.43359375, 'learning_rate': 0.00018333593721150216, 'epoch': 0.1}\n",
      "{'loss': 1.2515, 'grad_norm': 0.47265625, 'learning_rate': 0.0001833245748340383, 'epoch': 0.1}\n",
      "{'loss': 1.2137, 'grad_norm': 0.396484375, 'learning_rate': 0.0001833132124565744, 'epoch': 0.1}\n",
      "{'loss': 1.3141, 'grad_norm': 0.5859375, 'learning_rate': 0.00018330185007911056, 'epoch': 0.1}\n",
      "{'loss': 1.133, 'grad_norm': 0.58203125, 'learning_rate': 0.0001832904877016467, 'epoch': 0.1}\n",
      "{'loss': 1.328, 'grad_norm': 0.458984375, 'learning_rate': 0.00018327912532418284, 'epoch': 0.1}\n",
      "{'loss': 1.4314, 'grad_norm': 0.52734375, 'learning_rate': 0.000183267762946719, 'epoch': 0.1}\n",
      "{'loss': 1.4087, 'grad_norm': 0.466796875, 'learning_rate': 0.00018325640056925512, 'epoch': 0.1}\n",
      "{'loss': 1.1788, 'grad_norm': 0.482421875, 'learning_rate': 0.00018324503819179127, 'epoch': 0.1}\n",
      "{'loss': 1.0499, 'grad_norm': 0.81640625, 'learning_rate': 0.0001832336758143274, 'epoch': 0.1}\n",
      "{'loss': 1.4164, 'grad_norm': 0.42578125, 'learning_rate': 0.00018322231343686354, 'epoch': 0.1}\n",
      "{'loss': 1.1897, 'grad_norm': 0.578125, 'learning_rate': 0.00018321095105939967, 'epoch': 0.1}\n",
      "{'loss': 1.3514, 'grad_norm': 0.36328125, 'learning_rate': 0.00018319958868193582, 'epoch': 0.1}\n",
      "{'loss': 1.3325, 'grad_norm': 0.5078125, 'learning_rate': 0.00018318822630447197, 'epoch': 0.1}\n",
      "{'loss': 1.0513, 'grad_norm': 0.71875, 'learning_rate': 0.0001831768639270081, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2769, 'grad_norm': 0.404296875, 'learning_rate': 0.00018316550154954425, 'epoch': 0.1}\n",
      "{'loss': 1.3471, 'grad_norm': 0.55078125, 'learning_rate': 0.00018315413917208037, 'epoch': 0.1}\n",
      "{'loss': 1.3927, 'grad_norm': 0.380859375, 'learning_rate': 0.00018314277679461652, 'epoch': 0.1}\n",
      "{'loss': 1.2275, 'grad_norm': 0.48828125, 'learning_rate': 0.00018313141441715265, 'epoch': 0.1}\n",
      "{'loss': 1.2106, 'grad_norm': 0.69140625, 'learning_rate': 0.00018312005203968877, 'epoch': 0.1}\n",
      "{'loss': 1.3226, 'grad_norm': 0.5625, 'learning_rate': 0.00018310868966222495, 'epoch': 0.1}\n",
      "{'loss': 1.2592, 'grad_norm': 0.59765625, 'learning_rate': 0.00018309732728476107, 'epoch': 0.1}\n",
      "{'loss': 1.3329, 'grad_norm': 0.3671875, 'learning_rate': 0.00018308596490729722, 'epoch': 0.1}\n",
      "{'loss': 1.4159, 'grad_norm': 0.51171875, 'learning_rate': 0.00018307460252983335, 'epoch': 0.1}\n",
      "{'loss': 1.182, 'grad_norm': 0.79296875, 'learning_rate': 0.0001830632401523695, 'epoch': 0.1}\n",
      "{'loss': 1.4109, 'grad_norm': 0.478515625, 'learning_rate': 0.00018305187777490562, 'epoch': 0.1}\n",
      "{'loss': 1.2424, 'grad_norm': 0.578125, 'learning_rate': 0.00018304051539744175, 'epoch': 0.1}\n",
      "{'loss': 1.3907, 'grad_norm': 0.365234375, 'learning_rate': 0.00018302915301997793, 'epoch': 0.1}\n",
      "{'loss': 1.2917, 'grad_norm': 0.4609375, 'learning_rate': 0.00018301779064251405, 'epoch': 0.1}\n",
      "{'loss': 1.173, 'grad_norm': 0.50390625, 'learning_rate': 0.0001830064282650502, 'epoch': 0.1}\n",
      "{'loss': 1.3908, 'grad_norm': 0.404296875, 'learning_rate': 0.00018299506588758633, 'epoch': 0.1}\n",
      "{'loss': 1.2169, 'grad_norm': 0.5859375, 'learning_rate': 0.00018298370351012245, 'epoch': 0.1}\n",
      "{'loss': 1.2848, 'grad_norm': 0.435546875, 'learning_rate': 0.0001829723411326586, 'epoch': 0.1}\n",
      "{'loss': 1.2966, 'grad_norm': 0.408203125, 'learning_rate': 0.00018296097875519475, 'epoch': 0.1}\n",
      "{'loss': 1.0923, 'grad_norm': 0.54296875, 'learning_rate': 0.0001829496163777309, 'epoch': 0.1}\n",
      "{'loss': 1.3364, 'grad_norm': 0.4609375, 'learning_rate': 0.00018293825400026703, 'epoch': 0.1}\n",
      "{'loss': 1.1353, 'grad_norm': 0.486328125, 'learning_rate': 0.00018292689162280315, 'epoch': 0.1}\n",
      "{'loss': 1.2887, 'grad_norm': 0.375, 'learning_rate': 0.0001829155292453393, 'epoch': 0.1}\n",
      "{'loss': 1.3026, 'grad_norm': 0.55859375, 'learning_rate': 0.00018290416686787543, 'epoch': 0.1}\n",
      "{'loss': 1.1411, 'grad_norm': 0.82421875, 'learning_rate': 0.00018289280449041158, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3099, 'grad_norm': 0.46484375, 'learning_rate': 0.00018288144211294773, 'epoch': 0.1}\n",
      "{'loss': 1.35, 'grad_norm': 0.671875, 'learning_rate': 0.00018287007973548386, 'epoch': 0.1}\n",
      "{'loss': 1.3243, 'grad_norm': 0.439453125, 'learning_rate': 0.00018285871735802, 'epoch': 0.1}\n",
      "{'loss': 1.3428, 'grad_norm': 0.435546875, 'learning_rate': 0.00018284735498055613, 'epoch': 0.1}\n",
      "{'loss': 1.1694, 'grad_norm': 0.353515625, 'learning_rate': 0.00018283599260309228, 'epoch': 0.1}\n",
      "{'loss': 1.4163, 'grad_norm': 0.45703125, 'learning_rate': 0.0001828246302256284, 'epoch': 0.1}\n",
      "{'loss': 1.3442, 'grad_norm': 0.625, 'learning_rate': 0.00018281326784816456, 'epoch': 0.1}\n",
      "{'loss': 1.346, 'grad_norm': 0.373046875, 'learning_rate': 0.0001828019054707007, 'epoch': 0.1}\n",
      "{'loss': 1.2985, 'grad_norm': 0.59765625, 'learning_rate': 0.00018279054309323684, 'epoch': 0.1}\n",
      "{'loss': 1.2164, 'grad_norm': 0.73828125, 'learning_rate': 0.000182779180715773, 'epoch': 0.1}\n",
      "{'loss': 1.414, 'grad_norm': 0.427734375, 'learning_rate': 0.0001827678183383091, 'epoch': 0.1}\n",
      "{'loss': 1.4025, 'grad_norm': 0.60546875, 'learning_rate': 0.00018275645596084526, 'epoch': 0.1}\n",
      "{'loss': 1.3335, 'grad_norm': 0.373046875, 'learning_rate': 0.0001827450935833814, 'epoch': 0.1}\n",
      "{'loss': 1.4159, 'grad_norm': 0.5859375, 'learning_rate': 0.0001827337312059175, 'epoch': 0.1}\n",
      "{'loss': 1.1947, 'grad_norm': 0.74609375, 'learning_rate': 0.0001827223688284537, 'epoch': 0.1}\n",
      "{'loss': 1.4231, 'grad_norm': 0.423828125, 'learning_rate': 0.00018271100645098981, 'epoch': 0.1}\n",
      "{'loss': 1.2905, 'grad_norm': 0.54296875, 'learning_rate': 0.00018269964407352597, 'epoch': 0.1}\n",
      "{'loss': 1.3414, 'grad_norm': 0.365234375, 'learning_rate': 0.0001826882816960621, 'epoch': 0.1}\n",
      "{'loss': 1.4243, 'grad_norm': 0.4765625, 'learning_rate': 0.00018267691931859824, 'epoch': 0.1}\n",
      "{'loss': 1.2262, 'grad_norm': 0.8125, 'learning_rate': 0.00018266555694113437, 'epoch': 0.1}\n",
      "{'loss': 1.33, 'grad_norm': 0.5234375, 'learning_rate': 0.00018265419456367052, 'epoch': 0.1}\n",
      "{'loss': 1.2046, 'grad_norm': 0.42578125, 'learning_rate': 0.00018264283218620667, 'epoch': 0.1}\n",
      "{'loss': 1.329, 'grad_norm': 0.39453125, 'learning_rate': 0.0001826314698087428, 'epoch': 0.1}\n",
      "{'loss': 1.2545, 'grad_norm': 0.462890625, 'learning_rate': 0.00018262010743127894, 'epoch': 0.1}\n",
      "{'loss': 1.2131, 'grad_norm': 0.69921875, 'learning_rate': 0.00018260874505381507, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3806, 'grad_norm': 0.45703125, 'learning_rate': 0.0001825973826763512, 'epoch': 0.1}\n",
      "{'loss': 1.2315, 'grad_norm': 0.578125, 'learning_rate': 0.00018258602029888734, 'epoch': 0.1}\n",
      "{'loss': 1.2611, 'grad_norm': 0.37890625, 'learning_rate': 0.0001825746579214235, 'epoch': 0.1}\n",
      "{'loss': 1.305, 'grad_norm': 0.466796875, 'learning_rate': 0.00018256329554395965, 'epoch': 0.1}\n",
      "{'loss': 1.2537, 'grad_norm': 0.8203125, 'learning_rate': 0.00018255193316649577, 'epoch': 0.1}\n",
      "{'loss': 1.4194, 'grad_norm': 0.388671875, 'learning_rate': 0.0001825405707890319, 'epoch': 0.1}\n",
      "{'loss': 1.2502, 'grad_norm': 0.765625, 'learning_rate': 0.00018252920841156805, 'epoch': 0.1}\n",
      "{'loss': 1.2764, 'grad_norm': 0.400390625, 'learning_rate': 0.00018251784603410417, 'epoch': 0.1}\n",
      "{'loss': 1.275, 'grad_norm': 0.482421875, 'learning_rate': 0.00018250648365664032, 'epoch': 0.1}\n",
      "{'loss': 1.2712, 'grad_norm': 0.69921875, 'learning_rate': 0.00018249512127917647, 'epoch': 0.1}\n",
      "{'loss': 1.459, 'grad_norm': 0.404296875, 'learning_rate': 0.0001824837589017126, 'epoch': 0.1}\n",
      "{'loss': 1.1508, 'grad_norm': 1.046875, 'learning_rate': 0.00018247239652424875, 'epoch': 0.1}\n",
      "{'loss': 1.168, 'grad_norm': 0.458984375, 'learning_rate': 0.00018246103414678487, 'epoch': 0.1}\n",
      "{'loss': 1.3408, 'grad_norm': 0.546875, 'learning_rate': 0.00018244967176932102, 'epoch': 0.1}\n",
      "{'loss': 1.1252, 'grad_norm': 0.89453125, 'learning_rate': 0.00018243830939185715, 'epoch': 0.1}\n",
      "{'loss': 1.3581, 'grad_norm': 0.5234375, 'learning_rate': 0.0001824269470143933, 'epoch': 0.1}\n",
      "{'loss': 1.2638, 'grad_norm': 0.64453125, 'learning_rate': 0.00018241558463692945, 'epoch': 0.1}\n",
      "{'loss': 1.3138, 'grad_norm': 0.373046875, 'learning_rate': 0.00018240422225946558, 'epoch': 0.1}\n",
      "{'loss': 1.3489, 'grad_norm': 0.46484375, 'learning_rate': 0.00018239285988200173, 'epoch': 0.1}\n",
      "{'loss': 1.1787, 'grad_norm': 0.83203125, 'learning_rate': 0.00018238149750453785, 'epoch': 0.1}\n",
      "{'loss': 1.3722, 'grad_norm': 0.349609375, 'learning_rate': 0.000182370135127074, 'epoch': 0.1}\n",
      "{'loss': 1.2282, 'grad_norm': 0.53125, 'learning_rate': 0.00018235877274961013, 'epoch': 0.1}\n",
      "{'loss': 1.2113, 'grad_norm': 0.427734375, 'learning_rate': 0.00018234741037214625, 'epoch': 0.1}\n",
      "{'loss': 1.4162, 'grad_norm': 0.36328125, 'learning_rate': 0.00018233604799468243, 'epoch': 0.1}\n",
      "{'loss': 1.1073, 'grad_norm': 0.65625, 'learning_rate': 0.00018232468561721855, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4338, 'grad_norm': 0.482421875, 'learning_rate': 0.0001823133232397547, 'epoch': 0.1}\n",
      "{'loss': 1.2641, 'grad_norm': 0.34765625, 'learning_rate': 0.00018230196086229083, 'epoch': 0.1}\n",
      "{'loss': 1.3553, 'grad_norm': 0.3671875, 'learning_rate': 0.00018229059848482698, 'epoch': 0.1}\n",
      "{'loss': 1.3367, 'grad_norm': 0.447265625, 'learning_rate': 0.0001822792361073631, 'epoch': 0.1}\n",
      "{'loss': 1.2276, 'grad_norm': 1.1640625, 'learning_rate': 0.00018226787372989926, 'epoch': 0.1}\n",
      "{'loss': 1.4188, 'grad_norm': 0.416015625, 'learning_rate': 0.0001822565113524354, 'epoch': 0.1}\n",
      "{'loss': 1.2196, 'grad_norm': 0.68359375, 'learning_rate': 0.00018224514897497153, 'epoch': 0.1}\n",
      "{'loss': 1.3294, 'grad_norm': 0.345703125, 'learning_rate': 0.00018223378659750768, 'epoch': 0.1}\n",
      "{'loss': 1.3168, 'grad_norm': 0.46875, 'learning_rate': 0.0001822224242200438, 'epoch': 0.1}\n",
      "{'loss': 1.1171, 'grad_norm': 0.546875, 'learning_rate': 0.00018221106184257993, 'epoch': 0.1}\n",
      "{'loss': 1.3701, 'grad_norm': 0.408203125, 'learning_rate': 0.00018219969946511608, 'epoch': 0.1}\n",
      "{'loss': 1.173, 'grad_norm': 0.48828125, 'learning_rate': 0.00018218833708765224, 'epoch': 0.1}\n",
      "{'loss': 1.2752, 'grad_norm': 0.4765625, 'learning_rate': 0.0001821769747101884, 'epoch': 0.1}\n",
      "{'loss': 1.2818, 'grad_norm': 0.5546875, 'learning_rate': 0.0001821656123327245, 'epoch': 0.1}\n",
      "{'loss': 1.109, 'grad_norm': 0.46484375, 'learning_rate': 0.00018215424995526064, 'epoch': 0.1}\n",
      "{'loss': 1.4026, 'grad_norm': 0.427734375, 'learning_rate': 0.0001821428875777968, 'epoch': 0.1}\n",
      "{'loss': 1.2138, 'grad_norm': 0.5859375, 'learning_rate': 0.0001821315252003329, 'epoch': 0.1}\n",
      "{'loss': 1.2414, 'grad_norm': 0.40234375, 'learning_rate': 0.00018212016282286906, 'epoch': 0.11}\n",
      "{'loss': 1.3391, 'grad_norm': 0.484375, 'learning_rate': 0.00018210880044540521, 'epoch': 0.11}\n",
      "{'loss': 1.1389, 'grad_norm': 0.65234375, 'learning_rate': 0.00018209743806794134, 'epoch': 0.11}\n",
      "{'loss': 1.3999, 'grad_norm': 0.47265625, 'learning_rate': 0.0001820860756904775, 'epoch': 0.11}\n",
      "{'loss': 1.1826, 'grad_norm': 0.5546875, 'learning_rate': 0.00018207471331301361, 'epoch': 0.11}\n",
      "{'loss': 1.2001, 'grad_norm': 0.462890625, 'learning_rate': 0.00018206335093554977, 'epoch': 0.11}\n",
      "{'loss': 1.2542, 'grad_norm': 0.609375, 'learning_rate': 0.0001820519885580859, 'epoch': 0.11}\n",
      "{'loss': 1.1007, 'grad_norm': 0.30859375, 'learning_rate': 0.00018204062618062204, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4004, 'grad_norm': 0.466796875, 'learning_rate': 0.0001820292638031582, 'epoch': 0.11}\n",
      "{'loss': 1.2357, 'grad_norm': 0.71484375, 'learning_rate': 0.00018201790142569432, 'epoch': 0.11}\n",
      "{'loss': 1.2905, 'grad_norm': 0.35546875, 'learning_rate': 0.00018200653904823047, 'epoch': 0.11}\n",
      "{'loss': 1.3181, 'grad_norm': 0.53125, 'learning_rate': 0.0001819951766707666, 'epoch': 0.11}\n",
      "{'loss': 1.1381, 'grad_norm': 0.5234375, 'learning_rate': 0.00018198381429330274, 'epoch': 0.11}\n",
      "{'loss': 1.3189, 'grad_norm': 0.4453125, 'learning_rate': 0.00018197245191583887, 'epoch': 0.11}\n",
      "{'loss': 1.2947, 'grad_norm': 0.53515625, 'learning_rate': 0.00018196108953837502, 'epoch': 0.11}\n",
      "{'loss': 1.2631, 'grad_norm': 0.376953125, 'learning_rate': 0.00018194972716091117, 'epoch': 0.11}\n",
      "{'loss': 1.3255, 'grad_norm': 0.51953125, 'learning_rate': 0.0001819383647834473, 'epoch': 0.11}\n",
      "{'loss': 1.1005, 'grad_norm': 0.349609375, 'learning_rate': 0.00018192700240598345, 'epoch': 0.11}\n",
      "{'loss': 1.5827, 'grad_norm': 0.5, 'learning_rate': 0.00018191564002851957, 'epoch': 0.11}\n",
      "{'loss': 1.2834, 'grad_norm': 0.62109375, 'learning_rate': 0.00018190427765105572, 'epoch': 0.11}\n",
      "{'loss': 1.2697, 'grad_norm': 0.318359375, 'learning_rate': 0.00018189291527359185, 'epoch': 0.11}\n",
      "{'loss': 1.3571, 'grad_norm': 0.443359375, 'learning_rate': 0.000181881552896128, 'epoch': 0.11}\n",
      "{'loss': 1.075, 'grad_norm': 0.66796875, 'learning_rate': 0.00018187019051866415, 'epoch': 0.11}\n",
      "{'loss': 1.3993, 'grad_norm': 0.36328125, 'learning_rate': 0.00018185882814120027, 'epoch': 0.11}\n",
      "{'loss': 1.3268, 'grad_norm': 0.5859375, 'learning_rate': 0.00018184746576373643, 'epoch': 0.11}\n",
      "{'loss': 1.322, 'grad_norm': 0.484375, 'learning_rate': 0.00018183610338627255, 'epoch': 0.11}\n",
      "{'loss': 1.2688, 'grad_norm': 0.5390625, 'learning_rate': 0.00018182474100880867, 'epoch': 0.11}\n",
      "{'loss': 1.2282, 'grad_norm': 0.6796875, 'learning_rate': 0.00018181337863134483, 'epoch': 0.11}\n",
      "{'loss': 1.2836, 'grad_norm': 0.40625, 'learning_rate': 0.00018180201625388098, 'epoch': 0.11}\n",
      "{'loss': 1.2399, 'grad_norm': 0.4921875, 'learning_rate': 0.00018179065387641713, 'epoch': 0.11}\n",
      "{'loss': 1.2095, 'grad_norm': 0.52734375, 'learning_rate': 0.00018177929149895325, 'epoch': 0.11}\n",
      "{'loss': 1.3191, 'grad_norm': 0.63671875, 'learning_rate': 0.00018176792912148938, 'epoch': 0.11}\n",
      "{'loss': 1.0772, 'grad_norm': 0.486328125, 'learning_rate': 0.00018175656674402553, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2978, 'grad_norm': 0.52734375, 'learning_rate': 0.00018174520436656165, 'epoch': 0.11}\n",
      "{'loss': 1.2901, 'grad_norm': 0.5625, 'learning_rate': 0.0001817338419890978, 'epoch': 0.11}\n",
      "{'loss': 1.3221, 'grad_norm': 0.4453125, 'learning_rate': 0.00018172247961163396, 'epoch': 0.11}\n",
      "{'loss': 1.2885, 'grad_norm': 0.64453125, 'learning_rate': 0.00018171111723417008, 'epoch': 0.11}\n",
      "{'loss': 1.1434, 'grad_norm': 0.71484375, 'learning_rate': 0.00018169975485670623, 'epoch': 0.11}\n",
      "{'loss': 1.465, 'grad_norm': 0.38671875, 'learning_rate': 0.00018168839247924236, 'epoch': 0.11}\n",
      "{'loss': 1.1574, 'grad_norm': 0.3984375, 'learning_rate': 0.0001816770301017785, 'epoch': 0.11}\n",
      "{'loss': 1.231, 'grad_norm': 0.49609375, 'learning_rate': 0.00018166566772431463, 'epoch': 0.11}\n",
      "{'loss': 1.303, 'grad_norm': 0.482421875, 'learning_rate': 0.00018165430534685078, 'epoch': 0.11}\n",
      "{'loss': 1.1889, 'grad_norm': 0.6328125, 'learning_rate': 0.00018164294296938693, 'epoch': 0.11}\n",
      "{'loss': 1.3454, 'grad_norm': 0.474609375, 'learning_rate': 0.00018163158059192306, 'epoch': 0.11}\n",
      "{'loss': 1.2321, 'grad_norm': 0.44921875, 'learning_rate': 0.0001816202182144592, 'epoch': 0.11}\n",
      "{'loss': 1.3217, 'grad_norm': 0.4375, 'learning_rate': 0.00018160885583699533, 'epoch': 0.11}\n",
      "{'loss': 1.3056, 'grad_norm': 0.48046875, 'learning_rate': 0.00018159749345953149, 'epoch': 0.11}\n",
      "{'loss': 1.127, 'grad_norm': 1.09375, 'learning_rate': 0.0001815861310820676, 'epoch': 0.11}\n",
      "{'loss': 1.3261, 'grad_norm': 0.419921875, 'learning_rate': 0.00018157476870460376, 'epoch': 0.11}\n",
      "{'loss': 1.2557, 'grad_norm': 0.63671875, 'learning_rate': 0.0001815634063271399, 'epoch': 0.11}\n",
      "{'loss': 1.1939, 'grad_norm': 0.37109375, 'learning_rate': 0.00018155204394967604, 'epoch': 0.11}\n",
      "{'loss': 1.3608, 'grad_norm': 0.62890625, 'learning_rate': 0.0001815406815722122, 'epoch': 0.11}\n",
      "{'loss': 1.2083, 'grad_norm': 0.68359375, 'learning_rate': 0.0001815293191947483, 'epoch': 0.11}\n",
      "{'loss': 1.4155, 'grad_norm': 0.4765625, 'learning_rate': 0.00018151795681728446, 'epoch': 0.11}\n",
      "{'loss': 1.1839, 'grad_norm': 0.6484375, 'learning_rate': 0.0001815065944398206, 'epoch': 0.11}\n",
      "{'loss': 1.2789, 'grad_norm': 0.439453125, 'learning_rate': 0.00018149523206235674, 'epoch': 0.11}\n",
      "{'loss': 1.3276, 'grad_norm': 0.578125, 'learning_rate': 0.0001814838696848929, 'epoch': 0.11}\n",
      "{'loss': 1.1502, 'grad_norm': 0.47265625, 'learning_rate': 0.00018147250730742902, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.389, 'grad_norm': 0.390625, 'learning_rate': 0.00018146114492996517, 'epoch': 0.11}\n",
      "{'loss': 1.4181, 'grad_norm': 0.470703125, 'learning_rate': 0.0001814497825525013, 'epoch': 0.11}\n",
      "{'loss': 1.1712, 'grad_norm': 0.328125, 'learning_rate': 0.00018143842017503742, 'epoch': 0.11}\n",
      "{'loss': 1.2787, 'grad_norm': 0.44140625, 'learning_rate': 0.00018142705779757357, 'epoch': 0.11}\n",
      "{'loss': 1.201, 'grad_norm': 0.5859375, 'learning_rate': 0.00018141569542010972, 'epoch': 0.11}\n",
      "{'loss': 1.4761, 'grad_norm': 0.376953125, 'learning_rate': 0.00018140433304264587, 'epoch': 0.11}\n",
      "{'loss': 1.3978, 'grad_norm': 0.494140625, 'learning_rate': 0.000181392970665182, 'epoch': 0.11}\n",
      "{'loss': 1.3747, 'grad_norm': 0.392578125, 'learning_rate': 0.00018138160828771812, 'epoch': 0.11}\n",
      "{'loss': 1.2069, 'grad_norm': 0.470703125, 'learning_rate': 0.00018137024591025427, 'epoch': 0.11}\n",
      "{'loss': 1.1001, 'grad_norm': 0.251953125, 'learning_rate': 0.0001813588835327904, 'epoch': 0.11}\n",
      "{'loss': 1.2902, 'grad_norm': 0.47265625, 'learning_rate': 0.00018134752115532655, 'epoch': 0.11}\n",
      "{'loss': 1.2548, 'grad_norm': 0.640625, 'learning_rate': 0.0001813361587778627, 'epoch': 0.11}\n",
      "{'loss': 1.2984, 'grad_norm': 0.4453125, 'learning_rate': 0.00018132479640039882, 'epoch': 0.11}\n",
      "{'loss': 1.3432, 'grad_norm': 0.5078125, 'learning_rate': 0.00018131343402293497, 'epoch': 0.11}\n",
      "{'loss': 1.1753, 'grad_norm': 0.2734375, 'learning_rate': 0.0001813020716454711, 'epoch': 0.11}\n",
      "{'loss': 1.4686, 'grad_norm': 0.46484375, 'learning_rate': 0.00018129070926800725, 'epoch': 0.11}\n",
      "{'loss': 1.3517, 'grad_norm': 0.8125, 'learning_rate': 0.00018127934689054337, 'epoch': 0.11}\n",
      "{'loss': 1.2996, 'grad_norm': 0.396484375, 'learning_rate': 0.00018126798451307952, 'epoch': 0.11}\n",
      "{'loss': 1.4003, 'grad_norm': 0.458984375, 'learning_rate': 0.00018125662213561568, 'epoch': 0.11}\n",
      "{'loss': 1.1044, 'grad_norm': 0.6953125, 'learning_rate': 0.0001812452597581518, 'epoch': 0.11}\n",
      "{'loss': 1.2853, 'grad_norm': 0.384765625, 'learning_rate': 0.00018123389738068795, 'epoch': 0.11}\n",
      "{'loss': 1.2334, 'grad_norm': 0.6484375, 'learning_rate': 0.00018122253500322408, 'epoch': 0.11}\n",
      "{'loss': 1.2284, 'grad_norm': 0.33203125, 'learning_rate': 0.00018121117262576023, 'epoch': 0.11}\n",
      "{'loss': 1.2183, 'grad_norm': 0.4765625, 'learning_rate': 0.00018119981024829635, 'epoch': 0.11}\n",
      "{'loss': 1.1998, 'grad_norm': 0.5078125, 'learning_rate': 0.0001811884478708325, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.229, 'grad_norm': 0.337890625, 'learning_rate': 0.00018117708549336865, 'epoch': 0.11}\n",
      "{'loss': 1.1911, 'grad_norm': 0.6796875, 'learning_rate': 0.00018116572311590478, 'epoch': 0.11}\n",
      "{'loss': 1.1853, 'grad_norm': 0.396484375, 'learning_rate': 0.00018115436073844093, 'epoch': 0.11}\n",
      "{'loss': 1.3318, 'grad_norm': 0.466796875, 'learning_rate': 0.00018114299836097705, 'epoch': 0.11}\n",
      "{'loss': 1.1536, 'grad_norm': 0.337890625, 'learning_rate': 0.0001811316359835132, 'epoch': 0.11}\n",
      "{'loss': 1.505, 'grad_norm': 0.427734375, 'learning_rate': 0.00018112027360604933, 'epoch': 0.11}\n",
      "{'loss': 1.3354, 'grad_norm': 0.63671875, 'learning_rate': 0.00018110891122858548, 'epoch': 0.11}\n",
      "{'loss': 1.3234, 'grad_norm': 0.37890625, 'learning_rate': 0.00018109754885112163, 'epoch': 0.11}\n",
      "{'loss': 1.2827, 'grad_norm': 0.416015625, 'learning_rate': 0.00018108618647365776, 'epoch': 0.11}\n",
      "{'loss': 1.263, 'grad_norm': 0.6171875, 'learning_rate': 0.0001810748240961939, 'epoch': 0.11}\n",
      "{'loss': 1.4439, 'grad_norm': 0.474609375, 'learning_rate': 0.00018106346171873003, 'epoch': 0.11}\n",
      "{'loss': 1.3758, 'grad_norm': 0.50390625, 'learning_rate': 0.00018105209934126616, 'epoch': 0.11}\n",
      "{'loss': 1.3168, 'grad_norm': 0.353515625, 'learning_rate': 0.0001810407369638023, 'epoch': 0.11}\n",
      "{'loss': 1.2888, 'grad_norm': 0.5078125, 'learning_rate': 0.00018102937458633846, 'epoch': 0.11}\n",
      "{'loss': 1.1916, 'grad_norm': 0.78125, 'learning_rate': 0.0001810180122088746, 'epoch': 0.11}\n",
      "{'loss': 1.4417, 'grad_norm': 0.51171875, 'learning_rate': 0.00018100664983141074, 'epoch': 0.11}\n",
      "{'loss': 1.2016, 'grad_norm': 0.5859375, 'learning_rate': 0.00018099528745394686, 'epoch': 0.11}\n",
      "{'loss': 1.2598, 'grad_norm': 0.396484375, 'learning_rate': 0.000180983925076483, 'epoch': 0.11}\n",
      "{'loss': 1.3316, 'grad_norm': 0.447265625, 'learning_rate': 0.00018097256269901914, 'epoch': 0.11}\n",
      "{'loss': 1.2979, 'grad_norm': 0.71875, 'learning_rate': 0.0001809612003215553, 'epoch': 0.11}\n",
      "{'loss': 1.3446, 'grad_norm': 0.421875, 'learning_rate': 0.00018094983794409144, 'epoch': 0.11}\n",
      "{'loss': 1.1965, 'grad_norm': 0.5, 'learning_rate': 0.00018093847556662756, 'epoch': 0.11}\n",
      "{'loss': 1.2581, 'grad_norm': 0.37890625, 'learning_rate': 0.00018092711318916371, 'epoch': 0.11}\n",
      "{'loss': 1.37, 'grad_norm': 0.498046875, 'learning_rate': 0.00018091575081169984, 'epoch': 0.11}\n",
      "{'loss': 1.1436, 'grad_norm': 0.91015625, 'learning_rate': 0.000180904388434236, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4155, 'grad_norm': 0.515625, 'learning_rate': 0.00018089302605677211, 'epoch': 0.11}\n",
      "{'loss': 1.1835, 'grad_norm': 0.47265625, 'learning_rate': 0.00018088166367930827, 'epoch': 0.11}\n",
      "{'loss': 1.2644, 'grad_norm': 0.423828125, 'learning_rate': 0.00018087030130184442, 'epoch': 0.11}\n",
      "{'loss': 1.3429, 'grad_norm': 0.54296875, 'learning_rate': 0.00018085893892438054, 'epoch': 0.11}\n",
      "{'loss': 1.2616, 'grad_norm': 0.73828125, 'learning_rate': 0.0001808475765469167, 'epoch': 0.11}\n",
      "{'loss': 1.4414, 'grad_norm': 0.490234375, 'learning_rate': 0.00018083621416945282, 'epoch': 0.11}\n",
      "{'loss': 1.2767, 'grad_norm': 0.48828125, 'learning_rate': 0.00018082485179198897, 'epoch': 0.11}\n",
      "{'loss': 1.2503, 'grad_norm': 0.39453125, 'learning_rate': 0.0001808134894145251, 'epoch': 0.11}\n",
      "{'loss': 1.3451, 'grad_norm': 0.703125, 'learning_rate': 0.00018080212703706124, 'epoch': 0.11}\n",
      "{'loss': 1.0463, 'grad_norm': 0.6640625, 'learning_rate': 0.0001807907646595974, 'epoch': 0.11}\n",
      "{'loss': 1.5209, 'grad_norm': 0.50390625, 'learning_rate': 0.00018077940228213352, 'epoch': 0.11}\n",
      "{'loss': 1.1532, 'grad_norm': 0.6484375, 'learning_rate': 0.00018076803990466967, 'epoch': 0.11}\n",
      "{'loss': 1.2127, 'grad_norm': 0.361328125, 'learning_rate': 0.0001807566775272058, 'epoch': 0.11}\n",
      "{'loss': 1.3915, 'grad_norm': 0.482421875, 'learning_rate': 0.00018074531514974195, 'epoch': 0.11}\n",
      "{'loss': 1.1667, 'grad_norm': 0.52734375, 'learning_rate': 0.00018073395277227807, 'epoch': 0.11}\n",
      "{'loss': 1.3898, 'grad_norm': 0.54296875, 'learning_rate': 0.00018072259039481422, 'epoch': 0.11}\n",
      "{'loss': 1.285, 'grad_norm': 0.546875, 'learning_rate': 0.00018071122801735037, 'epoch': 0.11}\n",
      "{'loss': 1.2959, 'grad_norm': 0.34765625, 'learning_rate': 0.0001806998656398865, 'epoch': 0.11}\n",
      "{'loss': 1.4249, 'grad_norm': 0.37890625, 'learning_rate': 0.00018068850326242265, 'epoch': 0.11}\n",
      "{'loss': 1.1528, 'grad_norm': 1.2890625, 'learning_rate': 0.00018067714088495877, 'epoch': 0.11}\n",
      "{'loss': 1.3116, 'grad_norm': 0.8203125, 'learning_rate': 0.0001806657785074949, 'epoch': 0.11}\n",
      "{'loss': 1.2592, 'grad_norm': 0.68359375, 'learning_rate': 0.00018065441613003105, 'epoch': 0.11}\n",
      "{'loss': 1.1912, 'grad_norm': 0.326171875, 'learning_rate': 0.0001806430537525672, 'epoch': 0.11}\n",
      "{'loss': 1.1891, 'grad_norm': 0.53515625, 'learning_rate': 0.00018063169137510335, 'epoch': 0.11}\n",
      "{'loss': 1.2343, 'grad_norm': 0.875, 'learning_rate': 0.00018062032899763948, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3358, 'grad_norm': 0.431640625, 'learning_rate': 0.0001806089666201756, 'epoch': 0.11}\n",
      "{'loss': 1.1552, 'grad_norm': 0.640625, 'learning_rate': 0.00018059760424271175, 'epoch': 0.11}\n",
      "{'loss': 1.2324, 'grad_norm': 0.400390625, 'learning_rate': 0.00018058624186524788, 'epoch': 0.11}\n",
      "{'loss': 1.2959, 'grad_norm': 0.46875, 'learning_rate': 0.00018057487948778406, 'epoch': 0.11}\n",
      "{'loss': 1.2288, 'grad_norm': 0.90234375, 'learning_rate': 0.00018056351711032018, 'epoch': 0.11}\n",
      "{'loss': 1.4718, 'grad_norm': 0.451171875, 'learning_rate': 0.0001805521547328563, 'epoch': 0.11}\n",
      "{'loss': 1.1365, 'grad_norm': 0.703125, 'learning_rate': 0.00018054079235539246, 'epoch': 0.11}\n",
      "{'loss': 1.3361, 'grad_norm': 0.443359375, 'learning_rate': 0.00018052942997792858, 'epoch': 0.11}\n",
      "{'loss': 1.3655, 'grad_norm': 0.52734375, 'learning_rate': 0.00018051806760046473, 'epoch': 0.11}\n",
      "{'loss': 1.2132, 'grad_norm': 0.484375, 'learning_rate': 0.00018050670522300086, 'epoch': 0.11}\n",
      "{'loss': 1.5244, 'grad_norm': 0.59375, 'learning_rate': 0.000180495342845537, 'epoch': 0.11}\n",
      "{'loss': 1.1948, 'grad_norm': 0.53515625, 'learning_rate': 0.00018048398046807316, 'epoch': 0.11}\n",
      "{'loss': 1.234, 'grad_norm': 0.3515625, 'learning_rate': 0.00018047261809060928, 'epoch': 0.11}\n",
      "{'loss': 1.3778, 'grad_norm': 0.447265625, 'learning_rate': 0.00018046125571314543, 'epoch': 0.11}\n",
      "{'loss': 1.084, 'grad_norm': 0.7265625, 'learning_rate': 0.00018044989333568156, 'epoch': 0.11}\n",
      "{'loss': 1.4661, 'grad_norm': 0.53125, 'learning_rate': 0.0001804385309582177, 'epoch': 0.11}\n",
      "{'loss': 1.2392, 'grad_norm': 0.67578125, 'learning_rate': 0.00018042716858075383, 'epoch': 0.11}\n",
      "{'loss': 1.1913, 'grad_norm': 0.455078125, 'learning_rate': 0.00018041580620328999, 'epoch': 0.11}\n",
      "{'loss': 1.4008, 'grad_norm': 0.53125, 'learning_rate': 0.00018040444382582614, 'epoch': 0.11}\n",
      "{'loss': 1.1704, 'grad_norm': 0.7421875, 'learning_rate': 0.00018039308144836226, 'epoch': 0.11}\n",
      "{'loss': 1.3237, 'grad_norm': 0.447265625, 'learning_rate': 0.0001803817190708984, 'epoch': 0.11}\n",
      "{'loss': 1.2923, 'grad_norm': 1.015625, 'learning_rate': 0.00018037035669343454, 'epoch': 0.11}\n",
      "{'loss': 1.2227, 'grad_norm': 0.373046875, 'learning_rate': 0.0001803589943159707, 'epoch': 0.11}\n",
      "{'loss': 1.2761, 'grad_norm': 0.455078125, 'learning_rate': 0.0001803476319385068, 'epoch': 0.11}\n",
      "{'loss': 1.1633, 'grad_norm': 0.6484375, 'learning_rate': 0.00018033626956104296, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5292, 'grad_norm': 0.439453125, 'learning_rate': 0.00018032490718357912, 'epoch': 0.11}\n",
      "{'loss': 1.1751, 'grad_norm': 0.51953125, 'learning_rate': 0.00018031354480611524, 'epoch': 0.11}\n",
      "{'loss': 1.2045, 'grad_norm': 0.35546875, 'learning_rate': 0.0001803021824286514, 'epoch': 0.11}\n",
      "{'loss': 1.3165, 'grad_norm': 0.5859375, 'learning_rate': 0.00018029082005118752, 'epoch': 0.11}\n",
      "{'loss': 1.1036, 'grad_norm': 0.765625, 'learning_rate': 0.00018027945767372364, 'epoch': 0.11}\n",
      "{'loss': 1.3057, 'grad_norm': 0.44140625, 'learning_rate': 0.00018026809529625982, 'epoch': 0.11}\n",
      "{'loss': 1.244, 'grad_norm': 0.62109375, 'learning_rate': 0.00018025673291879594, 'epoch': 0.11}\n",
      "{'loss': 1.288, 'grad_norm': 0.345703125, 'learning_rate': 0.0001802453705413321, 'epoch': 0.11}\n",
      "{'loss': 1.3474, 'grad_norm': 0.546875, 'learning_rate': 0.00018023400816386822, 'epoch': 0.11}\n",
      "{'loss': 1.1948, 'grad_norm': 0.6875, 'learning_rate': 0.00018022264578640434, 'epoch': 0.11}\n",
      "{'loss': 1.416, 'grad_norm': 0.390625, 'learning_rate': 0.0001802112834089405, 'epoch': 0.11}\n",
      "{'loss': 1.29, 'grad_norm': 0.703125, 'learning_rate': 0.00018019992103147662, 'epoch': 0.11}\n",
      "{'loss': 1.2211, 'grad_norm': 0.34375, 'learning_rate': 0.0001801885586540128, 'epoch': 0.11}\n",
      "{'loss': 1.2935, 'grad_norm': 0.46484375, 'learning_rate': 0.00018017719627654892, 'epoch': 0.11}\n",
      "{'loss': 1.1941, 'grad_norm': 0.953125, 'learning_rate': 0.00018016583389908505, 'epoch': 0.11}\n",
      "{'loss': 1.3747, 'grad_norm': 0.48828125, 'learning_rate': 0.0001801544715216212, 'epoch': 0.11}\n",
      "{'loss': 1.2999, 'grad_norm': 0.546875, 'learning_rate': 0.00018014310914415732, 'epoch': 0.11}\n",
      "{'loss': 1.2146, 'grad_norm': 0.328125, 'learning_rate': 0.00018013174676669347, 'epoch': 0.11}\n",
      "{'loss': 1.3104, 'grad_norm': 0.56640625, 'learning_rate': 0.0001801203843892296, 'epoch': 0.11}\n",
      "{'loss': 1.1531, 'grad_norm': 0.6484375, 'learning_rate': 0.00018010902201176575, 'epoch': 0.11}\n",
      "{'loss': 1.4481, 'grad_norm': 0.60546875, 'learning_rate': 0.0001800976596343019, 'epoch': 0.11}\n",
      "{'loss': 1.2318, 'grad_norm': 0.5390625, 'learning_rate': 0.00018008629725683802, 'epoch': 0.11}\n",
      "{'loss': 1.2835, 'grad_norm': 0.4296875, 'learning_rate': 0.00018007493487937418, 'epoch': 0.11}\n",
      "{'loss': 1.3215, 'grad_norm': 0.5546875, 'learning_rate': 0.0001800635725019103, 'epoch': 0.11}\n",
      "{'loss': 1.1986, 'grad_norm': 0.7734375, 'learning_rate': 0.00018005221012444645, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3845, 'grad_norm': 0.423828125, 'learning_rate': 0.00018004084774698258, 'epoch': 0.11}\n",
      "{'loss': 1.3268, 'grad_norm': 0.404296875, 'learning_rate': 0.00018002948536951873, 'epoch': 0.11}\n",
      "{'loss': 1.3475, 'grad_norm': 0.447265625, 'learning_rate': 0.00018001812299205488, 'epoch': 0.11}\n",
      "{'loss': 1.231, 'grad_norm': 0.640625, 'learning_rate': 0.000180006760614591, 'epoch': 0.11}\n",
      "{'loss': 1.1027, 'grad_norm': 1.3046875, 'learning_rate': 0.00017999539823712715, 'epoch': 0.11}\n",
      "{'loss': 1.3924, 'grad_norm': 0.404296875, 'learning_rate': 0.00017998403585966328, 'epoch': 0.11}\n",
      "{'loss': 1.1719, 'grad_norm': 0.58984375, 'learning_rate': 0.00017997267348219943, 'epoch': 0.11}\n",
      "{'loss': 1.3199, 'grad_norm': 0.318359375, 'learning_rate': 0.00017996131110473555, 'epoch': 0.11}\n",
      "{'loss': 1.3816, 'grad_norm': 0.421875, 'learning_rate': 0.0001799499487272717, 'epoch': 0.11}\n",
      "{'loss': 1.1447, 'grad_norm': 1.1015625, 'learning_rate': 0.00017993858634980786, 'epoch': 0.11}\n",
      "{'loss': 1.3271, 'grad_norm': 0.5, 'learning_rate': 0.00017992722397234398, 'epoch': 0.11}\n",
      "{'loss': 1.3084, 'grad_norm': 1.0078125, 'learning_rate': 0.00017991586159488013, 'epoch': 0.11}\n",
      "{'loss': 1.3609, 'grad_norm': 0.380859375, 'learning_rate': 0.00017990449921741626, 'epoch': 0.11}\n",
      "{'loss': 1.3139, 'grad_norm': 0.546875, 'learning_rate': 0.00017989313683995238, 'epoch': 0.11}\n",
      "{'loss': 1.1412, 'grad_norm': 0.44921875, 'learning_rate': 0.00017988177446248856, 'epoch': 0.11}\n",
      "{'loss': 1.4563, 'grad_norm': 0.3984375, 'learning_rate': 0.00017987041208502468, 'epoch': 0.11}\n",
      "{'loss': 1.2986, 'grad_norm': 0.68359375, 'learning_rate': 0.00017985904970756084, 'epoch': 0.11}\n",
      "{'loss': 1.4371, 'grad_norm': 0.369140625, 'learning_rate': 0.00017984768733009696, 'epoch': 0.11}\n",
      "{'loss': 1.3686, 'grad_norm': 0.6875, 'learning_rate': 0.00017983632495263308, 'epoch': 0.12}\n",
      "{'loss': 1.2866, 'grad_norm': 0.65234375, 'learning_rate': 0.00017982496257516924, 'epoch': 0.12}\n",
      "{'loss': 1.4474, 'grad_norm': 0.416015625, 'learning_rate': 0.00017981360019770536, 'epoch': 0.12}\n",
      "{'loss': 1.1627, 'grad_norm': 0.70703125, 'learning_rate': 0.00017980223782024154, 'epoch': 0.12}\n",
      "{'loss': 1.3194, 'grad_norm': 0.451171875, 'learning_rate': 0.00017979087544277766, 'epoch': 0.12}\n",
      "{'loss': 1.2416, 'grad_norm': 0.4609375, 'learning_rate': 0.0001797795130653138, 'epoch': 0.12}\n",
      "{'loss': 1.1064, 'grad_norm': 0.69921875, 'learning_rate': 0.00017976815068784994, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3448, 'grad_norm': 0.48828125, 'learning_rate': 0.00017975678831038606, 'epoch': 0.12}\n",
      "{'loss': 1.1635, 'grad_norm': 0.546875, 'learning_rate': 0.00017974542593292221, 'epoch': 0.12}\n",
      "{'loss': 1.2393, 'grad_norm': 0.357421875, 'learning_rate': 0.00017973406355545834, 'epoch': 0.12}\n",
      "{'loss': 1.2451, 'grad_norm': 0.59375, 'learning_rate': 0.0001797227011779945, 'epoch': 0.12}\n",
      "{'loss': 1.1865, 'grad_norm': 1.109375, 'learning_rate': 0.00017971133880053064, 'epoch': 0.12}\n",
      "{'loss': 1.2509, 'grad_norm': 0.392578125, 'learning_rate': 0.00017969997642306677, 'epoch': 0.12}\n",
      "{'loss': 1.2526, 'grad_norm': 0.67578125, 'learning_rate': 0.00017968861404560292, 'epoch': 0.12}\n",
      "{'loss': 1.3249, 'grad_norm': 0.427734375, 'learning_rate': 0.00017967725166813904, 'epoch': 0.12}\n",
      "{'loss': 1.2493, 'grad_norm': 0.46875, 'learning_rate': 0.0001796658892906752, 'epoch': 0.12}\n",
      "{'loss': 1.2706, 'grad_norm': 0.70703125, 'learning_rate': 0.00017965452691321132, 'epoch': 0.12}\n",
      "{'loss': 1.5338, 'grad_norm': 0.41796875, 'learning_rate': 0.00017964316453574747, 'epoch': 0.12}\n",
      "{'loss': 1.1015, 'grad_norm': 0.640625, 'learning_rate': 0.00017963180215828362, 'epoch': 0.12}\n",
      "{'loss': 1.3153, 'grad_norm': 0.373046875, 'learning_rate': 0.00017962043978081974, 'epoch': 0.12}\n",
      "{'loss': 1.259, 'grad_norm': 0.458984375, 'learning_rate': 0.0001796090774033559, 'epoch': 0.12}\n",
      "{'loss': 1.0046, 'grad_norm': 0.421875, 'learning_rate': 0.00017959771502589202, 'epoch': 0.12}\n",
      "{'loss': 1.4094, 'grad_norm': 0.515625, 'learning_rate': 0.00017958635264842817, 'epoch': 0.12}\n",
      "{'loss': 1.176, 'grad_norm': 0.5390625, 'learning_rate': 0.00017957499027096432, 'epoch': 0.12}\n",
      "{'loss': 1.2, 'grad_norm': 0.408203125, 'learning_rate': 0.00017956362789350045, 'epoch': 0.12}\n",
      "{'loss': 1.3492, 'grad_norm': 0.62109375, 'learning_rate': 0.0001795522655160366, 'epoch': 0.12}\n",
      "{'loss': 1.2472, 'grad_norm': 0.54296875, 'learning_rate': 0.00017954090313857272, 'epoch': 0.12}\n",
      "{'loss': 1.4132, 'grad_norm': 0.419921875, 'learning_rate': 0.00017952954076110887, 'epoch': 0.12}\n",
      "{'loss': 1.2058, 'grad_norm': 0.5546875, 'learning_rate': 0.000179518178383645, 'epoch': 0.12}\n",
      "{'loss': 1.1527, 'grad_norm': 0.4765625, 'learning_rate': 0.00017950681600618112, 'epoch': 0.12}\n",
      "{'loss': 1.3712, 'grad_norm': 0.5625, 'learning_rate': 0.0001794954536287173, 'epoch': 0.12}\n",
      "{'loss': 1.2031, 'grad_norm': 0.6015625, 'learning_rate': 0.00017948409125125343, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3846, 'grad_norm': 0.484375, 'learning_rate': 0.00017947272887378958, 'epoch': 0.12}\n",
      "{'loss': 1.2786, 'grad_norm': 0.56640625, 'learning_rate': 0.0001794613664963257, 'epoch': 0.12}\n",
      "{'loss': 1.2735, 'grad_norm': 0.349609375, 'learning_rate': 0.00017945000411886183, 'epoch': 0.12}\n",
      "{'loss': 1.2035, 'grad_norm': 0.54296875, 'learning_rate': 0.00017943864174139798, 'epoch': 0.12}\n",
      "{'loss': 1.1293, 'grad_norm': 0.890625, 'learning_rate': 0.0001794272793639341, 'epoch': 0.12}\n",
      "{'loss': 1.5313, 'grad_norm': 0.5234375, 'learning_rate': 0.00017941591698647028, 'epoch': 0.12}\n",
      "{'loss': 1.2093, 'grad_norm': 0.40625, 'learning_rate': 0.0001794045546090064, 'epoch': 0.12}\n",
      "{'loss': 1.1726, 'grad_norm': 0.423828125, 'learning_rate': 0.00017939319223154253, 'epoch': 0.12}\n",
      "{'loss': 1.3151, 'grad_norm': 0.4765625, 'learning_rate': 0.00017938182985407868, 'epoch': 0.12}\n",
      "{'loss': 1.2937, 'grad_norm': 0.95703125, 'learning_rate': 0.0001793704674766148, 'epoch': 0.12}\n",
      "{'loss': 1.551, 'grad_norm': 0.48828125, 'learning_rate': 0.00017935910509915096, 'epoch': 0.12}\n",
      "{'loss': 1.232, 'grad_norm': 0.55078125, 'learning_rate': 0.00017934774272168708, 'epoch': 0.12}\n",
      "{'loss': 1.2842, 'grad_norm': 0.419921875, 'learning_rate': 0.00017933638034422323, 'epoch': 0.12}\n",
      "{'loss': 1.3473, 'grad_norm': 0.5703125, 'learning_rate': 0.00017932501796675938, 'epoch': 0.12}\n",
      "{'loss': 1.1973, 'grad_norm': 0.75, 'learning_rate': 0.0001793136555892955, 'epoch': 0.12}\n",
      "{'loss': 1.3302, 'grad_norm': 0.41796875, 'learning_rate': 0.00017930229321183166, 'epoch': 0.12}\n",
      "{'loss': 1.3368, 'grad_norm': 0.64453125, 'learning_rate': 0.00017929093083436778, 'epoch': 0.12}\n",
      "{'loss': 1.357, 'grad_norm': 0.38671875, 'learning_rate': 0.00017927956845690393, 'epoch': 0.12}\n",
      "{'loss': 1.2927, 'grad_norm': 0.474609375, 'learning_rate': 0.00017926820607944006, 'epoch': 0.12}\n",
      "{'loss': 1.2727, 'grad_norm': 1.09375, 'learning_rate': 0.0001792568437019762, 'epoch': 0.12}\n",
      "{'loss': 1.439, 'grad_norm': 0.3828125, 'learning_rate': 0.00017924548132451236, 'epoch': 0.12}\n",
      "{'loss': 1.194, 'grad_norm': 0.45703125, 'learning_rate': 0.00017923411894704848, 'epoch': 0.12}\n",
      "{'loss': 1.0883, 'grad_norm': 0.365234375, 'learning_rate': 0.00017922275656958464, 'epoch': 0.12}\n",
      "{'loss': 1.3738, 'grad_norm': 0.54296875, 'learning_rate': 0.00017921139419212076, 'epoch': 0.12}\n",
      "{'loss': 1.2018, 'grad_norm': 0.6484375, 'learning_rate': 0.0001792000318146569, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4442, 'grad_norm': 0.435546875, 'learning_rate': 0.00017918866943719306, 'epoch': 0.12}\n",
      "{'loss': 1.2725, 'grad_norm': 0.60546875, 'learning_rate': 0.0001791773070597292, 'epoch': 0.12}\n",
      "{'loss': 1.2343, 'grad_norm': 0.34765625, 'learning_rate': 0.00017916594468226534, 'epoch': 0.12}\n",
      "{'loss': 1.262, 'grad_norm': 0.455078125, 'learning_rate': 0.00017915458230480146, 'epoch': 0.12}\n",
      "{'loss': 1.2874, 'grad_norm': 0.828125, 'learning_rate': 0.00017914321992733761, 'epoch': 0.12}\n",
      "{'loss': 1.3284, 'grad_norm': 0.49609375, 'learning_rate': 0.00017913185754987374, 'epoch': 0.12}\n",
      "{'loss': 1.2457, 'grad_norm': 0.69140625, 'learning_rate': 0.00017912049517240986, 'epoch': 0.12}\n",
      "{'loss': 1.198, 'grad_norm': 0.375, 'learning_rate': 0.00017910913279494604, 'epoch': 0.12}\n",
      "{'loss': 1.2941, 'grad_norm': 0.515625, 'learning_rate': 0.00017909777041748217, 'epoch': 0.12}\n",
      "{'loss': 1.2839, 'grad_norm': 0.84375, 'learning_rate': 0.00017908640804001832, 'epoch': 0.12}\n",
      "{'loss': 1.3772, 'grad_norm': 0.490234375, 'learning_rate': 0.00017907504566255444, 'epoch': 0.12}\n",
      "{'loss': 1.1472, 'grad_norm': 0.62890625, 'learning_rate': 0.00017906368328509057, 'epoch': 0.12}\n",
      "{'loss': 1.3166, 'grad_norm': 0.421875, 'learning_rate': 0.00017905232090762672, 'epoch': 0.12}\n",
      "{'loss': 1.3376, 'grad_norm': 0.515625, 'learning_rate': 0.00017904095853016284, 'epoch': 0.12}\n",
      "{'loss': 1.1539, 'grad_norm': 1.296875, 'learning_rate': 0.00017902959615269902, 'epoch': 0.12}\n",
      "{'loss': 1.4056, 'grad_norm': 0.451171875, 'learning_rate': 0.00017901823377523514, 'epoch': 0.12}\n",
      "{'loss': 1.2651, 'grad_norm': 0.82421875, 'learning_rate': 0.00017900687139777127, 'epoch': 0.12}\n",
      "{'loss': 1.305, 'grad_norm': 0.365234375, 'learning_rate': 0.00017899550902030742, 'epoch': 0.12}\n",
      "{'loss': 1.433, 'grad_norm': 0.5625, 'learning_rate': 0.00017898414664284354, 'epoch': 0.12}\n",
      "{'loss': 1.1309, 'grad_norm': 0.94921875, 'learning_rate': 0.0001789727842653797, 'epoch': 0.12}\n",
      "{'loss': 1.4619, 'grad_norm': 0.546875, 'learning_rate': 0.00017896142188791582, 'epoch': 0.12}\n",
      "{'loss': 1.2291, 'grad_norm': 0.80859375, 'learning_rate': 0.00017895005951045197, 'epoch': 0.12}\n",
      "{'loss': 1.2331, 'grad_norm': 0.392578125, 'learning_rate': 0.00017893869713298812, 'epoch': 0.12}\n",
      "{'loss': 1.2639, 'grad_norm': 0.474609375, 'learning_rate': 0.00017892733475552425, 'epoch': 0.12}\n",
      "{'loss': 1.1502, 'grad_norm': 1.03125, 'learning_rate': 0.0001789159723780604, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3738, 'grad_norm': 0.4296875, 'learning_rate': 0.00017890461000059652, 'epoch': 0.12}\n",
      "{'loss': 1.2288, 'grad_norm': 0.57421875, 'learning_rate': 0.00017889324762313267, 'epoch': 0.12}\n",
      "{'loss': 1.3723, 'grad_norm': 0.421875, 'learning_rate': 0.00017888188524566883, 'epoch': 0.12}\n",
      "{'loss': 1.3049, 'grad_norm': 0.52734375, 'learning_rate': 0.00017887052286820495, 'epoch': 0.12}\n",
      "{'loss': 1.0192, 'grad_norm': 0.921875, 'learning_rate': 0.0001788591604907411, 'epoch': 0.12}\n",
      "{'loss': 1.4162, 'grad_norm': 0.56640625, 'learning_rate': 0.00017884779811327723, 'epoch': 0.12}\n",
      "{'loss': 1.2847, 'grad_norm': 0.43359375, 'learning_rate': 0.00017883643573581338, 'epoch': 0.12}\n",
      "{'loss': 1.2389, 'grad_norm': 0.4140625, 'learning_rate': 0.0001788250733583495, 'epoch': 0.12}\n",
      "{'loss': 1.3675, 'grad_norm': 0.72265625, 'learning_rate': 0.00017881371098088565, 'epoch': 0.12}\n",
      "{'loss': 1.209, 'grad_norm': 0.671875, 'learning_rate': 0.0001788023486034218, 'epoch': 0.12}\n",
      "{'loss': 1.4238, 'grad_norm': 0.45703125, 'learning_rate': 0.00017879098622595793, 'epoch': 0.12}\n",
      "{'loss': 1.2319, 'grad_norm': 0.55078125, 'learning_rate': 0.00017877962384849408, 'epoch': 0.12}\n",
      "{'loss': 1.4108, 'grad_norm': 0.451171875, 'learning_rate': 0.0001787682614710302, 'epoch': 0.12}\n",
      "{'loss': 1.374, 'grad_norm': 0.50390625, 'learning_rate': 0.00017875689909356636, 'epoch': 0.12}\n",
      "{'loss': 1.123, 'grad_norm': 2.09375, 'learning_rate': 0.00017874553671610248, 'epoch': 0.12}\n",
      "{'loss': 1.4895, 'grad_norm': 0.55078125, 'learning_rate': 0.0001787341743386386, 'epoch': 0.12}\n",
      "{'loss': 1.216, 'grad_norm': 0.6171875, 'learning_rate': 0.00017872281196117478, 'epoch': 0.12}\n",
      "{'loss': 1.2734, 'grad_norm': 0.412109375, 'learning_rate': 0.0001787114495837109, 'epoch': 0.12}\n",
      "{'loss': 1.4237, 'grad_norm': 0.458984375, 'learning_rate': 0.00017870008720624706, 'epoch': 0.12}\n",
      "{'loss': 1.2469, 'grad_norm': 0.54296875, 'learning_rate': 0.00017868872482878318, 'epoch': 0.12}\n",
      "{'loss': 1.3813, 'grad_norm': 0.455078125, 'learning_rate': 0.0001786773624513193, 'epoch': 0.12}\n",
      "{'loss': 1.2836, 'grad_norm': 0.58984375, 'learning_rate': 0.00017866600007385546, 'epoch': 0.12}\n",
      "{'loss': 1.1776, 'grad_norm': 0.451171875, 'learning_rate': 0.00017865463769639158, 'epoch': 0.12}\n",
      "{'loss': 1.2168, 'grad_norm': 0.54296875, 'learning_rate': 0.00017864327531892776, 'epoch': 0.12}\n",
      "{'loss': 1.0948, 'grad_norm': 1.2421875, 'learning_rate': 0.00017863191294146389, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.439, 'grad_norm': 0.390625, 'learning_rate': 0.000178620550564, 'epoch': 0.12}\n",
      "{'loss': 1.2316, 'grad_norm': 0.52734375, 'learning_rate': 0.00017860918818653616, 'epoch': 0.12}\n",
      "{'loss': 1.4314, 'grad_norm': 0.314453125, 'learning_rate': 0.00017859782580907229, 'epoch': 0.12}\n",
      "{'loss': 1.2794, 'grad_norm': 0.51171875, 'learning_rate': 0.00017858646343160844, 'epoch': 0.12}\n",
      "{'loss': 1.2407, 'grad_norm': 0.8671875, 'learning_rate': 0.00017857510105414456, 'epoch': 0.12}\n",
      "{'loss': 1.4601, 'grad_norm': 0.482421875, 'learning_rate': 0.0001785637386766807, 'epoch': 0.12}\n",
      "{'loss': 1.2862, 'grad_norm': 0.66796875, 'learning_rate': 0.00017855237629921686, 'epoch': 0.12}\n",
      "{'loss': 1.3481, 'grad_norm': 0.37890625, 'learning_rate': 0.000178541013921753, 'epoch': 0.12}\n",
      "{'loss': 1.3491, 'grad_norm': 0.44921875, 'learning_rate': 0.00017852965154428914, 'epoch': 0.12}\n",
      "{'loss': 1.1995, 'grad_norm': 0.7578125, 'learning_rate': 0.00017851828916682526, 'epoch': 0.12}\n",
      "{'loss': 1.5001, 'grad_norm': 0.65234375, 'learning_rate': 0.00017850692678936142, 'epoch': 0.12}\n",
      "{'loss': 1.2009, 'grad_norm': 0.60546875, 'learning_rate': 0.00017849556441189757, 'epoch': 0.12}\n",
      "{'loss': 1.2725, 'grad_norm': 0.333984375, 'learning_rate': 0.0001784842020344337, 'epoch': 0.12}\n",
      "{'loss': 1.3939, 'grad_norm': 0.5078125, 'learning_rate': 0.00017847283965696984, 'epoch': 0.12}\n",
      "{'loss': 1.2125, 'grad_norm': 0.57421875, 'learning_rate': 0.00017846147727950597, 'epoch': 0.12}\n",
      "{'loss': 1.4687, 'grad_norm': 0.4296875, 'learning_rate': 0.00017845011490204212, 'epoch': 0.12}\n",
      "{'loss': 1.1868, 'grad_norm': 0.5703125, 'learning_rate': 0.00017843875252457824, 'epoch': 0.12}\n",
      "{'loss': 1.2581, 'grad_norm': 0.40625, 'learning_rate': 0.0001784273901471144, 'epoch': 0.12}\n",
      "{'loss': 1.3582, 'grad_norm': 0.423828125, 'learning_rate': 0.00017841602776965055, 'epoch': 0.12}\n",
      "{'loss': 1.1767, 'grad_norm': 0.72265625, 'learning_rate': 0.00017840466539218667, 'epoch': 0.12}\n",
      "{'loss': 1.3708, 'grad_norm': 0.4609375, 'learning_rate': 0.00017839330301472282, 'epoch': 0.12}\n",
      "{'loss': 1.2154, 'grad_norm': 0.53125, 'learning_rate': 0.00017838194063725895, 'epoch': 0.12}\n",
      "{'loss': 1.2302, 'grad_norm': 0.451171875, 'learning_rate': 0.0001783705782597951, 'epoch': 0.12}\n",
      "{'loss': 1.2154, 'grad_norm': 0.4765625, 'learning_rate': 0.00017835921588233122, 'epoch': 0.12}\n",
      "{'loss': 1.3027, 'grad_norm': 1.640625, 'learning_rate': 0.00017834785350486735, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5388, 'grad_norm': 0.65234375, 'learning_rate': 0.00017833649112740352, 'epoch': 0.12}\n",
      "{'loss': 1.2043, 'grad_norm': 0.60546875, 'learning_rate': 0.00017832512874993965, 'epoch': 0.12}\n",
      "{'loss': 1.1888, 'grad_norm': 0.48828125, 'learning_rate': 0.0001783137663724758, 'epoch': 0.12}\n",
      "{'loss': 1.1959, 'grad_norm': 0.56640625, 'learning_rate': 0.00017830240399501192, 'epoch': 0.12}\n",
      "{'loss': 1.3619, 'grad_norm': 0.90625, 'learning_rate': 0.00017829104161754805, 'epoch': 0.12}\n",
      "{'loss': 1.4302, 'grad_norm': 0.470703125, 'learning_rate': 0.0001782796792400842, 'epoch': 0.12}\n",
      "{'loss': 1.2201, 'grad_norm': 0.478515625, 'learning_rate': 0.00017826831686262032, 'epoch': 0.12}\n",
      "{'loss': 1.2174, 'grad_norm': 0.33984375, 'learning_rate': 0.0001782569544851565, 'epoch': 0.12}\n",
      "{'loss': 1.3618, 'grad_norm': 0.498046875, 'learning_rate': 0.00017824559210769263, 'epoch': 0.12}\n",
      "{'loss': 1.0557, 'grad_norm': 0.44921875, 'learning_rate': 0.00017823422973022875, 'epoch': 0.12}\n",
      "{'loss': 1.4926, 'grad_norm': 0.5078125, 'learning_rate': 0.0001782228673527649, 'epoch': 0.12}\n",
      "{'loss': 1.1474, 'grad_norm': 0.74609375, 'learning_rate': 0.00017821150497530103, 'epoch': 0.12}\n",
      "{'loss': 1.2243, 'grad_norm': 0.36328125, 'learning_rate': 0.00017820014259783718, 'epoch': 0.12}\n",
      "{'loss': 1.2493, 'grad_norm': 0.5625, 'learning_rate': 0.00017818878022037333, 'epoch': 0.12}\n",
      "{'loss': 1.0849, 'grad_norm': 0.25390625, 'learning_rate': 0.00017817741784290945, 'epoch': 0.12}\n",
      "{'loss': 1.3664, 'grad_norm': 0.431640625, 'learning_rate': 0.0001781660554654456, 'epoch': 0.12}\n",
      "{'loss': 1.2458, 'grad_norm': 0.474609375, 'learning_rate': 0.00017815469308798173, 'epoch': 0.12}\n",
      "{'loss': 1.3504, 'grad_norm': 0.373046875, 'learning_rate': 0.00017814333071051788, 'epoch': 0.12}\n",
      "{'loss': 1.3591, 'grad_norm': 0.515625, 'learning_rate': 0.000178131968333054, 'epoch': 0.12}\n",
      "{'loss': 1.1644, 'grad_norm': 0.400390625, 'learning_rate': 0.00017812060595559016, 'epoch': 0.12}\n",
      "{'loss': 1.4684, 'grad_norm': 0.455078125, 'learning_rate': 0.0001781092435781263, 'epoch': 0.12}\n",
      "{'loss': 1.25, 'grad_norm': 0.77734375, 'learning_rate': 0.00017809788120066243, 'epoch': 0.12}\n",
      "{'loss': 1.3197, 'grad_norm': 0.41796875, 'learning_rate': 0.00017808651882319858, 'epoch': 0.12}\n",
      "{'loss': 1.3992, 'grad_norm': 0.52734375, 'learning_rate': 0.0001780751564457347, 'epoch': 0.12}\n",
      "{'loss': 1.2323, 'grad_norm': 0.82421875, 'learning_rate': 0.00017806379406827086, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.478, 'grad_norm': 0.466796875, 'learning_rate': 0.00017805243169080698, 'epoch': 0.12}\n",
      "{'loss': 1.1881, 'grad_norm': 0.60546875, 'learning_rate': 0.00017804106931334314, 'epoch': 0.12}\n",
      "{'loss': 1.3247, 'grad_norm': 0.4140625, 'learning_rate': 0.0001780297069358793, 'epoch': 0.12}\n",
      "{'loss': 1.2284, 'grad_norm': 0.5, 'learning_rate': 0.0001780183445584154, 'epoch': 0.12}\n",
      "{'loss': 1.0334, 'grad_norm': 0.69921875, 'learning_rate': 0.00017800698218095156, 'epoch': 0.12}\n",
      "{'loss': 1.3949, 'grad_norm': 0.384765625, 'learning_rate': 0.0001779956198034877, 'epoch': 0.12}\n",
      "{'loss': 1.2848, 'grad_norm': 0.5859375, 'learning_rate': 0.00017798425742602384, 'epoch': 0.12}\n",
      "{'loss': 1.3495, 'grad_norm': 0.39453125, 'learning_rate': 0.00017797289504855996, 'epoch': 0.12}\n",
      "{'loss': 1.2785, 'grad_norm': 0.46875, 'learning_rate': 0.0001779615326710961, 'epoch': 0.12}\n",
      "{'loss': 1.1435, 'grad_norm': 0.59765625, 'learning_rate': 0.00017795017029363227, 'epoch': 0.12}\n",
      "{'loss': 1.5202, 'grad_norm': 0.419921875, 'learning_rate': 0.0001779388079161684, 'epoch': 0.12}\n",
      "{'loss': 1.3011, 'grad_norm': 0.62890625, 'learning_rate': 0.00017792744553870454, 'epoch': 0.12}\n",
      "{'loss': 1.269, 'grad_norm': 0.396484375, 'learning_rate': 0.00017791608316124067, 'epoch': 0.12}\n",
      "{'loss': 1.3617, 'grad_norm': 0.4921875, 'learning_rate': 0.0001779047207837768, 'epoch': 0.12}\n",
      "{'loss': 1.323, 'grad_norm': 0.8125, 'learning_rate': 0.00017789335840631294, 'epoch': 0.12}\n",
      "{'loss': 1.3832, 'grad_norm': 0.357421875, 'learning_rate': 0.00017788199602884907, 'epoch': 0.12}\n",
      "{'loss': 1.1948, 'grad_norm': 0.5703125, 'learning_rate': 0.00017787063365138524, 'epoch': 0.12}\n",
      "{'loss': 1.1776, 'grad_norm': 0.373046875, 'learning_rate': 0.00017785927127392137, 'epoch': 0.12}\n",
      "{'loss': 1.284, 'grad_norm': 0.484375, 'learning_rate': 0.0001778479088964575, 'epoch': 0.12}\n",
      "{'loss': 1.2081, 'grad_norm': 0.75, 'learning_rate': 0.00017783654651899364, 'epoch': 0.12}\n",
      "{'loss': 1.4223, 'grad_norm': 0.4375, 'learning_rate': 0.00017782518414152977, 'epoch': 0.12}\n",
      "{'loss': 1.2906, 'grad_norm': 0.5546875, 'learning_rate': 0.00017781382176406592, 'epoch': 0.12}\n",
      "{'loss': 1.3482, 'grad_norm': 0.365234375, 'learning_rate': 0.00017780245938660207, 'epoch': 0.12}\n",
      "{'loss': 1.2897, 'grad_norm': 0.546875, 'learning_rate': 0.0001777910970091382, 'epoch': 0.12}\n",
      "{'loss': 1.2743, 'grad_norm': 0.6171875, 'learning_rate': 0.00017777973463167435, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3182, 'grad_norm': 0.56640625, 'learning_rate': 0.00017776837225421047, 'epoch': 0.12}\n",
      "{'loss': 1.1485, 'grad_norm': 0.5, 'learning_rate': 0.00017775700987674662, 'epoch': 0.12}\n",
      "{'loss': 1.2892, 'grad_norm': 0.48828125, 'learning_rate': 0.00017774564749928275, 'epoch': 0.12}\n",
      "{'loss': 1.4199, 'grad_norm': 0.58984375, 'learning_rate': 0.0001777342851218189, 'epoch': 0.12}\n",
      "{'loss': 1.1218, 'grad_norm': 1.5390625, 'learning_rate': 0.00017772292274435505, 'epoch': 0.12}\n",
      "{'loss': 1.6259, 'grad_norm': 0.419921875, 'learning_rate': 0.00017771156036689117, 'epoch': 0.12}\n",
      "{'loss': 1.1909, 'grad_norm': 0.482421875, 'learning_rate': 0.00017770019798942733, 'epoch': 0.12}\n",
      "{'loss': 1.2842, 'grad_norm': 0.349609375, 'learning_rate': 0.00017768883561196345, 'epoch': 0.12}\n",
      "{'loss': 1.317, 'grad_norm': 0.439453125, 'learning_rate': 0.0001776774732344996, 'epoch': 0.12}\n",
      "{'loss': 1.1769, 'grad_norm': 0.8515625, 'learning_rate': 0.00017766611085703573, 'epoch': 0.12}\n",
      "{'loss': 1.4654, 'grad_norm': 0.5, 'learning_rate': 0.00017765474847957188, 'epoch': 0.12}\n",
      "{'loss': 1.284, 'grad_norm': 0.70703125, 'learning_rate': 0.00017764338610210803, 'epoch': 0.12}\n",
      "{'loss': 1.2588, 'grad_norm': 0.40625, 'learning_rate': 0.00017763202372464415, 'epoch': 0.12}\n",
      "{'loss': 1.3541, 'grad_norm': 0.59765625, 'learning_rate': 0.0001776206613471803, 'epoch': 0.12}\n",
      "{'loss': 1.1353, 'grad_norm': 0.9296875, 'learning_rate': 0.00017760929896971643, 'epoch': 0.12}\n",
      "{'loss': 1.3763, 'grad_norm': 0.375, 'learning_rate': 0.00017759793659225258, 'epoch': 0.12}\n",
      "{'loss': 1.2585, 'grad_norm': 0.56640625, 'learning_rate': 0.0001775865742147887, 'epoch': 0.12}\n",
      "{'loss': 1.3653, 'grad_norm': 0.33203125, 'learning_rate': 0.00017757521183732483, 'epoch': 0.12}\n",
      "{'loss': 1.3316, 'grad_norm': 0.50390625, 'learning_rate': 0.000177563849459861, 'epoch': 0.12}\n",
      "{'loss': 1.1236, 'grad_norm': 0.51953125, 'learning_rate': 0.00017755248708239713, 'epoch': 0.12}\n",
      "{'loss': 1.3986, 'grad_norm': 0.37890625, 'learning_rate': 0.00017754112470493328, 'epoch': 0.13}\n",
      "{'loss': 1.2599, 'grad_norm': 0.5625, 'learning_rate': 0.0001775297623274694, 'epoch': 0.13}\n",
      "{'loss': 1.2702, 'grad_norm': 0.373046875, 'learning_rate': 0.00017751839995000553, 'epoch': 0.13}\n",
      "{'loss': 1.2213, 'grad_norm': 0.53125, 'learning_rate': 0.00017750703757254168, 'epoch': 0.13}\n",
      "{'loss': 1.0618, 'grad_norm': 0.7890625, 'learning_rate': 0.00017749567519507783, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3709, 'grad_norm': 0.408203125, 'learning_rate': 0.00017748431281761399, 'epoch': 0.13}\n",
      "{'loss': 1.2462, 'grad_norm': 0.7109375, 'learning_rate': 0.0001774729504401501, 'epoch': 0.13}\n",
      "{'loss': 1.1796, 'grad_norm': 0.4609375, 'learning_rate': 0.00017746158806268623, 'epoch': 0.13}\n",
      "{'loss': 1.3572, 'grad_norm': 0.50390625, 'learning_rate': 0.00017745022568522239, 'epoch': 0.13}\n",
      "{'loss': 1.1572, 'grad_norm': 0.72265625, 'learning_rate': 0.0001774388633077585, 'epoch': 0.13}\n",
      "{'loss': 1.3409, 'grad_norm': 0.41796875, 'learning_rate': 0.00017742750093029466, 'epoch': 0.13}\n",
      "{'loss': 1.1541, 'grad_norm': 0.703125, 'learning_rate': 0.0001774161385528308, 'epoch': 0.13}\n",
      "{'loss': 1.3077, 'grad_norm': 0.36328125, 'learning_rate': 0.00017740477617536694, 'epoch': 0.13}\n",
      "{'loss': 1.2703, 'grad_norm': 0.4921875, 'learning_rate': 0.0001773934137979031, 'epoch': 0.13}\n",
      "{'loss': 1.1574, 'grad_norm': 0.76953125, 'learning_rate': 0.0001773820514204392, 'epoch': 0.13}\n",
      "{'loss': 1.4585, 'grad_norm': 0.484375, 'learning_rate': 0.00017737068904297536, 'epoch': 0.13}\n",
      "{'loss': 1.3029, 'grad_norm': 0.96875, 'learning_rate': 0.0001773593266655115, 'epoch': 0.13}\n",
      "{'loss': 1.4159, 'grad_norm': 0.365234375, 'learning_rate': 0.00017734796428804764, 'epoch': 0.13}\n",
      "{'loss': 1.2262, 'grad_norm': 0.54296875, 'learning_rate': 0.0001773366019105838, 'epoch': 0.13}\n",
      "{'loss': 1.1303, 'grad_norm': 0.8046875, 'learning_rate': 0.00017732523953311992, 'epoch': 0.13}\n",
      "{'loss': 1.3129, 'grad_norm': 0.69140625, 'learning_rate': 0.00017731387715565607, 'epoch': 0.13}\n",
      "{'loss': 1.2197, 'grad_norm': 0.54296875, 'learning_rate': 0.0001773025147781922, 'epoch': 0.13}\n",
      "{'loss': 1.2505, 'grad_norm': 0.5625, 'learning_rate': 0.00017729115240072834, 'epoch': 0.13}\n",
      "{'loss': 1.3075, 'grad_norm': 0.45703125, 'learning_rate': 0.00017727979002326447, 'epoch': 0.13}\n",
      "{'loss': 1.1369, 'grad_norm': 0.78515625, 'learning_rate': 0.00017726842764580062, 'epoch': 0.13}\n",
      "{'loss': 1.4334, 'grad_norm': 0.484375, 'learning_rate': 0.00017725706526833677, 'epoch': 0.13}\n",
      "{'loss': 1.2478, 'grad_norm': 0.63671875, 'learning_rate': 0.0001772457028908729, 'epoch': 0.13}\n",
      "{'loss': 1.2221, 'grad_norm': 0.435546875, 'learning_rate': 0.00017723434051340905, 'epoch': 0.13}\n",
      "{'loss': 1.3626, 'grad_norm': 0.404296875, 'learning_rate': 0.00017722297813594517, 'epoch': 0.13}\n",
      "{'loss': 1.2001, 'grad_norm': 0.69921875, 'learning_rate': 0.00017721161575848132, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3748, 'grad_norm': 0.451171875, 'learning_rate': 0.00017720025338101745, 'epoch': 0.13}\n",
      "{'loss': 1.2342, 'grad_norm': 0.55859375, 'learning_rate': 0.00017718889100355357, 'epoch': 0.13}\n",
      "{'loss': 1.2998, 'grad_norm': 0.462890625, 'learning_rate': 0.00017717752862608975, 'epoch': 0.13}\n",
      "{'loss': 1.2125, 'grad_norm': 0.390625, 'learning_rate': 0.00017716616624862587, 'epoch': 0.13}\n",
      "{'loss': 1.1803, 'grad_norm': 0.58984375, 'learning_rate': 0.00017715480387116202, 'epoch': 0.13}\n",
      "{'loss': 1.3752, 'grad_norm': 0.447265625, 'learning_rate': 0.00017714344149369815, 'epoch': 0.13}\n",
      "{'loss': 1.1603, 'grad_norm': 0.447265625, 'learning_rate': 0.00017713207911623427, 'epoch': 0.13}\n",
      "{'loss': 1.349, 'grad_norm': 0.39453125, 'learning_rate': 0.00017712071673877042, 'epoch': 0.13}\n",
      "{'loss': 1.3866, 'grad_norm': 0.640625, 'learning_rate': 0.00017710935436130658, 'epoch': 0.13}\n",
      "{'loss': 1.1788, 'grad_norm': 0.66796875, 'learning_rate': 0.00017709799198384273, 'epoch': 0.13}\n",
      "{'loss': 1.386, 'grad_norm': 0.42578125, 'learning_rate': 0.00017708662960637885, 'epoch': 0.13}\n",
      "{'loss': 1.2096, 'grad_norm': 0.81640625, 'learning_rate': 0.00017707526722891498, 'epoch': 0.13}\n",
      "{'loss': 1.2855, 'grad_norm': 0.419921875, 'learning_rate': 0.00017706390485145113, 'epoch': 0.13}\n",
      "{'loss': 1.3013, 'grad_norm': 0.53125, 'learning_rate': 0.00017705254247398725, 'epoch': 0.13}\n",
      "{'loss': 1.2264, 'grad_norm': 0.59375, 'learning_rate': 0.0001770411800965234, 'epoch': 0.13}\n",
      "{'loss': 1.4291, 'grad_norm': 0.36328125, 'learning_rate': 0.00017702981771905955, 'epoch': 0.13}\n",
      "{'loss': 1.4078, 'grad_norm': 0.54296875, 'learning_rate': 0.00017701845534159568, 'epoch': 0.13}\n",
      "{'loss': 1.3568, 'grad_norm': 0.421875, 'learning_rate': 0.00017700709296413183, 'epoch': 0.13}\n",
      "{'loss': 1.3199, 'grad_norm': 0.455078125, 'learning_rate': 0.00017699573058666795, 'epoch': 0.13}\n",
      "{'loss': 1.2585, 'grad_norm': 0.9765625, 'learning_rate': 0.0001769843682092041, 'epoch': 0.13}\n",
      "{'loss': 1.2679, 'grad_norm': 0.5234375, 'learning_rate': 0.00017697300583174023, 'epoch': 0.13}\n",
      "{'loss': 1.1153, 'grad_norm': 0.5859375, 'learning_rate': 0.00017696164345427638, 'epoch': 0.13}\n",
      "{'loss': 1.2304, 'grad_norm': 0.373046875, 'learning_rate': 0.00017695028107681253, 'epoch': 0.13}\n",
      "{'loss': 1.3928, 'grad_norm': 0.54296875, 'learning_rate': 0.00017693891869934866, 'epoch': 0.13}\n",
      "{'loss': 1.1588, 'grad_norm': 0.25390625, 'learning_rate': 0.0001769275563218848, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4325, 'grad_norm': 0.484375, 'learning_rate': 0.00017691619394442093, 'epoch': 0.13}\n",
      "{'loss': 1.22, 'grad_norm': 0.48046875, 'learning_rate': 0.00017690483156695708, 'epoch': 0.13}\n",
      "{'loss': 1.426, 'grad_norm': 0.34765625, 'learning_rate': 0.0001768934691894932, 'epoch': 0.13}\n",
      "{'loss': 1.2661, 'grad_norm': 0.484375, 'learning_rate': 0.00017688210681202936, 'epoch': 0.13}\n",
      "{'loss': 1.0998, 'grad_norm': 0.859375, 'learning_rate': 0.0001768707444345655, 'epoch': 0.13}\n",
      "{'loss': 1.4371, 'grad_norm': 0.373046875, 'learning_rate': 0.00017685938205710164, 'epoch': 0.13}\n",
      "{'loss': 1.2512, 'grad_norm': 0.578125, 'learning_rate': 0.0001768480196796378, 'epoch': 0.13}\n",
      "{'loss': 1.2287, 'grad_norm': 0.484375, 'learning_rate': 0.0001768366573021739, 'epoch': 0.13}\n",
      "{'loss': 1.2277, 'grad_norm': 1.390625, 'learning_rate': 0.00017682529492471006, 'epoch': 0.13}\n",
      "{'loss': 1.1727, 'grad_norm': 0.451171875, 'learning_rate': 0.0001768139325472462, 'epoch': 0.13}\n",
      "{'loss': 1.3811, 'grad_norm': 0.494140625, 'learning_rate': 0.00017680257016978234, 'epoch': 0.13}\n",
      "{'loss': 1.1509, 'grad_norm': 0.45703125, 'learning_rate': 0.0001767912077923185, 'epoch': 0.13}\n",
      "{'loss': 1.333, 'grad_norm': 0.341796875, 'learning_rate': 0.00017677984541485461, 'epoch': 0.13}\n",
      "{'loss': 1.3916, 'grad_norm': 0.474609375, 'learning_rate': 0.00017676848303739077, 'epoch': 0.13}\n",
      "{'loss': 1.1025, 'grad_norm': 0.74609375, 'learning_rate': 0.0001767571206599269, 'epoch': 0.13}\n",
      "{'loss': 1.4411, 'grad_norm': 0.58984375, 'learning_rate': 0.00017674575828246301, 'epoch': 0.13}\n",
      "{'loss': 1.2571, 'grad_norm': 0.76953125, 'learning_rate': 0.00017673439590499917, 'epoch': 0.13}\n",
      "{'loss': 1.2213, 'grad_norm': 0.431640625, 'learning_rate': 0.00017672303352753532, 'epoch': 0.13}\n",
      "{'loss': 1.354, 'grad_norm': 0.474609375, 'learning_rate': 0.00017671167115007147, 'epoch': 0.13}\n",
      "{'loss': 1.1009, 'grad_norm': 0.67578125, 'learning_rate': 0.0001767003087726076, 'epoch': 0.13}\n",
      "{'loss': 1.3435, 'grad_norm': 0.404296875, 'learning_rate': 0.00017668894639514372, 'epoch': 0.13}\n",
      "{'loss': 1.3131, 'grad_norm': 0.5, 'learning_rate': 0.00017667758401767987, 'epoch': 0.13}\n",
      "{'loss': 1.1437, 'grad_norm': 0.404296875, 'learning_rate': 0.000176666221640216, 'epoch': 0.13}\n",
      "{'loss': 1.3566, 'grad_norm': 0.63671875, 'learning_rate': 0.00017665485926275214, 'epoch': 0.13}\n",
      "{'loss': 1.0856, 'grad_norm': 0.91796875, 'learning_rate': 0.0001766434968852883, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3646, 'grad_norm': 0.46875, 'learning_rate': 0.00017663213450782442, 'epoch': 0.13}\n",
      "{'loss': 1.2709, 'grad_norm': 0.609375, 'learning_rate': 0.00017662077213036057, 'epoch': 0.13}\n",
      "{'loss': 1.2635, 'grad_norm': 0.359375, 'learning_rate': 0.0001766094097528967, 'epoch': 0.13}\n",
      "{'loss': 1.3783, 'grad_norm': 0.53125, 'learning_rate': 0.00017659804737543285, 'epoch': 0.13}\n",
      "{'loss': 1.0729, 'grad_norm': 0.71484375, 'learning_rate': 0.00017658668499796897, 'epoch': 0.13}\n",
      "{'loss': 1.4213, 'grad_norm': 0.431640625, 'learning_rate': 0.00017657532262050512, 'epoch': 0.13}\n",
      "{'loss': 1.2384, 'grad_norm': 0.68359375, 'learning_rate': 0.00017656396024304127, 'epoch': 0.13}\n",
      "{'loss': 1.1982, 'grad_norm': 0.458984375, 'learning_rate': 0.0001765525978655774, 'epoch': 0.13}\n",
      "{'loss': 1.3939, 'grad_norm': 0.486328125, 'learning_rate': 0.00017654123548811355, 'epoch': 0.13}\n",
      "{'loss': 1.1539, 'grad_norm': 0.4609375, 'learning_rate': 0.00017652987311064967, 'epoch': 0.13}\n",
      "{'loss': 1.3476, 'grad_norm': 0.482421875, 'learning_rate': 0.00017651851073318583, 'epoch': 0.13}\n",
      "{'loss': 1.2585, 'grad_norm': 0.7265625, 'learning_rate': 0.00017650714835572195, 'epoch': 0.13}\n",
      "{'loss': 1.3132, 'grad_norm': 0.357421875, 'learning_rate': 0.0001764957859782581, 'epoch': 0.13}\n",
      "{'loss': 1.3405, 'grad_norm': 0.447265625, 'learning_rate': 0.00017648442360079425, 'epoch': 0.13}\n",
      "{'loss': 1.1578, 'grad_norm': 1.3359375, 'learning_rate': 0.00017647306122333038, 'epoch': 0.13}\n",
      "{'loss': 1.4405, 'grad_norm': 0.57421875, 'learning_rate': 0.00017646169884586653, 'epoch': 0.13}\n",
      "{'loss': 1.1409, 'grad_norm': 0.486328125, 'learning_rate': 0.00017645033646840265, 'epoch': 0.13}\n",
      "{'loss': 1.2719, 'grad_norm': 0.375, 'learning_rate': 0.0001764389740909388, 'epoch': 0.13}\n",
      "{'loss': 1.3339, 'grad_norm': 0.51171875, 'learning_rate': 0.00017642761171347493, 'epoch': 0.13}\n",
      "{'loss': 1.1515, 'grad_norm': 0.470703125, 'learning_rate': 0.00017641624933601108, 'epoch': 0.13}\n",
      "{'loss': 1.342, 'grad_norm': 0.46484375, 'learning_rate': 0.00017640488695854723, 'epoch': 0.13}\n",
      "{'loss': 1.3552, 'grad_norm': 0.7734375, 'learning_rate': 0.00017639352458108336, 'epoch': 0.13}\n",
      "{'loss': 1.1664, 'grad_norm': 0.412109375, 'learning_rate': 0.0001763821622036195, 'epoch': 0.13}\n",
      "{'loss': 1.3254, 'grad_norm': 0.53125, 'learning_rate': 0.00017637079982615563, 'epoch': 0.13}\n",
      "{'loss': 1.1449, 'grad_norm': 1.2265625, 'learning_rate': 0.00017635943744869176, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4145, 'grad_norm': 0.62890625, 'learning_rate': 0.0001763480750712279, 'epoch': 0.13}\n",
      "{'loss': 1.1768, 'grad_norm': 0.58984375, 'learning_rate': 0.00017633671269376406, 'epoch': 0.13}\n",
      "{'loss': 1.2864, 'grad_norm': 0.4609375, 'learning_rate': 0.0001763253503163002, 'epoch': 0.13}\n",
      "{'loss': 1.2191, 'grad_norm': 0.48046875, 'learning_rate': 0.00017631398793883633, 'epoch': 0.13}\n",
      "{'loss': 1.0717, 'grad_norm': 0.8515625, 'learning_rate': 0.00017630262556137246, 'epoch': 0.13}\n",
      "{'loss': 1.3486, 'grad_norm': 0.5234375, 'learning_rate': 0.0001762912631839086, 'epoch': 0.13}\n",
      "{'loss': 1.1892, 'grad_norm': 0.419921875, 'learning_rate': 0.00017627990080644473, 'epoch': 0.13}\n",
      "{'loss': 1.3412, 'grad_norm': 0.421875, 'learning_rate': 0.00017626853842898089, 'epoch': 0.13}\n",
      "{'loss': 1.3034, 'grad_norm': 0.5703125, 'learning_rate': 0.00017625717605151704, 'epoch': 0.13}\n",
      "{'loss': 1.2677, 'grad_norm': 0.69921875, 'learning_rate': 0.00017624581367405316, 'epoch': 0.13}\n",
      "{'loss': 1.2608, 'grad_norm': 0.45703125, 'learning_rate': 0.0001762344512965893, 'epoch': 0.13}\n",
      "{'loss': 1.2662, 'grad_norm': 0.72265625, 'learning_rate': 0.00017622308891912544, 'epoch': 0.13}\n",
      "{'loss': 1.3737, 'grad_norm': 0.466796875, 'learning_rate': 0.0001762117265416616, 'epoch': 0.13}\n",
      "{'loss': 1.364, 'grad_norm': 0.41796875, 'learning_rate': 0.0001762003641641977, 'epoch': 0.13}\n",
      "{'loss': 1.1433, 'grad_norm': 1.0703125, 'learning_rate': 0.00017618900178673386, 'epoch': 0.13}\n",
      "{'loss': 1.4038, 'grad_norm': 0.435546875, 'learning_rate': 0.00017617763940927002, 'epoch': 0.13}\n",
      "{'loss': 1.1157, 'grad_norm': 0.5625, 'learning_rate': 0.00017616627703180614, 'epoch': 0.13}\n",
      "{'loss': 1.2535, 'grad_norm': 0.37109375, 'learning_rate': 0.0001761549146543423, 'epoch': 0.13}\n",
      "{'loss': 1.3497, 'grad_norm': 0.51953125, 'learning_rate': 0.00017614355227687842, 'epoch': 0.13}\n",
      "{'loss': 1.0685, 'grad_norm': 0.470703125, 'learning_rate': 0.00017613218989941457, 'epoch': 0.13}\n",
      "{'loss': 1.4158, 'grad_norm': 0.39453125, 'learning_rate': 0.0001761208275219507, 'epoch': 0.13}\n",
      "{'loss': 1.2919, 'grad_norm': 0.57421875, 'learning_rate': 0.00017610946514448684, 'epoch': 0.13}\n",
      "{'loss': 1.2468, 'grad_norm': 0.53515625, 'learning_rate': 0.000176098102767023, 'epoch': 0.13}\n",
      "{'loss': 1.4793, 'grad_norm': 0.3984375, 'learning_rate': 0.00017608674038955912, 'epoch': 0.13}\n",
      "{'loss': 1.1143, 'grad_norm': 1.0390625, 'learning_rate': 0.00017607537801209527, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4867, 'grad_norm': 0.5625, 'learning_rate': 0.0001760640156346314, 'epoch': 0.13}\n",
      "{'loss': 1.261, 'grad_norm': 0.56640625, 'learning_rate': 0.00017605265325716754, 'epoch': 0.13}\n",
      "{'loss': 1.3618, 'grad_norm': 0.37109375, 'learning_rate': 0.00017604129087970367, 'epoch': 0.13}\n",
      "{'loss': 1.3055, 'grad_norm': 0.431640625, 'learning_rate': 0.00017602992850223982, 'epoch': 0.13}\n",
      "{'loss': 1.1574, 'grad_norm': 1.78125, 'learning_rate': 0.00017601856612477597, 'epoch': 0.13}\n",
      "{'loss': 1.3781, 'grad_norm': 0.453125, 'learning_rate': 0.0001760072037473121, 'epoch': 0.13}\n",
      "{'loss': 1.2709, 'grad_norm': 0.6328125, 'learning_rate': 0.00017599584136984825, 'epoch': 0.13}\n",
      "{'loss': 1.2538, 'grad_norm': 0.3515625, 'learning_rate': 0.00017598447899238437, 'epoch': 0.13}\n",
      "{'loss': 1.3761, 'grad_norm': 0.50390625, 'learning_rate': 0.0001759731166149205, 'epoch': 0.13}\n",
      "{'loss': 1.1149, 'grad_norm': 0.8046875, 'learning_rate': 0.00017596175423745665, 'epoch': 0.13}\n",
      "{'loss': 1.408, 'grad_norm': 0.353515625, 'learning_rate': 0.0001759503918599928, 'epoch': 0.13}\n",
      "{'loss': 1.2195, 'grad_norm': 0.5859375, 'learning_rate': 0.00017593902948252895, 'epoch': 0.13}\n",
      "{'loss': 1.4782, 'grad_norm': 0.56640625, 'learning_rate': 0.00017592766710506507, 'epoch': 0.13}\n",
      "{'loss': 1.4066, 'grad_norm': 0.431640625, 'learning_rate': 0.0001759163047276012, 'epoch': 0.13}\n",
      "{'loss': 1.1684, 'grad_norm': 0.416015625, 'learning_rate': 0.00017590494235013735, 'epoch': 0.13}\n",
      "{'loss': 1.3712, 'grad_norm': 0.42578125, 'learning_rate': 0.00017589357997267347, 'epoch': 0.13}\n",
      "{'loss': 1.252, 'grad_norm': 0.60546875, 'learning_rate': 0.00017588221759520963, 'epoch': 0.13}\n",
      "{'loss': 1.3243, 'grad_norm': 0.416015625, 'learning_rate': 0.00017587085521774578, 'epoch': 0.13}\n",
      "{'loss': 1.2949, 'grad_norm': 0.4453125, 'learning_rate': 0.0001758594928402819, 'epoch': 0.13}\n",
      "{'loss': 1.1061, 'grad_norm': 0.48046875, 'learning_rate': 0.00017584813046281805, 'epoch': 0.13}\n",
      "{'loss': 1.4702, 'grad_norm': 0.421875, 'learning_rate': 0.00017583676808535418, 'epoch': 0.13}\n",
      "{'loss': 1.1259, 'grad_norm': 0.59765625, 'learning_rate': 0.00017582540570789033, 'epoch': 0.13}\n",
      "{'loss': 1.3461, 'grad_norm': 0.3984375, 'learning_rate': 0.00017581404333042645, 'epoch': 0.13}\n",
      "{'loss': 1.3402, 'grad_norm': 0.625, 'learning_rate': 0.0001758026809529626, 'epoch': 0.13}\n",
      "{'loss': 1.1599, 'grad_norm': 0.84765625, 'learning_rate': 0.00017579131857549876, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.353, 'grad_norm': 0.5234375, 'learning_rate': 0.00017577995619803488, 'epoch': 0.13}\n",
      "{'loss': 1.1531, 'grad_norm': 0.546875, 'learning_rate': 0.00017576859382057103, 'epoch': 0.13}\n",
      "{'loss': 1.2895, 'grad_norm': 0.8671875, 'learning_rate': 0.00017575723144310716, 'epoch': 0.13}\n",
      "{'loss': 1.2528, 'grad_norm': 0.59375, 'learning_rate': 0.0001757458690656433, 'epoch': 0.13}\n",
      "{'loss': 1.1266, 'grad_norm': 0.78515625, 'learning_rate': 0.00017573450668817943, 'epoch': 0.13}\n",
      "{'loss': 1.377, 'grad_norm': 0.55859375, 'learning_rate': 0.00017572314431071558, 'epoch': 0.13}\n",
      "{'loss': 1.2472, 'grad_norm': 0.71875, 'learning_rate': 0.00017571178193325173, 'epoch': 0.13}\n",
      "{'loss': 1.2233, 'grad_norm': 0.43359375, 'learning_rate': 0.00017570041955578786, 'epoch': 0.13}\n",
      "{'loss': 1.2568, 'grad_norm': 0.484375, 'learning_rate': 0.000175689057178324, 'epoch': 0.13}\n",
      "{'loss': 1.1149, 'grad_norm': 1.078125, 'learning_rate': 0.00017567769480086013, 'epoch': 0.13}\n",
      "{'loss': 1.3818, 'grad_norm': 0.419921875, 'learning_rate': 0.00017566633242339629, 'epoch': 0.13}\n",
      "{'loss': 1.2808, 'grad_norm': 0.7578125, 'learning_rate': 0.0001756549700459324, 'epoch': 0.13}\n",
      "{'loss': 1.2765, 'grad_norm': 0.390625, 'learning_rate': 0.00017564360766846856, 'epoch': 0.13}\n",
      "{'loss': 1.1853, 'grad_norm': 0.466796875, 'learning_rate': 0.0001756322452910047, 'epoch': 0.13}\n",
      "{'loss': 1.1895, 'grad_norm': 0.68359375, 'learning_rate': 0.00017562088291354084, 'epoch': 0.13}\n",
      "{'loss': 1.3593, 'grad_norm': 0.48046875, 'learning_rate': 0.000175609520536077, 'epoch': 0.13}\n",
      "{'loss': 1.2561, 'grad_norm': 0.6640625, 'learning_rate': 0.0001755981581586131, 'epoch': 0.13}\n",
      "{'loss': 1.1985, 'grad_norm': 0.453125, 'learning_rate': 0.00017558679578114924, 'epoch': 0.13}\n",
      "{'loss': 1.3004, 'grad_norm': 0.61328125, 'learning_rate': 0.0001755754334036854, 'epoch': 0.13}\n",
      "{'loss': 1.1215, 'grad_norm': 0.6171875, 'learning_rate': 0.00017556407102622154, 'epoch': 0.13}\n",
      "{'loss': 1.6033, 'grad_norm': 0.439453125, 'learning_rate': 0.0001755527086487577, 'epoch': 0.13}\n",
      "{'loss': 1.2123, 'grad_norm': 0.55859375, 'learning_rate': 0.00017554134627129382, 'epoch': 0.13}\n",
      "{'loss': 1.2869, 'grad_norm': 0.380859375, 'learning_rate': 0.00017552998389382994, 'epoch': 0.13}\n",
      "{'loss': 1.3565, 'grad_norm': 0.54296875, 'learning_rate': 0.0001755186215163661, 'epoch': 0.13}\n",
      "{'loss': 1.1434, 'grad_norm': 0.82421875, 'learning_rate': 0.00017550725913890222, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3854, 'grad_norm': 0.51953125, 'learning_rate': 0.00017549589676143837, 'epoch': 0.13}\n",
      "{'loss': 1.1979, 'grad_norm': 0.5390625, 'learning_rate': 0.00017548453438397452, 'epoch': 0.13}\n",
      "{'loss': 1.231, 'grad_norm': 0.36328125, 'learning_rate': 0.00017547317200651064, 'epoch': 0.13}\n",
      "{'loss': 1.238, 'grad_norm': 0.5234375, 'learning_rate': 0.0001754618096290468, 'epoch': 0.13}\n",
      "{'loss': 1.1159, 'grad_norm': 0.73828125, 'learning_rate': 0.00017545044725158292, 'epoch': 0.13}\n",
      "{'loss': 1.3581, 'grad_norm': 0.455078125, 'learning_rate': 0.00017543908487411907, 'epoch': 0.13}\n",
      "{'loss': 1.3091, 'grad_norm': 0.69921875, 'learning_rate': 0.0001754277224966552, 'epoch': 0.13}\n",
      "{'loss': 1.0669, 'grad_norm': 0.466796875, 'learning_rate': 0.00017541636011919135, 'epoch': 0.13}\n",
      "{'loss': 1.335, 'grad_norm': 0.5234375, 'learning_rate': 0.0001754049977417275, 'epoch': 0.13}\n",
      "{'loss': 1.0528, 'grad_norm': 0.6796875, 'learning_rate': 0.00017539363536426362, 'epoch': 0.13}\n",
      "{'loss': 1.4268, 'grad_norm': 0.5, 'learning_rate': 0.00017538227298679977, 'epoch': 0.13}\n",
      "{'loss': 1.3961, 'grad_norm': 0.85546875, 'learning_rate': 0.0001753709106093359, 'epoch': 0.13}\n",
      "{'loss': 1.1305, 'grad_norm': 0.435546875, 'learning_rate': 0.00017535954823187205, 'epoch': 0.13}\n",
      "{'loss': 1.2923, 'grad_norm': 0.50390625, 'learning_rate': 0.00017534818585440817, 'epoch': 0.13}\n",
      "{'loss': 1.1622, 'grad_norm': 0.9375, 'learning_rate': 0.00017533682347694432, 'epoch': 0.13}\n",
      "{'loss': 1.328, 'grad_norm': 0.46484375, 'learning_rate': 0.00017532546109948048, 'epoch': 0.13}\n",
      "{'loss': 1.3356, 'grad_norm': 0.66015625, 'learning_rate': 0.0001753140987220166, 'epoch': 0.13}\n",
      "{'loss': 1.064, 'grad_norm': 0.376953125, 'learning_rate': 0.00017530273634455275, 'epoch': 0.13}\n",
      "{'loss': 1.2911, 'grad_norm': 0.53125, 'learning_rate': 0.00017529137396708888, 'epoch': 0.13}\n",
      "{'loss': 1.0928, 'grad_norm': 0.74609375, 'learning_rate': 0.00017528001158962503, 'epoch': 0.13}\n",
      "{'loss': 1.3912, 'grad_norm': 0.48046875, 'learning_rate': 0.00017526864921216115, 'epoch': 0.13}\n",
      "{'loss': 1.3639, 'grad_norm': 0.451171875, 'learning_rate': 0.0001752572868346973, 'epoch': 0.14}\n",
      "{'loss': 1.3193, 'grad_norm': 0.3359375, 'learning_rate': 0.00017524592445723345, 'epoch': 0.14}\n",
      "{'loss': 1.2933, 'grad_norm': 0.6875, 'learning_rate': 0.00017523456207976958, 'epoch': 0.14}\n",
      "{'loss': 1.1758, 'grad_norm': 1.46875, 'learning_rate': 0.00017522319970230573, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3746, 'grad_norm': 0.515625, 'learning_rate': 0.00017521183732484185, 'epoch': 0.14}\n",
      "{'loss': 1.2755, 'grad_norm': 0.37890625, 'learning_rate': 0.00017520047494737798, 'epoch': 0.14}\n",
      "{'loss': 1.5216, 'grad_norm': 0.43359375, 'learning_rate': 0.00017518911256991413, 'epoch': 0.14}\n",
      "{'loss': 1.3897, 'grad_norm': 0.7109375, 'learning_rate': 0.00017517775019245028, 'epoch': 0.14}\n",
      "{'loss': 1.2277, 'grad_norm': 0.97265625, 'learning_rate': 0.00017516638781498643, 'epoch': 0.14}\n",
      "{'loss': 1.4748, 'grad_norm': 0.482421875, 'learning_rate': 0.00017515502543752256, 'epoch': 0.14}\n",
      "{'loss': 1.3783, 'grad_norm': 0.484375, 'learning_rate': 0.00017514366306005868, 'epoch': 0.14}\n",
      "{'loss': 1.2694, 'grad_norm': 0.380859375, 'learning_rate': 0.00017513230068259483, 'epoch': 0.14}\n",
      "{'loss': 1.1934, 'grad_norm': 0.625, 'learning_rate': 0.00017512093830513096, 'epoch': 0.14}\n",
      "{'loss': 1.1172, 'grad_norm': 0.46484375, 'learning_rate': 0.00017510957592766714, 'epoch': 0.14}\n",
      "{'loss': 1.3338, 'grad_norm': 0.5078125, 'learning_rate': 0.00017509821355020326, 'epoch': 0.14}\n",
      "{'loss': 1.2328, 'grad_norm': 0.42578125, 'learning_rate': 0.00017508685117273938, 'epoch': 0.14}\n",
      "{'loss': 1.3187, 'grad_norm': 0.3828125, 'learning_rate': 0.00017507548879527554, 'epoch': 0.14}\n",
      "{'loss': 1.4251, 'grad_norm': 0.439453125, 'learning_rate': 0.00017506412641781166, 'epoch': 0.14}\n",
      "{'loss': 1.1667, 'grad_norm': 0.76171875, 'learning_rate': 0.0001750527640403478, 'epoch': 0.14}\n",
      "{'loss': 1.3479, 'grad_norm': 0.58203125, 'learning_rate': 0.00017504140166288394, 'epoch': 0.14}\n",
      "{'loss': 1.2559, 'grad_norm': 0.6875, 'learning_rate': 0.0001750300392854201, 'epoch': 0.14}\n",
      "{'loss': 1.2062, 'grad_norm': 0.5859375, 'learning_rate': 0.00017501867690795624, 'epoch': 0.14}\n",
      "{'loss': 1.2167, 'grad_norm': 0.62109375, 'learning_rate': 0.00017500731453049236, 'epoch': 0.14}\n",
      "{'loss': 1.2369, 'grad_norm': 0.94921875, 'learning_rate': 0.00017499595215302851, 'epoch': 0.14}\n",
      "{'loss': 1.3086, 'grad_norm': 0.5078125, 'learning_rate': 0.00017498458977556464, 'epoch': 0.14}\n",
      "{'loss': 1.3013, 'grad_norm': 0.58203125, 'learning_rate': 0.0001749732273981008, 'epoch': 0.14}\n",
      "{'loss': 1.1339, 'grad_norm': 0.4921875, 'learning_rate': 0.00017496186502063691, 'epoch': 0.14}\n",
      "{'loss': 1.1991, 'grad_norm': 0.546875, 'learning_rate': 0.00017495050264317307, 'epoch': 0.14}\n",
      "{'loss': 1.2312, 'grad_norm': 0.77734375, 'learning_rate': 0.00017493914026570922, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3648, 'grad_norm': 0.435546875, 'learning_rate': 0.00017492777788824534, 'epoch': 0.14}\n",
      "{'loss': 1.3229, 'grad_norm': 0.81640625, 'learning_rate': 0.0001749164155107815, 'epoch': 0.14}\n",
      "{'loss': 1.3568, 'grad_norm': 0.458984375, 'learning_rate': 0.00017490505313331762, 'epoch': 0.14}\n",
      "{'loss': 1.2478, 'grad_norm': 0.66015625, 'learning_rate': 0.00017489369075585377, 'epoch': 0.14}\n",
      "{'loss': 1.144, 'grad_norm': 0.427734375, 'learning_rate': 0.0001748823283783899, 'epoch': 0.14}\n",
      "{'loss': 1.4027, 'grad_norm': 0.75, 'learning_rate': 0.00017487096600092604, 'epoch': 0.14}\n",
      "{'loss': 1.2885, 'grad_norm': 0.6328125, 'learning_rate': 0.0001748596036234622, 'epoch': 0.14}\n",
      "{'loss': 1.2653, 'grad_norm': 0.369140625, 'learning_rate': 0.00017484824124599832, 'epoch': 0.14}\n",
      "{'loss': 1.4024, 'grad_norm': 0.466796875, 'learning_rate': 0.00017483687886853447, 'epoch': 0.14}\n",
      "{'loss': 1.1822, 'grad_norm': 0.7734375, 'learning_rate': 0.0001748255164910706, 'epoch': 0.14}\n",
      "{'loss': 1.419, 'grad_norm': 0.45703125, 'learning_rate': 0.00017481415411360672, 'epoch': 0.14}\n",
      "{'loss': 1.3173, 'grad_norm': 0.7578125, 'learning_rate': 0.00017480279173614287, 'epoch': 0.14}\n",
      "{'loss': 1.2418, 'grad_norm': 0.294921875, 'learning_rate': 0.00017479142935867902, 'epoch': 0.14}\n",
      "{'loss': 1.3292, 'grad_norm': 0.62109375, 'learning_rate': 0.00017478006698121517, 'epoch': 0.14}\n",
      "{'loss': 1.1392, 'grad_norm': 0.51953125, 'learning_rate': 0.0001747687046037513, 'epoch': 0.14}\n",
      "{'loss': 1.332, 'grad_norm': 0.48828125, 'learning_rate': 0.00017475734222628742, 'epoch': 0.14}\n",
      "{'loss': 1.3667, 'grad_norm': 0.63671875, 'learning_rate': 0.00017474597984882357, 'epoch': 0.14}\n",
      "{'loss': 1.2749, 'grad_norm': 0.396484375, 'learning_rate': 0.0001747346174713597, 'epoch': 0.14}\n",
      "{'loss': 1.2614, 'grad_norm': 0.51171875, 'learning_rate': 0.00017472325509389588, 'epoch': 0.14}\n",
      "{'loss': 1.1499, 'grad_norm': 0.7890625, 'learning_rate': 0.000174711892716432, 'epoch': 0.14}\n",
      "{'loss': 1.4261, 'grad_norm': 0.408203125, 'learning_rate': 0.00017470053033896813, 'epoch': 0.14}\n",
      "{'loss': 1.2321, 'grad_norm': 0.6015625, 'learning_rate': 0.00017468916796150428, 'epoch': 0.14}\n",
      "{'loss': 1.2841, 'grad_norm': 0.474609375, 'learning_rate': 0.0001746778055840404, 'epoch': 0.14}\n",
      "{'loss': 1.3894, 'grad_norm': 0.486328125, 'learning_rate': 0.00017466644320657655, 'epoch': 0.14}\n",
      "{'loss': 1.1842, 'grad_norm': 0.92578125, 'learning_rate': 0.00017465508082911268, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2785, 'grad_norm': 0.44921875, 'learning_rate': 0.00017464371845164883, 'epoch': 0.14}\n",
      "{'loss': 1.2872, 'grad_norm': 0.67578125, 'learning_rate': 0.00017463235607418498, 'epoch': 0.14}\n",
      "{'loss': 1.2714, 'grad_norm': 0.458984375, 'learning_rate': 0.0001746209936967211, 'epoch': 0.14}\n",
      "{'loss': 1.345, 'grad_norm': 0.55078125, 'learning_rate': 0.00017460963131925726, 'epoch': 0.14}\n",
      "{'loss': 1.2373, 'grad_norm': 0.447265625, 'learning_rate': 0.00017459826894179338, 'epoch': 0.14}\n",
      "{'loss': 1.3199, 'grad_norm': 0.44921875, 'learning_rate': 0.00017458690656432953, 'epoch': 0.14}\n",
      "{'loss': 1.251, 'grad_norm': 0.66796875, 'learning_rate': 0.00017457554418686566, 'epoch': 0.14}\n",
      "{'loss': 1.3499, 'grad_norm': 0.390625, 'learning_rate': 0.0001745641818094018, 'epoch': 0.14}\n",
      "{'loss': 1.3071, 'grad_norm': 0.62890625, 'learning_rate': 0.00017455281943193796, 'epoch': 0.14}\n",
      "{'loss': 1.0921, 'grad_norm': 0.400390625, 'learning_rate': 0.00017454145705447408, 'epoch': 0.14}\n",
      "{'loss': 1.4367, 'grad_norm': 0.40625, 'learning_rate': 0.00017453009467701023, 'epoch': 0.14}\n",
      "{'loss': 1.1624, 'grad_norm': 0.62890625, 'learning_rate': 0.00017451873229954636, 'epoch': 0.14}\n",
      "{'loss': 1.3019, 'grad_norm': 0.453125, 'learning_rate': 0.0001745073699220825, 'epoch': 0.14}\n",
      "{'loss': 1.258, 'grad_norm': 0.54296875, 'learning_rate': 0.00017449600754461863, 'epoch': 0.14}\n",
      "{'loss': 1.1922, 'grad_norm': 0.46484375, 'learning_rate': 0.00017448464516715479, 'epoch': 0.14}\n",
      "{'loss': 1.4052, 'grad_norm': 0.3984375, 'learning_rate': 0.00017447328278969094, 'epoch': 0.14}\n",
      "{'loss': 1.169, 'grad_norm': 0.5703125, 'learning_rate': 0.00017446192041222706, 'epoch': 0.14}\n",
      "{'loss': 1.358, 'grad_norm': 0.345703125, 'learning_rate': 0.0001744505580347632, 'epoch': 0.14}\n",
      "{'loss': 1.3926, 'grad_norm': 0.5625, 'learning_rate': 0.00017443919565729934, 'epoch': 0.14}\n",
      "{'loss': 1.1816, 'grad_norm': 0.69140625, 'learning_rate': 0.00017442783327983546, 'epoch': 0.14}\n",
      "{'loss': 1.3909, 'grad_norm': 0.400390625, 'learning_rate': 0.00017441647090237164, 'epoch': 0.14}\n",
      "{'loss': 1.2106, 'grad_norm': 0.55859375, 'learning_rate': 0.00017440510852490776, 'epoch': 0.14}\n",
      "{'loss': 1.3087, 'grad_norm': 0.3671875, 'learning_rate': 0.00017439374614744392, 'epoch': 0.14}\n",
      "{'loss': 1.2755, 'grad_norm': 0.373046875, 'learning_rate': 0.00017438238376998004, 'epoch': 0.14}\n",
      "{'loss': 1.3252, 'grad_norm': 0.8046875, 'learning_rate': 0.00017437102139251616, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3618, 'grad_norm': 0.51953125, 'learning_rate': 0.00017435965901505232, 'epoch': 0.14}\n",
      "{'loss': 1.2248, 'grad_norm': 0.423828125, 'learning_rate': 0.00017434829663758844, 'epoch': 0.14}\n",
      "{'loss': 1.1898, 'grad_norm': 0.4140625, 'learning_rate': 0.00017433693426012462, 'epoch': 0.14}\n",
      "{'loss': 1.3176, 'grad_norm': 0.484375, 'learning_rate': 0.00017432557188266074, 'epoch': 0.14}\n",
      "{'loss': 1.2456, 'grad_norm': 0.703125, 'learning_rate': 0.00017431420950519687, 'epoch': 0.14}\n",
      "{'loss': 1.4008, 'grad_norm': 0.55078125, 'learning_rate': 0.00017430284712773302, 'epoch': 0.14}\n",
      "{'loss': 1.2884, 'grad_norm': 0.5390625, 'learning_rate': 0.00017429148475026914, 'epoch': 0.14}\n",
      "{'loss': 1.2443, 'grad_norm': 0.44140625, 'learning_rate': 0.0001742801223728053, 'epoch': 0.14}\n",
      "{'loss': 1.2354, 'grad_norm': 0.5703125, 'learning_rate': 0.00017426875999534142, 'epoch': 0.14}\n",
      "{'loss': 1.2666, 'grad_norm': 0.65234375, 'learning_rate': 0.00017425739761787757, 'epoch': 0.14}\n",
      "{'loss': 1.353, 'grad_norm': 0.4921875, 'learning_rate': 0.00017424603524041372, 'epoch': 0.14}\n",
      "{'loss': 1.1926, 'grad_norm': 0.640625, 'learning_rate': 0.00017423467286294985, 'epoch': 0.14}\n",
      "{'loss': 1.1942, 'grad_norm': 0.3671875, 'learning_rate': 0.000174223310485486, 'epoch': 0.14}\n",
      "{'loss': 1.242, 'grad_norm': 0.462890625, 'learning_rate': 0.00017421194810802212, 'epoch': 0.14}\n",
      "{'loss': 1.1389, 'grad_norm': 0.73828125, 'learning_rate': 0.00017420058573055827, 'epoch': 0.14}\n",
      "{'loss': 1.4303, 'grad_norm': 0.400390625, 'learning_rate': 0.0001741892233530944, 'epoch': 0.14}\n",
      "{'loss': 1.2455, 'grad_norm': 0.65234375, 'learning_rate': 0.00017417786097563055, 'epoch': 0.14}\n",
      "{'loss': 1.4038, 'grad_norm': 0.37109375, 'learning_rate': 0.0001741664985981667, 'epoch': 0.14}\n",
      "{'loss': 1.3641, 'grad_norm': 0.58984375, 'learning_rate': 0.00017415513622070282, 'epoch': 0.14}\n",
      "{'loss': 1.0969, 'grad_norm': 1.1484375, 'learning_rate': 0.00017414377384323898, 'epoch': 0.14}\n",
      "{'loss': 1.3755, 'grad_norm': 0.4296875, 'learning_rate': 0.0001741324114657751, 'epoch': 0.14}\n",
      "{'loss': 1.3065, 'grad_norm': 0.5234375, 'learning_rate': 0.00017412104908831125, 'epoch': 0.14}\n",
      "{'loss': 1.2873, 'grad_norm': 0.46875, 'learning_rate': 0.00017410968671084738, 'epoch': 0.14}\n",
      "{'loss': 1.3499, 'grad_norm': 0.546875, 'learning_rate': 0.00017409832433338353, 'epoch': 0.14}\n",
      "{'loss': 1.1124, 'grad_norm': 0.38671875, 'learning_rate': 0.00017408696195591968, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2796, 'grad_norm': 0.37890625, 'learning_rate': 0.0001740755995784558, 'epoch': 0.14}\n",
      "{'loss': 1.2569, 'grad_norm': 0.92578125, 'learning_rate': 0.00017406423720099195, 'epoch': 0.14}\n",
      "{'loss': 1.1939, 'grad_norm': 0.369140625, 'learning_rate': 0.00017405287482352808, 'epoch': 0.14}\n",
      "{'loss': 1.4035, 'grad_norm': 0.4921875, 'learning_rate': 0.0001740415124460642, 'epoch': 0.14}\n",
      "{'loss': 1.0957, 'grad_norm': 0.52734375, 'learning_rate': 0.00017403015006860038, 'epoch': 0.14}\n",
      "{'loss': 1.4258, 'grad_norm': 0.400390625, 'learning_rate': 0.0001740187876911365, 'epoch': 0.14}\n",
      "{'loss': 1.1992, 'grad_norm': 0.5390625, 'learning_rate': 0.00017400742531367266, 'epoch': 0.14}\n",
      "{'loss': 1.2558, 'grad_norm': 0.46484375, 'learning_rate': 0.00017399606293620878, 'epoch': 0.14}\n",
      "{'loss': 1.1538, 'grad_norm': 0.55859375, 'learning_rate': 0.0001739847005587449, 'epoch': 0.14}\n",
      "{'loss': 1.1673, 'grad_norm': 0.7109375, 'learning_rate': 0.00017397333818128106, 'epoch': 0.14}\n",
      "{'loss': 1.4393, 'grad_norm': 0.6015625, 'learning_rate': 0.00017396197580381718, 'epoch': 0.14}\n",
      "{'loss': 1.181, 'grad_norm': 0.54296875, 'learning_rate': 0.00017395061342635336, 'epoch': 0.14}\n",
      "{'loss': 1.2905, 'grad_norm': 0.427734375, 'learning_rate': 0.00017393925104888948, 'epoch': 0.14}\n",
      "{'loss': 1.2931, 'grad_norm': 0.66015625, 'learning_rate': 0.0001739278886714256, 'epoch': 0.14}\n",
      "{'loss': 1.175, 'grad_norm': 0.6953125, 'learning_rate': 0.00017391652629396176, 'epoch': 0.14}\n",
      "{'loss': 1.4624, 'grad_norm': 0.47265625, 'learning_rate': 0.00017390516391649788, 'epoch': 0.14}\n",
      "{'loss': 1.349, 'grad_norm': 0.66796875, 'learning_rate': 0.00017389380153903404, 'epoch': 0.14}\n",
      "{'loss': 1.2547, 'grad_norm': 0.380859375, 'learning_rate': 0.00017388243916157016, 'epoch': 0.14}\n",
      "{'loss': 1.3558, 'grad_norm': 0.54296875, 'learning_rate': 0.0001738710767841063, 'epoch': 0.14}\n",
      "{'loss': 1.0932, 'grad_norm': 1.2578125, 'learning_rate': 0.00017385971440664246, 'epoch': 0.14}\n",
      "{'loss': 1.412, 'grad_norm': 0.470703125, 'learning_rate': 0.0001738483520291786, 'epoch': 0.14}\n",
      "{'loss': 1.2607, 'grad_norm': 0.7578125, 'learning_rate': 0.00017383698965171474, 'epoch': 0.14}\n",
      "{'loss': 1.2052, 'grad_norm': 0.451171875, 'learning_rate': 0.00017382562727425086, 'epoch': 0.14}\n",
      "{'loss': 1.3579, 'grad_norm': 0.4921875, 'learning_rate': 0.00017381426489678701, 'epoch': 0.14}\n",
      "{'loss': 1.1561, 'grad_norm': 0.46484375, 'learning_rate': 0.00017380290251932314, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5199, 'grad_norm': 0.54296875, 'learning_rate': 0.0001737915401418593, 'epoch': 0.14}\n",
      "{'loss': 1.3207, 'grad_norm': 0.5234375, 'learning_rate': 0.00017378017776439544, 'epoch': 0.14}\n",
      "{'loss': 1.3839, 'grad_norm': 0.4296875, 'learning_rate': 0.00017376881538693157, 'epoch': 0.14}\n",
      "{'loss': 1.3016, 'grad_norm': 0.51953125, 'learning_rate': 0.00017375745300946772, 'epoch': 0.14}\n",
      "{'loss': 1.2025, 'grad_norm': 0.76953125, 'learning_rate': 0.00017374609063200384, 'epoch': 0.14}\n",
      "{'loss': 1.3251, 'grad_norm': 0.54296875, 'learning_rate': 0.00017373472825454, 'epoch': 0.14}\n",
      "{'loss': 1.2884, 'grad_norm': 0.73046875, 'learning_rate': 0.00017372336587707614, 'epoch': 0.14}\n",
      "{'loss': 1.2998, 'grad_norm': 0.400390625, 'learning_rate': 0.00017371200349961227, 'epoch': 0.14}\n",
      "{'loss': 1.3535, 'grad_norm': 0.462890625, 'learning_rate': 0.00017370064112214842, 'epoch': 0.14}\n",
      "{'loss': 1.215, 'grad_norm': 0.65625, 'learning_rate': 0.00017368927874468454, 'epoch': 0.14}\n",
      "{'loss': 1.2008, 'grad_norm': 0.40234375, 'learning_rate': 0.0001736779163672207, 'epoch': 0.14}\n",
      "{'loss': 1.2076, 'grad_norm': 0.609375, 'learning_rate': 0.00017366655398975682, 'epoch': 0.14}\n",
      "{'loss': 1.3597, 'grad_norm': 0.392578125, 'learning_rate': 0.00017365519161229294, 'epoch': 0.14}\n",
      "{'loss': 1.3752, 'grad_norm': 0.5078125, 'learning_rate': 0.00017364382923482912, 'epoch': 0.14}\n",
      "{'loss': 1.1065, 'grad_norm': 0.80078125, 'learning_rate': 0.00017363246685736525, 'epoch': 0.14}\n",
      "{'loss': 1.4013, 'grad_norm': 0.419921875, 'learning_rate': 0.0001736211044799014, 'epoch': 0.14}\n",
      "{'loss': 1.2468, 'grad_norm': 0.74609375, 'learning_rate': 0.00017360974210243752, 'epoch': 0.14}\n",
      "{'loss': 1.2674, 'grad_norm': 0.421875, 'learning_rate': 0.00017359837972497365, 'epoch': 0.14}\n",
      "{'loss': 1.2987, 'grad_norm': 0.46484375, 'learning_rate': 0.0001735870173475098, 'epoch': 0.14}\n",
      "{'loss': 1.1861, 'grad_norm': 0.65625, 'learning_rate': 0.00017357565497004592, 'epoch': 0.14}\n",
      "{'loss': 1.464, 'grad_norm': 0.60546875, 'learning_rate': 0.0001735642925925821, 'epoch': 0.14}\n",
      "{'loss': 1.3938, 'grad_norm': 0.5234375, 'learning_rate': 0.00017355293021511823, 'epoch': 0.14}\n",
      "{'loss': 1.3048, 'grad_norm': 0.41015625, 'learning_rate': 0.00017354156783765435, 'epoch': 0.14}\n",
      "{'loss': 1.2768, 'grad_norm': 0.5, 'learning_rate': 0.0001735302054601905, 'epoch': 0.14}\n",
      "{'loss': 1.208, 'grad_norm': 0.55078125, 'learning_rate': 0.00017351884308272663, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3134, 'grad_norm': 0.4609375, 'learning_rate': 0.00017350748070526278, 'epoch': 0.14}\n",
      "{'loss': 1.2623, 'grad_norm': 0.81640625, 'learning_rate': 0.0001734961183277989, 'epoch': 0.14}\n",
      "{'loss': 1.3928, 'grad_norm': 0.462890625, 'learning_rate': 0.00017348475595033505, 'epoch': 0.14}\n",
      "{'loss': 1.2772, 'grad_norm': 0.392578125, 'learning_rate': 0.0001734733935728712, 'epoch': 0.14}\n",
      "{'loss': 1.1526, 'grad_norm': 0.5546875, 'learning_rate': 0.00017346203119540733, 'epoch': 0.14}\n",
      "{'loss': 1.4417, 'grad_norm': 0.470703125, 'learning_rate': 0.00017345066881794348, 'epoch': 0.14}\n",
      "{'loss': 1.2811, 'grad_norm': 0.625, 'learning_rate': 0.0001734393064404796, 'epoch': 0.14}\n",
      "{'loss': 1.1109, 'grad_norm': 0.36328125, 'learning_rate': 0.00017342794406301576, 'epoch': 0.14}\n",
      "{'loss': 1.255, 'grad_norm': 0.6484375, 'learning_rate': 0.00017341658168555188, 'epoch': 0.14}\n",
      "{'loss': 1.0955, 'grad_norm': 0.88671875, 'learning_rate': 0.00017340521930808803, 'epoch': 0.14}\n",
      "{'loss': 1.4037, 'grad_norm': 0.390625, 'learning_rate': 0.00017339385693062418, 'epoch': 0.14}\n",
      "{'loss': 1.1434, 'grad_norm': 0.546875, 'learning_rate': 0.0001733824945531603, 'epoch': 0.14}\n",
      "{'loss': 1.2235, 'grad_norm': 0.44140625, 'learning_rate': 0.00017337113217569646, 'epoch': 0.14}\n",
      "{'loss': 1.3415, 'grad_norm': 0.5078125, 'learning_rate': 0.00017335976979823258, 'epoch': 0.14}\n",
      "{'loss': 1.0626, 'grad_norm': 1.1796875, 'learning_rate': 0.00017334840742076873, 'epoch': 0.14}\n",
      "{'loss': 1.303, 'grad_norm': 0.4453125, 'learning_rate': 0.00017333704504330489, 'epoch': 0.14}\n",
      "{'loss': 1.2862, 'grad_norm': 0.734375, 'learning_rate': 0.000173325682665841, 'epoch': 0.14}\n",
      "{'loss': 1.2328, 'grad_norm': 0.43359375, 'learning_rate': 0.00017331432028837716, 'epoch': 0.14}\n",
      "{'loss': 1.3846, 'grad_norm': 0.5859375, 'learning_rate': 0.00017330295791091329, 'epoch': 0.14}\n",
      "{'loss': 1.1403, 'grad_norm': 0.5078125, 'learning_rate': 0.00017329159553344944, 'epoch': 0.14}\n",
      "{'loss': 1.3136, 'grad_norm': 0.51953125, 'learning_rate': 0.00017328023315598556, 'epoch': 0.14}\n",
      "{'loss': 1.1132, 'grad_norm': 0.7265625, 'learning_rate': 0.00017326887077852169, 'epoch': 0.14}\n",
      "{'loss': 1.1276, 'grad_norm': 0.423828125, 'learning_rate': 0.00017325750840105786, 'epoch': 0.14}\n",
      "{'loss': 1.2589, 'grad_norm': 0.68359375, 'learning_rate': 0.000173246146023594, 'epoch': 0.14}\n",
      "{'loss': 1.1817, 'grad_norm': 0.357421875, 'learning_rate': 0.00017323478364613014, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3769, 'grad_norm': 0.58203125, 'learning_rate': 0.00017322342126866626, 'epoch': 0.14}\n",
      "{'loss': 1.3347, 'grad_norm': 0.49609375, 'learning_rate': 0.0001732120588912024, 'epoch': 0.14}\n",
      "{'loss': 1.194, 'grad_norm': 0.44921875, 'learning_rate': 0.00017320069651373854, 'epoch': 0.14}\n",
      "{'loss': 1.281, 'grad_norm': 0.5, 'learning_rate': 0.00017318933413627466, 'epoch': 0.14}\n",
      "{'loss': 1.1655, 'grad_norm': 0.95703125, 'learning_rate': 0.00017317797175881084, 'epoch': 0.14}\n",
      "{'loss': 1.4435, 'grad_norm': 0.50390625, 'learning_rate': 0.00017316660938134697, 'epoch': 0.14}\n",
      "{'loss': 1.2115, 'grad_norm': 0.5390625, 'learning_rate': 0.0001731552470038831, 'epoch': 0.14}\n",
      "{'loss': 1.31, 'grad_norm': 0.40625, 'learning_rate': 0.00017314388462641924, 'epoch': 0.14}\n",
      "{'loss': 1.3538, 'grad_norm': 0.5078125, 'learning_rate': 0.00017313252224895537, 'epoch': 0.14}\n",
      "{'loss': 1.2115, 'grad_norm': 0.92578125, 'learning_rate': 0.00017312115987149152, 'epoch': 0.14}\n",
      "{'loss': 1.5685, 'grad_norm': 0.470703125, 'learning_rate': 0.00017310979749402764, 'epoch': 0.14}\n",
      "{'loss': 1.1081, 'grad_norm': 0.59375, 'learning_rate': 0.0001730984351165638, 'epoch': 0.14}\n",
      "{'loss': 1.2157, 'grad_norm': 0.34765625, 'learning_rate': 0.00017308707273909995, 'epoch': 0.14}\n",
      "{'loss': 1.3211, 'grad_norm': 0.53515625, 'learning_rate': 0.00017307571036163607, 'epoch': 0.14}\n",
      "{'loss': 1.1973, 'grad_norm': 0.6953125, 'learning_rate': 0.00017306434798417222, 'epoch': 0.14}\n",
      "{'loss': 1.4212, 'grad_norm': 0.55078125, 'learning_rate': 0.00017305298560670835, 'epoch': 0.14}\n",
      "{'loss': 1.1813, 'grad_norm': 0.6328125, 'learning_rate': 0.0001730416232292445, 'epoch': 0.14}\n",
      "{'loss': 1.2973, 'grad_norm': 0.51171875, 'learning_rate': 0.00017303026085178065, 'epoch': 0.14}\n",
      "{'loss': 1.3003, 'grad_norm': 0.62109375, 'learning_rate': 0.00017301889847431677, 'epoch': 0.14}\n",
      "{'loss': 1.2621, 'grad_norm': 1.125, 'learning_rate': 0.00017300753609685292, 'epoch': 0.14}\n",
      "{'loss': 1.417, 'grad_norm': 0.494140625, 'learning_rate': 0.00017299617371938905, 'epoch': 0.14}\n",
      "{'loss': 1.2786, 'grad_norm': 0.578125, 'learning_rate': 0.0001729848113419252, 'epoch': 0.14}\n",
      "{'loss': 1.2039, 'grad_norm': 0.41796875, 'learning_rate': 0.00017297344896446132, 'epoch': 0.14}\n",
      "{'loss': 1.088, 'grad_norm': 0.56640625, 'learning_rate': 0.00017296208658699748, 'epoch': 0.15}\n",
      "{'loss': 1.0575, 'grad_norm': 0.7265625, 'learning_rate': 0.00017295072420953363, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4726, 'grad_norm': 0.609375, 'learning_rate': 0.00017293936183206975, 'epoch': 0.15}\n",
      "{'loss': 1.2824, 'grad_norm': 0.61328125, 'learning_rate': 0.0001729279994546059, 'epoch': 0.15}\n",
      "{'loss': 1.2789, 'grad_norm': 0.49609375, 'learning_rate': 0.00017291663707714203, 'epoch': 0.15}\n",
      "{'loss': 1.3871, 'grad_norm': 0.5390625, 'learning_rate': 0.00017290527469967818, 'epoch': 0.15}\n",
      "{'loss': 1.1868, 'grad_norm': 0.66796875, 'learning_rate': 0.0001728939123222143, 'epoch': 0.15}\n",
      "{'loss': 1.3778, 'grad_norm': 0.48046875, 'learning_rate': 0.00017288254994475043, 'epoch': 0.15}\n",
      "{'loss': 1.1481, 'grad_norm': 0.71875, 'learning_rate': 0.0001728711875672866, 'epoch': 0.15}\n",
      "{'loss': 1.2995, 'grad_norm': 0.44140625, 'learning_rate': 0.00017285982518982273, 'epoch': 0.15}\n",
      "{'loss': 1.2824, 'grad_norm': 0.46875, 'learning_rate': 0.00017284846281235888, 'epoch': 0.15}\n",
      "{'loss': 1.2176, 'grad_norm': 0.81640625, 'learning_rate': 0.000172837100434895, 'epoch': 0.15}\n",
      "{'loss': 1.4019, 'grad_norm': 0.57421875, 'learning_rate': 0.00017282573805743113, 'epoch': 0.15}\n",
      "{'loss': 1.2479, 'grad_norm': 0.6171875, 'learning_rate': 0.00017281437567996728, 'epoch': 0.15}\n",
      "{'loss': 1.2883, 'grad_norm': 0.357421875, 'learning_rate': 0.0001728030133025034, 'epoch': 0.15}\n",
      "{'loss': 1.348, 'grad_norm': 0.484375, 'learning_rate': 0.00017279165092503958, 'epoch': 0.15}\n",
      "{'loss': 1.1982, 'grad_norm': 0.3828125, 'learning_rate': 0.0001727802885475757, 'epoch': 0.15}\n",
      "{'loss': 1.3217, 'grad_norm': 0.5625, 'learning_rate': 0.00017276892617011183, 'epoch': 0.15}\n",
      "{'loss': 1.145, 'grad_norm': 0.6328125, 'learning_rate': 0.00017275756379264798, 'epoch': 0.15}\n",
      "{'loss': 1.2655, 'grad_norm': 0.39453125, 'learning_rate': 0.0001727462014151841, 'epoch': 0.15}\n",
      "{'loss': 1.289, 'grad_norm': 0.55859375, 'learning_rate': 0.00017273483903772026, 'epoch': 0.15}\n",
      "{'loss': 1.1665, 'grad_norm': 0.8671875, 'learning_rate': 0.0001727234766602564, 'epoch': 0.15}\n",
      "{'loss': 1.2773, 'grad_norm': 0.400390625, 'learning_rate': 0.00017271211428279253, 'epoch': 0.15}\n",
      "{'loss': 1.085, 'grad_norm': 0.5625, 'learning_rate': 0.00017270075190532869, 'epoch': 0.15}\n",
      "{'loss': 1.1938, 'grad_norm': 0.498046875, 'learning_rate': 0.0001726893895278648, 'epoch': 0.15}\n",
      "{'loss': 1.4593, 'grad_norm': 0.52734375, 'learning_rate': 0.00017267802715040096, 'epoch': 0.15}\n",
      "{'loss': 1.1569, 'grad_norm': 0.38671875, 'learning_rate': 0.00017266666477293709, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3648, 'grad_norm': 0.419921875, 'learning_rate': 0.00017265530239547324, 'epoch': 0.15}\n",
      "{'loss': 1.2049, 'grad_norm': 0.55078125, 'learning_rate': 0.0001726439400180094, 'epoch': 0.15}\n",
      "{'loss': 1.1346, 'grad_norm': 0.376953125, 'learning_rate': 0.0001726325776405455, 'epoch': 0.15}\n",
      "{'loss': 1.2305, 'grad_norm': 0.43359375, 'learning_rate': 0.00017262121526308166, 'epoch': 0.15}\n",
      "{'loss': 1.1524, 'grad_norm': 0.73046875, 'learning_rate': 0.0001726098528856178, 'epoch': 0.15}\n",
      "{'loss': 1.3814, 'grad_norm': 0.53515625, 'learning_rate': 0.00017259849050815394, 'epoch': 0.15}\n",
      "{'loss': 1.3995, 'grad_norm': 0.91796875, 'learning_rate': 0.00017258712813069006, 'epoch': 0.15}\n",
      "{'loss': 1.3106, 'grad_norm': 0.4140625, 'learning_rate': 0.00017257576575322622, 'epoch': 0.15}\n",
      "{'loss': 1.2978, 'grad_norm': 0.49609375, 'learning_rate': 0.00017256440337576237, 'epoch': 0.15}\n",
      "{'loss': 1.0091, 'grad_norm': 0.498046875, 'learning_rate': 0.0001725530409982985, 'epoch': 0.15}\n",
      "{'loss': 1.524, 'grad_norm': 0.486328125, 'learning_rate': 0.00017254167862083464, 'epoch': 0.15}\n",
      "{'loss': 1.1752, 'grad_norm': 0.60546875, 'learning_rate': 0.00017253031624337077, 'epoch': 0.15}\n",
      "{'loss': 1.3055, 'grad_norm': 0.44140625, 'learning_rate': 0.00017251895386590692, 'epoch': 0.15}\n",
      "{'loss': 1.3476, 'grad_norm': 0.56640625, 'learning_rate': 0.00017250759148844304, 'epoch': 0.15}\n",
      "{'loss': 1.1296, 'grad_norm': 0.48828125, 'learning_rate': 0.00017249622911097917, 'epoch': 0.15}\n",
      "{'loss': 1.534, 'grad_norm': 0.400390625, 'learning_rate': 0.00017248486673351535, 'epoch': 0.15}\n",
      "{'loss': 1.1197, 'grad_norm': 0.5546875, 'learning_rate': 0.00017247350435605147, 'epoch': 0.15}\n",
      "{'loss': 1.1953, 'grad_norm': 0.416015625, 'learning_rate': 0.00017246214197858762, 'epoch': 0.15}\n",
      "{'loss': 1.2498, 'grad_norm': 0.578125, 'learning_rate': 0.00017245077960112375, 'epoch': 0.15}\n",
      "{'loss': 1.2737, 'grad_norm': 0.61328125, 'learning_rate': 0.00017243941722365987, 'epoch': 0.15}\n",
      "{'loss': 1.3278, 'grad_norm': 0.59375, 'learning_rate': 0.00017242805484619602, 'epoch': 0.15}\n",
      "{'loss': 1.228, 'grad_norm': 0.94921875, 'learning_rate': 0.00017241669246873215, 'epoch': 0.15}\n",
      "{'loss': 1.2605, 'grad_norm': 0.42578125, 'learning_rate': 0.00017240533009126832, 'epoch': 0.15}\n",
      "{'loss': 1.3648, 'grad_norm': 0.61328125, 'learning_rate': 0.00017239396771380445, 'epoch': 0.15}\n",
      "{'loss': 1.1734, 'grad_norm': 0.71875, 'learning_rate': 0.00017238260533634057, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4514, 'grad_norm': 0.52734375, 'learning_rate': 0.00017237124295887672, 'epoch': 0.15}\n",
      "{'loss': 1.2967, 'grad_norm': 0.66796875, 'learning_rate': 0.00017235988058141285, 'epoch': 0.15}\n",
      "{'loss': 1.32, 'grad_norm': 0.392578125, 'learning_rate': 0.000172348518203949, 'epoch': 0.15}\n",
      "{'loss': 1.3108, 'grad_norm': 0.578125, 'learning_rate': 0.00017233715582648515, 'epoch': 0.15}\n",
      "{'loss': 1.1833, 'grad_norm': 0.640625, 'learning_rate': 0.00017232579344902128, 'epoch': 0.15}\n",
      "{'loss': 1.425, 'grad_norm': 0.365234375, 'learning_rate': 0.00017231443107155743, 'epoch': 0.15}\n",
      "{'loss': 1.2676, 'grad_norm': 0.58203125, 'learning_rate': 0.00017230306869409355, 'epoch': 0.15}\n",
      "{'loss': 1.3969, 'grad_norm': 0.408203125, 'learning_rate': 0.0001722917063166297, 'epoch': 0.15}\n",
      "{'loss': 1.2164, 'grad_norm': 0.52734375, 'learning_rate': 0.00017228034393916583, 'epoch': 0.15}\n",
      "{'loss': 1.2236, 'grad_norm': 0.6953125, 'learning_rate': 0.00017226898156170198, 'epoch': 0.15}\n",
      "{'loss': 1.3822, 'grad_norm': 0.4375, 'learning_rate': 0.00017225761918423813, 'epoch': 0.15}\n",
      "{'loss': 1.1844, 'grad_norm': 0.890625, 'learning_rate': 0.00017224625680677425, 'epoch': 0.15}\n",
      "{'loss': 1.3198, 'grad_norm': 0.44140625, 'learning_rate': 0.0001722348944293104, 'epoch': 0.15}\n",
      "{'loss': 1.3092, 'grad_norm': 0.60546875, 'learning_rate': 0.00017222353205184653, 'epoch': 0.15}\n",
      "{'loss': 1.2934, 'grad_norm': 0.73046875, 'learning_rate': 0.00017221216967438268, 'epoch': 0.15}\n",
      "{'loss': 1.3082, 'grad_norm': 0.57421875, 'learning_rate': 0.0001722008072969188, 'epoch': 0.15}\n",
      "{'loss': 1.2024, 'grad_norm': 0.63671875, 'learning_rate': 0.00017218944491945496, 'epoch': 0.15}\n",
      "{'loss': 1.1528, 'grad_norm': 0.41796875, 'learning_rate': 0.0001721780825419911, 'epoch': 0.15}\n",
      "{'loss': 1.3932, 'grad_norm': 0.65625, 'learning_rate': 0.00017216672016452723, 'epoch': 0.15}\n",
      "{'loss': 1.0847, 'grad_norm': 0.9375, 'learning_rate': 0.00017215535778706338, 'epoch': 0.15}\n",
      "{'loss': 1.2271, 'grad_norm': 0.44140625, 'learning_rate': 0.0001721439954095995, 'epoch': 0.15}\n",
      "{'loss': 1.2005, 'grad_norm': 0.66015625, 'learning_rate': 0.00017213263303213566, 'epoch': 0.15}\n",
      "{'loss': 1.3161, 'grad_norm': 0.380859375, 'learning_rate': 0.00017212127065467178, 'epoch': 0.15}\n",
      "{'loss': 1.2942, 'grad_norm': 0.703125, 'learning_rate': 0.0001721099082772079, 'epoch': 0.15}\n",
      "{'loss': 1.1601, 'grad_norm': 0.57421875, 'learning_rate': 0.0001720985458997441, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3444, 'grad_norm': 0.462890625, 'learning_rate': 0.0001720871835222802, 'epoch': 0.15}\n",
      "{'loss': 1.1938, 'grad_norm': 0.546875, 'learning_rate': 0.00017207582114481636, 'epoch': 0.15}\n",
      "{'loss': 1.3022, 'grad_norm': 0.453125, 'learning_rate': 0.0001720644587673525, 'epoch': 0.15}\n",
      "{'loss': 1.3644, 'grad_norm': 0.48046875, 'learning_rate': 0.0001720530963898886, 'epoch': 0.15}\n",
      "{'loss': 1.2028, 'grad_norm': 0.56640625, 'learning_rate': 0.00017204173401242476, 'epoch': 0.15}\n",
      "{'loss': 1.4609, 'grad_norm': 0.419921875, 'learning_rate': 0.00017203037163496091, 'epoch': 0.15}\n",
      "{'loss': 1.1773, 'grad_norm': 0.65625, 'learning_rate': 0.00017201900925749707, 'epoch': 0.15}\n",
      "{'loss': 1.1561, 'grad_norm': 0.439453125, 'learning_rate': 0.0001720076468800332, 'epoch': 0.15}\n",
      "{'loss': 1.2873, 'grad_norm': 0.59765625, 'learning_rate': 0.00017199628450256931, 'epoch': 0.15}\n",
      "{'loss': 1.2022, 'grad_norm': 0.8515625, 'learning_rate': 0.00017198492212510547, 'epoch': 0.15}\n",
      "{'loss': 1.4195, 'grad_norm': 0.625, 'learning_rate': 0.0001719735597476416, 'epoch': 0.15}\n",
      "{'loss': 1.2192, 'grad_norm': 0.61328125, 'learning_rate': 0.00017196219737017774, 'epoch': 0.15}\n",
      "{'loss': 1.3402, 'grad_norm': 0.390625, 'learning_rate': 0.0001719508349927139, 'epoch': 0.15}\n",
      "{'loss': 1.2495, 'grad_norm': 0.65234375, 'learning_rate': 0.00017193947261525002, 'epoch': 0.15}\n",
      "{'loss': 1.0205, 'grad_norm': 0.31640625, 'learning_rate': 0.00017192811023778617, 'epoch': 0.15}\n",
      "{'loss': 1.461, 'grad_norm': 0.494140625, 'learning_rate': 0.0001719167478603223, 'epoch': 0.15}\n",
      "{'loss': 1.3069, 'grad_norm': 0.546875, 'learning_rate': 0.00017190538548285844, 'epoch': 0.15}\n",
      "{'loss': 1.3415, 'grad_norm': 0.44140625, 'learning_rate': 0.00017189402310539457, 'epoch': 0.15}\n",
      "{'loss': 1.2951, 'grad_norm': 0.53125, 'learning_rate': 0.00017188266072793072, 'epoch': 0.15}\n",
      "{'loss': 1.2385, 'grad_norm': 0.65234375, 'learning_rate': 0.00017187129835046687, 'epoch': 0.15}\n",
      "{'loss': 1.3749, 'grad_norm': 0.5234375, 'learning_rate': 0.000171859935973003, 'epoch': 0.15}\n",
      "{'loss': 1.2812, 'grad_norm': 0.69921875, 'learning_rate': 0.00017184857359553915, 'epoch': 0.15}\n",
      "{'loss': 1.1758, 'grad_norm': 0.41796875, 'learning_rate': 0.00017183721121807527, 'epoch': 0.15}\n",
      "{'loss': 1.2647, 'grad_norm': 0.4921875, 'learning_rate': 0.00017182584884061142, 'epoch': 0.15}\n",
      "{'loss': 1.0875, 'grad_norm': 0.8984375, 'learning_rate': 0.00017181448646314755, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2636, 'grad_norm': 0.5390625, 'learning_rate': 0.0001718031240856837, 'epoch': 0.15}\n",
      "{'loss': 1.2981, 'grad_norm': 0.86328125, 'learning_rate': 0.00017179176170821985, 'epoch': 0.15}\n",
      "{'loss': 1.2446, 'grad_norm': 0.369140625, 'learning_rate': 0.00017178039933075597, 'epoch': 0.15}\n",
      "{'loss': 1.2713, 'grad_norm': 0.58984375, 'learning_rate': 0.00017176903695329213, 'epoch': 0.15}\n",
      "{'loss': 1.0126, 'grad_norm': 0.51171875, 'learning_rate': 0.00017175767457582825, 'epoch': 0.15}\n",
      "{'loss': 1.391, 'grad_norm': 0.55078125, 'learning_rate': 0.0001717463121983644, 'epoch': 0.15}\n",
      "{'loss': 1.138, 'grad_norm': 0.8671875, 'learning_rate': 0.00017173494982090053, 'epoch': 0.15}\n",
      "{'loss': 1.2897, 'grad_norm': 0.40234375, 'learning_rate': 0.00017172358744343665, 'epoch': 0.15}\n",
      "{'loss': 1.3067, 'grad_norm': 0.44921875, 'learning_rate': 0.00017171222506597283, 'epoch': 0.15}\n",
      "{'loss': 1.0925, 'grad_norm': 1.03125, 'learning_rate': 0.00017170086268850895, 'epoch': 0.15}\n",
      "{'loss': 1.3199, 'grad_norm': 0.4296875, 'learning_rate': 0.0001716895003110451, 'epoch': 0.15}\n",
      "{'loss': 1.1452, 'grad_norm': 0.78125, 'learning_rate': 0.00017167813793358123, 'epoch': 0.15}\n",
      "{'loss': 1.3649, 'grad_norm': 0.458984375, 'learning_rate': 0.00017166677555611735, 'epoch': 0.15}\n",
      "{'loss': 1.3372, 'grad_norm': 0.671875, 'learning_rate': 0.0001716554131786535, 'epoch': 0.15}\n",
      "{'loss': 1.0566, 'grad_norm': 0.6640625, 'learning_rate': 0.00017164405080118966, 'epoch': 0.15}\n",
      "{'loss': 1.4463, 'grad_norm': 0.447265625, 'learning_rate': 0.0001716326884237258, 'epoch': 0.15}\n",
      "{'loss': 1.1898, 'grad_norm': 0.4765625, 'learning_rate': 0.00017162132604626193, 'epoch': 0.15}\n",
      "{'loss': 1.2224, 'grad_norm': 0.375, 'learning_rate': 0.00017160996366879806, 'epoch': 0.15}\n",
      "{'loss': 1.3484, 'grad_norm': 0.81640625, 'learning_rate': 0.0001715986012913342, 'epoch': 0.15}\n",
      "{'loss': 1.0428, 'grad_norm': 0.224609375, 'learning_rate': 0.00017158723891387033, 'epoch': 0.15}\n",
      "{'loss': 1.344, 'grad_norm': 0.4609375, 'learning_rate': 0.00017157587653640648, 'epoch': 0.15}\n",
      "{'loss': 1.3236, 'grad_norm': 0.75, 'learning_rate': 0.00017156451415894263, 'epoch': 0.15}\n",
      "{'loss': 1.3381, 'grad_norm': 0.36328125, 'learning_rate': 0.00017155315178147876, 'epoch': 0.15}\n",
      "{'loss': 1.3668, 'grad_norm': 0.5078125, 'learning_rate': 0.0001715417894040149, 'epoch': 0.15}\n",
      "{'loss': 1.1218, 'grad_norm': 0.74609375, 'learning_rate': 0.00017153042702655103, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3934, 'grad_norm': 0.5390625, 'learning_rate': 0.00017151906464908719, 'epoch': 0.15}\n",
      "{'loss': 1.2101, 'grad_norm': 1.03125, 'learning_rate': 0.0001715077022716233, 'epoch': 0.15}\n",
      "{'loss': 1.3344, 'grad_norm': 0.376953125, 'learning_rate': 0.00017149633989415946, 'epoch': 0.15}\n",
      "{'loss': 1.2387, 'grad_norm': 0.55078125, 'learning_rate': 0.0001714849775166956, 'epoch': 0.15}\n",
      "{'loss': 1.1072, 'grad_norm': 0.353515625, 'learning_rate': 0.00017147361513923174, 'epoch': 0.15}\n",
      "{'loss': 1.4004, 'grad_norm': 0.470703125, 'learning_rate': 0.0001714622527617679, 'epoch': 0.15}\n",
      "{'loss': 1.2599, 'grad_norm': 0.390625, 'learning_rate': 0.000171450890384304, 'epoch': 0.15}\n",
      "{'loss': 1.1939, 'grad_norm': 0.33984375, 'learning_rate': 0.00017143952800684016, 'epoch': 0.15}\n",
      "{'loss': 1.2399, 'grad_norm': 0.5234375, 'learning_rate': 0.0001714281656293763, 'epoch': 0.15}\n",
      "{'loss': 1.0816, 'grad_norm': 0.62890625, 'learning_rate': 0.00017141680325191244, 'epoch': 0.15}\n",
      "{'loss': 1.3989, 'grad_norm': 0.423828125, 'learning_rate': 0.0001714054408744486, 'epoch': 0.15}\n",
      "{'loss': 1.1891, 'grad_norm': 0.5, 'learning_rate': 0.00017139407849698472, 'epoch': 0.15}\n",
      "{'loss': 1.2499, 'grad_norm': 0.384765625, 'learning_rate': 0.00017138271611952087, 'epoch': 0.15}\n",
      "{'loss': 1.2774, 'grad_norm': 0.40625, 'learning_rate': 0.000171371353742057, 'epoch': 0.15}\n",
      "{'loss': 1.0898, 'grad_norm': 0.99609375, 'learning_rate': 0.00017135999136459314, 'epoch': 0.15}\n",
      "{'loss': 1.4443, 'grad_norm': 0.390625, 'learning_rate': 0.00017134862898712927, 'epoch': 0.15}\n",
      "{'loss': 1.1927, 'grad_norm': 0.8359375, 'learning_rate': 0.00017133726660966542, 'epoch': 0.15}\n",
      "{'loss': 1.2469, 'grad_norm': 0.3515625, 'learning_rate': 0.00017132590423220157, 'epoch': 0.15}\n",
      "{'loss': 1.2647, 'grad_norm': 0.59375, 'learning_rate': 0.0001713145418547377, 'epoch': 0.15}\n",
      "{'loss': 1.1897, 'grad_norm': 0.66796875, 'learning_rate': 0.00017130317947727385, 'epoch': 0.15}\n",
      "{'loss': 1.3799, 'grad_norm': 0.49609375, 'learning_rate': 0.00017129181709980997, 'epoch': 0.15}\n",
      "{'loss': 1.2289, 'grad_norm': 0.62890625, 'learning_rate': 0.0001712804547223461, 'epoch': 0.15}\n",
      "{'loss': 1.1222, 'grad_norm': 0.400390625, 'learning_rate': 0.00017126909234488225, 'epoch': 0.15}\n",
      "{'loss': 1.2541, 'grad_norm': 0.70703125, 'learning_rate': 0.0001712577299674184, 'epoch': 0.15}\n",
      "{'loss': 1.1053, 'grad_norm': 0.73046875, 'learning_rate': 0.00017124636758995455, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4834, 'grad_norm': 0.6640625, 'learning_rate': 0.00017123500521249067, 'epoch': 0.15}\n",
      "{'loss': 1.2234, 'grad_norm': 0.54296875, 'learning_rate': 0.0001712236428350268, 'epoch': 0.15}\n",
      "{'loss': 1.3485, 'grad_norm': 0.400390625, 'learning_rate': 0.00017121228045756295, 'epoch': 0.15}\n",
      "{'loss': 1.2926, 'grad_norm': 0.765625, 'learning_rate': 0.00017120091808009907, 'epoch': 0.15}\n",
      "{'loss': 1.1703, 'grad_norm': 0.828125, 'learning_rate': 0.00017118955570263522, 'epoch': 0.15}\n",
      "{'loss': 1.4908, 'grad_norm': 0.515625, 'learning_rate': 0.00017117819332517138, 'epoch': 0.15}\n",
      "{'loss': 1.1392, 'grad_norm': 0.76953125, 'learning_rate': 0.0001711668309477075, 'epoch': 0.15}\n",
      "{'loss': 1.2821, 'grad_norm': 0.341796875, 'learning_rate': 0.00017115546857024365, 'epoch': 0.15}\n",
      "{'loss': 1.3382, 'grad_norm': 0.427734375, 'learning_rate': 0.00017114410619277978, 'epoch': 0.15}\n",
      "{'loss': 1.1022, 'grad_norm': 0.89453125, 'learning_rate': 0.00017113274381531593, 'epoch': 0.15}\n",
      "{'loss': 1.3966, 'grad_norm': 0.361328125, 'learning_rate': 0.00017112138143785205, 'epoch': 0.15}\n",
      "{'loss': 1.2786, 'grad_norm': 0.609375, 'learning_rate': 0.0001711100190603882, 'epoch': 0.15}\n",
      "{'loss': 1.3051, 'grad_norm': 0.42578125, 'learning_rate': 0.00017109865668292435, 'epoch': 0.15}\n",
      "{'loss': 1.2956, 'grad_norm': 0.42578125, 'learning_rate': 0.00017108729430546048, 'epoch': 0.15}\n",
      "{'loss': 1.2426, 'grad_norm': 0.82421875, 'learning_rate': 0.00017107593192799663, 'epoch': 0.15}\n",
      "{'loss': 1.475, 'grad_norm': 0.458984375, 'learning_rate': 0.00017106456955053275, 'epoch': 0.15}\n",
      "{'loss': 1.2292, 'grad_norm': 0.59765625, 'learning_rate': 0.0001710532071730689, 'epoch': 0.15}\n",
      "{'loss': 1.2223, 'grad_norm': 0.38671875, 'learning_rate': 0.00017104184479560503, 'epoch': 0.15}\n",
      "{'loss': 1.3535, 'grad_norm': 0.74609375, 'learning_rate': 0.00017103048241814118, 'epoch': 0.15}\n",
      "{'loss': 1.1616, 'grad_norm': 0.71484375, 'learning_rate': 0.00017101912004067733, 'epoch': 0.15}\n",
      "{'loss': 1.3614, 'grad_norm': 0.49609375, 'learning_rate': 0.00017100775766321346, 'epoch': 0.15}\n",
      "{'loss': 1.1226, 'grad_norm': 0.51171875, 'learning_rate': 0.0001709963952857496, 'epoch': 0.15}\n",
      "{'loss': 1.0883, 'grad_norm': 0.384765625, 'learning_rate': 0.00017098503290828573, 'epoch': 0.15}\n",
      "{'loss': 1.1468, 'grad_norm': 0.48046875, 'learning_rate': 0.00017097367053082188, 'epoch': 0.15}\n",
      "{'loss': 1.091, 'grad_norm': 1.0, 'learning_rate': 0.000170962308153358, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4043, 'grad_norm': 0.5546875, 'learning_rate': 0.00017095094577589416, 'epoch': 0.15}\n",
      "{'loss': 1.257, 'grad_norm': 0.9453125, 'learning_rate': 0.0001709395833984303, 'epoch': 0.15}\n",
      "{'loss': 1.3344, 'grad_norm': 0.361328125, 'learning_rate': 0.00017092822102096644, 'epoch': 0.15}\n",
      "{'loss': 1.2065, 'grad_norm': 0.44140625, 'learning_rate': 0.0001709168586435026, 'epoch': 0.15}\n",
      "{'loss': 1.1036, 'grad_norm': 1.4765625, 'learning_rate': 0.0001709054962660387, 'epoch': 0.15}\n",
      "{'loss': 1.3485, 'grad_norm': 0.44921875, 'learning_rate': 0.00017089413388857484, 'epoch': 0.15}\n",
      "{'loss': 1.2558, 'grad_norm': 0.66796875, 'learning_rate': 0.000170882771511111, 'epoch': 0.15}\n",
      "{'loss': 1.2761, 'grad_norm': 0.3828125, 'learning_rate': 0.00017087140913364714, 'epoch': 0.15}\n",
      "{'loss': 1.2614, 'grad_norm': 0.6015625, 'learning_rate': 0.0001708600467561833, 'epoch': 0.15}\n",
      "{'loss': 1.1177, 'grad_norm': 0.6953125, 'learning_rate': 0.00017084868437871941, 'epoch': 0.15}\n",
      "{'loss': 1.4452, 'grad_norm': 0.447265625, 'learning_rate': 0.00017083732200125554, 'epoch': 0.15}\n",
      "{'loss': 1.2016, 'grad_norm': 0.5078125, 'learning_rate': 0.0001708259596237917, 'epoch': 0.15}\n",
      "{'loss': 1.3397, 'grad_norm': 0.453125, 'learning_rate': 0.00017081459724632781, 'epoch': 0.15}\n",
      "{'loss': 1.3211, 'grad_norm': 0.578125, 'learning_rate': 0.00017080323486886397, 'epoch': 0.15}\n",
      "{'loss': 1.1671, 'grad_norm': 0.703125, 'learning_rate': 0.00017079187249140012, 'epoch': 0.15}\n",
      "{'loss': 1.3522, 'grad_norm': 0.5625, 'learning_rate': 0.00017078051011393624, 'epoch': 0.15}\n",
      "{'loss': 1.1833, 'grad_norm': 0.56640625, 'learning_rate': 0.0001707691477364724, 'epoch': 0.15}\n",
      "{'loss': 1.2432, 'grad_norm': 0.353515625, 'learning_rate': 0.00017075778535900852, 'epoch': 0.15}\n",
      "{'loss': 1.3017, 'grad_norm': 0.40234375, 'learning_rate': 0.00017074642298154467, 'epoch': 0.15}\n",
      "{'loss': 1.1476, 'grad_norm': 1.2578125, 'learning_rate': 0.0001707350606040808, 'epoch': 0.15}\n",
      "{'loss': 1.3611, 'grad_norm': 0.41796875, 'learning_rate': 0.00017072369822661694, 'epoch': 0.15}\n",
      "{'loss': 1.2968, 'grad_norm': 0.73046875, 'learning_rate': 0.0001707123358491531, 'epoch': 0.15}\n",
      "{'loss': 1.232, 'grad_norm': 0.400390625, 'learning_rate': 0.00017070097347168922, 'epoch': 0.15}\n",
      "{'loss': 1.3376, 'grad_norm': 0.474609375, 'learning_rate': 0.00017068961109422537, 'epoch': 0.15}\n",
      "{'loss': 1.2071, 'grad_norm': 0.435546875, 'learning_rate': 0.0001706782487167615, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4552, 'grad_norm': 0.38671875, 'learning_rate': 0.00017066688633929765, 'epoch': 0.16}\n",
      "{'loss': 1.3534, 'grad_norm': 0.640625, 'learning_rate': 0.00017065552396183377, 'epoch': 0.16}\n",
      "{'loss': 1.2219, 'grad_norm': 0.455078125, 'learning_rate': 0.00017064416158436992, 'epoch': 0.16}\n",
      "{'loss': 1.3531, 'grad_norm': 0.578125, 'learning_rate': 0.00017063279920690607, 'epoch': 0.16}\n",
      "{'loss': 1.1115, 'grad_norm': 0.54296875, 'learning_rate': 0.0001706214368294422, 'epoch': 0.16}\n",
      "{'loss': 1.2015, 'grad_norm': 0.388671875, 'learning_rate': 0.00017061007445197835, 'epoch': 0.16}\n",
      "{'loss': 1.1907, 'grad_norm': 0.62890625, 'learning_rate': 0.00017059871207451447, 'epoch': 0.16}\n",
      "{'loss': 1.3059, 'grad_norm': 0.36328125, 'learning_rate': 0.00017058734969705063, 'epoch': 0.16}\n",
      "{'loss': 1.2105, 'grad_norm': 0.44140625, 'learning_rate': 0.00017057598731958675, 'epoch': 0.16}\n",
      "{'loss': 1.1929, 'grad_norm': 0.90625, 'learning_rate': 0.0001705646249421229, 'epoch': 0.16}\n",
      "{'loss': 1.4093, 'grad_norm': 0.59375, 'learning_rate': 0.00017055326256465905, 'epoch': 0.16}\n",
      "{'loss': 1.2107, 'grad_norm': 0.57421875, 'learning_rate': 0.00017054190018719518, 'epoch': 0.16}\n",
      "{'loss': 1.2635, 'grad_norm': 0.40234375, 'learning_rate': 0.00017053053780973133, 'epoch': 0.16}\n",
      "{'loss': 1.2209, 'grad_norm': 0.54296875, 'learning_rate': 0.00017051917543226745, 'epoch': 0.16}\n",
      "{'loss': 1.1865, 'grad_norm': 1.484375, 'learning_rate': 0.00017050781305480358, 'epoch': 0.16}\n",
      "{'loss': 1.2897, 'grad_norm': 0.466796875, 'learning_rate': 0.00017049645067733973, 'epoch': 0.16}\n",
      "{'loss': 1.3719, 'grad_norm': 0.68359375, 'learning_rate': 0.00017048508829987588, 'epoch': 0.16}\n",
      "{'loss': 1.3737, 'grad_norm': 0.380859375, 'learning_rate': 0.00017047372592241203, 'epoch': 0.16}\n",
      "{'loss': 1.3123, 'grad_norm': 0.5703125, 'learning_rate': 0.00017046236354494816, 'epoch': 0.16}\n",
      "{'loss': 1.132, 'grad_norm': 0.66015625, 'learning_rate': 0.00017045100116748428, 'epoch': 0.16}\n",
      "{'loss': 1.4126, 'grad_norm': 0.388671875, 'learning_rate': 0.00017043963879002043, 'epoch': 0.16}\n",
      "{'loss': 1.198, 'grad_norm': 0.6875, 'learning_rate': 0.00017042827641255656, 'epoch': 0.16}\n",
      "{'loss': 1.2253, 'grad_norm': 0.42578125, 'learning_rate': 0.0001704169140350927, 'epoch': 0.16}\n",
      "{'loss': 1.3012, 'grad_norm': 0.5078125, 'learning_rate': 0.00017040555165762886, 'epoch': 0.16}\n",
      "{'loss': 1.1731, 'grad_norm': 0.65234375, 'learning_rate': 0.00017039418928016498, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2604, 'grad_norm': 0.490234375, 'learning_rate': 0.00017038282690270113, 'epoch': 0.16}\n",
      "{'loss': 1.2157, 'grad_norm': 0.9453125, 'learning_rate': 0.00017037146452523726, 'epoch': 0.16}\n",
      "{'loss': 1.3081, 'grad_norm': 0.369140625, 'learning_rate': 0.0001703601021477734, 'epoch': 0.16}\n",
      "{'loss': 1.2645, 'grad_norm': 0.53515625, 'learning_rate': 0.00017034873977030953, 'epoch': 0.16}\n",
      "{'loss': 1.1347, 'grad_norm': 0.3359375, 'learning_rate': 0.00017033737739284569, 'epoch': 0.16}\n",
      "{'loss': 1.3909, 'grad_norm': 0.482421875, 'learning_rate': 0.00017032601501538184, 'epoch': 0.16}\n",
      "{'loss': 1.4437, 'grad_norm': 0.5703125, 'learning_rate': 0.00017031465263791796, 'epoch': 0.16}\n",
      "{'loss': 1.1952, 'grad_norm': 0.5, 'learning_rate': 0.0001703032902604541, 'epoch': 0.16}\n",
      "{'loss': 1.3299, 'grad_norm': 0.7578125, 'learning_rate': 0.00017029192788299024, 'epoch': 0.16}\n",
      "{'loss': 1.1616, 'grad_norm': 0.859375, 'learning_rate': 0.0001702805655055264, 'epoch': 0.16}\n",
      "{'loss': 1.4579, 'grad_norm': 0.45703125, 'learning_rate': 0.0001702692031280625, 'epoch': 0.16}\n",
      "{'loss': 1.217, 'grad_norm': 0.55859375, 'learning_rate': 0.00017025784075059866, 'epoch': 0.16}\n",
      "{'loss': 1.3218, 'grad_norm': 0.390625, 'learning_rate': 0.00017024647837313482, 'epoch': 0.16}\n",
      "{'loss': 1.3243, 'grad_norm': 0.5234375, 'learning_rate': 0.00017023511599567094, 'epoch': 0.16}\n",
      "{'loss': 1.1113, 'grad_norm': 0.31640625, 'learning_rate': 0.0001702237536182071, 'epoch': 0.16}\n",
      "{'loss': 1.5005, 'grad_norm': 0.52734375, 'learning_rate': 0.00017021239124074322, 'epoch': 0.16}\n",
      "{'loss': 1.1941, 'grad_norm': 0.59375, 'learning_rate': 0.00017020102886327937, 'epoch': 0.16}\n",
      "{'loss': 1.3315, 'grad_norm': 0.423828125, 'learning_rate': 0.0001701896664858155, 'epoch': 0.16}\n",
      "{'loss': 1.3422, 'grad_norm': 0.435546875, 'learning_rate': 0.00017017830410835164, 'epoch': 0.16}\n",
      "{'loss': 1.0839, 'grad_norm': 0.6875, 'learning_rate': 0.0001701669417308878, 'epoch': 0.16}\n",
      "{'loss': 1.4958, 'grad_norm': 0.4140625, 'learning_rate': 0.00017015557935342392, 'epoch': 0.16}\n",
      "{'loss': 1.1583, 'grad_norm': 0.408203125, 'learning_rate': 0.00017014421697596007, 'epoch': 0.16}\n",
      "{'loss': 1.2828, 'grad_norm': 0.3828125, 'learning_rate': 0.0001701328545984962, 'epoch': 0.16}\n",
      "{'loss': 1.2738, 'grad_norm': 0.5078125, 'learning_rate': 0.00017012149222103232, 'epoch': 0.16}\n",
      "{'loss': 1.1445, 'grad_norm': 0.78515625, 'learning_rate': 0.00017011012984356847, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4455, 'grad_norm': 0.421875, 'learning_rate': 0.00017009876746610462, 'epoch': 0.16}\n",
      "{'loss': 1.1699, 'grad_norm': 0.765625, 'learning_rate': 0.00017008740508864077, 'epoch': 0.16}\n",
      "{'loss': 1.2755, 'grad_norm': 0.48046875, 'learning_rate': 0.0001700760427111769, 'epoch': 0.16}\n",
      "{'loss': 1.2681, 'grad_norm': 0.5078125, 'learning_rate': 0.00017006468033371302, 'epoch': 0.16}\n",
      "{'loss': 1.2252, 'grad_norm': 0.8203125, 'learning_rate': 0.00017005331795624917, 'epoch': 0.16}\n",
      "{'loss': 1.2431, 'grad_norm': 0.423828125, 'learning_rate': 0.0001700419555787853, 'epoch': 0.16}\n",
      "{'loss': 1.2177, 'grad_norm': 0.55859375, 'learning_rate': 0.00017003059320132145, 'epoch': 0.16}\n",
      "{'loss': 1.192, 'grad_norm': 0.470703125, 'learning_rate': 0.0001700192308238576, 'epoch': 0.16}\n",
      "{'loss': 1.3022, 'grad_norm': 0.486328125, 'learning_rate': 0.00017000786844639372, 'epoch': 0.16}\n",
      "{'loss': 0.9348, 'grad_norm': 0.5, 'learning_rate': 0.00016999650606892988, 'epoch': 0.16}\n",
      "{'loss': 1.5349, 'grad_norm': 0.44921875, 'learning_rate': 0.000169985143691466, 'epoch': 0.16}\n",
      "{'loss': 1.1709, 'grad_norm': 0.73046875, 'learning_rate': 0.00016997378131400215, 'epoch': 0.16}\n",
      "{'loss': 1.1903, 'grad_norm': 0.38671875, 'learning_rate': 0.00016996241893653828, 'epoch': 0.16}\n",
      "{'loss': 1.362, 'grad_norm': 0.55859375, 'learning_rate': 0.00016995105655907443, 'epoch': 0.16}\n",
      "{'loss': 1.1299, 'grad_norm': 0.9375, 'learning_rate': 0.00016993969418161058, 'epoch': 0.16}\n",
      "{'loss': 1.4435, 'grad_norm': 0.359375, 'learning_rate': 0.0001699283318041467, 'epoch': 0.16}\n",
      "{'loss': 1.1901, 'grad_norm': 0.73046875, 'learning_rate': 0.00016991696942668285, 'epoch': 0.16}\n",
      "{'loss': 1.4544, 'grad_norm': 0.36328125, 'learning_rate': 0.00016990560704921898, 'epoch': 0.16}\n",
      "{'loss': 1.3581, 'grad_norm': 0.51953125, 'learning_rate': 0.00016989424467175513, 'epoch': 0.16}\n",
      "{'loss': 1.211, 'grad_norm': 0.78125, 'learning_rate': 0.00016988288229429125, 'epoch': 0.16}\n",
      "{'loss': 1.4917, 'grad_norm': 0.44921875, 'learning_rate': 0.0001698715199168274, 'epoch': 0.16}\n",
      "{'loss': 1.3974, 'grad_norm': 0.66015625, 'learning_rate': 0.00016986015753936356, 'epoch': 0.16}\n",
      "{'loss': 1.2751, 'grad_norm': 0.462890625, 'learning_rate': 0.00016984879516189968, 'epoch': 0.16}\n",
      "{'loss': 1.3553, 'grad_norm': 0.64453125, 'learning_rate': 0.00016983743278443583, 'epoch': 0.16}\n",
      "{'loss': 1.0598, 'grad_norm': 0.83984375, 'learning_rate': 0.00016982607040697196, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3448, 'grad_norm': 0.478515625, 'learning_rate': 0.0001698147080295081, 'epoch': 0.16}\n",
      "{'loss': 1.2306, 'grad_norm': 0.5859375, 'learning_rate': 0.00016980334565204423, 'epoch': 0.16}\n",
      "{'loss': 1.2994, 'grad_norm': 0.41015625, 'learning_rate': 0.00016979198327458038, 'epoch': 0.16}\n",
      "{'loss': 1.3011, 'grad_norm': 0.54296875, 'learning_rate': 0.00016978062089711654, 'epoch': 0.16}\n",
      "{'loss': 1.1458, 'grad_norm': 0.75, 'learning_rate': 0.00016976925851965266, 'epoch': 0.16}\n",
      "{'loss': 1.3889, 'grad_norm': 0.443359375, 'learning_rate': 0.0001697578961421888, 'epoch': 0.16}\n",
      "{'loss': 1.2873, 'grad_norm': 0.6015625, 'learning_rate': 0.00016974653376472494, 'epoch': 0.16}\n",
      "{'loss': 1.307, 'grad_norm': 0.404296875, 'learning_rate': 0.00016973517138726106, 'epoch': 0.16}\n",
      "{'loss': 1.37, 'grad_norm': 0.470703125, 'learning_rate': 0.0001697238090097972, 'epoch': 0.16}\n",
      "{'loss': 1.1408, 'grad_norm': 0.53515625, 'learning_rate': 0.00016971244663233336, 'epoch': 0.16}\n",
      "{'loss': 1.3443, 'grad_norm': 0.59375, 'learning_rate': 0.00016970108425486951, 'epoch': 0.16}\n",
      "{'loss': 1.2818, 'grad_norm': 0.8984375, 'learning_rate': 0.00016968972187740564, 'epoch': 0.16}\n",
      "{'loss': 1.4274, 'grad_norm': 0.412109375, 'learning_rate': 0.00016967835949994176, 'epoch': 0.16}\n",
      "{'loss': 1.3381, 'grad_norm': 0.65234375, 'learning_rate': 0.00016966699712247791, 'epoch': 0.16}\n",
      "{'loss': 1.2341, 'grad_norm': 0.91796875, 'learning_rate': 0.00016965563474501404, 'epoch': 0.16}\n",
      "{'loss': 1.5723, 'grad_norm': 0.578125, 'learning_rate': 0.0001696442723675502, 'epoch': 0.16}\n",
      "{'loss': 1.1882, 'grad_norm': 0.546875, 'learning_rate': 0.00016963290999008634, 'epoch': 0.16}\n",
      "{'loss': 1.1056, 'grad_norm': 0.404296875, 'learning_rate': 0.00016962154761262247, 'epoch': 0.16}\n",
      "{'loss': 1.3868, 'grad_norm': 0.4609375, 'learning_rate': 0.00016961018523515862, 'epoch': 0.16}\n",
      "{'loss': 1.1116, 'grad_norm': 0.6484375, 'learning_rate': 0.00016959882285769474, 'epoch': 0.16}\n",
      "{'loss': 1.3446, 'grad_norm': 0.50390625, 'learning_rate': 0.0001695874604802309, 'epoch': 0.16}\n",
      "{'loss': 1.2853, 'grad_norm': 0.5390625, 'learning_rate': 0.00016957609810276702, 'epoch': 0.16}\n",
      "{'loss': 1.2207, 'grad_norm': 0.3828125, 'learning_rate': 0.00016956473572530317, 'epoch': 0.16}\n",
      "{'loss': 1.4031, 'grad_norm': 0.56640625, 'learning_rate': 0.00016955337334783932, 'epoch': 0.16}\n",
      "{'loss': 1.162, 'grad_norm': 0.46484375, 'learning_rate': 0.00016954201097037544, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4038, 'grad_norm': 0.50390625, 'learning_rate': 0.0001695306485929116, 'epoch': 0.16}\n",
      "{'loss': 1.1809, 'grad_norm': 0.50390625, 'learning_rate': 0.00016951928621544772, 'epoch': 0.16}\n",
      "{'loss': 1.2301, 'grad_norm': 0.4453125, 'learning_rate': 0.00016950792383798387, 'epoch': 0.16}\n",
      "{'loss': 1.1189, 'grad_norm': 0.462890625, 'learning_rate': 0.00016949656146052, 'epoch': 0.16}\n",
      "{'loss': 1.1755, 'grad_norm': 0.640625, 'learning_rate': 0.00016948519908305615, 'epoch': 0.16}\n",
      "{'loss': 1.459, 'grad_norm': 0.408203125, 'learning_rate': 0.0001694738367055923, 'epoch': 0.16}\n",
      "{'loss': 1.3012, 'grad_norm': 0.7421875, 'learning_rate': 0.00016946247432812842, 'epoch': 0.16}\n",
      "{'loss': 1.2759, 'grad_norm': 0.3515625, 'learning_rate': 0.00016945111195066457, 'epoch': 0.16}\n",
      "{'loss': 1.2258, 'grad_norm': 0.345703125, 'learning_rate': 0.0001694397495732007, 'epoch': 0.16}\n",
      "{'loss': 1.0647, 'grad_norm': 0.37109375, 'learning_rate': 0.00016942838719573685, 'epoch': 0.16}\n",
      "{'loss': 1.3201, 'grad_norm': 0.47265625, 'learning_rate': 0.00016941702481827297, 'epoch': 0.16}\n",
      "{'loss': 1.2324, 'grad_norm': 0.62890625, 'learning_rate': 0.00016940566244080912, 'epoch': 0.16}\n",
      "{'loss': 1.3138, 'grad_norm': 0.359375, 'learning_rate': 0.00016939430006334528, 'epoch': 0.16}\n",
      "{'loss': 1.2651, 'grad_norm': 0.369140625, 'learning_rate': 0.0001693829376858814, 'epoch': 0.16}\n",
      "{'loss': 1.2522, 'grad_norm': 0.7734375, 'learning_rate': 0.00016937157530841755, 'epoch': 0.16}\n",
      "{'loss': 1.4189, 'grad_norm': 0.4375, 'learning_rate': 0.00016936021293095368, 'epoch': 0.16}\n",
      "{'loss': 1.1517, 'grad_norm': 0.66796875, 'learning_rate': 0.0001693488505534898, 'epoch': 0.16}\n",
      "{'loss': 1.262, 'grad_norm': 0.38671875, 'learning_rate': 0.00016933748817602595, 'epoch': 0.16}\n",
      "{'loss': 1.3512, 'grad_norm': 0.67578125, 'learning_rate': 0.0001693261257985621, 'epoch': 0.16}\n",
      "{'loss': 1.1038, 'grad_norm': 0.65625, 'learning_rate': 0.00016931476342109825, 'epoch': 0.16}\n",
      "{'loss': 1.4444, 'grad_norm': 0.466796875, 'learning_rate': 0.00016930340104363438, 'epoch': 0.16}\n",
      "{'loss': 1.1088, 'grad_norm': 0.58984375, 'learning_rate': 0.0001692920386661705, 'epoch': 0.16}\n",
      "{'loss': 1.387, 'grad_norm': 0.3515625, 'learning_rate': 0.00016928067628870665, 'epoch': 0.16}\n",
      "{'loss': 1.2161, 'grad_norm': 0.55078125, 'learning_rate': 0.00016926931391124278, 'epoch': 0.16}\n",
      "{'loss': 1.1451, 'grad_norm': 0.84375, 'learning_rate': 0.00016925795153377896, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3764, 'grad_norm': 0.5, 'learning_rate': 0.00016924658915631508, 'epoch': 0.16}\n",
      "{'loss': 1.251, 'grad_norm': 0.57421875, 'learning_rate': 0.0001692352267788512, 'epoch': 0.16}\n",
      "{'loss': 1.2201, 'grad_norm': 0.365234375, 'learning_rate': 0.00016922386440138736, 'epoch': 0.16}\n",
      "{'loss': 1.3625, 'grad_norm': 0.498046875, 'learning_rate': 0.00016921250202392348, 'epoch': 0.16}\n",
      "{'loss': 1.1108, 'grad_norm': 1.1171875, 'learning_rate': 0.00016920113964645963, 'epoch': 0.16}\n",
      "{'loss': 1.4884, 'grad_norm': 0.482421875, 'learning_rate': 0.00016918977726899576, 'epoch': 0.16}\n",
      "{'loss': 1.1918, 'grad_norm': 0.59375, 'learning_rate': 0.0001691784148915319, 'epoch': 0.16}\n",
      "{'loss': 1.3004, 'grad_norm': 0.4453125, 'learning_rate': 0.00016916705251406806, 'epoch': 0.16}\n",
      "{'loss': 1.2687, 'grad_norm': 0.8359375, 'learning_rate': 0.00016915569013660418, 'epoch': 0.16}\n",
      "{'loss': 1.0972, 'grad_norm': 0.62890625, 'learning_rate': 0.00016914432775914034, 'epoch': 0.16}\n",
      "{'loss': 1.3995, 'grad_norm': 0.439453125, 'learning_rate': 0.00016913296538167646, 'epoch': 0.16}\n",
      "{'loss': 1.2831, 'grad_norm': 0.69140625, 'learning_rate': 0.0001691216030042126, 'epoch': 0.16}\n",
      "{'loss': 1.3231, 'grad_norm': 0.435546875, 'learning_rate': 0.00016911024062674874, 'epoch': 0.16}\n",
      "{'loss': 1.333, 'grad_norm': 0.64453125, 'learning_rate': 0.0001690988782492849, 'epoch': 0.16}\n",
      "{'loss': 1.2278, 'grad_norm': 0.83203125, 'learning_rate': 0.00016908751587182104, 'epoch': 0.16}\n",
      "{'loss': 1.4217, 'grad_norm': 0.53515625, 'learning_rate': 0.00016907615349435716, 'epoch': 0.16}\n",
      "{'loss': 1.351, 'grad_norm': 0.6875, 'learning_rate': 0.00016906479111689331, 'epoch': 0.16}\n",
      "{'loss': 1.2691, 'grad_norm': 0.392578125, 'learning_rate': 0.00016905342873942944, 'epoch': 0.16}\n",
      "{'loss': 1.2649, 'grad_norm': 0.419921875, 'learning_rate': 0.0001690420663619656, 'epoch': 0.16}\n",
      "{'loss': 1.2211, 'grad_norm': 0.58203125, 'learning_rate': 0.00016903070398450171, 'epoch': 0.16}\n",
      "{'loss': 1.3099, 'grad_norm': 0.58984375, 'learning_rate': 0.00016901934160703787, 'epoch': 0.16}\n",
      "{'loss': 1.2449, 'grad_norm': 0.58984375, 'learning_rate': 0.00016900797922957402, 'epoch': 0.16}\n",
      "{'loss': 1.1433, 'grad_norm': 0.470703125, 'learning_rate': 0.00016899661685211014, 'epoch': 0.16}\n",
      "{'loss': 1.1776, 'grad_norm': 0.59375, 'learning_rate': 0.0001689852544746463, 'epoch': 0.16}\n",
      "{'loss': 1.2, 'grad_norm': 0.71484375, 'learning_rate': 0.00016897389209718242, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3855, 'grad_norm': 0.51171875, 'learning_rate': 0.00016896252971971854, 'epoch': 0.16}\n",
      "{'loss': 1.2398, 'grad_norm': 0.66015625, 'learning_rate': 0.00016895116734225472, 'epoch': 0.16}\n",
      "{'loss': 1.4978, 'grad_norm': 0.36328125, 'learning_rate': 0.00016893980496479084, 'epoch': 0.16}\n",
      "{'loss': 1.2403, 'grad_norm': 0.466796875, 'learning_rate': 0.000168928442587327, 'epoch': 0.16}\n",
      "{'loss': 1.085, 'grad_norm': 1.140625, 'learning_rate': 0.00016891708020986312, 'epoch': 0.16}\n",
      "{'loss': 1.4365, 'grad_norm': 0.484375, 'learning_rate': 0.00016890571783239924, 'epoch': 0.16}\n",
      "{'loss': 1.1817, 'grad_norm': 0.63671875, 'learning_rate': 0.0001688943554549354, 'epoch': 0.16}\n",
      "{'loss': 1.2673, 'grad_norm': 0.48828125, 'learning_rate': 0.00016888299307747152, 'epoch': 0.16}\n",
      "{'loss': 1.3567, 'grad_norm': 0.5390625, 'learning_rate': 0.0001688716307000077, 'epoch': 0.16}\n",
      "{'loss': 1.1845, 'grad_norm': 0.408203125, 'learning_rate': 0.00016886026832254382, 'epoch': 0.16}\n",
      "{'loss': 1.3329, 'grad_norm': 0.55078125, 'learning_rate': 0.00016884890594507995, 'epoch': 0.16}\n",
      "{'loss': 1.1528, 'grad_norm': 0.68359375, 'learning_rate': 0.0001688375435676161, 'epoch': 0.16}\n",
      "{'loss': 1.3265, 'grad_norm': 0.54296875, 'learning_rate': 0.00016882618119015222, 'epoch': 0.16}\n",
      "{'loss': 1.3889, 'grad_norm': 0.66015625, 'learning_rate': 0.00016881481881268837, 'epoch': 0.16}\n",
      "{'loss': 1.1729, 'grad_norm': 0.5859375, 'learning_rate': 0.0001688034564352245, 'epoch': 0.16}\n",
      "{'loss': 1.3697, 'grad_norm': 0.44140625, 'learning_rate': 0.00016879209405776065, 'epoch': 0.16}\n",
      "{'loss': 1.2475, 'grad_norm': 0.4765625, 'learning_rate': 0.0001687807316802968, 'epoch': 0.16}\n",
      "{'loss': 1.2162, 'grad_norm': 0.41015625, 'learning_rate': 0.00016876936930283293, 'epoch': 0.16}\n",
      "{'loss': 1.3508, 'grad_norm': 0.4765625, 'learning_rate': 0.00016875800692536908, 'epoch': 0.16}\n",
      "{'loss': 1.1934, 'grad_norm': 0.9140625, 'learning_rate': 0.0001687466445479052, 'epoch': 0.16}\n",
      "{'loss': 1.4207, 'grad_norm': 0.408203125, 'learning_rate': 0.00016873528217044135, 'epoch': 0.16}\n",
      "{'loss': 1.2843, 'grad_norm': 0.90625, 'learning_rate': 0.00016872391979297748, 'epoch': 0.16}\n",
      "{'loss': 1.2881, 'grad_norm': 0.431640625, 'learning_rate': 0.00016871255741551363, 'epoch': 0.16}\n",
      "{'loss': 1.3442, 'grad_norm': 0.4765625, 'learning_rate': 0.00016870119503804978, 'epoch': 0.16}\n",
      "{'loss': 1.0529, 'grad_norm': 1.1328125, 'learning_rate': 0.0001686898326605859, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5238, 'grad_norm': 0.51953125, 'learning_rate': 0.00016867847028312206, 'epoch': 0.16}\n",
      "{'loss': 1.1466, 'grad_norm': 0.5859375, 'learning_rate': 0.00016866710790565818, 'epoch': 0.16}\n",
      "{'loss': 1.3943, 'grad_norm': 0.5390625, 'learning_rate': 0.00016865574552819433, 'epoch': 0.16}\n",
      "{'loss': 1.2886, 'grad_norm': 0.5390625, 'learning_rate': 0.00016864438315073046, 'epoch': 0.16}\n",
      "{'loss': 1.2666, 'grad_norm': 0.625, 'learning_rate': 0.0001686330207732666, 'epoch': 0.16}\n",
      "{'loss': 1.3276, 'grad_norm': 0.40234375, 'learning_rate': 0.00016862165839580276, 'epoch': 0.16}\n",
      "{'loss': 1.2177, 'grad_norm': 0.6171875, 'learning_rate': 0.00016861029601833888, 'epoch': 0.16}\n",
      "{'loss': 1.3474, 'grad_norm': 0.37890625, 'learning_rate': 0.00016859893364087503, 'epoch': 0.16}\n",
      "{'loss': 1.3145, 'grad_norm': 0.48828125, 'learning_rate': 0.00016858757126341116, 'epoch': 0.16}\n",
      "{'loss': 1.1574, 'grad_norm': 0.5703125, 'learning_rate': 0.00016857620888594728, 'epoch': 0.16}\n",
      "{'loss': 1.4392, 'grad_norm': 0.435546875, 'learning_rate': 0.00016856484650848346, 'epoch': 0.16}\n",
      "{'loss': 1.2097, 'grad_norm': 0.55078125, 'learning_rate': 0.00016855348413101959, 'epoch': 0.16}\n",
      "{'loss': 1.1986, 'grad_norm': 0.361328125, 'learning_rate': 0.00016854212175355574, 'epoch': 0.16}\n",
      "{'loss': 1.2064, 'grad_norm': 0.51953125, 'learning_rate': 0.00016853075937609186, 'epoch': 0.16}\n",
      "{'loss': 1.1541, 'grad_norm': 1.046875, 'learning_rate': 0.00016851939699862799, 'epoch': 0.16}\n",
      "{'loss': 1.4632, 'grad_norm': 0.396484375, 'learning_rate': 0.00016850803462116414, 'epoch': 0.16}\n",
      "{'loss': 1.1919, 'grad_norm': 0.625, 'learning_rate': 0.00016849667224370026, 'epoch': 0.16}\n",
      "{'loss': 1.2778, 'grad_norm': 0.40625, 'learning_rate': 0.00016848530986623644, 'epoch': 0.16}\n",
      "{'loss': 1.2823, 'grad_norm': 0.5, 'learning_rate': 0.00016847394748877256, 'epoch': 0.16}\n",
      "{'loss': 1.1959, 'grad_norm': 0.46875, 'learning_rate': 0.0001684625851113087, 'epoch': 0.16}\n",
      "{'loss': 1.2527, 'grad_norm': 0.55859375, 'learning_rate': 0.00016845122273384484, 'epoch': 0.16}\n",
      "{'loss': 1.2574, 'grad_norm': 1.03125, 'learning_rate': 0.00016843986035638096, 'epoch': 0.16}\n",
      "{'loss': 1.1931, 'grad_norm': 0.396484375, 'learning_rate': 0.00016842849797891712, 'epoch': 0.16}\n",
      "{'loss': 1.3138, 'grad_norm': 0.62109375, 'learning_rate': 0.00016841713560145324, 'epoch': 0.16}\n",
      "{'loss': 1.1628, 'grad_norm': 0.9296875, 'learning_rate': 0.0001684057732239894, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3656, 'grad_norm': 0.419921875, 'learning_rate': 0.00016839441084652554, 'epoch': 0.16}\n",
      "{'loss': 1.1907, 'grad_norm': 0.5625, 'learning_rate': 0.00016838304846906167, 'epoch': 0.17}\n",
      "{'loss': 1.286, 'grad_norm': 0.375, 'learning_rate': 0.00016837168609159782, 'epoch': 0.17}\n",
      "{'loss': 1.1937, 'grad_norm': 0.4765625, 'learning_rate': 0.00016836032371413394, 'epoch': 0.17}\n",
      "{'loss': 1.1752, 'grad_norm': 0.6875, 'learning_rate': 0.0001683489613366701, 'epoch': 0.17}\n",
      "{'loss': 1.5061, 'grad_norm': 0.490234375, 'learning_rate': 0.00016833759895920622, 'epoch': 0.17}\n",
      "{'loss': 1.3477, 'grad_norm': 0.8671875, 'learning_rate': 0.00016832623658174237, 'epoch': 0.17}\n",
      "{'loss': 1.3483, 'grad_norm': 0.4765625, 'learning_rate': 0.00016831487420427852, 'epoch': 0.17}\n",
      "{'loss': 1.4167, 'grad_norm': 0.58984375, 'learning_rate': 0.00016830351182681465, 'epoch': 0.17}\n",
      "{'loss': 1.0713, 'grad_norm': 0.63671875, 'learning_rate': 0.0001682921494493508, 'epoch': 0.17}\n",
      "{'loss': 1.3576, 'grad_norm': 0.390625, 'learning_rate': 0.00016828078707188692, 'epoch': 0.17}\n",
      "{'loss': 1.138, 'grad_norm': 0.58984375, 'learning_rate': 0.00016826942469442307, 'epoch': 0.17}\n",
      "{'loss': 1.2033, 'grad_norm': 0.36328125, 'learning_rate': 0.00016825806231695922, 'epoch': 0.17}\n",
      "{'loss': 1.2573, 'grad_norm': 0.53515625, 'learning_rate': 0.00016824669993949535, 'epoch': 0.17}\n",
      "{'loss': 1.2254, 'grad_norm': 0.74609375, 'learning_rate': 0.0001682353375620315, 'epoch': 0.17}\n",
      "{'loss': 1.3897, 'grad_norm': 0.416015625, 'learning_rate': 0.00016822397518456762, 'epoch': 0.17}\n",
      "{'loss': 1.2484, 'grad_norm': 0.76171875, 'learning_rate': 0.00016821261280710378, 'epoch': 0.17}\n",
      "{'loss': 1.2147, 'grad_norm': 0.5, 'learning_rate': 0.0001682012504296399, 'epoch': 0.17}\n",
      "{'loss': 1.3179, 'grad_norm': 0.5078125, 'learning_rate': 0.00016818988805217602, 'epoch': 0.17}\n",
      "{'loss': 1.0725, 'grad_norm': 0.74609375, 'learning_rate': 0.0001681785256747122, 'epoch': 0.17}\n",
      "{'loss': 1.3514, 'grad_norm': 0.419921875, 'learning_rate': 0.00016816716329724833, 'epoch': 0.17}\n",
      "{'loss': 1.2345, 'grad_norm': 0.625, 'learning_rate': 0.00016815580091978448, 'epoch': 0.17}\n",
      "{'loss': 1.2025, 'grad_norm': 0.474609375, 'learning_rate': 0.0001681444385423206, 'epoch': 0.17}\n",
      "{'loss': 1.3274, 'grad_norm': 0.412109375, 'learning_rate': 0.00016813307616485673, 'epoch': 0.17}\n",
      "{'loss': 1.1827, 'grad_norm': 0.375, 'learning_rate': 0.00016812171378739288, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3058, 'grad_norm': 0.546875, 'learning_rate': 0.000168110351409929, 'epoch': 0.17}\n",
      "{'loss': 1.146, 'grad_norm': 0.578125, 'learning_rate': 0.00016809898903246518, 'epoch': 0.17}\n",
      "{'loss': 1.2105, 'grad_norm': 0.443359375, 'learning_rate': 0.0001680876266550013, 'epoch': 0.17}\n",
      "{'loss': 1.394, 'grad_norm': 0.63671875, 'learning_rate': 0.00016807626427753743, 'epoch': 0.17}\n",
      "{'loss': 1.0838, 'grad_norm': 0.63671875, 'learning_rate': 0.00016806490190007358, 'epoch': 0.17}\n",
      "{'loss': 1.4601, 'grad_norm': 0.74609375, 'learning_rate': 0.0001680535395226097, 'epoch': 0.17}\n",
      "{'loss': 1.2954, 'grad_norm': 0.46484375, 'learning_rate': 0.00016804217714514586, 'epoch': 0.17}\n",
      "{'loss': 1.2763, 'grad_norm': 0.474609375, 'learning_rate': 0.00016803081476768198, 'epoch': 0.17}\n",
      "{'loss': 1.2036, 'grad_norm': 0.52734375, 'learning_rate': 0.00016801945239021813, 'epoch': 0.17}\n",
      "{'loss': 1.0812, 'grad_norm': 1.046875, 'learning_rate': 0.00016800809001275428, 'epoch': 0.17}\n",
      "{'loss': 1.4324, 'grad_norm': 0.396484375, 'learning_rate': 0.0001679967276352904, 'epoch': 0.17}\n",
      "{'loss': 1.2415, 'grad_norm': 0.7109375, 'learning_rate': 0.00016798536525782656, 'epoch': 0.17}\n",
      "{'loss': 1.3032, 'grad_norm': 0.333984375, 'learning_rate': 0.00016797400288036268, 'epoch': 0.17}\n",
      "{'loss': 1.1586, 'grad_norm': 0.54296875, 'learning_rate': 0.00016796264050289884, 'epoch': 0.17}\n",
      "{'loss': 1.0333, 'grad_norm': 0.58984375, 'learning_rate': 0.00016795127812543496, 'epoch': 0.17}\n",
      "{'loss': 1.3887, 'grad_norm': 0.443359375, 'learning_rate': 0.0001679399157479711, 'epoch': 0.17}\n",
      "{'loss': 1.1982, 'grad_norm': 0.53515625, 'learning_rate': 0.00016792855337050726, 'epoch': 0.17}\n",
      "{'loss': 1.3223, 'grad_norm': 0.42578125, 'learning_rate': 0.0001679171909930434, 'epoch': 0.17}\n",
      "{'loss': 1.282, 'grad_norm': 0.4609375, 'learning_rate': 0.00016790582861557954, 'epoch': 0.17}\n",
      "{'loss': 1.022, 'grad_norm': 0.82421875, 'learning_rate': 0.00016789446623811566, 'epoch': 0.17}\n",
      "{'loss': 1.3875, 'grad_norm': 0.443359375, 'learning_rate': 0.00016788310386065181, 'epoch': 0.17}\n",
      "{'loss': 1.2615, 'grad_norm': 0.59765625, 'learning_rate': 0.00016787174148318797, 'epoch': 0.17}\n",
      "{'loss': 1.1461, 'grad_norm': 0.369140625, 'learning_rate': 0.0001678603791057241, 'epoch': 0.17}\n",
      "{'loss': 1.3487, 'grad_norm': 0.5234375, 'learning_rate': 0.00016784901672826024, 'epoch': 0.17}\n",
      "{'loss': 1.0817, 'grad_norm': 0.271484375, 'learning_rate': 0.00016783765435079637, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3977, 'grad_norm': 0.6328125, 'learning_rate': 0.00016782629197333252, 'epoch': 0.17}\n",
      "{'loss': 1.2299, 'grad_norm': 0.60546875, 'learning_rate': 0.00016781492959586864, 'epoch': 0.17}\n",
      "{'loss': 1.3774, 'grad_norm': 0.41015625, 'learning_rate': 0.00016780356721840477, 'epoch': 0.17}\n",
      "{'loss': 1.2379, 'grad_norm': 0.43359375, 'learning_rate': 0.00016779220484094094, 'epoch': 0.17}\n",
      "{'loss': 1.187, 'grad_norm': 0.63671875, 'learning_rate': 0.00016778084246347707, 'epoch': 0.17}\n",
      "{'loss': 1.2771, 'grad_norm': 0.388671875, 'learning_rate': 0.00016776948008601322, 'epoch': 0.17}\n",
      "{'loss': 1.1629, 'grad_norm': 0.9375, 'learning_rate': 0.00016775811770854934, 'epoch': 0.17}\n",
      "{'loss': 1.0561, 'grad_norm': 0.357421875, 'learning_rate': 0.00016774675533108547, 'epoch': 0.17}\n",
      "{'loss': 1.2311, 'grad_norm': 0.373046875, 'learning_rate': 0.00016773539295362162, 'epoch': 0.17}\n",
      "{'loss': 1.1451, 'grad_norm': 1.53125, 'learning_rate': 0.00016772403057615774, 'epoch': 0.17}\n",
      "{'loss': 1.4821, 'grad_norm': 0.41015625, 'learning_rate': 0.00016771266819869392, 'epoch': 0.17}\n",
      "{'loss': 1.2843, 'grad_norm': 0.49609375, 'learning_rate': 0.00016770130582123005, 'epoch': 0.17}\n",
      "{'loss': 1.3223, 'grad_norm': 0.39453125, 'learning_rate': 0.00016768994344376617, 'epoch': 0.17}\n",
      "{'loss': 1.2918, 'grad_norm': 0.486328125, 'learning_rate': 0.00016767858106630232, 'epoch': 0.17}\n",
      "{'loss': 1.1947, 'grad_norm': 0.439453125, 'learning_rate': 0.00016766721868883845, 'epoch': 0.17}\n",
      "{'loss': 1.3273, 'grad_norm': 0.37890625, 'learning_rate': 0.0001676558563113746, 'epoch': 0.17}\n",
      "{'loss': 1.1573, 'grad_norm': 0.7109375, 'learning_rate': 0.00016764449393391072, 'epoch': 0.17}\n",
      "{'loss': 1.2788, 'grad_norm': 0.494140625, 'learning_rate': 0.00016763313155644687, 'epoch': 0.17}\n",
      "{'loss': 1.3681, 'grad_norm': 0.5546875, 'learning_rate': 0.00016762176917898303, 'epoch': 0.17}\n",
      "{'loss': 1.0611, 'grad_norm': 1.0703125, 'learning_rate': 0.00016761040680151915, 'epoch': 0.17}\n",
      "{'loss': 1.3829, 'grad_norm': 0.43359375, 'learning_rate': 0.0001675990444240553, 'epoch': 0.17}\n",
      "{'loss': 1.266, 'grad_norm': 0.578125, 'learning_rate': 0.00016758768204659143, 'epoch': 0.17}\n",
      "{'loss': 1.2633, 'grad_norm': 0.478515625, 'learning_rate': 0.00016757631966912758, 'epoch': 0.17}\n",
      "{'loss': 1.2553, 'grad_norm': 0.8125, 'learning_rate': 0.00016756495729166373, 'epoch': 0.17}\n",
      "{'loss': 1.0643, 'grad_norm': 0.6015625, 'learning_rate': 0.00016755359491419985, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4765, 'grad_norm': 0.400390625, 'learning_rate': 0.000167542232536736, 'epoch': 0.17}\n",
      "{'loss': 1.1851, 'grad_norm': 0.5859375, 'learning_rate': 0.00016753087015927213, 'epoch': 0.17}\n",
      "{'loss': 1.275, 'grad_norm': 0.353515625, 'learning_rate': 0.00016751950778180828, 'epoch': 0.17}\n",
      "{'loss': 1.3772, 'grad_norm': 0.74609375, 'learning_rate': 0.0001675081454043444, 'epoch': 0.17}\n",
      "{'loss': 1.1061, 'grad_norm': 0.51953125, 'learning_rate': 0.00016749678302688056, 'epoch': 0.17}\n",
      "{'loss': 1.2844, 'grad_norm': 0.48046875, 'learning_rate': 0.0001674854206494167, 'epoch': 0.17}\n",
      "{'loss': 1.2589, 'grad_norm': 1.5234375, 'learning_rate': 0.00016747405827195283, 'epoch': 0.17}\n",
      "{'loss': 1.3881, 'grad_norm': 0.35546875, 'learning_rate': 0.00016746269589448898, 'epoch': 0.17}\n",
      "{'loss': 1.2991, 'grad_norm': 0.921875, 'learning_rate': 0.0001674513335170251, 'epoch': 0.17}\n",
      "{'loss': 1.0372, 'grad_norm': 0.71484375, 'learning_rate': 0.00016743997113956126, 'epoch': 0.17}\n",
      "{'loss': 1.208, 'grad_norm': 0.4375, 'learning_rate': 0.00016742860876209738, 'epoch': 0.17}\n",
      "{'loss': 1.1144, 'grad_norm': 0.48046875, 'learning_rate': 0.0001674172463846335, 'epoch': 0.17}\n",
      "{'loss': 1.1703, 'grad_norm': 0.40625, 'learning_rate': 0.00016740588400716969, 'epoch': 0.17}\n",
      "{'loss': 1.3737, 'grad_norm': 0.65234375, 'learning_rate': 0.0001673945216297058, 'epoch': 0.17}\n",
      "{'loss': 1.0689, 'grad_norm': 1.2578125, 'learning_rate': 0.00016738315925224196, 'epoch': 0.17}\n",
      "{'loss': 1.2993, 'grad_norm': 0.5078125, 'learning_rate': 0.00016737179687477809, 'epoch': 0.17}\n",
      "{'loss': 1.2117, 'grad_norm': 0.546875, 'learning_rate': 0.0001673604344973142, 'epoch': 0.17}\n",
      "{'loss': 1.151, 'grad_norm': 0.384765625, 'learning_rate': 0.00016734907211985036, 'epoch': 0.17}\n",
      "{'loss': 1.2514, 'grad_norm': 0.447265625, 'learning_rate': 0.00016733770974238649, 'epoch': 0.17}\n",
      "{'loss': 1.108, 'grad_norm': 0.50390625, 'learning_rate': 0.00016732634736492266, 'epoch': 0.17}\n",
      "{'loss': 1.4652, 'grad_norm': 0.419921875, 'learning_rate': 0.0001673149849874588, 'epoch': 0.17}\n",
      "{'loss': 1.2337, 'grad_norm': 0.6015625, 'learning_rate': 0.0001673036226099949, 'epoch': 0.17}\n",
      "{'loss': 1.1897, 'grad_norm': 0.427734375, 'learning_rate': 0.00016729226023253106, 'epoch': 0.17}\n",
      "{'loss': 1.173, 'grad_norm': 0.83984375, 'learning_rate': 0.0001672808978550672, 'epoch': 0.17}\n",
      "{'loss': 1.0726, 'grad_norm': 0.5234375, 'learning_rate': 0.00016726953547760334, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.459, 'grad_norm': 0.435546875, 'learning_rate': 0.00016725817310013946, 'epoch': 0.17}\n",
      "{'loss': 1.1896, 'grad_norm': 0.474609375, 'learning_rate': 0.00016724681072267562, 'epoch': 0.17}\n",
      "{'loss': 1.2448, 'grad_norm': 0.453125, 'learning_rate': 0.00016723544834521177, 'epoch': 0.17}\n",
      "{'loss': 1.1896, 'grad_norm': 0.75, 'learning_rate': 0.0001672240859677479, 'epoch': 0.17}\n",
      "{'loss': 1.0299, 'grad_norm': 0.9453125, 'learning_rate': 0.00016721272359028404, 'epoch': 0.17}\n",
      "{'loss': 1.3967, 'grad_norm': 0.45703125, 'learning_rate': 0.00016720136121282017, 'epoch': 0.17}\n",
      "{'loss': 1.2043, 'grad_norm': 0.69140625, 'learning_rate': 0.00016718999883535632, 'epoch': 0.17}\n",
      "{'loss': 1.2393, 'grad_norm': 0.37109375, 'learning_rate': 0.00016717863645789247, 'epoch': 0.17}\n",
      "{'loss': 1.2266, 'grad_norm': 0.416015625, 'learning_rate': 0.0001671672740804286, 'epoch': 0.17}\n",
      "{'loss': 1.1814, 'grad_norm': 0.59375, 'learning_rate': 0.00016715591170296475, 'epoch': 0.17}\n",
      "{'loss': 1.4251, 'grad_norm': 0.58984375, 'learning_rate': 0.00016714454932550087, 'epoch': 0.17}\n",
      "{'loss': 1.1402, 'grad_norm': 0.466796875, 'learning_rate': 0.00016713318694803702, 'epoch': 0.17}\n",
      "{'loss': 1.2637, 'grad_norm': 0.5390625, 'learning_rate': 0.00016712182457057315, 'epoch': 0.17}\n",
      "{'loss': 1.3481, 'grad_norm': 0.58203125, 'learning_rate': 0.0001671104621931093, 'epoch': 0.17}\n",
      "{'loss': 1.2545, 'grad_norm': 0.95703125, 'learning_rate': 0.00016709909981564545, 'epoch': 0.17}\n",
      "{'loss': 1.2804, 'grad_norm': 0.439453125, 'learning_rate': 0.00016708773743818157, 'epoch': 0.17}\n",
      "{'loss': 1.2683, 'grad_norm': 0.61328125, 'learning_rate': 0.00016707637506071772, 'epoch': 0.17}\n",
      "{'loss': 1.2955, 'grad_norm': 0.37109375, 'learning_rate': 0.00016706501268325385, 'epoch': 0.17}\n",
      "{'loss': 1.356, 'grad_norm': 0.703125, 'learning_rate': 0.00016705365030579, 'epoch': 0.17}\n",
      "{'loss': 1.1983, 'grad_norm': 0.6171875, 'learning_rate': 0.00016704228792832612, 'epoch': 0.17}\n",
      "{'loss': 1.3371, 'grad_norm': 0.46875, 'learning_rate': 0.00016703092555086225, 'epoch': 0.17}\n",
      "{'loss': 1.3471, 'grad_norm': 0.609375, 'learning_rate': 0.00016701956317339843, 'epoch': 0.17}\n",
      "{'loss': 1.2489, 'grad_norm': 0.4765625, 'learning_rate': 0.00016700820079593455, 'epoch': 0.17}\n",
      "{'loss': 1.3299, 'grad_norm': 0.50390625, 'learning_rate': 0.0001669968384184707, 'epoch': 0.17}\n",
      "{'loss': 1.0871, 'grad_norm': 0.435546875, 'learning_rate': 0.00016698547604100683, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3928, 'grad_norm': 0.392578125, 'learning_rate': 0.00016697411366354295, 'epoch': 0.17}\n",
      "{'loss': 1.2356, 'grad_norm': 0.51171875, 'learning_rate': 0.0001669627512860791, 'epoch': 0.17}\n",
      "{'loss': 1.2247, 'grad_norm': 0.455078125, 'learning_rate': 0.00016695138890861523, 'epoch': 0.17}\n",
      "{'loss': 1.2647, 'grad_norm': 0.455078125, 'learning_rate': 0.0001669400265311514, 'epoch': 0.17}\n",
      "{'loss': 1.0369, 'grad_norm': 0.6796875, 'learning_rate': 0.00016692866415368753, 'epoch': 0.17}\n",
      "{'loss': 1.3229, 'grad_norm': 0.44921875, 'learning_rate': 0.00016691730177622365, 'epoch': 0.17}\n",
      "{'loss': 1.1391, 'grad_norm': 0.85546875, 'learning_rate': 0.0001669059393987598, 'epoch': 0.17}\n",
      "{'loss': 1.3359, 'grad_norm': 0.40625, 'learning_rate': 0.00016689457702129593, 'epoch': 0.17}\n",
      "{'loss': 1.3105, 'grad_norm': 0.5546875, 'learning_rate': 0.00016688321464383208, 'epoch': 0.17}\n",
      "{'loss': 1.157, 'grad_norm': 0.4296875, 'learning_rate': 0.00016687185226636823, 'epoch': 0.17}\n",
      "{'loss': 1.3596, 'grad_norm': 0.55859375, 'learning_rate': 0.00016686048988890436, 'epoch': 0.17}\n",
      "{'loss': 1.1322, 'grad_norm': 0.76953125, 'learning_rate': 0.0001668491275114405, 'epoch': 0.17}\n",
      "{'loss': 1.1837, 'grad_norm': 0.32421875, 'learning_rate': 0.00016683776513397663, 'epoch': 0.17}\n",
      "{'loss': 1.395, 'grad_norm': 0.37890625, 'learning_rate': 0.00016682640275651278, 'epoch': 0.17}\n",
      "{'loss': 1.2093, 'grad_norm': 0.306640625, 'learning_rate': 0.0001668150403790489, 'epoch': 0.17}\n",
      "{'loss': 1.5204, 'grad_norm': 0.431640625, 'learning_rate': 0.00016680367800158506, 'epoch': 0.17}\n",
      "{'loss': 1.2787, 'grad_norm': 0.51953125, 'learning_rate': 0.0001667923156241212, 'epoch': 0.17}\n",
      "{'loss': 1.2193, 'grad_norm': 0.421875, 'learning_rate': 0.00016678095324665734, 'epoch': 0.17}\n",
      "{'loss': 1.1968, 'grad_norm': 0.4609375, 'learning_rate': 0.0001667695908691935, 'epoch': 0.17}\n",
      "{'loss': 1.0721, 'grad_norm': 0.7890625, 'learning_rate': 0.0001667582284917296, 'epoch': 0.17}\n",
      "{'loss': 1.4364, 'grad_norm': 0.50390625, 'learning_rate': 0.00016674686611426576, 'epoch': 0.17}\n",
      "{'loss': 1.2729, 'grad_norm': 0.6875, 'learning_rate': 0.0001667355037368019, 'epoch': 0.17}\n",
      "{'loss': 1.2314, 'grad_norm': 0.392578125, 'learning_rate': 0.00016672414135933804, 'epoch': 0.17}\n",
      "{'loss': 1.2743, 'grad_norm': 0.412109375, 'learning_rate': 0.0001667127789818742, 'epoch': 0.17}\n",
      "{'loss': 1.2058, 'grad_norm': 0.51171875, 'learning_rate': 0.00016670141660441031, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.37, 'grad_norm': 0.41015625, 'learning_rate': 0.00016669005422694647, 'epoch': 0.17}\n",
      "{'loss': 1.2095, 'grad_norm': 0.7421875, 'learning_rate': 0.0001666786918494826, 'epoch': 0.17}\n",
      "{'loss': 1.3104, 'grad_norm': 0.443359375, 'learning_rate': 0.00016666732947201874, 'epoch': 0.17}\n",
      "{'loss': 1.3109, 'grad_norm': 0.65625, 'learning_rate': 0.00016665596709455487, 'epoch': 0.17}\n",
      "{'loss': 1.2389, 'grad_norm': 0.90234375, 'learning_rate': 0.000166644604717091, 'epoch': 0.17}\n",
      "{'loss': 1.3746, 'grad_norm': 0.48046875, 'learning_rate': 0.00016663324233962717, 'epoch': 0.17}\n",
      "{'loss': 1.2765, 'grad_norm': 0.71875, 'learning_rate': 0.0001666218799621633, 'epoch': 0.17}\n",
      "{'loss': 1.2105, 'grad_norm': 0.4375, 'learning_rate': 0.00016661051758469944, 'epoch': 0.17}\n",
      "{'loss': 1.2807, 'grad_norm': 0.6484375, 'learning_rate': 0.00016659915520723557, 'epoch': 0.17}\n",
      "{'loss': 1.1114, 'grad_norm': 0.8203125, 'learning_rate': 0.0001665877928297717, 'epoch': 0.17}\n",
      "{'loss': 1.3715, 'grad_norm': 0.466796875, 'learning_rate': 0.00016657643045230784, 'epoch': 0.17}\n",
      "{'loss': 1.2685, 'grad_norm': 0.455078125, 'learning_rate': 0.00016656506807484397, 'epoch': 0.17}\n",
      "{'loss': 1.2757, 'grad_norm': 0.388671875, 'learning_rate': 0.00016655370569738015, 'epoch': 0.17}\n",
      "{'loss': 1.2802, 'grad_norm': 0.5078125, 'learning_rate': 0.00016654234331991627, 'epoch': 0.17}\n",
      "{'loss': 1.1048, 'grad_norm': 0.97265625, 'learning_rate': 0.0001665309809424524, 'epoch': 0.17}\n",
      "{'loss': 1.4702, 'grad_norm': 0.46875, 'learning_rate': 0.00016651961856498855, 'epoch': 0.17}\n",
      "{'loss': 1.1808, 'grad_norm': 0.6484375, 'learning_rate': 0.00016650825618752467, 'epoch': 0.17}\n",
      "{'loss': 1.1905, 'grad_norm': 0.427734375, 'learning_rate': 0.00016649689381006082, 'epoch': 0.17}\n",
      "{'loss': 1.2896, 'grad_norm': 0.69921875, 'learning_rate': 0.00016648553143259697, 'epoch': 0.17}\n",
      "{'loss': 1.2682, 'grad_norm': 0.6953125, 'learning_rate': 0.0001664741690551331, 'epoch': 0.17}\n",
      "{'loss': 1.3677, 'grad_norm': 0.423828125, 'learning_rate': 0.00016646280667766925, 'epoch': 0.17}\n",
      "{'loss': 1.1584, 'grad_norm': 0.7109375, 'learning_rate': 0.00016645144430020537, 'epoch': 0.17}\n",
      "{'loss': 1.2388, 'grad_norm': 0.4296875, 'learning_rate': 0.00016644008192274153, 'epoch': 0.17}\n",
      "{'loss': 1.2805, 'grad_norm': 0.65625, 'learning_rate': 0.00016642871954527765, 'epoch': 0.17}\n",
      "{'loss': 1.1447, 'grad_norm': 0.85546875, 'learning_rate': 0.0001664173571678138, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3878, 'grad_norm': 0.7109375, 'learning_rate': 0.00016640599479034995, 'epoch': 0.17}\n",
      "{'loss': 1.1832, 'grad_norm': 0.55859375, 'learning_rate': 0.00016639463241288608, 'epoch': 0.17}\n",
      "{'loss': 1.1939, 'grad_norm': 0.439453125, 'learning_rate': 0.00016638327003542223, 'epoch': 0.17}\n",
      "{'loss': 1.3268, 'grad_norm': 0.62109375, 'learning_rate': 0.00016637190765795835, 'epoch': 0.17}\n",
      "{'loss': 1.1182, 'grad_norm': 0.7734375, 'learning_rate': 0.0001663605452804945, 'epoch': 0.17}\n",
      "{'loss': 1.3303, 'grad_norm': 0.453125, 'learning_rate': 0.00016634918290303063, 'epoch': 0.17}\n",
      "{'loss': 1.1988, 'grad_norm': 0.55859375, 'learning_rate': 0.00016633782052556678, 'epoch': 0.17}\n",
      "{'loss': 1.2688, 'grad_norm': 0.37109375, 'learning_rate': 0.00016632645814810293, 'epoch': 0.17}\n",
      "{'loss': 1.3621, 'grad_norm': 0.498046875, 'learning_rate': 0.00016631509577063905, 'epoch': 0.17}\n",
      "{'loss': 1.1849, 'grad_norm': 0.92578125, 'learning_rate': 0.0001663037333931752, 'epoch': 0.17}\n",
      "{'loss': 1.4296, 'grad_norm': 0.39453125, 'learning_rate': 0.00016629237101571133, 'epoch': 0.17}\n",
      "{'loss': 1.1399, 'grad_norm': 0.498046875, 'learning_rate': 0.00016628100863824748, 'epoch': 0.17}\n",
      "{'loss': 1.2982, 'grad_norm': 0.34375, 'learning_rate': 0.0001662696462607836, 'epoch': 0.17}\n",
      "{'loss': 1.3749, 'grad_norm': 0.4453125, 'learning_rate': 0.00016625828388331973, 'epoch': 0.17}\n",
      "{'loss': 1.2191, 'grad_norm': 0.64453125, 'learning_rate': 0.0001662469215058559, 'epoch': 0.17}\n",
      "{'loss': 1.4592, 'grad_norm': 0.384765625, 'learning_rate': 0.00016623555912839203, 'epoch': 0.17}\n",
      "{'loss': 1.3265, 'grad_norm': 0.5859375, 'learning_rate': 0.00016622419675092818, 'epoch': 0.17}\n",
      "{'loss': 1.2009, 'grad_norm': 0.361328125, 'learning_rate': 0.0001662128343734643, 'epoch': 0.17}\n",
      "{'loss': 1.3834, 'grad_norm': 0.5390625, 'learning_rate': 0.00016620147199600043, 'epoch': 0.17}\n",
      "{'loss': 1.0178, 'grad_norm': 0.50390625, 'learning_rate': 0.00016619010961853658, 'epoch': 0.17}\n",
      "{'loss': 1.3416, 'grad_norm': 0.60546875, 'learning_rate': 0.00016617874724107274, 'epoch': 0.17}\n",
      "{'loss': 1.2084, 'grad_norm': 0.546875, 'learning_rate': 0.0001661673848636089, 'epoch': 0.17}\n",
      "{'loss': 1.2009, 'grad_norm': 0.435546875, 'learning_rate': 0.000166156022486145, 'epoch': 0.17}\n",
      "{'loss': 1.3022, 'grad_norm': 0.71875, 'learning_rate': 0.00016614466010868114, 'epoch': 0.17}\n",
      "{'loss': 1.1235, 'grad_norm': 1.0390625, 'learning_rate': 0.0001661332977312173, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3407, 'grad_norm': 0.49609375, 'learning_rate': 0.0001661219353537534, 'epoch': 0.17}\n",
      "{'loss': 1.2614, 'grad_norm': 0.400390625, 'learning_rate': 0.00016611057297628956, 'epoch': 0.17}\n",
      "{'loss': 1.293, 'grad_norm': 0.44140625, 'learning_rate': 0.00016609921059882571, 'epoch': 0.17}\n",
      "{'loss': 1.3053, 'grad_norm': 0.55078125, 'learning_rate': 0.00016608784822136184, 'epoch': 0.18}\n",
      "{'loss': 1.1486, 'grad_norm': 0.71875, 'learning_rate': 0.000166076485843898, 'epoch': 0.18}\n",
      "{'loss': 1.4876, 'grad_norm': 0.498046875, 'learning_rate': 0.00016606512346643411, 'epoch': 0.18}\n",
      "{'loss': 1.236, 'grad_norm': 0.6875, 'learning_rate': 0.00016605376108897027, 'epoch': 0.18}\n",
      "{'loss': 1.219, 'grad_norm': 0.3984375, 'learning_rate': 0.0001660423987115064, 'epoch': 0.18}\n",
      "{'loss': 1.2214, 'grad_norm': 0.5, 'learning_rate': 0.00016603103633404254, 'epoch': 0.18}\n",
      "{'loss': 1.2456, 'grad_norm': 0.828125, 'learning_rate': 0.0001660196739565787, 'epoch': 0.18}\n",
      "{'loss': 1.5043, 'grad_norm': 0.3828125, 'learning_rate': 0.00016600831157911482, 'epoch': 0.18}\n",
      "{'loss': 1.1429, 'grad_norm': 0.734375, 'learning_rate': 0.00016599694920165097, 'epoch': 0.18}\n",
      "{'loss': 1.1694, 'grad_norm': 0.369140625, 'learning_rate': 0.0001659855868241871, 'epoch': 0.18}\n",
      "{'loss': 1.2369, 'grad_norm': 0.5078125, 'learning_rate': 0.00016597422444672324, 'epoch': 0.18}\n",
      "{'loss': 1.0643, 'grad_norm': 0.625, 'learning_rate': 0.00016596286206925937, 'epoch': 0.18}\n",
      "{'loss': 1.3087, 'grad_norm': 0.48046875, 'learning_rate': 0.00016595149969179552, 'epoch': 0.18}\n",
      "{'loss': 1.2292, 'grad_norm': 0.62890625, 'learning_rate': 0.00016594013731433167, 'epoch': 0.18}\n",
      "{'loss': 1.3362, 'grad_norm': 0.439453125, 'learning_rate': 0.0001659287749368678, 'epoch': 0.18}\n",
      "{'loss': 1.2504, 'grad_norm': 0.69140625, 'learning_rate': 0.00016591741255940395, 'epoch': 0.18}\n",
      "{'loss': 1.1755, 'grad_norm': 0.49609375, 'learning_rate': 0.00016590605018194007, 'epoch': 0.18}\n",
      "{'loss': 1.4637, 'grad_norm': 0.56640625, 'learning_rate': 0.00016589468780447622, 'epoch': 0.18}\n",
      "{'loss': 1.2322, 'grad_norm': 0.5859375, 'learning_rate': 0.00016588332542701235, 'epoch': 0.18}\n",
      "{'loss': 1.3109, 'grad_norm': 0.39453125, 'learning_rate': 0.00016587196304954847, 'epoch': 0.18}\n",
      "{'loss': 1.1886, 'grad_norm': 0.478515625, 'learning_rate': 0.00016586060067208465, 'epoch': 0.18}\n",
      "{'loss': 1.2808, 'grad_norm': 0.6484375, 'learning_rate': 0.00016584923829462077, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2633, 'grad_norm': 0.427734375, 'learning_rate': 0.00016583787591715693, 'epoch': 0.18}\n",
      "{'loss': 1.2292, 'grad_norm': 0.89453125, 'learning_rate': 0.00016582651353969305, 'epoch': 0.18}\n",
      "{'loss': 1.2362, 'grad_norm': 0.3984375, 'learning_rate': 0.00016581515116222917, 'epoch': 0.18}\n",
      "{'loss': 1.266, 'grad_norm': 0.76171875, 'learning_rate': 0.00016580378878476533, 'epoch': 0.18}\n",
      "{'loss': 1.1181, 'grad_norm': 1.0546875, 'learning_rate': 0.00016579242640730148, 'epoch': 0.18}\n",
      "{'loss': 1.3519, 'grad_norm': 0.54296875, 'learning_rate': 0.00016578106402983763, 'epoch': 0.18}\n",
      "{'loss': 1.3683, 'grad_norm': 1.046875, 'learning_rate': 0.00016576970165237375, 'epoch': 0.18}\n",
      "{'loss': 1.0437, 'grad_norm': 0.34375, 'learning_rate': 0.00016575833927490988, 'epoch': 0.18}\n",
      "{'loss': 1.3932, 'grad_norm': 0.53515625, 'learning_rate': 0.00016574697689744603, 'epoch': 0.18}\n",
      "{'loss': 1.1431, 'grad_norm': 1.34375, 'learning_rate': 0.00016573561451998215, 'epoch': 0.18}\n",
      "{'loss': 1.2853, 'grad_norm': 0.6875, 'learning_rate': 0.0001657242521425183, 'epoch': 0.18}\n",
      "{'loss': 1.2785, 'grad_norm': 0.67578125, 'learning_rate': 0.00016571288976505446, 'epoch': 0.18}\n",
      "{'loss': 1.3301, 'grad_norm': 0.462890625, 'learning_rate': 0.00016570152738759058, 'epoch': 0.18}\n",
      "{'loss': 1.1487, 'grad_norm': 0.474609375, 'learning_rate': 0.00016569016501012673, 'epoch': 0.18}\n",
      "{'loss': 1.0353, 'grad_norm': 0.578125, 'learning_rate': 0.00016567880263266286, 'epoch': 0.18}\n",
      "{'loss': 1.3024, 'grad_norm': 0.455078125, 'learning_rate': 0.000165667440255199, 'epoch': 0.18}\n",
      "{'loss': 1.163, 'grad_norm': 0.73828125, 'learning_rate': 0.00016565607787773513, 'epoch': 0.18}\n",
      "{'loss': 1.2801, 'grad_norm': 0.498046875, 'learning_rate': 0.00016564471550027128, 'epoch': 0.18}\n",
      "{'loss': 1.281, 'grad_norm': 0.5625, 'learning_rate': 0.00016563335312280743, 'epoch': 0.18}\n",
      "{'loss': 1.2323, 'grad_norm': 0.85546875, 'learning_rate': 0.00016562199074534356, 'epoch': 0.18}\n",
      "{'loss': 1.4867, 'grad_norm': 0.5234375, 'learning_rate': 0.0001656106283678797, 'epoch': 0.18}\n",
      "{'loss': 1.2428, 'grad_norm': 0.58984375, 'learning_rate': 0.00016559926599041583, 'epoch': 0.18}\n",
      "{'loss': 1.1921, 'grad_norm': 0.404296875, 'learning_rate': 0.00016558790361295199, 'epoch': 0.18}\n",
      "{'loss': 1.3193, 'grad_norm': 0.4453125, 'learning_rate': 0.0001655765412354881, 'epoch': 0.18}\n",
      "{'loss': 1.0549, 'grad_norm': 0.412109375, 'learning_rate': 0.00016556517885802426, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.362, 'grad_norm': 0.5078125, 'learning_rate': 0.0001655538164805604, 'epoch': 0.18}\n",
      "{'loss': 1.1806, 'grad_norm': 0.494140625, 'learning_rate': 0.00016554245410309654, 'epoch': 0.18}\n",
      "{'loss': 1.2524, 'grad_norm': 0.443359375, 'learning_rate': 0.0001655310917256327, 'epoch': 0.18}\n",
      "{'loss': 1.321, 'grad_norm': 0.50390625, 'learning_rate': 0.0001655197293481688, 'epoch': 0.18}\n",
      "{'loss': 1.0909, 'grad_norm': 0.765625, 'learning_rate': 0.00016550836697070496, 'epoch': 0.18}\n",
      "{'loss': 1.3111, 'grad_norm': 0.427734375, 'learning_rate': 0.0001654970045932411, 'epoch': 0.18}\n",
      "{'loss': 1.2229, 'grad_norm': 0.671875, 'learning_rate': 0.00016548564221577724, 'epoch': 0.18}\n",
      "{'loss': 1.2913, 'grad_norm': 0.35546875, 'learning_rate': 0.0001654742798383134, 'epoch': 0.18}\n",
      "{'loss': 1.3128, 'grad_norm': 0.5625, 'learning_rate': 0.00016546291746084952, 'epoch': 0.18}\n",
      "{'loss': 1.1799, 'grad_norm': 0.984375, 'learning_rate': 0.00016545155508338567, 'epoch': 0.18}\n",
      "{'loss': 1.4842, 'grad_norm': 0.51171875, 'learning_rate': 0.0001654401927059218, 'epoch': 0.18}\n",
      "{'loss': 1.1915, 'grad_norm': 0.75, 'learning_rate': 0.00016542883032845792, 'epoch': 0.18}\n",
      "{'loss': 1.1359, 'grad_norm': 0.48828125, 'learning_rate': 0.00016541746795099407, 'epoch': 0.18}\n",
      "{'loss': 1.1849, 'grad_norm': 0.625, 'learning_rate': 0.00016540610557353022, 'epoch': 0.18}\n",
      "{'loss': 1.2619, 'grad_norm': 1.0234375, 'learning_rate': 0.00016539474319606637, 'epoch': 0.18}\n",
      "{'loss': 1.4234, 'grad_norm': 0.490234375, 'learning_rate': 0.0001653833808186025, 'epoch': 0.18}\n",
      "{'loss': 1.2256, 'grad_norm': 0.6171875, 'learning_rate': 0.00016537201844113862, 'epoch': 0.18}\n",
      "{'loss': 1.1842, 'grad_norm': 0.4609375, 'learning_rate': 0.00016536065606367477, 'epoch': 0.18}\n",
      "{'loss': 1.3367, 'grad_norm': 0.48046875, 'learning_rate': 0.0001653492936862109, 'epoch': 0.18}\n",
      "{'loss': 1.0548, 'grad_norm': 0.66015625, 'learning_rate': 0.00016533793130874705, 'epoch': 0.18}\n",
      "{'loss': 1.3122, 'grad_norm': 0.51953125, 'learning_rate': 0.0001653265689312832, 'epoch': 0.18}\n",
      "{'loss': 1.254, 'grad_norm': 0.7578125, 'learning_rate': 0.00016531520655381932, 'epoch': 0.18}\n",
      "{'loss': 1.2497, 'grad_norm': 0.45703125, 'learning_rate': 0.00016530384417635547, 'epoch': 0.18}\n",
      "{'loss': 1.1696, 'grad_norm': 0.546875, 'learning_rate': 0.0001652924817988916, 'epoch': 0.18}\n",
      "{'loss': 1.2414, 'grad_norm': 0.97265625, 'learning_rate': 0.00016528111942142775, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4538, 'grad_norm': 0.443359375, 'learning_rate': 0.00016526975704396387, 'epoch': 0.18}\n",
      "{'loss': 1.2231, 'grad_norm': 0.7578125, 'learning_rate': 0.00016525839466650002, 'epoch': 0.18}\n",
      "{'loss': 1.1982, 'grad_norm': 0.4375, 'learning_rate': 0.00016524703228903618, 'epoch': 0.18}\n",
      "{'loss': 1.2928, 'grad_norm': 0.49609375, 'learning_rate': 0.0001652356699115723, 'epoch': 0.18}\n",
      "{'loss': 1.1907, 'grad_norm': 0.6640625, 'learning_rate': 0.00016522430753410845, 'epoch': 0.18}\n",
      "{'loss': 1.3517, 'grad_norm': 0.470703125, 'learning_rate': 0.00016521294515664458, 'epoch': 0.18}\n",
      "{'loss': 1.1835, 'grad_norm': 0.48828125, 'learning_rate': 0.00016520158277918073, 'epoch': 0.18}\n",
      "{'loss': 1.385, 'grad_norm': 0.375, 'learning_rate': 0.00016519022040171685, 'epoch': 0.18}\n",
      "{'loss': 1.3431, 'grad_norm': 0.66015625, 'learning_rate': 0.000165178858024253, 'epoch': 0.18}\n",
      "{'loss': 1.2088, 'grad_norm': 1.078125, 'learning_rate': 0.00016516749564678915, 'epoch': 0.18}\n",
      "{'loss': 1.3072, 'grad_norm': 0.56640625, 'learning_rate': 0.00016515613326932528, 'epoch': 0.18}\n",
      "{'loss': 1.2998, 'grad_norm': 0.6328125, 'learning_rate': 0.00016514477089186143, 'epoch': 0.18}\n",
      "{'loss': 1.2811, 'grad_norm': 0.4140625, 'learning_rate': 0.00016513340851439755, 'epoch': 0.18}\n",
      "{'loss': 1.3073, 'grad_norm': 0.8515625, 'learning_rate': 0.0001651220461369337, 'epoch': 0.18}\n",
      "{'loss': 1.2084, 'grad_norm': 0.94921875, 'learning_rate': 0.00016511068375946983, 'epoch': 0.18}\n",
      "{'loss': 1.3705, 'grad_norm': 0.5, 'learning_rate': 0.00016509932138200598, 'epoch': 0.18}\n",
      "{'loss': 1.1792, 'grad_norm': 0.546875, 'learning_rate': 0.00016508795900454213, 'epoch': 0.18}\n",
      "{'loss': 1.2349, 'grad_norm': 0.455078125, 'learning_rate': 0.00016507659662707826, 'epoch': 0.18}\n",
      "{'loss': 1.18, 'grad_norm': 0.5859375, 'learning_rate': 0.0001650652342496144, 'epoch': 0.18}\n",
      "{'loss': 1.0522, 'grad_norm': 1.03125, 'learning_rate': 0.00016505387187215053, 'epoch': 0.18}\n",
      "{'loss': 1.2836, 'grad_norm': 0.58203125, 'learning_rate': 0.00016504250949468666, 'epoch': 0.18}\n",
      "{'loss': 1.1758, 'grad_norm': 0.60546875, 'learning_rate': 0.0001650311471172228, 'epoch': 0.18}\n",
      "{'loss': 1.1277, 'grad_norm': 0.4375, 'learning_rate': 0.00016501978473975896, 'epoch': 0.18}\n",
      "{'loss': 1.3607, 'grad_norm': 0.57421875, 'learning_rate': 0.0001650084223622951, 'epoch': 0.18}\n",
      "{'loss': 1.118, 'grad_norm': 0.349609375, 'learning_rate': 0.00016499705998483124, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3606, 'grad_norm': 0.404296875, 'learning_rate': 0.00016498569760736736, 'epoch': 0.18}\n",
      "{'loss': 1.3298, 'grad_norm': 0.59765625, 'learning_rate': 0.0001649743352299035, 'epoch': 0.18}\n",
      "{'loss': 1.2473, 'grad_norm': 0.3671875, 'learning_rate': 0.00016496297285243964, 'epoch': 0.18}\n",
      "{'loss': 1.2631, 'grad_norm': 0.6171875, 'learning_rate': 0.0001649516104749758, 'epoch': 0.18}\n",
      "{'loss': 1.1744, 'grad_norm': 1.1171875, 'learning_rate': 0.00016494024809751194, 'epoch': 0.18}\n",
      "{'loss': 1.4257, 'grad_norm': 0.58203125, 'learning_rate': 0.00016492888572004806, 'epoch': 0.18}\n",
      "{'loss': 1.1059, 'grad_norm': 0.5078125, 'learning_rate': 0.00016491752334258421, 'epoch': 0.18}\n",
      "{'loss': 1.1898, 'grad_norm': 0.44921875, 'learning_rate': 0.00016490616096512034, 'epoch': 0.18}\n",
      "{'loss': 1.228, 'grad_norm': 0.5703125, 'learning_rate': 0.0001648947985876565, 'epoch': 0.18}\n",
      "{'loss': 1.1369, 'grad_norm': 0.63671875, 'learning_rate': 0.00016488343621019261, 'epoch': 0.18}\n",
      "{'loss': 1.3408, 'grad_norm': 0.455078125, 'learning_rate': 0.00016487207383272877, 'epoch': 0.18}\n",
      "{'loss': 1.2359, 'grad_norm': 0.6171875, 'learning_rate': 0.00016486071145526492, 'epoch': 0.18}\n",
      "{'loss': 1.1941, 'grad_norm': 0.45703125, 'learning_rate': 0.00016484934907780104, 'epoch': 0.18}\n",
      "{'loss': 1.3345, 'grad_norm': 0.515625, 'learning_rate': 0.0001648379867003372, 'epoch': 0.18}\n",
      "{'loss': 1.1403, 'grad_norm': 0.7578125, 'learning_rate': 0.00016482662432287332, 'epoch': 0.18}\n",
      "{'loss': 1.4027, 'grad_norm': 0.462890625, 'learning_rate': 0.00016481526194540947, 'epoch': 0.18}\n",
      "{'loss': 1.2206, 'grad_norm': 0.63671875, 'learning_rate': 0.0001648038995679456, 'epoch': 0.18}\n",
      "{'loss': 1.2866, 'grad_norm': 0.396484375, 'learning_rate': 0.00016479253719048174, 'epoch': 0.18}\n",
      "{'loss': 1.2472, 'grad_norm': 0.73046875, 'learning_rate': 0.0001647811748130179, 'epoch': 0.18}\n",
      "{'loss': 1.1415, 'grad_norm': 0.9375, 'learning_rate': 0.00016476981243555402, 'epoch': 0.18}\n",
      "{'loss': 1.4736, 'grad_norm': 0.447265625, 'learning_rate': 0.00016475845005809017, 'epoch': 0.18}\n",
      "{'loss': 1.2191, 'grad_norm': 0.609375, 'learning_rate': 0.0001647470876806263, 'epoch': 0.18}\n",
      "{'loss': 1.2724, 'grad_norm': 0.431640625, 'learning_rate': 0.00016473572530316245, 'epoch': 0.18}\n",
      "{'loss': 1.1694, 'grad_norm': 0.5625, 'learning_rate': 0.00016472436292569857, 'epoch': 0.18}\n",
      "{'loss': 1.0283, 'grad_norm': 0.37890625, 'learning_rate': 0.00016471300054823472, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3595, 'grad_norm': 0.58203125, 'learning_rate': 0.00016470163817077087, 'epoch': 0.18}\n",
      "{'loss': 1.2613, 'grad_norm': 0.54296875, 'learning_rate': 0.000164690275793307, 'epoch': 0.18}\n",
      "{'loss': 1.1908, 'grad_norm': 0.439453125, 'learning_rate': 0.00016467891341584315, 'epoch': 0.18}\n",
      "{'loss': 1.2312, 'grad_norm': 0.453125, 'learning_rate': 0.00016466755103837927, 'epoch': 0.18}\n",
      "{'loss': 1.096, 'grad_norm': 0.419921875, 'learning_rate': 0.0001646561886609154, 'epoch': 0.18}\n",
      "{'loss': 1.4056, 'grad_norm': 0.6015625, 'learning_rate': 0.00016464482628345155, 'epoch': 0.18}\n",
      "{'loss': 1.1874, 'grad_norm': 0.515625, 'learning_rate': 0.0001646334639059877, 'epoch': 0.18}\n",
      "{'loss': 1.3006, 'grad_norm': 0.59765625, 'learning_rate': 0.00016462210152852385, 'epoch': 0.18}\n",
      "{'loss': 1.2047, 'grad_norm': 0.54296875, 'learning_rate': 0.00016461073915105998, 'epoch': 0.18}\n",
      "{'loss': 1.1127, 'grad_norm': 0.462890625, 'learning_rate': 0.0001645993767735961, 'epoch': 0.18}\n",
      "{'loss': 1.2826, 'grad_norm': 0.423828125, 'learning_rate': 0.00016458801439613225, 'epoch': 0.18}\n",
      "{'loss': 1.1802, 'grad_norm': 0.83984375, 'learning_rate': 0.00016457665201866838, 'epoch': 0.18}\n",
      "{'loss': 1.2184, 'grad_norm': 0.369140625, 'learning_rate': 0.00016456528964120453, 'epoch': 0.18}\n",
      "{'loss': 1.3154, 'grad_norm': 0.7265625, 'learning_rate': 0.00016455392726374068, 'epoch': 0.18}\n",
      "{'loss': 1.0574, 'grad_norm': 1.0703125, 'learning_rate': 0.0001645425648862768, 'epoch': 0.18}\n",
      "{'loss': 1.2968, 'grad_norm': 0.5703125, 'learning_rate': 0.00016453120250881296, 'epoch': 0.18}\n",
      "{'loss': 1.2023, 'grad_norm': 0.73046875, 'learning_rate': 0.00016451984013134908, 'epoch': 0.18}\n",
      "{'loss': 1.1478, 'grad_norm': 0.443359375, 'learning_rate': 0.00016450847775388523, 'epoch': 0.18}\n",
      "{'loss': 1.3101, 'grad_norm': 0.478515625, 'learning_rate': 0.00016449711537642136, 'epoch': 0.18}\n",
      "{'loss': 1.1502, 'grad_norm': 0.53125, 'learning_rate': 0.00016448575299895753, 'epoch': 0.18}\n",
      "{'loss': 1.3212, 'grad_norm': 0.482421875, 'learning_rate': 0.00016447439062149366, 'epoch': 0.18}\n",
      "{'loss': 1.2596, 'grad_norm': 0.70703125, 'learning_rate': 0.00016446302824402978, 'epoch': 0.18}\n",
      "{'loss': 1.2767, 'grad_norm': 0.4921875, 'learning_rate': 0.00016445166586656593, 'epoch': 0.18}\n",
      "{'loss': 1.371, 'grad_norm': 0.671875, 'learning_rate': 0.00016444030348910206, 'epoch': 0.18}\n",
      "{'loss': 1.0694, 'grad_norm': 0.79296875, 'learning_rate': 0.0001644289411116382, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3261, 'grad_norm': 0.56640625, 'learning_rate': 0.00016441757873417433, 'epoch': 0.18}\n",
      "{'loss': 1.2493, 'grad_norm': 0.466796875, 'learning_rate': 0.00016440621635671049, 'epoch': 0.18}\n",
      "{'loss': 1.3164, 'grad_norm': 0.40234375, 'learning_rate': 0.00016439485397924664, 'epoch': 0.18}\n",
      "{'loss': 1.242, 'grad_norm': 0.47265625, 'learning_rate': 0.00016438349160178276, 'epoch': 0.18}\n",
      "{'loss': 1.1572, 'grad_norm': 0.890625, 'learning_rate': 0.0001643721292243189, 'epoch': 0.18}\n",
      "{'loss': 1.3552, 'grad_norm': 0.59765625, 'learning_rate': 0.00016436076684685504, 'epoch': 0.18}\n",
      "{'loss': 1.2761, 'grad_norm': 0.80078125, 'learning_rate': 0.0001643494044693912, 'epoch': 0.18}\n",
      "{'loss': 1.2353, 'grad_norm': 0.412109375, 'learning_rate': 0.0001643380420919273, 'epoch': 0.18}\n",
      "{'loss': 1.271, 'grad_norm': 0.41796875, 'learning_rate': 0.00016432667971446346, 'epoch': 0.18}\n",
      "{'loss': 1.1965, 'grad_norm': 0.67578125, 'learning_rate': 0.00016431531733699962, 'epoch': 0.18}\n",
      "{'loss': 1.4185, 'grad_norm': 0.435546875, 'learning_rate': 0.00016430395495953574, 'epoch': 0.18}\n",
      "{'loss': 1.244, 'grad_norm': 0.56640625, 'learning_rate': 0.0001642925925820719, 'epoch': 0.18}\n",
      "{'loss': 1.1837, 'grad_norm': 0.408203125, 'learning_rate': 0.00016428123020460802, 'epoch': 0.18}\n",
      "{'loss': 1.3025, 'grad_norm': 0.486328125, 'learning_rate': 0.00016426986782714414, 'epoch': 0.18}\n",
      "{'loss': 1.2618, 'grad_norm': 1.046875, 'learning_rate': 0.0001642585054496803, 'epoch': 0.18}\n",
      "{'loss': 1.2093, 'grad_norm': 0.474609375, 'learning_rate': 0.00016424714307221644, 'epoch': 0.18}\n",
      "{'loss': 1.2124, 'grad_norm': 0.54296875, 'learning_rate': 0.0001642357806947526, 'epoch': 0.18}\n",
      "{'loss': 1.3575, 'grad_norm': 0.484375, 'learning_rate': 0.00016422441831728872, 'epoch': 0.18}\n",
      "{'loss': 1.361, 'grad_norm': 0.65625, 'learning_rate': 0.00016421305593982484, 'epoch': 0.18}\n",
      "{'loss': 1.0647, 'grad_norm': 0.51171875, 'learning_rate': 0.000164201693562361, 'epoch': 0.18}\n",
      "{'loss': 1.299, 'grad_norm': 0.52734375, 'learning_rate': 0.00016419033118489712, 'epoch': 0.18}\n",
      "{'loss': 1.1265, 'grad_norm': 0.57421875, 'learning_rate': 0.00016417896880743327, 'epoch': 0.18}\n",
      "{'loss': 1.263, 'grad_norm': 0.36328125, 'learning_rate': 0.00016416760642996942, 'epoch': 0.18}\n",
      "{'loss': 1.1996, 'grad_norm': 0.66796875, 'learning_rate': 0.00016415624405250555, 'epoch': 0.18}\n",
      "{'loss': 1.0734, 'grad_norm': 0.56640625, 'learning_rate': 0.0001641448816750417, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.241, 'grad_norm': 0.5, 'learning_rate': 0.00016413351929757782, 'epoch': 0.18}\n",
      "{'loss': 1.2045, 'grad_norm': 0.625, 'learning_rate': 0.00016412215692011397, 'epoch': 0.18}\n",
      "{'loss': 1.2742, 'grad_norm': 0.408203125, 'learning_rate': 0.0001641107945426501, 'epoch': 0.18}\n",
      "{'loss': 1.2547, 'grad_norm': 0.62109375, 'learning_rate': 0.00016409943216518628, 'epoch': 0.18}\n",
      "{'loss': 1.1977, 'grad_norm': 1.046875, 'learning_rate': 0.0001640880697877224, 'epoch': 0.18}\n",
      "{'loss': 1.2689, 'grad_norm': 0.42578125, 'learning_rate': 0.00016407670741025852, 'epoch': 0.18}\n",
      "{'loss': 1.2406, 'grad_norm': 0.703125, 'learning_rate': 0.00016406534503279468, 'epoch': 0.18}\n",
      "{'loss': 1.3085, 'grad_norm': 0.35546875, 'learning_rate': 0.0001640539826553308, 'epoch': 0.18}\n",
      "{'loss': 1.2195, 'grad_norm': 0.80859375, 'learning_rate': 0.00016404262027786695, 'epoch': 0.18}\n",
      "{'loss': 1.1551, 'grad_norm': 0.48828125, 'learning_rate': 0.00016403125790040308, 'epoch': 0.18}\n",
      "{'loss': 1.4577, 'grad_norm': 0.498046875, 'learning_rate': 0.00016401989552293923, 'epoch': 0.18}\n",
      "{'loss': 1.2226, 'grad_norm': 0.9375, 'learning_rate': 0.00016400853314547538, 'epoch': 0.18}\n",
      "{'loss': 1.2435, 'grad_norm': 0.44140625, 'learning_rate': 0.0001639971707680115, 'epoch': 0.18}\n",
      "{'loss': 1.2717, 'grad_norm': 0.57421875, 'learning_rate': 0.00016398580839054765, 'epoch': 0.18}\n",
      "{'loss': 1.2955, 'grad_norm': 0.5703125, 'learning_rate': 0.00016397444601308378, 'epoch': 0.18}\n",
      "{'loss': 1.4075, 'grad_norm': 0.515625, 'learning_rate': 0.00016396308363561993, 'epoch': 0.18}\n",
      "{'loss': 1.2294, 'grad_norm': 0.546875, 'learning_rate': 0.00016395172125815605, 'epoch': 0.18}\n",
      "{'loss': 1.1608, 'grad_norm': 0.400390625, 'learning_rate': 0.0001639403588806922, 'epoch': 0.18}\n",
      "{'loss': 1.1098, 'grad_norm': 0.474609375, 'learning_rate': 0.00016392899650322836, 'epoch': 0.18}\n",
      "{'loss': 1.2119, 'grad_norm': 0.88671875, 'learning_rate': 0.00016391763412576448, 'epoch': 0.18}\n",
      "{'loss': 1.3636, 'grad_norm': 0.4296875, 'learning_rate': 0.00016390627174830063, 'epoch': 0.18}\n",
      "{'loss': 1.1993, 'grad_norm': 0.4765625, 'learning_rate': 0.00016389490937083676, 'epoch': 0.18}\n",
      "{'loss': 1.2276, 'grad_norm': 0.466796875, 'learning_rate': 0.00016388354699337288, 'epoch': 0.18}\n",
      "{'loss': 1.2968, 'grad_norm': 0.703125, 'learning_rate': 0.00016387218461590903, 'epoch': 0.18}\n",
      "{'loss': 1.1247, 'grad_norm': 0.4921875, 'learning_rate': 0.00016386082223844518, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3389, 'grad_norm': 0.44140625, 'learning_rate': 0.00016384945986098134, 'epoch': 0.18}\n",
      "{'loss': 1.0987, 'grad_norm': 0.828125, 'learning_rate': 0.00016383809748351746, 'epoch': 0.18}\n",
      "{'loss': 1.2929, 'grad_norm': 0.400390625, 'learning_rate': 0.00016382673510605358, 'epoch': 0.18}\n",
      "{'loss': 1.2953, 'grad_norm': 0.5234375, 'learning_rate': 0.00016381537272858974, 'epoch': 0.18}\n",
      "{'loss': 1.1022, 'grad_norm': 0.765625, 'learning_rate': 0.00016380401035112586, 'epoch': 0.18}\n",
      "{'loss': 1.3949, 'grad_norm': 0.43359375, 'learning_rate': 0.00016379264797366204, 'epoch': 0.19}\n",
      "{'loss': 1.2087, 'grad_norm': 0.75, 'learning_rate': 0.00016378128559619816, 'epoch': 0.19}\n",
      "{'loss': 1.2654, 'grad_norm': 0.44140625, 'learning_rate': 0.0001637699232187343, 'epoch': 0.19}\n",
      "{'loss': 1.2961, 'grad_norm': 0.50390625, 'learning_rate': 0.00016375856084127044, 'epoch': 0.19}\n",
      "{'loss': 1.3032, 'grad_norm': 0.78125, 'learning_rate': 0.00016374719846380656, 'epoch': 0.19}\n",
      "{'loss': 1.4196, 'grad_norm': 0.42578125, 'learning_rate': 0.00016373583608634271, 'epoch': 0.19}\n",
      "{'loss': 1.307, 'grad_norm': 0.6953125, 'learning_rate': 0.00016372447370887884, 'epoch': 0.19}\n",
      "{'loss': 1.3744, 'grad_norm': 0.396484375, 'learning_rate': 0.00016371311133141502, 'epoch': 0.19}\n",
      "{'loss': 1.36, 'grad_norm': 0.5546875, 'learning_rate': 0.00016370174895395114, 'epoch': 0.19}\n",
      "{'loss': 1.133, 'grad_norm': 0.63671875, 'learning_rate': 0.00016369038657648727, 'epoch': 0.19}\n",
      "{'loss': 1.4447, 'grad_norm': 0.40234375, 'learning_rate': 0.00016367902419902342, 'epoch': 0.19}\n",
      "{'loss': 1.2162, 'grad_norm': 0.65234375, 'learning_rate': 0.00016366766182155954, 'epoch': 0.19}\n",
      "{'loss': 1.2049, 'grad_norm': 0.369140625, 'learning_rate': 0.0001636562994440957, 'epoch': 0.19}\n",
      "{'loss': 1.2128, 'grad_norm': 0.5234375, 'learning_rate': 0.00016364493706663182, 'epoch': 0.19}\n",
      "{'loss': 1.2192, 'grad_norm': 1.0078125, 'learning_rate': 0.00016363357468916797, 'epoch': 0.19}\n",
      "{'loss': 1.2599, 'grad_norm': 0.4921875, 'learning_rate': 0.00016362221231170412, 'epoch': 0.19}\n",
      "{'loss': 1.2776, 'grad_norm': 0.828125, 'learning_rate': 0.00016361084993424024, 'epoch': 0.19}\n",
      "{'loss': 1.2826, 'grad_norm': 0.44140625, 'learning_rate': 0.0001635994875567764, 'epoch': 0.19}\n",
      "{'loss': 1.2479, 'grad_norm': 0.380859375, 'learning_rate': 0.00016358812517931252, 'epoch': 0.19}\n",
      "{'loss': 1.0665, 'grad_norm': 0.7421875, 'learning_rate': 0.00016357676280184867, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2921, 'grad_norm': 0.4765625, 'learning_rate': 0.0001635654004243848, 'epoch': 0.19}\n",
      "{'loss': 1.2044, 'grad_norm': 0.93359375, 'learning_rate': 0.00016355403804692095, 'epoch': 0.19}\n",
      "{'loss': 1.1285, 'grad_norm': 0.494140625, 'learning_rate': 0.0001635426756694571, 'epoch': 0.19}\n",
      "{'loss': 1.3135, 'grad_norm': 0.5, 'learning_rate': 0.00016353131329199322, 'epoch': 0.19}\n",
      "{'loss': 1.1879, 'grad_norm': 0.546875, 'learning_rate': 0.00016351995091452937, 'epoch': 0.19}\n",
      "{'loss': 1.4942, 'grad_norm': 0.49609375, 'learning_rate': 0.0001635085885370655, 'epoch': 0.19}\n",
      "{'loss': 1.261, 'grad_norm': 0.88671875, 'learning_rate': 0.00016349722615960162, 'epoch': 0.19}\n",
      "{'loss': 1.3704, 'grad_norm': 0.345703125, 'learning_rate': 0.00016348586378213777, 'epoch': 0.19}\n",
      "{'loss': 1.3178, 'grad_norm': 0.5234375, 'learning_rate': 0.00016347450140467393, 'epoch': 0.19}\n",
      "{'loss': 1.0646, 'grad_norm': 0.875, 'learning_rate': 0.00016346313902721008, 'epoch': 0.19}\n",
      "{'loss': 1.56, 'grad_norm': 0.53125, 'learning_rate': 0.0001634517766497462, 'epoch': 0.19}\n",
      "{'loss': 1.2342, 'grad_norm': 0.6328125, 'learning_rate': 0.00016344041427228233, 'epoch': 0.19}\n",
      "{'loss': 1.2651, 'grad_norm': 0.43359375, 'learning_rate': 0.00016342905189481848, 'epoch': 0.19}\n",
      "{'loss': 1.2319, 'grad_norm': 0.515625, 'learning_rate': 0.0001634176895173546, 'epoch': 0.19}\n",
      "{'loss': 1.1745, 'grad_norm': 0.84375, 'learning_rate': 0.00016340632713989078, 'epoch': 0.19}\n",
      "{'loss': 1.4273, 'grad_norm': 0.45703125, 'learning_rate': 0.0001633949647624269, 'epoch': 0.19}\n",
      "{'loss': 1.1848, 'grad_norm': 0.640625, 'learning_rate': 0.00016338360238496303, 'epoch': 0.19}\n",
      "{'loss': 1.1907, 'grad_norm': 0.419921875, 'learning_rate': 0.00016337224000749918, 'epoch': 0.19}\n",
      "{'loss': 1.2283, 'grad_norm': 0.470703125, 'learning_rate': 0.0001633608776300353, 'epoch': 0.19}\n",
      "{'loss': 1.1885, 'grad_norm': 0.6640625, 'learning_rate': 0.00016334951525257146, 'epoch': 0.19}\n",
      "{'loss': 1.4458, 'grad_norm': 0.46484375, 'learning_rate': 0.00016333815287510758, 'epoch': 0.19}\n",
      "{'loss': 1.2452, 'grad_norm': 0.6640625, 'learning_rate': 0.00016332679049764376, 'epoch': 0.19}\n",
      "{'loss': 1.3626, 'grad_norm': 0.373046875, 'learning_rate': 0.00016331542812017988, 'epoch': 0.19}\n",
      "{'loss': 1.3743, 'grad_norm': 0.77734375, 'learning_rate': 0.000163304065742716, 'epoch': 0.19}\n",
      "{'loss': 1.0978, 'grad_norm': 0.4140625, 'learning_rate': 0.00016329270336525216, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4187, 'grad_norm': 0.451171875, 'learning_rate': 0.00016328134098778828, 'epoch': 0.19}\n",
      "{'loss': 1.1953, 'grad_norm': 0.734375, 'learning_rate': 0.00016326997861032443, 'epoch': 0.19}\n",
      "{'loss': 1.1535, 'grad_norm': 0.3515625, 'learning_rate': 0.00016325861623286056, 'epoch': 0.19}\n",
      "{'loss': 1.2722, 'grad_norm': 0.6015625, 'learning_rate': 0.0001632472538553967, 'epoch': 0.19}\n",
      "{'loss': 1.1464, 'grad_norm': 1.625, 'learning_rate': 0.00016323589147793286, 'epoch': 0.19}\n",
      "{'loss': 1.3352, 'grad_norm': 0.51953125, 'learning_rate': 0.00016322452910046899, 'epoch': 0.19}\n",
      "{'loss': 1.2468, 'grad_norm': 0.7265625, 'learning_rate': 0.00016321316672300514, 'epoch': 0.19}\n",
      "{'loss': 1.3166, 'grad_norm': 0.498046875, 'learning_rate': 0.00016320180434554126, 'epoch': 0.19}\n",
      "{'loss': 1.1936, 'grad_norm': 0.6640625, 'learning_rate': 0.0001631904419680774, 'epoch': 0.19}\n",
      "{'loss': 1.1571, 'grad_norm': 0.72265625, 'learning_rate': 0.00016317907959061354, 'epoch': 0.19}\n",
      "{'loss': 1.4478, 'grad_norm': 0.58203125, 'learning_rate': 0.0001631677172131497, 'epoch': 0.19}\n",
      "{'loss': 1.3198, 'grad_norm': 0.490234375, 'learning_rate': 0.00016315635483568584, 'epoch': 0.19}\n",
      "{'loss': 1.2476, 'grad_norm': 0.33203125, 'learning_rate': 0.00016314499245822196, 'epoch': 0.19}\n",
      "{'loss': 1.2117, 'grad_norm': 0.55859375, 'learning_rate': 0.00016313363008075811, 'epoch': 0.19}\n",
      "{'loss': 1.1759, 'grad_norm': 1.4296875, 'learning_rate': 0.00016312226770329424, 'epoch': 0.19}\n",
      "{'loss': 1.3861, 'grad_norm': 0.458984375, 'learning_rate': 0.00016311090532583036, 'epoch': 0.19}\n",
      "{'loss': 1.0396, 'grad_norm': 0.43359375, 'learning_rate': 0.00016309954294836654, 'epoch': 0.19}\n",
      "{'loss': 1.188, 'grad_norm': 0.40625, 'learning_rate': 0.00016308818057090267, 'epoch': 0.19}\n",
      "{'loss': 1.3091, 'grad_norm': 0.455078125, 'learning_rate': 0.00016307681819343882, 'epoch': 0.19}\n",
      "{'loss': 1.2074, 'grad_norm': 0.7421875, 'learning_rate': 0.00016306545581597494, 'epoch': 0.19}\n",
      "{'loss': 1.3569, 'grad_norm': 0.6640625, 'learning_rate': 0.00016305409343851107, 'epoch': 0.19}\n",
      "{'loss': 1.1577, 'grad_norm': 0.7421875, 'learning_rate': 0.00016304273106104722, 'epoch': 0.19}\n",
      "{'loss': 1.1997, 'grad_norm': 0.44921875, 'learning_rate': 0.00016303136868358334, 'epoch': 0.19}\n",
      "{'loss': 1.3068, 'grad_norm': 0.5546875, 'learning_rate': 0.00016302000630611952, 'epoch': 0.19}\n",
      "{'loss': 1.0773, 'grad_norm': 0.9140625, 'learning_rate': 0.00016300864392865564, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3461, 'grad_norm': 0.4609375, 'learning_rate': 0.00016299728155119177, 'epoch': 0.19}\n",
      "{'loss': 1.2097, 'grad_norm': 0.50390625, 'learning_rate': 0.00016298591917372792, 'epoch': 0.19}\n",
      "{'loss': 1.2634, 'grad_norm': 0.58984375, 'learning_rate': 0.00016297455679626404, 'epoch': 0.19}\n",
      "{'loss': 1.2048, 'grad_norm': 0.55859375, 'learning_rate': 0.0001629631944188002, 'epoch': 0.19}\n",
      "{'loss': 1.0733, 'grad_norm': 0.62109375, 'learning_rate': 0.00016295183204133632, 'epoch': 0.19}\n",
      "{'loss': 1.3516, 'grad_norm': 0.5546875, 'learning_rate': 0.0001629404696638725, 'epoch': 0.19}\n",
      "{'loss': 1.2474, 'grad_norm': 0.7265625, 'learning_rate': 0.00016292910728640862, 'epoch': 0.19}\n",
      "{'loss': 1.165, 'grad_norm': 0.451171875, 'learning_rate': 0.00016291774490894475, 'epoch': 0.19}\n",
      "{'loss': 1.1635, 'grad_norm': 0.5, 'learning_rate': 0.0001629063825314809, 'epoch': 0.19}\n",
      "{'loss': 1.1031, 'grad_norm': 0.5390625, 'learning_rate': 0.00016289502015401702, 'epoch': 0.19}\n",
      "{'loss': 1.3074, 'grad_norm': 0.421875, 'learning_rate': 0.00016288365777655317, 'epoch': 0.19}\n",
      "{'loss': 1.191, 'grad_norm': 0.671875, 'learning_rate': 0.0001628722953990893, 'epoch': 0.19}\n",
      "{'loss': 1.454, 'grad_norm': 0.37109375, 'learning_rate': 0.00016286093302162545, 'epoch': 0.19}\n",
      "{'loss': 1.2788, 'grad_norm': 0.54296875, 'learning_rate': 0.0001628495706441616, 'epoch': 0.19}\n",
      "{'loss': 1.0538, 'grad_norm': 0.7578125, 'learning_rate': 0.00016283820826669773, 'epoch': 0.19}\n",
      "{'loss': 1.4863, 'grad_norm': 0.419921875, 'learning_rate': 0.00016282684588923388, 'epoch': 0.19}\n",
      "{'loss': 1.2566, 'grad_norm': 0.6640625, 'learning_rate': 0.00016281548351177, 'epoch': 0.19}\n",
      "{'loss': 1.281, 'grad_norm': 0.35546875, 'learning_rate': 0.00016280412113430615, 'epoch': 0.19}\n",
      "{'loss': 1.2534, 'grad_norm': 0.52734375, 'learning_rate': 0.00016279275875684228, 'epoch': 0.19}\n",
      "{'loss': 1.0113, 'grad_norm': 0.5703125, 'learning_rate': 0.00016278139637937843, 'epoch': 0.19}\n",
      "{'loss': 1.4999, 'grad_norm': 0.439453125, 'learning_rate': 0.00016277003400191458, 'epoch': 0.19}\n",
      "{'loss': 1.2517, 'grad_norm': 0.462890625, 'learning_rate': 0.0001627586716244507, 'epoch': 0.19}\n",
      "{'loss': 1.1349, 'grad_norm': 0.427734375, 'learning_rate': 0.00016274730924698686, 'epoch': 0.19}\n",
      "{'loss': 1.37, 'grad_norm': 0.419921875, 'learning_rate': 0.00016273594686952298, 'epoch': 0.19}\n",
      "{'loss': 1.1559, 'grad_norm': 0.60546875, 'learning_rate': 0.0001627245844920591, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5122, 'grad_norm': 0.44921875, 'learning_rate': 0.00016271322211459528, 'epoch': 0.19}\n",
      "{'loss': 1.2733, 'grad_norm': 0.6484375, 'learning_rate': 0.0001627018597371314, 'epoch': 0.19}\n",
      "{'loss': 1.2121, 'grad_norm': 0.4140625, 'learning_rate': 0.00016269049735966756, 'epoch': 0.19}\n",
      "{'loss': 1.2264, 'grad_norm': 0.78515625, 'learning_rate': 0.00016267913498220368, 'epoch': 0.19}\n",
      "{'loss': 1.2029, 'grad_norm': 0.890625, 'learning_rate': 0.0001626677726047398, 'epoch': 0.19}\n",
      "{'loss': 1.3412, 'grad_norm': 0.49609375, 'learning_rate': 0.00016265641022727596, 'epoch': 0.19}\n",
      "{'loss': 1.2475, 'grad_norm': 0.96484375, 'learning_rate': 0.00016264504784981208, 'epoch': 0.19}\n",
      "{'loss': 1.0746, 'grad_norm': 0.41796875, 'learning_rate': 0.00016263368547234826, 'epoch': 0.19}\n",
      "{'loss': 1.3369, 'grad_norm': 0.62109375, 'learning_rate': 0.00016262232309488439, 'epoch': 0.19}\n",
      "{'loss': 1.1467, 'grad_norm': 0.8203125, 'learning_rate': 0.00016261096071742054, 'epoch': 0.19}\n",
      "{'loss': 1.4315, 'grad_norm': 0.6171875, 'learning_rate': 0.00016259959833995666, 'epoch': 0.19}\n",
      "{'loss': 1.2767, 'grad_norm': 0.6015625, 'learning_rate': 0.00016258823596249279, 'epoch': 0.19}\n",
      "{'loss': 1.3182, 'grad_norm': 0.478515625, 'learning_rate': 0.00016257687358502894, 'epoch': 0.19}\n",
      "{'loss': 1.3858, 'grad_norm': 0.52734375, 'learning_rate': 0.00016256551120756506, 'epoch': 0.19}\n",
      "{'loss': 1.259, 'grad_norm': 0.73828125, 'learning_rate': 0.00016255414883010124, 'epoch': 0.19}\n",
      "{'loss': 1.35, 'grad_norm': 0.498046875, 'learning_rate': 0.00016254278645263736, 'epoch': 0.19}\n",
      "{'loss': 1.1531, 'grad_norm': 0.94921875, 'learning_rate': 0.0001625314240751735, 'epoch': 0.19}\n",
      "{'loss': 1.1563, 'grad_norm': 0.384765625, 'learning_rate': 0.00016252006169770964, 'epoch': 0.19}\n",
      "{'loss': 1.4114, 'grad_norm': 0.5234375, 'learning_rate': 0.00016250869932024576, 'epoch': 0.19}\n",
      "{'loss': 1.1432, 'grad_norm': 0.7265625, 'learning_rate': 0.00016249733694278192, 'epoch': 0.19}\n",
      "{'loss': 1.3743, 'grad_norm': 0.4140625, 'learning_rate': 0.00016248597456531804, 'epoch': 0.19}\n",
      "{'loss': 1.1242, 'grad_norm': 0.71484375, 'learning_rate': 0.0001624746121878542, 'epoch': 0.19}\n",
      "{'loss': 1.3234, 'grad_norm': 0.50390625, 'learning_rate': 0.00016246324981039034, 'epoch': 0.19}\n",
      "{'loss': 1.4261, 'grad_norm': 0.4765625, 'learning_rate': 0.00016245188743292647, 'epoch': 0.19}\n",
      "{'loss': 1.2824, 'grad_norm': 0.79296875, 'learning_rate': 0.00016244052505546262, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3476, 'grad_norm': 0.4921875, 'learning_rate': 0.00016242916267799874, 'epoch': 0.19}\n",
      "{'loss': 1.4006, 'grad_norm': 0.59765625, 'learning_rate': 0.0001624178003005349, 'epoch': 0.19}\n",
      "{'loss': 1.2002, 'grad_norm': 0.3359375, 'learning_rate': 0.00016240643792307105, 'epoch': 0.19}\n",
      "{'loss': 1.3553, 'grad_norm': 0.52734375, 'learning_rate': 0.00016239507554560717, 'epoch': 0.19}\n",
      "{'loss': 1.3343, 'grad_norm': 0.8671875, 'learning_rate': 0.00016238371316814332, 'epoch': 0.19}\n",
      "{'loss': 1.3075, 'grad_norm': 0.74609375, 'learning_rate': 0.00016237235079067945, 'epoch': 0.19}\n",
      "{'loss': 1.2162, 'grad_norm': 0.77734375, 'learning_rate': 0.0001623609884132156, 'epoch': 0.19}\n",
      "{'loss': 1.2674, 'grad_norm': 0.498046875, 'learning_rate': 0.00016234962603575172, 'epoch': 0.19}\n",
      "{'loss': 1.2577, 'grad_norm': 0.53515625, 'learning_rate': 0.00016233826365828785, 'epoch': 0.19}\n",
      "{'loss': 1.1653, 'grad_norm': 0.578125, 'learning_rate': 0.00016232690128082402, 'epoch': 0.19}\n",
      "{'loss': 1.2949, 'grad_norm': 0.42578125, 'learning_rate': 0.00016231553890336015, 'epoch': 0.19}\n",
      "{'loss': 1.2783, 'grad_norm': 0.6328125, 'learning_rate': 0.0001623041765258963, 'epoch': 0.19}\n",
      "{'loss': 1.2817, 'grad_norm': 0.466796875, 'learning_rate': 0.00016229281414843242, 'epoch': 0.19}\n",
      "{'loss': 1.2579, 'grad_norm': 0.5234375, 'learning_rate': 0.00016228145177096855, 'epoch': 0.19}\n",
      "{'loss': 1.2492, 'grad_norm': 0.8515625, 'learning_rate': 0.0001622700893935047, 'epoch': 0.19}\n",
      "{'loss': 1.4767, 'grad_norm': 0.46484375, 'learning_rate': 0.00016225872701604082, 'epoch': 0.19}\n",
      "{'loss': 1.1693, 'grad_norm': 0.58203125, 'learning_rate': 0.000162247364638577, 'epoch': 0.19}\n",
      "{'loss': 1.3003, 'grad_norm': 0.373046875, 'learning_rate': 0.00016223600226111313, 'epoch': 0.19}\n",
      "{'loss': 1.2077, 'grad_norm': 0.6015625, 'learning_rate': 0.00016222463988364928, 'epoch': 0.19}\n",
      "{'loss': 1.0215, 'grad_norm': 1.3515625, 'learning_rate': 0.0001622132775061854, 'epoch': 0.19}\n",
      "{'loss': 1.2858, 'grad_norm': 0.65625, 'learning_rate': 0.00016220191512872153, 'epoch': 0.19}\n",
      "{'loss': 1.1951, 'grad_norm': 0.5703125, 'learning_rate': 0.00016219055275125768, 'epoch': 0.19}\n",
      "{'loss': 1.227, 'grad_norm': 0.388671875, 'learning_rate': 0.0001621791903737938, 'epoch': 0.19}\n",
      "{'loss': 1.2634, 'grad_norm': 0.82421875, 'learning_rate': 0.00016216782799632998, 'epoch': 0.19}\n",
      "{'loss': 0.9827, 'grad_norm': 0.455078125, 'learning_rate': 0.0001621564656188661, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.508, 'grad_norm': 0.46875, 'learning_rate': 0.00016214510324140223, 'epoch': 0.19}\n",
      "{'loss': 1.1784, 'grad_norm': 0.5390625, 'learning_rate': 0.00016213374086393838, 'epoch': 0.19}\n",
      "{'loss': 1.2609, 'grad_norm': 0.392578125, 'learning_rate': 0.0001621223784864745, 'epoch': 0.19}\n",
      "{'loss': 1.3204, 'grad_norm': 0.474609375, 'learning_rate': 0.00016211101610901066, 'epoch': 0.19}\n",
      "{'loss': 1.1901, 'grad_norm': 0.765625, 'learning_rate': 0.00016209965373154678, 'epoch': 0.19}\n",
      "{'loss': 1.3679, 'grad_norm': 0.4140625, 'learning_rate': 0.00016208829135408293, 'epoch': 0.19}\n",
      "{'loss': 1.2137, 'grad_norm': 0.83203125, 'learning_rate': 0.00016207692897661908, 'epoch': 0.19}\n",
      "{'loss': 1.2599, 'grad_norm': 0.54296875, 'learning_rate': 0.0001620655665991552, 'epoch': 0.19}\n",
      "{'loss': 1.2158, 'grad_norm': 0.56640625, 'learning_rate': 0.00016205420422169136, 'epoch': 0.19}\n",
      "{'loss': 1.0769, 'grad_norm': 0.84375, 'learning_rate': 0.00016204284184422748, 'epoch': 0.19}\n",
      "{'loss': 1.2662, 'grad_norm': 0.66015625, 'learning_rate': 0.00016203147946676364, 'epoch': 0.19}\n",
      "{'loss': 1.2186, 'grad_norm': 0.80859375, 'learning_rate': 0.0001620201170892998, 'epoch': 0.19}\n",
      "{'loss': 1.3211, 'grad_norm': 0.5078125, 'learning_rate': 0.0001620087547118359, 'epoch': 0.19}\n",
      "{'loss': 1.2801, 'grad_norm': 0.470703125, 'learning_rate': 0.00016199739233437206, 'epoch': 0.19}\n",
      "{'loss': 1.0894, 'grad_norm': 0.796875, 'learning_rate': 0.0001619860299569082, 'epoch': 0.19}\n",
      "{'loss': 1.2967, 'grad_norm': 0.4375, 'learning_rate': 0.00016197466757944434, 'epoch': 0.19}\n",
      "{'loss': 1.1777, 'grad_norm': 0.86328125, 'learning_rate': 0.00016196330520198046, 'epoch': 0.19}\n",
      "{'loss': 1.2857, 'grad_norm': 0.4296875, 'learning_rate': 0.0001619519428245166, 'epoch': 0.19}\n",
      "{'loss': 1.3334, 'grad_norm': 0.72265625, 'learning_rate': 0.00016194058044705277, 'epoch': 0.19}\n",
      "{'loss': 1.1689, 'grad_norm': 0.5234375, 'learning_rate': 0.0001619292180695889, 'epoch': 0.19}\n",
      "{'loss': 1.3212, 'grad_norm': 0.416015625, 'learning_rate': 0.00016191785569212504, 'epoch': 0.19}\n",
      "{'loss': 1.1509, 'grad_norm': 0.55078125, 'learning_rate': 0.00016190649331466117, 'epoch': 0.19}\n",
      "{'loss': 1.2058, 'grad_norm': 0.3828125, 'learning_rate': 0.0001618951309371973, 'epoch': 0.19}\n",
      "{'loss': 1.3143, 'grad_norm': 0.671875, 'learning_rate': 0.00016188376855973344, 'epoch': 0.19}\n",
      "{'loss': 1.0303, 'grad_norm': 0.48046875, 'learning_rate': 0.00016187240618226957, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3433, 'grad_norm': 0.384765625, 'learning_rate': 0.00016186104380480574, 'epoch': 0.19}\n",
      "{'loss': 1.2506, 'grad_norm': 0.98828125, 'learning_rate': 0.00016184968142734187, 'epoch': 0.19}\n",
      "{'loss': 1.1856, 'grad_norm': 0.423828125, 'learning_rate': 0.00016183831904987802, 'epoch': 0.19}\n",
      "{'loss': 1.2574, 'grad_norm': 0.73046875, 'learning_rate': 0.00016182695667241414, 'epoch': 0.19}\n",
      "{'loss': 1.1755, 'grad_norm': 0.90234375, 'learning_rate': 0.00016181559429495027, 'epoch': 0.19}\n",
      "{'loss': 1.4086, 'grad_norm': 0.65625, 'learning_rate': 0.00016180423191748642, 'epoch': 0.19}\n",
      "{'loss': 1.1846, 'grad_norm': 0.52734375, 'learning_rate': 0.00016179286954002254, 'epoch': 0.19}\n",
      "{'loss': 1.071, 'grad_norm': 0.384765625, 'learning_rate': 0.00016178150716255872, 'epoch': 0.19}\n",
      "{'loss': 1.0617, 'grad_norm': 0.6484375, 'learning_rate': 0.00016177014478509485, 'epoch': 0.19}\n",
      "{'loss': 1.0579, 'grad_norm': 1.046875, 'learning_rate': 0.00016175878240763097, 'epoch': 0.19}\n",
      "{'loss': 1.4691, 'grad_norm': 0.396484375, 'learning_rate': 0.00016174742003016712, 'epoch': 0.19}\n",
      "{'loss': 1.1453, 'grad_norm': 0.734375, 'learning_rate': 0.00016173605765270325, 'epoch': 0.19}\n",
      "{'loss': 1.3244, 'grad_norm': 0.5234375, 'learning_rate': 0.0001617246952752394, 'epoch': 0.19}\n",
      "{'loss': 1.2058, 'grad_norm': 0.62890625, 'learning_rate': 0.00016171333289777555, 'epoch': 0.19}\n",
      "{'loss': 1.0392, 'grad_norm': 0.9296875, 'learning_rate': 0.00016170197052031167, 'epoch': 0.19}\n",
      "{'loss': 1.4262, 'grad_norm': 0.5078125, 'learning_rate': 0.00016169060814284783, 'epoch': 0.19}\n",
      "{'loss': 1.1496, 'grad_norm': 0.62890625, 'learning_rate': 0.00016167924576538395, 'epoch': 0.19}\n",
      "{'loss': 1.329, 'grad_norm': 0.5078125, 'learning_rate': 0.0001616678833879201, 'epoch': 0.19}\n",
      "{'loss': 1.3976, 'grad_norm': 0.54296875, 'learning_rate': 0.00016165652101045623, 'epoch': 0.19}\n",
      "{'loss': 1.1776, 'grad_norm': 1.0, 'learning_rate': 0.00016164515863299238, 'epoch': 0.19}\n",
      "{'loss': 1.4346, 'grad_norm': 0.53515625, 'learning_rate': 0.00016163379625552853, 'epoch': 0.19}\n",
      "{'loss': 1.248, 'grad_norm': 0.80078125, 'learning_rate': 0.00016162243387806465, 'epoch': 0.19}\n",
      "{'loss': 1.2372, 'grad_norm': 0.546875, 'learning_rate': 0.0001616110715006008, 'epoch': 0.19}\n",
      "{'loss': 1.2454, 'grad_norm': 0.5390625, 'learning_rate': 0.00016159970912313693, 'epoch': 0.19}\n",
      "{'loss': 1.0196, 'grad_norm': 1.078125, 'learning_rate': 0.00016158834674567308, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3498, 'grad_norm': 0.421875, 'learning_rate': 0.0001615769843682092, 'epoch': 0.19}\n",
      "{'loss': 1.2046, 'grad_norm': 0.515625, 'learning_rate': 0.00016156562199074533, 'epoch': 0.19}\n",
      "{'loss': 1.2782, 'grad_norm': 0.39453125, 'learning_rate': 0.0001615542596132815, 'epoch': 0.19}\n",
      "{'loss': 1.3058, 'grad_norm': 0.5859375, 'learning_rate': 0.00016154289723581763, 'epoch': 0.19}\n",
      "{'loss': 1.1431, 'grad_norm': 0.69140625, 'learning_rate': 0.00016153153485835378, 'epoch': 0.19}\n",
      "{'loss': 1.346, 'grad_norm': 0.40625, 'learning_rate': 0.0001615201724808899, 'epoch': 0.19}\n",
      "{'loss': 1.1936, 'grad_norm': 0.55859375, 'learning_rate': 0.00016150881010342603, 'epoch': 0.2}\n",
      "{'loss': 1.2229, 'grad_norm': 0.408203125, 'learning_rate': 0.00016149744772596218, 'epoch': 0.2}\n",
      "{'loss': 1.2502, 'grad_norm': 0.78515625, 'learning_rate': 0.0001614860853484983, 'epoch': 0.2}\n",
      "{'loss': 1.1083, 'grad_norm': 1.171875, 'learning_rate': 0.00016147472297103449, 'epoch': 0.2}\n",
      "{'loss': 1.4244, 'grad_norm': 0.65234375, 'learning_rate': 0.0001614633605935706, 'epoch': 0.2}\n",
      "{'loss': 1.2203, 'grad_norm': 0.65234375, 'learning_rate': 0.00016145199821610676, 'epoch': 0.2}\n",
      "{'loss': 1.2384, 'grad_norm': 0.5546875, 'learning_rate': 0.00016144063583864289, 'epoch': 0.2}\n",
      "{'loss': 1.2036, 'grad_norm': 0.59765625, 'learning_rate': 0.000161429273461179, 'epoch': 0.2}\n",
      "{'loss': 1.0701, 'grad_norm': 0.53125, 'learning_rate': 0.00016141791108371516, 'epoch': 0.2}\n",
      "{'loss': 1.3294, 'grad_norm': 0.59765625, 'learning_rate': 0.00016140654870625129, 'epoch': 0.2}\n",
      "{'loss': 1.1657, 'grad_norm': 1.25, 'learning_rate': 0.00016139518632878746, 'epoch': 0.2}\n",
      "{'loss': 1.3187, 'grad_norm': 0.443359375, 'learning_rate': 0.0001613838239513236, 'epoch': 0.2}\n",
      "{'loss': 1.3787, 'grad_norm': 0.6796875, 'learning_rate': 0.0001613724615738597, 'epoch': 0.2}\n",
      "{'loss': 1.1071, 'grad_norm': 0.875, 'learning_rate': 0.00016136109919639586, 'epoch': 0.2}\n",
      "{'loss': 1.2253, 'grad_norm': 0.466796875, 'learning_rate': 0.000161349736818932, 'epoch': 0.2}\n",
      "{'loss': 1.2048, 'grad_norm': 0.5234375, 'learning_rate': 0.00016133837444146814, 'epoch': 0.2}\n",
      "{'loss': 1.3485, 'grad_norm': 0.404296875, 'learning_rate': 0.0001613270120640043, 'epoch': 0.2}\n",
      "{'loss': 1.2889, 'grad_norm': 0.62109375, 'learning_rate': 0.00016131564968654042, 'epoch': 0.2}\n",
      "{'loss': 1.1846, 'grad_norm': 0.77734375, 'learning_rate': 0.00016130428730907657, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3728, 'grad_norm': 0.42578125, 'learning_rate': 0.0001612929249316127, 'epoch': 0.2}\n",
      "{'loss': 1.1821, 'grad_norm': 0.78515625, 'learning_rate': 0.00016128156255414884, 'epoch': 0.2}\n",
      "{'loss': 1.291, 'grad_norm': 0.443359375, 'learning_rate': 0.00016127020017668497, 'epoch': 0.2}\n",
      "{'loss': 1.262, 'grad_norm': 0.5078125, 'learning_rate': 0.00016125883779922112, 'epoch': 0.2}\n",
      "{'loss': 1.12, 'grad_norm': 0.65625, 'learning_rate': 0.00016124747542175727, 'epoch': 0.2}\n",
      "{'loss': 1.5349, 'grad_norm': 0.6796875, 'learning_rate': 0.0001612361130442934, 'epoch': 0.2}\n",
      "{'loss': 1.2396, 'grad_norm': 0.73046875, 'learning_rate': 0.00016122475066682955, 'epoch': 0.2}\n",
      "{'loss': 1.3662, 'grad_norm': 0.3828125, 'learning_rate': 0.00016121338828936567, 'epoch': 0.2}\n",
      "{'loss': 1.4055, 'grad_norm': 0.49609375, 'learning_rate': 0.00016120202591190182, 'epoch': 0.2}\n",
      "{'loss': 1.2475, 'grad_norm': 0.83203125, 'learning_rate': 0.00016119066353443795, 'epoch': 0.2}\n",
      "{'loss': 1.3652, 'grad_norm': 0.5859375, 'learning_rate': 0.00016117930115697407, 'epoch': 0.2}\n",
      "{'loss': 1.1785, 'grad_norm': 0.5703125, 'learning_rate': 0.00016116793877951025, 'epoch': 0.2}\n",
      "{'loss': 1.2439, 'grad_norm': 0.390625, 'learning_rate': 0.00016115657640204637, 'epoch': 0.2}\n",
      "{'loss': 1.2329, 'grad_norm': 0.484375, 'learning_rate': 0.00016114521402458252, 'epoch': 0.2}\n",
      "{'loss': 1.1919, 'grad_norm': 0.67578125, 'learning_rate': 0.00016113385164711865, 'epoch': 0.2}\n",
      "{'loss': 1.3527, 'grad_norm': 0.47265625, 'learning_rate': 0.00016112248926965477, 'epoch': 0.2}\n",
      "{'loss': 1.053, 'grad_norm': 0.53125, 'learning_rate': 0.00016111112689219092, 'epoch': 0.2}\n",
      "{'loss': 1.3729, 'grad_norm': 0.353515625, 'learning_rate': 0.00016109976451472705, 'epoch': 0.2}\n",
      "{'loss': 1.2851, 'grad_norm': 0.546875, 'learning_rate': 0.00016108840213726323, 'epoch': 0.2}\n",
      "{'loss': 1.202, 'grad_norm': 0.95703125, 'learning_rate': 0.00016107703975979935, 'epoch': 0.2}\n",
      "{'loss': 1.3781, 'grad_norm': 0.5390625, 'learning_rate': 0.0001610656773823355, 'epoch': 0.2}\n",
      "{'loss': 1.3393, 'grad_norm': 0.59375, 'learning_rate': 0.00016105431500487163, 'epoch': 0.2}\n",
      "{'loss': 1.1976, 'grad_norm': 0.470703125, 'learning_rate': 0.00016104295262740775, 'epoch': 0.2}\n",
      "{'loss': 1.2388, 'grad_norm': 0.65625, 'learning_rate': 0.0001610315902499439, 'epoch': 0.2}\n",
      "{'loss': 1.1379, 'grad_norm': 1.5078125, 'learning_rate': 0.00016102022787248005, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.314, 'grad_norm': 0.4765625, 'learning_rate': 0.0001610088654950162, 'epoch': 0.2}\n",
      "{'loss': 1.1876, 'grad_norm': 0.703125, 'learning_rate': 0.00016099750311755233, 'epoch': 0.2}\n",
      "{'loss': 1.283, 'grad_norm': 0.396484375, 'learning_rate': 0.00016098614074008845, 'epoch': 0.2}\n",
      "{'loss': 1.2506, 'grad_norm': 0.88671875, 'learning_rate': 0.0001609747783626246, 'epoch': 0.2}\n",
      "{'loss': 1.0375, 'grad_norm': 1.1640625, 'learning_rate': 0.00016096341598516073, 'epoch': 0.2}\n",
      "{'loss': 1.3355, 'grad_norm': 0.404296875, 'learning_rate': 0.00016095205360769688, 'epoch': 0.2}\n",
      "{'loss': 1.1236, 'grad_norm': 0.765625, 'learning_rate': 0.00016094069123023303, 'epoch': 0.2}\n",
      "{'loss': 1.2103, 'grad_norm': 0.484375, 'learning_rate': 0.00016092932885276916, 'epoch': 0.2}\n",
      "{'loss': 1.1601, 'grad_norm': 0.73828125, 'learning_rate': 0.0001609179664753053, 'epoch': 0.2}\n",
      "{'loss': 1.093, 'grad_norm': 0.7578125, 'learning_rate': 0.00016090660409784143, 'epoch': 0.2}\n",
      "{'loss': 1.382, 'grad_norm': 0.6484375, 'learning_rate': 0.00016089524172037758, 'epoch': 0.2}\n",
      "{'loss': 1.1816, 'grad_norm': 0.5625, 'learning_rate': 0.0001608838793429137, 'epoch': 0.2}\n",
      "{'loss': 1.1965, 'grad_norm': 0.37109375, 'learning_rate': 0.00016087251696544986, 'epoch': 0.2}\n",
      "{'loss': 1.3352, 'grad_norm': 0.53125, 'learning_rate': 0.000160861154587986, 'epoch': 0.2}\n",
      "{'loss': 1.1607, 'grad_norm': 0.455078125, 'learning_rate': 0.00016084979221052214, 'epoch': 0.2}\n",
      "{'loss': 1.4121, 'grad_norm': 0.61328125, 'learning_rate': 0.0001608384298330583, 'epoch': 0.2}\n",
      "{'loss': 1.2405, 'grad_norm': 0.66015625, 'learning_rate': 0.0001608270674555944, 'epoch': 0.2}\n",
      "{'loss': 1.14, 'grad_norm': 0.412109375, 'learning_rate': 0.00016081570507813056, 'epoch': 0.2}\n",
      "{'loss': 1.2736, 'grad_norm': 0.44140625, 'learning_rate': 0.0001608043427006667, 'epoch': 0.2}\n",
      "{'loss': 1.2441, 'grad_norm': 0.60546875, 'learning_rate': 0.0001607929803232028, 'epoch': 0.2}\n",
      "{'loss': 1.3533, 'grad_norm': 0.6015625, 'learning_rate': 0.000160781617945739, 'epoch': 0.2}\n",
      "{'loss': 1.2577, 'grad_norm': 0.6015625, 'learning_rate': 0.00016077025556827511, 'epoch': 0.2}\n",
      "{'loss': 1.2149, 'grad_norm': 0.54296875, 'learning_rate': 0.00016075889319081127, 'epoch': 0.2}\n",
      "{'loss': 1.3182, 'grad_norm': 0.5078125, 'learning_rate': 0.0001607475308133474, 'epoch': 0.2}\n",
      "{'loss': 1.1047, 'grad_norm': 1.046875, 'learning_rate': 0.00016073616843588351, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3775, 'grad_norm': 0.40234375, 'learning_rate': 0.00016072480605841967, 'epoch': 0.2}\n",
      "{'loss': 1.0992, 'grad_norm': 0.79296875, 'learning_rate': 0.00016071344368095582, 'epoch': 0.2}\n",
      "{'loss': 1.2799, 'grad_norm': 0.48828125, 'learning_rate': 0.00016070208130349197, 'epoch': 0.2}\n",
      "{'loss': 1.143, 'grad_norm': 0.54296875, 'learning_rate': 0.0001606907189260281, 'epoch': 0.2}\n",
      "{'loss': 1.1741, 'grad_norm': 0.419921875, 'learning_rate': 0.00016067935654856424, 'epoch': 0.2}\n",
      "{'loss': 1.3943, 'grad_norm': 0.44921875, 'learning_rate': 0.00016066799417110037, 'epoch': 0.2}\n",
      "{'loss': 1.1792, 'grad_norm': 0.546875, 'learning_rate': 0.0001606566317936365, 'epoch': 0.2}\n",
      "{'loss': 1.1957, 'grad_norm': 0.423828125, 'learning_rate': 0.00016064526941617264, 'epoch': 0.2}\n",
      "{'loss': 1.3417, 'grad_norm': 0.423828125, 'learning_rate': 0.0001606339070387088, 'epoch': 0.2}\n",
      "{'loss': 1.1469, 'grad_norm': 0.671875, 'learning_rate': 0.00016062254466124495, 'epoch': 0.2}\n",
      "{'loss': 1.3818, 'grad_norm': 0.6328125, 'learning_rate': 0.00016061118228378107, 'epoch': 0.2}\n",
      "{'loss': 1.1825, 'grad_norm': 0.88671875, 'learning_rate': 0.0001605998199063172, 'epoch': 0.2}\n",
      "{'loss': 1.2438, 'grad_norm': 0.3984375, 'learning_rate': 0.00016058845752885335, 'epoch': 0.2}\n",
      "{'loss': 1.2741, 'grad_norm': 0.6171875, 'learning_rate': 0.00016057709515138947, 'epoch': 0.2}\n",
      "{'loss': 1.033, 'grad_norm': 0.72265625, 'learning_rate': 0.00016056573277392562, 'epoch': 0.2}\n",
      "{'loss': 1.4624, 'grad_norm': 0.51171875, 'learning_rate': 0.00016055437039646177, 'epoch': 0.2}\n",
      "{'loss': 1.1285, 'grad_norm': 0.427734375, 'learning_rate': 0.0001605430080189979, 'epoch': 0.2}\n",
      "{'loss': 1.0428, 'grad_norm': 0.466796875, 'learning_rate': 0.00016053164564153405, 'epoch': 0.2}\n",
      "{'loss': 1.2241, 'grad_norm': 0.546875, 'learning_rate': 0.00016052028326407017, 'epoch': 0.2}\n",
      "{'loss': 1.275, 'grad_norm': 0.84765625, 'learning_rate': 0.00016050892088660633, 'epoch': 0.2}\n",
      "{'loss': 1.3614, 'grad_norm': 0.392578125, 'learning_rate': 0.00016049755850914245, 'epoch': 0.2}\n",
      "{'loss': 1.1906, 'grad_norm': 0.46484375, 'learning_rate': 0.0001604861961316786, 'epoch': 0.2}\n",
      "{'loss': 1.3239, 'grad_norm': 0.390625, 'learning_rate': 0.00016047483375421475, 'epoch': 0.2}\n",
      "{'loss': 1.3622, 'grad_norm': 0.49609375, 'learning_rate': 0.00016046347137675088, 'epoch': 0.2}\n",
      "{'loss': 1.1476, 'grad_norm': 0.75, 'learning_rate': 0.00016045210899928703, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.323, 'grad_norm': 0.52734375, 'learning_rate': 0.00016044074662182315, 'epoch': 0.2}\n",
      "{'loss': 1.1142, 'grad_norm': 0.734375, 'learning_rate': 0.0001604293842443593, 'epoch': 0.2}\n",
      "{'loss': 1.2357, 'grad_norm': 0.388671875, 'learning_rate': 0.00016041802186689543, 'epoch': 0.2}\n",
      "{'loss': 1.2645, 'grad_norm': 0.51953125, 'learning_rate': 0.00016040665948943155, 'epoch': 0.2}\n",
      "{'loss': 1.1108, 'grad_norm': 0.7265625, 'learning_rate': 0.00016039529711196773, 'epoch': 0.2}\n",
      "{'loss': 1.3481, 'grad_norm': 0.458984375, 'learning_rate': 0.00016038393473450386, 'epoch': 0.2}\n",
      "{'loss': 1.2032, 'grad_norm': 0.6015625, 'learning_rate': 0.00016037257235704, 'epoch': 0.2}\n",
      "{'loss': 1.2086, 'grad_norm': 0.62109375, 'learning_rate': 0.00016036120997957613, 'epoch': 0.2}\n",
      "{'loss': 1.1281, 'grad_norm': 0.71875, 'learning_rate': 0.00016034984760211226, 'epoch': 0.2}\n",
      "{'loss': 1.1212, 'grad_norm': 0.640625, 'learning_rate': 0.0001603384852246484, 'epoch': 0.2}\n",
      "{'loss': 1.377, 'grad_norm': 0.578125, 'learning_rate': 0.00016032712284718456, 'epoch': 0.2}\n",
      "{'loss': 1.2955, 'grad_norm': 0.54296875, 'learning_rate': 0.0001603157604697207, 'epoch': 0.2}\n",
      "{'loss': 1.2532, 'grad_norm': 0.46875, 'learning_rate': 0.00016030439809225683, 'epoch': 0.2}\n",
      "{'loss': 1.3794, 'grad_norm': 0.50390625, 'learning_rate': 0.00016029303571479299, 'epoch': 0.2}\n",
      "{'loss': 1.1514, 'grad_norm': 0.5703125, 'learning_rate': 0.0001602816733373291, 'epoch': 0.2}\n",
      "{'loss': 1.4649, 'grad_norm': 0.43359375, 'learning_rate': 0.00016027031095986523, 'epoch': 0.2}\n",
      "{'loss': 1.2554, 'grad_norm': 0.8125, 'learning_rate': 0.00016025894858240139, 'epoch': 0.2}\n",
      "{'loss': 1.2424, 'grad_norm': 0.46484375, 'learning_rate': 0.00016024758620493754, 'epoch': 0.2}\n",
      "{'loss': 1.2966, 'grad_norm': 0.57421875, 'learning_rate': 0.0001602362238274737, 'epoch': 0.2}\n",
      "{'loss': 1.145, 'grad_norm': 0.71484375, 'learning_rate': 0.0001602248614500098, 'epoch': 0.2}\n",
      "{'loss': 1.3475, 'grad_norm': 0.5, 'learning_rate': 0.00016021349907254594, 'epoch': 0.2}\n",
      "{'loss': 1.3022, 'grad_norm': 0.91015625, 'learning_rate': 0.0001602021366950821, 'epoch': 0.2}\n",
      "{'loss': 1.1459, 'grad_norm': 0.416015625, 'learning_rate': 0.0001601907743176182, 'epoch': 0.2}\n",
      "{'loss': 1.2856, 'grad_norm': 0.57421875, 'learning_rate': 0.00016017941194015436, 'epoch': 0.2}\n",
      "{'loss': 1.2081, 'grad_norm': 0.58203125, 'learning_rate': 0.00016016804956269052, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3626, 'grad_norm': 0.482421875, 'learning_rate': 0.00016015668718522664, 'epoch': 0.2}\n",
      "{'loss': 1.2431, 'grad_norm': 0.71484375, 'learning_rate': 0.0001601453248077628, 'epoch': 0.2}\n",
      "{'loss': 1.2694, 'grad_norm': 0.419921875, 'learning_rate': 0.00016013396243029892, 'epoch': 0.2}\n",
      "{'loss': 1.2826, 'grad_norm': 0.5703125, 'learning_rate': 0.00016012260005283507, 'epoch': 0.2}\n",
      "{'loss': 1.0804, 'grad_norm': 0.8046875, 'learning_rate': 0.0001601112376753712, 'epoch': 0.2}\n",
      "{'loss': 1.3577, 'grad_norm': 0.5234375, 'learning_rate': 0.00016009987529790734, 'epoch': 0.2}\n",
      "{'loss': 1.1232, 'grad_norm': 0.482421875, 'learning_rate': 0.0001600885129204435, 'epoch': 0.2}\n",
      "{'loss': 1.2725, 'grad_norm': 0.466796875, 'learning_rate': 0.00016007715054297962, 'epoch': 0.2}\n",
      "{'loss': 1.355, 'grad_norm': 0.68359375, 'learning_rate': 0.00016006578816551577, 'epoch': 0.2}\n",
      "{'loss': 1.1815, 'grad_norm': 0.83203125, 'learning_rate': 0.0001600544257880519, 'epoch': 0.2}\n",
      "{'loss': 1.1598, 'grad_norm': 0.51171875, 'learning_rate': 0.00016004306341058805, 'epoch': 0.2}\n",
      "{'loss': 1.2504, 'grad_norm': 0.62890625, 'learning_rate': 0.00016003170103312417, 'epoch': 0.2}\n",
      "{'loss': 1.2609, 'grad_norm': 0.48046875, 'learning_rate': 0.00016002033865566032, 'epoch': 0.2}\n",
      "{'loss': 1.2917, 'grad_norm': 0.55078125, 'learning_rate': 0.00016000897627819647, 'epoch': 0.2}\n",
      "{'loss': 1.0492, 'grad_norm': 0.412109375, 'learning_rate': 0.0001599976139007326, 'epoch': 0.2}\n",
      "{'loss': 1.455, 'grad_norm': 0.4609375, 'learning_rate': 0.00015998625152326875, 'epoch': 0.2}\n",
      "{'loss': 1.3135, 'grad_norm': 0.75, 'learning_rate': 0.00015997488914580487, 'epoch': 0.2}\n",
      "{'loss': 1.274, 'grad_norm': 0.44921875, 'learning_rate': 0.000159963526768341, 'epoch': 0.2}\n",
      "{'loss': 1.2695, 'grad_norm': 0.478515625, 'learning_rate': 0.00015995216439087715, 'epoch': 0.2}\n",
      "{'loss': 1.128, 'grad_norm': 1.0078125, 'learning_rate': 0.0001599408020134133, 'epoch': 0.2}\n",
      "{'loss': 1.3102, 'grad_norm': 0.55078125, 'learning_rate': 0.00015992943963594945, 'epoch': 0.2}\n",
      "{'loss': 1.3893, 'grad_norm': 0.56640625, 'learning_rate': 0.00015991807725848558, 'epoch': 0.2}\n",
      "{'loss': 1.2189, 'grad_norm': 0.5703125, 'learning_rate': 0.00015990671488102173, 'epoch': 0.2}\n",
      "{'loss': 1.3335, 'grad_norm': 0.75390625, 'learning_rate': 0.00015989535250355785, 'epoch': 0.2}\n",
      "{'loss': 1.1436, 'grad_norm': 0.77734375, 'learning_rate': 0.00015988399012609398, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2557, 'grad_norm': 0.40625, 'learning_rate': 0.00015987262774863013, 'epoch': 0.2}\n",
      "{'loss': 1.3251, 'grad_norm': 0.59375, 'learning_rate': 0.00015986126537116628, 'epoch': 0.2}\n",
      "{'loss': 1.2267, 'grad_norm': 0.388671875, 'learning_rate': 0.00015984990299370243, 'epoch': 0.2}\n",
      "{'loss': 1.4118, 'grad_norm': 0.498046875, 'learning_rate': 0.00015983854061623855, 'epoch': 0.2}\n",
      "{'loss': 1.0847, 'grad_norm': 0.6875, 'learning_rate': 0.00015982717823877468, 'epoch': 0.2}\n",
      "{'loss': 1.3698, 'grad_norm': 0.40625, 'learning_rate': 0.00015981581586131083, 'epoch': 0.2}\n",
      "{'loss': 1.2162, 'grad_norm': 0.5703125, 'learning_rate': 0.00015980445348384695, 'epoch': 0.2}\n",
      "{'loss': 1.4082, 'grad_norm': 0.400390625, 'learning_rate': 0.0001597930911063831, 'epoch': 0.2}\n",
      "{'loss': 1.2697, 'grad_norm': 0.70703125, 'learning_rate': 0.00015978172872891926, 'epoch': 0.2}\n",
      "{'loss': 1.1447, 'grad_norm': 0.84375, 'learning_rate': 0.00015977036635145538, 'epoch': 0.2}\n",
      "{'loss': 1.395, 'grad_norm': 0.484375, 'learning_rate': 0.00015975900397399153, 'epoch': 0.2}\n",
      "{'loss': 1.1886, 'grad_norm': 0.890625, 'learning_rate': 0.00015974764159652766, 'epoch': 0.2}\n",
      "{'loss': 1.3078, 'grad_norm': 0.380859375, 'learning_rate': 0.0001597362792190638, 'epoch': 0.2}\n",
      "{'loss': 1.2937, 'grad_norm': 0.62890625, 'learning_rate': 0.00015972491684159993, 'epoch': 0.2}\n",
      "{'loss': 1.1139, 'grad_norm': 0.875, 'learning_rate': 0.00015971355446413608, 'epoch': 0.2}\n",
      "{'loss': 1.3727, 'grad_norm': 0.44921875, 'learning_rate': 0.00015970219208667223, 'epoch': 0.2}\n",
      "{'loss': 1.2723, 'grad_norm': 0.490234375, 'learning_rate': 0.00015969082970920836, 'epoch': 0.2}\n",
      "{'loss': 1.3734, 'grad_norm': 0.423828125, 'learning_rate': 0.0001596794673317445, 'epoch': 0.2}\n",
      "{'loss': 1.3784, 'grad_norm': 0.57421875, 'learning_rate': 0.00015966810495428063, 'epoch': 0.2}\n",
      "{'loss': 1.1131, 'grad_norm': 0.65625, 'learning_rate': 0.00015965674257681679, 'epoch': 0.2}\n",
      "{'loss': 1.2324, 'grad_norm': 0.443359375, 'learning_rate': 0.0001596453801993529, 'epoch': 0.2}\n",
      "{'loss': 1.2081, 'grad_norm': 0.6953125, 'learning_rate': 0.00015963401782188906, 'epoch': 0.2}\n",
      "{'loss': 1.1738, 'grad_norm': 0.546875, 'learning_rate': 0.0001596226554444252, 'epoch': 0.2}\n",
      "{'loss': 1.2395, 'grad_norm': 0.5859375, 'learning_rate': 0.00015961129306696134, 'epoch': 0.2}\n",
      "{'loss': 1.13, 'grad_norm': 0.765625, 'learning_rate': 0.0001595999306894975, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3663, 'grad_norm': 0.431640625, 'learning_rate': 0.0001595885683120336, 'epoch': 0.2}\n",
      "{'loss': 1.2458, 'grad_norm': 0.6171875, 'learning_rate': 0.00015957720593456974, 'epoch': 0.2}\n",
      "{'loss': 1.2204, 'grad_norm': 0.41015625, 'learning_rate': 0.0001595658435571059, 'epoch': 0.2}\n",
      "{'loss': 1.2855, 'grad_norm': 0.58984375, 'learning_rate': 0.00015955448117964204, 'epoch': 0.2}\n",
      "{'loss': 1.1149, 'grad_norm': 0.72265625, 'learning_rate': 0.0001595431188021782, 'epoch': 0.2}\n",
      "{'loss': 1.39, 'grad_norm': 0.46484375, 'learning_rate': 0.00015953175642471432, 'epoch': 0.2}\n",
      "{'loss': 1.3049, 'grad_norm': 0.92578125, 'learning_rate': 0.00015952039404725047, 'epoch': 0.2}\n",
      "{'loss': 1.2482, 'grad_norm': 0.416015625, 'learning_rate': 0.0001595090316697866, 'epoch': 0.2}\n",
      "{'loss': 1.368, 'grad_norm': 0.6640625, 'learning_rate': 0.00015949766929232272, 'epoch': 0.2}\n",
      "{'loss': 1.0918, 'grad_norm': 0.8984375, 'learning_rate': 0.00015948630691485887, 'epoch': 0.2}\n",
      "{'loss': 1.2904, 'grad_norm': 0.48828125, 'learning_rate': 0.00015947494453739502, 'epoch': 0.2}\n",
      "{'loss': 1.2272, 'grad_norm': 0.8515625, 'learning_rate': 0.00015946358215993117, 'epoch': 0.2}\n",
      "{'loss': 1.2878, 'grad_norm': 0.416015625, 'learning_rate': 0.0001594522197824673, 'epoch': 0.2}\n",
      "{'loss': 1.3118, 'grad_norm': 0.427734375, 'learning_rate': 0.00015944085740500342, 'epoch': 0.2}\n",
      "{'loss': 1.1261, 'grad_norm': 0.6328125, 'learning_rate': 0.00015942949502753957, 'epoch': 0.2}\n",
      "{'loss': 1.346, 'grad_norm': 0.46875, 'learning_rate': 0.0001594181326500757, 'epoch': 0.2}\n",
      "{'loss': 1.1908, 'grad_norm': 0.86328125, 'learning_rate': 0.00015940677027261185, 'epoch': 0.2}\n",
      "{'loss': 1.1488, 'grad_norm': 0.388671875, 'learning_rate': 0.000159395407895148, 'epoch': 0.2}\n",
      "{'loss': 1.167, 'grad_norm': 0.61328125, 'learning_rate': 0.00015938404551768412, 'epoch': 0.2}\n",
      "{'loss': 1.0115, 'grad_norm': 0.79296875, 'learning_rate': 0.00015937268314022027, 'epoch': 0.2}\n",
      "{'loss': 1.4384, 'grad_norm': 0.365234375, 'learning_rate': 0.0001593613207627564, 'epoch': 0.2}\n",
      "{'loss': 1.2481, 'grad_norm': 0.578125, 'learning_rate': 0.00015934995838529255, 'epoch': 0.2}\n",
      "{'loss': 1.2725, 'grad_norm': 0.5, 'learning_rate': 0.00015933859600782867, 'epoch': 0.2}\n",
      "{'loss': 1.279, 'grad_norm': 0.55078125, 'learning_rate': 0.00015932723363036482, 'epoch': 0.2}\n",
      "{'loss': 1.1331, 'grad_norm': 0.7890625, 'learning_rate': 0.00015931587125290098, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4957, 'grad_norm': 0.443359375, 'learning_rate': 0.0001593045088754371, 'epoch': 0.2}\n",
      "{'loss': 1.1789, 'grad_norm': 0.59375, 'learning_rate': 0.00015929314649797325, 'epoch': 0.2}\n",
      "{'loss': 1.186, 'grad_norm': 0.458984375, 'learning_rate': 0.00015928178412050938, 'epoch': 0.2}\n",
      "{'loss': 1.3734, 'grad_norm': 0.404296875, 'learning_rate': 0.00015927042174304553, 'epoch': 0.2}\n",
      "{'loss': 1.1531, 'grad_norm': 0.99609375, 'learning_rate': 0.00015925905936558165, 'epoch': 0.2}\n",
      "{'loss': 1.3717, 'grad_norm': 0.5, 'learning_rate': 0.0001592476969881178, 'epoch': 0.2}\n",
      "{'loss': 1.2734, 'grad_norm': 0.70703125, 'learning_rate': 0.00015923633461065395, 'epoch': 0.2}\n",
      "{'loss': 1.2017, 'grad_norm': 0.4296875, 'learning_rate': 0.00015922497223319008, 'epoch': 0.2}\n",
      "{'loss': 1.3023, 'grad_norm': 0.48828125, 'learning_rate': 0.00015921360985572623, 'epoch': 0.21}\n",
      "{'loss': 1.1528, 'grad_norm': 0.58203125, 'learning_rate': 0.00015920224747826235, 'epoch': 0.21}\n",
      "{'loss': 1.3597, 'grad_norm': 0.515625, 'learning_rate': 0.00015919088510079848, 'epoch': 0.21}\n",
      "{'loss': 1.1378, 'grad_norm': 0.52734375, 'learning_rate': 0.00015917952272333463, 'epoch': 0.21}\n",
      "{'loss': 1.1611, 'grad_norm': 0.609375, 'learning_rate': 0.00015916816034587078, 'epoch': 0.21}\n",
      "{'loss': 1.3468, 'grad_norm': 0.515625, 'learning_rate': 0.00015915679796840693, 'epoch': 0.21}\n",
      "{'loss': 1.1254, 'grad_norm': 0.8046875, 'learning_rate': 0.00015914543559094306, 'epoch': 0.21}\n",
      "{'loss': 1.3473, 'grad_norm': 0.65234375, 'learning_rate': 0.0001591340732134792, 'epoch': 0.21}\n",
      "{'loss': 1.2172, 'grad_norm': 0.5625, 'learning_rate': 0.00015912271083601533, 'epoch': 0.21}\n",
      "{'loss': 1.2266, 'grad_norm': 0.4765625, 'learning_rate': 0.00015911134845855146, 'epoch': 0.21}\n",
      "{'loss': 1.3363, 'grad_norm': 0.5546875, 'learning_rate': 0.0001590999860810876, 'epoch': 0.21}\n",
      "{'loss': 1.2329, 'grad_norm': 1.8046875, 'learning_rate': 0.00015908862370362376, 'epoch': 0.21}\n",
      "{'loss': 1.408, 'grad_norm': 0.421875, 'learning_rate': 0.0001590772613261599, 'epoch': 0.21}\n",
      "{'loss': 1.2276, 'grad_norm': 0.66796875, 'learning_rate': 0.00015906589894869604, 'epoch': 0.21}\n",
      "{'loss': 1.2269, 'grad_norm': 0.466796875, 'learning_rate': 0.00015905453657123216, 'epoch': 0.21}\n",
      "{'loss': 1.2587, 'grad_norm': 0.4765625, 'learning_rate': 0.0001590431741937683, 'epoch': 0.21}\n",
      "{'loss': 1.1408, 'grad_norm': 0.4140625, 'learning_rate': 0.00015903181181630444, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4082, 'grad_norm': 0.4375, 'learning_rate': 0.0001590204494388406, 'epoch': 0.21}\n",
      "{'loss': 1.1206, 'grad_norm': 0.55859375, 'learning_rate': 0.00015900908706137674, 'epoch': 0.21}\n",
      "{'loss': 1.2925, 'grad_norm': 0.404296875, 'learning_rate': 0.00015899772468391286, 'epoch': 0.21}\n",
      "{'loss': 1.2246, 'grad_norm': 0.73046875, 'learning_rate': 0.00015898636230644901, 'epoch': 0.21}\n",
      "{'loss': 1.0237, 'grad_norm': 0.7578125, 'learning_rate': 0.00015897499992898514, 'epoch': 0.21}\n",
      "{'loss': 1.3798, 'grad_norm': 0.65625, 'learning_rate': 0.0001589636375515213, 'epoch': 0.21}\n",
      "{'loss': 1.3047, 'grad_norm': 0.75390625, 'learning_rate': 0.00015895227517405741, 'epoch': 0.21}\n",
      "{'loss': 1.3238, 'grad_norm': 0.373046875, 'learning_rate': 0.00015894091279659357, 'epoch': 0.21}\n",
      "{'loss': 1.2847, 'grad_norm': 0.47265625, 'learning_rate': 0.00015892955041912972, 'epoch': 0.21}\n",
      "{'loss': 1.2056, 'grad_norm': 1.0234375, 'learning_rate': 0.00015891818804166584, 'epoch': 0.21}\n",
      "{'loss': 1.3839, 'grad_norm': 0.486328125, 'learning_rate': 0.000158906825664202, 'epoch': 0.21}\n",
      "{'loss': 1.346, 'grad_norm': 0.74609375, 'learning_rate': 0.00015889546328673812, 'epoch': 0.21}\n",
      "{'loss': 1.2866, 'grad_norm': 0.34375, 'learning_rate': 0.00015888410090927427, 'epoch': 0.21}\n",
      "{'loss': 1.3625, 'grad_norm': 0.63671875, 'learning_rate': 0.0001588727385318104, 'epoch': 0.21}\n",
      "{'loss': 1.1969, 'grad_norm': 0.3984375, 'learning_rate': 0.00015886137615434654, 'epoch': 0.21}\n",
      "{'loss': 1.3629, 'grad_norm': 0.462890625, 'learning_rate': 0.0001588500137768827, 'epoch': 0.21}\n",
      "{'loss': 1.1095, 'grad_norm': 0.72265625, 'learning_rate': 0.00015883865139941882, 'epoch': 0.21}\n",
      "{'loss': 1.2634, 'grad_norm': 0.45703125, 'learning_rate': 0.00015882728902195497, 'epoch': 0.21}\n",
      "{'loss': 1.3204, 'grad_norm': 0.58984375, 'learning_rate': 0.0001588159266444911, 'epoch': 0.21}\n",
      "{'loss': 1.144, 'grad_norm': 0.55859375, 'learning_rate': 0.00015880456426702722, 'epoch': 0.21}\n",
      "{'loss': 1.3526, 'grad_norm': 0.490234375, 'learning_rate': 0.00015879320188956337, 'epoch': 0.21}\n",
      "{'loss': 1.2316, 'grad_norm': 0.59765625, 'learning_rate': 0.00015878183951209952, 'epoch': 0.21}\n",
      "{'loss': 1.193, 'grad_norm': 0.515625, 'learning_rate': 0.00015877047713463567, 'epoch': 0.21}\n",
      "{'loss': 1.2714, 'grad_norm': 0.55859375, 'learning_rate': 0.0001587591147571718, 'epoch': 0.21}\n",
      "{'loss': 1.2641, 'grad_norm': 0.87109375, 'learning_rate': 0.00015874775237970795, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2707, 'grad_norm': 0.4375, 'learning_rate': 0.00015873639000224407, 'epoch': 0.21}\n",
      "{'loss': 1.2042, 'grad_norm': 0.5234375, 'learning_rate': 0.0001587250276247802, 'epoch': 0.21}\n",
      "{'loss': 1.0958, 'grad_norm': 0.41015625, 'learning_rate': 0.00015871366524731635, 'epoch': 0.21}\n",
      "{'loss': 1.1879, 'grad_norm': 0.49609375, 'learning_rate': 0.0001587023028698525, 'epoch': 0.21}\n",
      "{'loss': 1.1016, 'grad_norm': 0.9140625, 'learning_rate': 0.00015869094049238865, 'epoch': 0.21}\n",
      "{'loss': 1.4124, 'grad_norm': 0.40234375, 'learning_rate': 0.00015867957811492478, 'epoch': 0.21}\n",
      "{'loss': 1.1343, 'grad_norm': 0.6015625, 'learning_rate': 0.0001586682157374609, 'epoch': 0.21}\n",
      "{'loss': 1.2165, 'grad_norm': 0.458984375, 'learning_rate': 0.00015865685335999705, 'epoch': 0.21}\n",
      "{'loss': 1.3242, 'grad_norm': 0.458984375, 'learning_rate': 0.00015864549098253318, 'epoch': 0.21}\n",
      "{'loss': 1.0673, 'grad_norm': 0.4609375, 'learning_rate': 0.00015863412860506936, 'epoch': 0.21}\n",
      "{'loss': 1.4079, 'grad_norm': 0.458984375, 'learning_rate': 0.00015862276622760548, 'epoch': 0.21}\n",
      "{'loss': 1.2985, 'grad_norm': 0.921875, 'learning_rate': 0.0001586114038501416, 'epoch': 0.21}\n",
      "{'loss': 1.2749, 'grad_norm': 0.486328125, 'learning_rate': 0.00015860004147267776, 'epoch': 0.21}\n",
      "{'loss': 1.2102, 'grad_norm': 0.625, 'learning_rate': 0.00015858867909521388, 'epoch': 0.21}\n",
      "{'loss': 1.0393, 'grad_norm': 1.125, 'learning_rate': 0.00015857731671775003, 'epoch': 0.21}\n",
      "{'loss': 1.282, 'grad_norm': 0.41015625, 'learning_rate': 0.00015856595434028616, 'epoch': 0.21}\n",
      "{'loss': 1.2263, 'grad_norm': 0.8828125, 'learning_rate': 0.0001585545919628223, 'epoch': 0.21}\n",
      "{'loss': 1.1756, 'grad_norm': 0.484375, 'learning_rate': 0.00015854322958535846, 'epoch': 0.21}\n",
      "{'loss': 1.2609, 'grad_norm': 0.62890625, 'learning_rate': 0.00015853186720789458, 'epoch': 0.21}\n",
      "{'loss': 1.0844, 'grad_norm': 0.7265625, 'learning_rate': 0.00015852050483043073, 'epoch': 0.21}\n",
      "{'loss': 1.3841, 'grad_norm': 0.45703125, 'learning_rate': 0.00015850914245296686, 'epoch': 0.21}\n",
      "{'loss': 1.1875, 'grad_norm': 0.76171875, 'learning_rate': 0.000158497780075503, 'epoch': 0.21}\n",
      "{'loss': 1.2312, 'grad_norm': 0.4453125, 'learning_rate': 0.00015848641769803913, 'epoch': 0.21}\n",
      "{'loss': 1.2965, 'grad_norm': 0.546875, 'learning_rate': 0.00015847505532057529, 'epoch': 0.21}\n",
      "{'loss': 1.1546, 'grad_norm': 0.921875, 'learning_rate': 0.00015846369294311144, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3952, 'grad_norm': 0.435546875, 'learning_rate': 0.00015845233056564756, 'epoch': 0.21}\n",
      "{'loss': 1.2707, 'grad_norm': 0.486328125, 'learning_rate': 0.0001584409681881837, 'epoch': 0.21}\n",
      "{'loss': 1.2569, 'grad_norm': 0.43359375, 'learning_rate': 0.00015842960581071984, 'epoch': 0.21}\n",
      "{'loss': 1.1872, 'grad_norm': 0.66796875, 'learning_rate': 0.00015841824343325596, 'epoch': 0.21}\n",
      "{'loss': 1.2332, 'grad_norm': 0.69140625, 'learning_rate': 0.0001584068810557921, 'epoch': 0.21}\n",
      "{'loss': 1.3846, 'grad_norm': 0.49609375, 'learning_rate': 0.00015839551867832826, 'epoch': 0.21}\n",
      "{'loss': 1.242, 'grad_norm': 0.57421875, 'learning_rate': 0.00015838415630086442, 'epoch': 0.21}\n",
      "{'loss': 1.1554, 'grad_norm': 0.326171875, 'learning_rate': 0.00015837279392340054, 'epoch': 0.21}\n",
      "{'loss': 1.3217, 'grad_norm': 0.5390625, 'learning_rate': 0.0001583614315459367, 'epoch': 0.21}\n",
      "{'loss': 1.1655, 'grad_norm': 0.75, 'learning_rate': 0.00015835006916847282, 'epoch': 0.21}\n",
      "{'loss': 1.3244, 'grad_norm': 0.68359375, 'learning_rate': 0.00015833870679100894, 'epoch': 0.21}\n",
      "{'loss': 1.2201, 'grad_norm': 0.66015625, 'learning_rate': 0.0001583273444135451, 'epoch': 0.21}\n",
      "{'loss': 1.1435, 'grad_norm': 0.53515625, 'learning_rate': 0.00015831598203608124, 'epoch': 0.21}\n",
      "{'loss': 1.243, 'grad_norm': 0.703125, 'learning_rate': 0.0001583046196586174, 'epoch': 0.21}\n",
      "{'loss': 1.1423, 'grad_norm': 0.86328125, 'learning_rate': 0.00015829325728115352, 'epoch': 0.21}\n",
      "{'loss': 1.3044, 'grad_norm': 0.455078125, 'learning_rate': 0.00015828189490368964, 'epoch': 0.21}\n",
      "{'loss': 1.3351, 'grad_norm': 0.6328125, 'learning_rate': 0.0001582705325262258, 'epoch': 0.21}\n",
      "{'loss': 1.1688, 'grad_norm': 0.53125, 'learning_rate': 0.00015825917014876192, 'epoch': 0.21}\n",
      "{'loss': 1.3464, 'grad_norm': 0.72265625, 'learning_rate': 0.0001582478077712981, 'epoch': 0.21}\n",
      "{'loss': 1.1168, 'grad_norm': 0.34375, 'learning_rate': 0.00015823644539383422, 'epoch': 0.21}\n",
      "{'loss': 1.3827, 'grad_norm': 0.48046875, 'learning_rate': 0.00015822508301637035, 'epoch': 0.21}\n",
      "{'loss': 1.2365, 'grad_norm': 0.71875, 'learning_rate': 0.0001582137206389065, 'epoch': 0.21}\n",
      "{'loss': 1.2843, 'grad_norm': 0.62109375, 'learning_rate': 0.00015820235826144262, 'epoch': 0.21}\n",
      "{'loss': 1.2408, 'grad_norm': 0.53515625, 'learning_rate': 0.00015819099588397877, 'epoch': 0.21}\n",
      "{'loss': 1.0881, 'grad_norm': 1.15625, 'learning_rate': 0.0001581796335065149, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3352, 'grad_norm': 0.421875, 'learning_rate': 0.00015816827112905105, 'epoch': 0.21}\n",
      "{'loss': 1.275, 'grad_norm': 0.63671875, 'learning_rate': 0.0001581569087515872, 'epoch': 0.21}\n",
      "{'loss': 1.1696, 'grad_norm': 0.390625, 'learning_rate': 0.00015814554637412332, 'epoch': 0.21}\n",
      "{'loss': 1.3, 'grad_norm': 0.7578125, 'learning_rate': 0.00015813418399665948, 'epoch': 0.21}\n",
      "{'loss': 1.1361, 'grad_norm': 0.294921875, 'learning_rate': 0.0001581228216191956, 'epoch': 0.21}\n",
      "{'loss': 1.5602, 'grad_norm': 0.75390625, 'learning_rate': 0.00015811145924173175, 'epoch': 0.21}\n",
      "{'loss': 1.2012, 'grad_norm': 0.59375, 'learning_rate': 0.00015810009686426788, 'epoch': 0.21}\n",
      "{'loss': 1.2663, 'grad_norm': 0.515625, 'learning_rate': 0.00015808873448680403, 'epoch': 0.21}\n",
      "{'loss': 1.33, 'grad_norm': 0.78515625, 'learning_rate': 0.00015807737210934018, 'epoch': 0.21}\n",
      "{'loss': 1.0322, 'grad_norm': 1.0, 'learning_rate': 0.0001580660097318763, 'epoch': 0.21}\n",
      "{'loss': 1.4295, 'grad_norm': 0.59765625, 'learning_rate': 0.00015805464735441245, 'epoch': 0.21}\n",
      "{'loss': 1.1685, 'grad_norm': 0.53515625, 'learning_rate': 0.00015804328497694858, 'epoch': 0.21}\n",
      "{'loss': 1.158, 'grad_norm': 0.3828125, 'learning_rate': 0.00015803192259948473, 'epoch': 0.21}\n",
      "{'loss': 1.2367, 'grad_norm': 0.380859375, 'learning_rate': 0.00015802056022202085, 'epoch': 0.21}\n",
      "{'loss': 1.1269, 'grad_norm': 0.890625, 'learning_rate': 0.000158009197844557, 'epoch': 0.21}\n",
      "{'loss': 1.3114, 'grad_norm': 0.5390625, 'learning_rate': 0.00015799783546709316, 'epoch': 0.21}\n",
      "{'loss': 1.1089, 'grad_norm': 0.408203125, 'learning_rate': 0.00015798647308962928, 'epoch': 0.21}\n",
      "{'loss': 1.293, 'grad_norm': 0.4453125, 'learning_rate': 0.00015797511071216543, 'epoch': 0.21}\n",
      "{'loss': 1.3327, 'grad_norm': 0.73828125, 'learning_rate': 0.00015796374833470156, 'epoch': 0.21}\n",
      "{'loss': 1.1246, 'grad_norm': 1.109375, 'learning_rate': 0.00015795238595723768, 'epoch': 0.21}\n",
      "{'loss': 1.3402, 'grad_norm': 0.478515625, 'learning_rate': 0.00015794102357977386, 'epoch': 0.21}\n",
      "{'loss': 1.2979, 'grad_norm': 0.7578125, 'learning_rate': 0.00015792966120230998, 'epoch': 0.21}\n",
      "{'loss': 1.1988, 'grad_norm': 0.41796875, 'learning_rate': 0.00015791829882484614, 'epoch': 0.21}\n",
      "{'loss': 1.2331, 'grad_norm': 0.484375, 'learning_rate': 0.00015790693644738226, 'epoch': 0.21}\n",
      "{'loss': 1.1042, 'grad_norm': 0.53125, 'learning_rate': 0.00015789557406991838, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4775, 'grad_norm': 0.58203125, 'learning_rate': 0.00015788421169245454, 'epoch': 0.21}\n",
      "{'loss': 1.194, 'grad_norm': 0.69140625, 'learning_rate': 0.00015787284931499066, 'epoch': 0.21}\n",
      "{'loss': 1.292, 'grad_norm': 0.4375, 'learning_rate': 0.00015786148693752684, 'epoch': 0.21}\n",
      "{'loss': 1.2622, 'grad_norm': 0.6015625, 'learning_rate': 0.00015785012456006296, 'epoch': 0.21}\n",
      "{'loss': 1.0854, 'grad_norm': 0.734375, 'learning_rate': 0.0001578387621825991, 'epoch': 0.21}\n",
      "{'loss': 1.3629, 'grad_norm': 0.47265625, 'learning_rate': 0.00015782739980513524, 'epoch': 0.21}\n",
      "{'loss': 1.1603, 'grad_norm': 0.5703125, 'learning_rate': 0.00015781603742767136, 'epoch': 0.21}\n",
      "{'loss': 1.1872, 'grad_norm': 0.45703125, 'learning_rate': 0.00015780467505020751, 'epoch': 0.21}\n",
      "{'loss': 1.324, 'grad_norm': 0.578125, 'learning_rate': 0.00015779331267274364, 'epoch': 0.21}\n",
      "{'loss': 1.0681, 'grad_norm': 0.578125, 'learning_rate': 0.0001577819502952798, 'epoch': 0.21}\n",
      "{'loss': 1.4501, 'grad_norm': 0.5078125, 'learning_rate': 0.00015777058791781594, 'epoch': 0.21}\n",
      "{'loss': 1.177, 'grad_norm': 0.546875, 'learning_rate': 0.00015775922554035207, 'epoch': 0.21}\n",
      "{'loss': 1.2466, 'grad_norm': 0.333984375, 'learning_rate': 0.00015774786316288822, 'epoch': 0.21}\n",
      "{'loss': 1.1934, 'grad_norm': 0.765625, 'learning_rate': 0.00015773650078542434, 'epoch': 0.21}\n",
      "{'loss': 1.0089, 'grad_norm': 0.66796875, 'learning_rate': 0.0001577251384079605, 'epoch': 0.21}\n",
      "{'loss': 1.3028, 'grad_norm': 0.38671875, 'learning_rate': 0.00015771377603049662, 'epoch': 0.21}\n",
      "{'loss': 1.1017, 'grad_norm': 0.8203125, 'learning_rate': 0.00015770241365303277, 'epoch': 0.21}\n",
      "{'loss': 1.3152, 'grad_norm': 0.357421875, 'learning_rate': 0.00015769105127556892, 'epoch': 0.21}\n",
      "{'loss': 1.3114, 'grad_norm': 0.6953125, 'learning_rate': 0.00015767968889810504, 'epoch': 0.21}\n",
      "{'loss': 1.0818, 'grad_norm': 0.447265625, 'learning_rate': 0.0001576683265206412, 'epoch': 0.21}\n",
      "{'loss': 1.3628, 'grad_norm': 0.57421875, 'learning_rate': 0.00015765696414317732, 'epoch': 0.21}\n",
      "{'loss': 1.1184, 'grad_norm': 0.515625, 'learning_rate': 0.00015764560176571347, 'epoch': 0.21}\n",
      "{'loss': 1.3177, 'grad_norm': 0.412109375, 'learning_rate': 0.0001576342393882496, 'epoch': 0.21}\n",
      "{'loss': 1.285, 'grad_norm': 0.59375, 'learning_rate': 0.00015762287701078575, 'epoch': 0.21}\n",
      "{'loss': 1.1305, 'grad_norm': 0.84765625, 'learning_rate': 0.0001576115146333219, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4124, 'grad_norm': 0.609375, 'learning_rate': 0.00015760015225585802, 'epoch': 0.21}\n",
      "{'loss': 1.2602, 'grad_norm': 0.734375, 'learning_rate': 0.00015758878987839417, 'epoch': 0.21}\n",
      "{'loss': 1.1999, 'grad_norm': 0.359375, 'learning_rate': 0.0001575774275009303, 'epoch': 0.21}\n",
      "{'loss': 1.3583, 'grad_norm': 0.6328125, 'learning_rate': 0.00015756606512346642, 'epoch': 0.21}\n",
      "{'loss': 1.1383, 'grad_norm': 0.87109375, 'learning_rate': 0.0001575547027460026, 'epoch': 0.21}\n",
      "{'loss': 1.2054, 'grad_norm': 0.5625, 'learning_rate': 0.00015754334036853873, 'epoch': 0.21}\n",
      "{'loss': 1.384, 'grad_norm': 0.66015625, 'learning_rate': 0.00015753197799107488, 'epoch': 0.21}\n",
      "{'loss': 1.1582, 'grad_norm': 0.376953125, 'learning_rate': 0.000157520615613611, 'epoch': 0.21}\n",
      "{'loss': 1.3118, 'grad_norm': 0.59765625, 'learning_rate': 0.00015750925323614713, 'epoch': 0.21}\n",
      "{'loss': 1.0903, 'grad_norm': 1.390625, 'learning_rate': 0.00015749789085868328, 'epoch': 0.21}\n",
      "{'loss': 1.4267, 'grad_norm': 0.62890625, 'learning_rate': 0.0001574865284812194, 'epoch': 0.21}\n",
      "{'loss': 1.2032, 'grad_norm': 0.59375, 'learning_rate': 0.00015747516610375558, 'epoch': 0.21}\n",
      "{'loss': 1.3052, 'grad_norm': 0.421875, 'learning_rate': 0.0001574638037262917, 'epoch': 0.21}\n",
      "{'loss': 1.3231, 'grad_norm': 0.51953125, 'learning_rate': 0.00015745244134882783, 'epoch': 0.21}\n",
      "{'loss': 1.3067, 'grad_norm': 0.9609375, 'learning_rate': 0.00015744107897136398, 'epoch': 0.21}\n",
      "{'loss': 1.4398, 'grad_norm': 0.498046875, 'learning_rate': 0.0001574297165939001, 'epoch': 0.21}\n",
      "{'loss': 1.2254, 'grad_norm': 0.8828125, 'learning_rate': 0.00015741835421643626, 'epoch': 0.21}\n",
      "{'loss': 1.2582, 'grad_norm': 0.546875, 'learning_rate': 0.00015740699183897238, 'epoch': 0.21}\n",
      "{'loss': 1.2063, 'grad_norm': 0.51171875, 'learning_rate': 0.00015739562946150853, 'epoch': 0.21}\n",
      "{'loss': 1.0212, 'grad_norm': 0.515625, 'learning_rate': 0.00015738426708404468, 'epoch': 0.21}\n",
      "{'loss': 1.3087, 'grad_norm': 0.369140625, 'learning_rate': 0.0001573729047065808, 'epoch': 0.21}\n",
      "{'loss': 1.1406, 'grad_norm': 0.52734375, 'learning_rate': 0.00015736154232911696, 'epoch': 0.21}\n",
      "{'loss': 1.3256, 'grad_norm': 0.423828125, 'learning_rate': 0.00015735017995165308, 'epoch': 0.21}\n",
      "{'loss': 1.2538, 'grad_norm': 0.53125, 'learning_rate': 0.00015733881757418923, 'epoch': 0.21}\n",
      "{'loss': 1.0185, 'grad_norm': 0.6953125, 'learning_rate': 0.00015732745519672536, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.374, 'grad_norm': 0.40625, 'learning_rate': 0.0001573160928192615, 'epoch': 0.21}\n",
      "{'loss': 1.1639, 'grad_norm': 0.58984375, 'learning_rate': 0.00015730473044179766, 'epoch': 0.21}\n",
      "{'loss': 1.3036, 'grad_norm': 0.451171875, 'learning_rate': 0.00015729336806433379, 'epoch': 0.21}\n",
      "{'loss': 1.1918, 'grad_norm': 0.71484375, 'learning_rate': 0.00015728200568686994, 'epoch': 0.21}\n",
      "{'loss': 1.2284, 'grad_norm': 0.69140625, 'learning_rate': 0.00015727064330940606, 'epoch': 0.21}\n",
      "{'loss': 1.419, 'grad_norm': 0.47265625, 'learning_rate': 0.0001572592809319422, 'epoch': 0.21}\n",
      "{'loss': 1.1051, 'grad_norm': 0.6953125, 'learning_rate': 0.00015724791855447836, 'epoch': 0.21}\n",
      "{'loss': 1.3392, 'grad_norm': 0.41015625, 'learning_rate': 0.0001572365561770145, 'epoch': 0.21}\n",
      "{'loss': 1.3285, 'grad_norm': 0.380859375, 'learning_rate': 0.00015722519379955064, 'epoch': 0.21}\n",
      "{'loss': 1.1222, 'grad_norm': 0.83203125, 'learning_rate': 0.00015721383142208676, 'epoch': 0.21}\n",
      "{'loss': 1.3407, 'grad_norm': 0.5234375, 'learning_rate': 0.00015720246904462292, 'epoch': 0.21}\n",
      "{'loss': 1.1672, 'grad_norm': 0.498046875, 'learning_rate': 0.00015719110666715904, 'epoch': 0.21}\n",
      "{'loss': 1.1384, 'grad_norm': 0.51171875, 'learning_rate': 0.00015717974428969516, 'epoch': 0.21}\n",
      "{'loss': 1.254, 'grad_norm': 0.81640625, 'learning_rate': 0.00015716838191223134, 'epoch': 0.21}\n",
      "{'loss': 1.1869, 'grad_norm': 0.86328125, 'learning_rate': 0.00015715701953476747, 'epoch': 0.21}\n",
      "{'loss': 1.4889, 'grad_norm': 0.546875, 'learning_rate': 0.00015714565715730362, 'epoch': 0.21}\n",
      "{'loss': 1.2831, 'grad_norm': 0.8671875, 'learning_rate': 0.00015713429477983974, 'epoch': 0.21}\n",
      "{'loss': 1.1586, 'grad_norm': 0.4296875, 'learning_rate': 0.00015712293240237587, 'epoch': 0.21}\n",
      "{'loss': 1.2562, 'grad_norm': 0.48828125, 'learning_rate': 0.00015711157002491202, 'epoch': 0.21}\n",
      "{'loss': 1.2555, 'grad_norm': 0.64453125, 'learning_rate': 0.00015710020764744814, 'epoch': 0.21}\n",
      "{'loss': 1.3792, 'grad_norm': 0.60546875, 'learning_rate': 0.00015708884526998432, 'epoch': 0.21}\n",
      "{'loss': 1.3084, 'grad_norm': 0.55859375, 'learning_rate': 0.00015707748289252045, 'epoch': 0.21}\n",
      "{'loss': 1.2993, 'grad_norm': 0.41015625, 'learning_rate': 0.00015706612051505657, 'epoch': 0.21}\n",
      "{'loss': 1.2994, 'grad_norm': 0.58203125, 'learning_rate': 0.00015705475813759272, 'epoch': 0.21}\n",
      "{'loss': 1.2438, 'grad_norm': 0.55859375, 'learning_rate': 0.00015704339576012885, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3902, 'grad_norm': 0.4921875, 'learning_rate': 0.000157032033382665, 'epoch': 0.21}\n",
      "{'loss': 1.1365, 'grad_norm': 0.67578125, 'learning_rate': 0.00015702067100520112, 'epoch': 0.21}\n",
      "{'loss': 1.1352, 'grad_norm': 0.466796875, 'learning_rate': 0.00015700930862773727, 'epoch': 0.21}\n",
      "{'loss': 1.3633, 'grad_norm': 0.51953125, 'learning_rate': 0.00015699794625027342, 'epoch': 0.21}\n",
      "{'loss': 1.2318, 'grad_norm': 0.85546875, 'learning_rate': 0.00015698658387280955, 'epoch': 0.21}\n",
      "{'loss': 1.4342, 'grad_norm': 0.431640625, 'learning_rate': 0.0001569752214953457, 'epoch': 0.21}\n",
      "{'loss': 1.3021, 'grad_norm': 0.625, 'learning_rate': 0.00015696385911788182, 'epoch': 0.21}\n",
      "{'loss': 1.2305, 'grad_norm': 0.49609375, 'learning_rate': 0.00015695249674041798, 'epoch': 0.21}\n",
      "{'loss': 1.3113, 'grad_norm': 0.65234375, 'learning_rate': 0.00015694113436295413, 'epoch': 0.21}\n",
      "{'loss': 1.0555, 'grad_norm': 0.46875, 'learning_rate': 0.00015692977198549025, 'epoch': 0.22}\n",
      "{'loss': 1.4437, 'grad_norm': 0.5078125, 'learning_rate': 0.0001569184096080264, 'epoch': 0.22}\n",
      "{'loss': 1.2431, 'grad_norm': 0.53125, 'learning_rate': 0.00015690704723056253, 'epoch': 0.22}\n",
      "{'loss': 1.2608, 'grad_norm': 0.62890625, 'learning_rate': 0.00015689568485309868, 'epoch': 0.22}\n",
      "{'loss': 1.3006, 'grad_norm': 0.71484375, 'learning_rate': 0.0001568843224756348, 'epoch': 0.22}\n",
      "{'loss': 1.0789, 'grad_norm': 0.625, 'learning_rate': 0.00015687296009817095, 'epoch': 0.22}\n",
      "{'loss': 1.2474, 'grad_norm': 0.455078125, 'learning_rate': 0.0001568615977207071, 'epoch': 0.22}\n",
      "{'loss': 1.2035, 'grad_norm': 0.55859375, 'learning_rate': 0.00015685023534324323, 'epoch': 0.22}\n",
      "{'loss': 1.2702, 'grad_norm': 0.46875, 'learning_rate': 0.00015683887296577938, 'epoch': 0.22}\n",
      "{'loss': 1.2695, 'grad_norm': 0.49609375, 'learning_rate': 0.0001568275105883155, 'epoch': 0.22}\n",
      "{'loss': 1.1011, 'grad_norm': 0.71875, 'learning_rate': 0.00015681614821085166, 'epoch': 0.22}\n",
      "{'loss': 1.3575, 'grad_norm': 0.546875, 'learning_rate': 0.00015680478583338778, 'epoch': 0.22}\n",
      "{'loss': 1.1738, 'grad_norm': 0.68359375, 'learning_rate': 0.0001567934234559239, 'epoch': 0.22}\n",
      "{'loss': 1.1413, 'grad_norm': 0.421875, 'learning_rate': 0.00015678206107846008, 'epoch': 0.22}\n",
      "{'loss': 1.332, 'grad_norm': 0.7890625, 'learning_rate': 0.0001567706987009962, 'epoch': 0.22}\n",
      "{'loss': 1.0793, 'grad_norm': 0.416015625, 'learning_rate': 0.00015675933632353236, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.317, 'grad_norm': 0.462890625, 'learning_rate': 0.00015674797394606848, 'epoch': 0.22}\n",
      "{'loss': 1.2166, 'grad_norm': 0.609375, 'learning_rate': 0.0001567366115686046, 'epoch': 0.22}\n",
      "{'loss': 1.1959, 'grad_norm': 0.4609375, 'learning_rate': 0.00015672524919114076, 'epoch': 0.22}\n",
      "{'loss': 1.2424, 'grad_norm': 0.51171875, 'learning_rate': 0.00015671388681367688, 'epoch': 0.22}\n",
      "{'loss': 1.0858, 'grad_norm': 0.67578125, 'learning_rate': 0.00015670252443621306, 'epoch': 0.22}\n",
      "{'loss': 1.5275, 'grad_norm': 0.490234375, 'learning_rate': 0.00015669116205874919, 'epoch': 0.22}\n",
      "{'loss': 1.2169, 'grad_norm': 0.640625, 'learning_rate': 0.0001566797996812853, 'epoch': 0.22}\n",
      "{'loss': 1.2103, 'grad_norm': 0.4140625, 'learning_rate': 0.00015666843730382146, 'epoch': 0.22}\n",
      "{'loss': 1.2293, 'grad_norm': 0.70703125, 'learning_rate': 0.0001566570749263576, 'epoch': 0.22}\n",
      "{'loss': 1.0434, 'grad_norm': 0.9375, 'learning_rate': 0.00015664571254889374, 'epoch': 0.22}\n",
      "{'loss': 1.2669, 'grad_norm': 0.49609375, 'learning_rate': 0.00015663435017142986, 'epoch': 0.22}\n",
      "{'loss': 1.2106, 'grad_norm': 0.7890625, 'learning_rate': 0.00015662298779396601, 'epoch': 0.22}\n",
      "{'loss': 1.3374, 'grad_norm': 0.44921875, 'learning_rate': 0.00015661162541650216, 'epoch': 0.22}\n",
      "{'loss': 1.2902, 'grad_norm': 0.482421875, 'learning_rate': 0.0001566002630390383, 'epoch': 0.22}\n",
      "{'loss': 1.1713, 'grad_norm': 0.73046875, 'learning_rate': 0.00015658890066157444, 'epoch': 0.22}\n",
      "{'loss': 1.3435, 'grad_norm': 0.6640625, 'learning_rate': 0.00015657753828411056, 'epoch': 0.22}\n",
      "{'loss': 1.1349, 'grad_norm': 0.51953125, 'learning_rate': 0.00015656617590664672, 'epoch': 0.22}\n",
      "{'loss': 1.1958, 'grad_norm': 0.5078125, 'learning_rate': 0.00015655481352918287, 'epoch': 0.22}\n",
      "{'loss': 1.2158, 'grad_norm': 0.55859375, 'learning_rate': 0.000156543451151719, 'epoch': 0.22}\n",
      "{'loss': 1.2105, 'grad_norm': 0.95703125, 'learning_rate': 0.00015653208877425514, 'epoch': 0.22}\n",
      "{'loss': 1.4389, 'grad_norm': 0.49609375, 'learning_rate': 0.00015652072639679127, 'epoch': 0.22}\n",
      "{'loss': 1.1352, 'grad_norm': 0.57421875, 'learning_rate': 0.00015650936401932742, 'epoch': 0.22}\n",
      "{'loss': 1.238, 'grad_norm': 0.375, 'learning_rate': 0.00015649800164186354, 'epoch': 0.22}\n",
      "{'loss': 1.2377, 'grad_norm': 0.58203125, 'learning_rate': 0.0001564866392643997, 'epoch': 0.22}\n",
      "{'loss': 1.218, 'grad_norm': 1.03125, 'learning_rate': 0.00015647527688693585, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3875, 'grad_norm': 0.4453125, 'learning_rate': 0.00015646391450947197, 'epoch': 0.22}\n",
      "{'loss': 1.1597, 'grad_norm': 0.73046875, 'learning_rate': 0.00015645255213200812, 'epoch': 0.22}\n",
      "{'loss': 1.1549, 'grad_norm': 0.4609375, 'learning_rate': 0.00015644118975454425, 'epoch': 0.22}\n",
      "{'loss': 1.359, 'grad_norm': 0.52734375, 'learning_rate': 0.0001564298273770804, 'epoch': 0.22}\n",
      "{'loss': 1.1381, 'grad_norm': 1.09375, 'learning_rate': 0.00015641846499961652, 'epoch': 0.22}\n",
      "{'loss': 1.4207, 'grad_norm': 0.7578125, 'learning_rate': 0.00015640710262215265, 'epoch': 0.22}\n",
      "{'loss': 1.2037, 'grad_norm': 0.65234375, 'learning_rate': 0.00015639574024468882, 'epoch': 0.22}\n",
      "{'loss': 1.2048, 'grad_norm': 0.4609375, 'learning_rate': 0.00015638437786722495, 'epoch': 0.22}\n",
      "{'loss': 1.2311, 'grad_norm': 0.49609375, 'learning_rate': 0.0001563730154897611, 'epoch': 0.22}\n",
      "{'loss': 1.1147, 'grad_norm': 0.78125, 'learning_rate': 0.00015636165311229722, 'epoch': 0.22}\n",
      "{'loss': 1.315, 'grad_norm': 0.470703125, 'learning_rate': 0.00015635029073483335, 'epoch': 0.22}\n",
      "{'loss': 1.1464, 'grad_norm': 0.57421875, 'learning_rate': 0.0001563389283573695, 'epoch': 0.22}\n",
      "{'loss': 1.1887, 'grad_norm': 0.453125, 'learning_rate': 0.00015632756597990562, 'epoch': 0.22}\n",
      "{'loss': 1.4348, 'grad_norm': 0.73828125, 'learning_rate': 0.0001563162036024418, 'epoch': 0.22}\n",
      "{'loss': 1.11, 'grad_norm': 0.48046875, 'learning_rate': 0.00015630484122497793, 'epoch': 0.22}\n",
      "{'loss': 1.2536, 'grad_norm': 0.3359375, 'learning_rate': 0.00015629347884751405, 'epoch': 0.22}\n",
      "{'loss': 1.27, 'grad_norm': 0.63671875, 'learning_rate': 0.0001562821164700502, 'epoch': 0.22}\n",
      "{'loss': 1.1664, 'grad_norm': 0.41796875, 'learning_rate': 0.00015627075409258633, 'epoch': 0.22}\n",
      "{'loss': 1.192, 'grad_norm': 0.61328125, 'learning_rate': 0.00015625939171512248, 'epoch': 0.22}\n",
      "{'loss': 0.996, 'grad_norm': 0.66015625, 'learning_rate': 0.00015624802933765863, 'epoch': 0.22}\n",
      "{'loss': 1.3114, 'grad_norm': 0.4375, 'learning_rate': 0.00015623666696019475, 'epoch': 0.22}\n",
      "{'loss': 1.0736, 'grad_norm': 1.3828125, 'learning_rate': 0.0001562253045827309, 'epoch': 0.22}\n",
      "{'loss': 1.2335, 'grad_norm': 0.404296875, 'learning_rate': 0.00015621394220526703, 'epoch': 0.22}\n",
      "{'loss': 1.2216, 'grad_norm': 0.4140625, 'learning_rate': 0.00015620257982780318, 'epoch': 0.22}\n",
      "{'loss': 1.2123, 'grad_norm': 0.609375, 'learning_rate': 0.0001561912174503393, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3764, 'grad_norm': 0.478515625, 'learning_rate': 0.00015617985507287546, 'epoch': 0.22}\n",
      "{'loss': 1.2295, 'grad_norm': 0.62890625, 'learning_rate': 0.0001561684926954116, 'epoch': 0.22}\n",
      "{'loss': 1.3822, 'grad_norm': 0.52734375, 'learning_rate': 0.00015615713031794773, 'epoch': 0.22}\n",
      "{'loss': 1.3576, 'grad_norm': 0.50390625, 'learning_rate': 0.00015614576794048388, 'epoch': 0.22}\n",
      "{'loss': 1.0385, 'grad_norm': 0.546875, 'learning_rate': 0.00015613440556302, 'epoch': 0.22}\n",
      "{'loss': 1.3104, 'grad_norm': 0.419921875, 'learning_rate': 0.00015612304318555616, 'epoch': 0.22}\n",
      "{'loss': 1.2213, 'grad_norm': 0.46484375, 'learning_rate': 0.00015611168080809228, 'epoch': 0.22}\n",
      "{'loss': 1.1753, 'grad_norm': 0.490234375, 'learning_rate': 0.00015610031843062844, 'epoch': 0.22}\n",
      "{'loss': 1.3196, 'grad_norm': 0.55859375, 'learning_rate': 0.0001560889560531646, 'epoch': 0.22}\n",
      "{'loss': 1.1096, 'grad_norm': 0.75, 'learning_rate': 0.0001560775936757007, 'epoch': 0.22}\n",
      "{'loss': 1.5076, 'grad_norm': 0.5625, 'learning_rate': 0.00015606623129823686, 'epoch': 0.22}\n",
      "{'loss': 1.1909, 'grad_norm': 0.56640625, 'learning_rate': 0.000156054868920773, 'epoch': 0.22}\n",
      "{'loss': 1.267, 'grad_norm': 0.435546875, 'learning_rate': 0.00015604350654330914, 'epoch': 0.22}\n",
      "{'loss': 1.3272, 'grad_norm': 0.6015625, 'learning_rate': 0.00015603214416584526, 'epoch': 0.22}\n",
      "{'loss': 1.0831, 'grad_norm': 0.72265625, 'learning_rate': 0.0001560207817883814, 'epoch': 0.22}\n",
      "{'loss': 1.3516, 'grad_norm': 0.5703125, 'learning_rate': 0.00015600941941091757, 'epoch': 0.22}\n",
      "{'loss': 1.0636, 'grad_norm': 0.7265625, 'learning_rate': 0.0001559980570334537, 'epoch': 0.22}\n",
      "{'loss': 1.3617, 'grad_norm': 0.361328125, 'learning_rate': 0.00015598669465598984, 'epoch': 0.22}\n",
      "{'loss': 1.3054, 'grad_norm': 0.515625, 'learning_rate': 0.00015597533227852597, 'epoch': 0.22}\n",
      "{'loss': 1.1327, 'grad_norm': 1.2421875, 'learning_rate': 0.0001559639699010621, 'epoch': 0.22}\n",
      "{'loss': 1.4534, 'grad_norm': 0.4375, 'learning_rate': 0.00015595260752359824, 'epoch': 0.22}\n",
      "{'loss': 1.1721, 'grad_norm': 0.54296875, 'learning_rate': 0.00015594124514613437, 'epoch': 0.22}\n",
      "{'loss': 1.2877, 'grad_norm': 0.50390625, 'learning_rate': 0.00015592988276867054, 'epoch': 0.22}\n",
      "{'loss': 1.3245, 'grad_norm': 0.56640625, 'learning_rate': 0.00015591852039120667, 'epoch': 0.22}\n",
      "{'loss': 1.1123, 'grad_norm': 0.8203125, 'learning_rate': 0.0001559071580137428, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3063, 'grad_norm': 0.57421875, 'learning_rate': 0.00015589579563627894, 'epoch': 0.22}\n",
      "{'loss': 1.3032, 'grad_norm': 0.7890625, 'learning_rate': 0.00015588443325881507, 'epoch': 0.22}\n",
      "{'loss': 1.259, 'grad_norm': 0.390625, 'learning_rate': 0.00015587307088135122, 'epoch': 0.22}\n",
      "{'loss': 1.2903, 'grad_norm': 0.76171875, 'learning_rate': 0.00015586170850388737, 'epoch': 0.22}\n",
      "{'loss': 1.1698, 'grad_norm': 1.1328125, 'learning_rate': 0.0001558503461264235, 'epoch': 0.22}\n",
      "{'loss': 1.4593, 'grad_norm': 0.52734375, 'learning_rate': 0.00015583898374895965, 'epoch': 0.22}\n",
      "{'loss': 1.2702, 'grad_norm': 0.71875, 'learning_rate': 0.00015582762137149577, 'epoch': 0.22}\n",
      "{'loss': 1.4536, 'grad_norm': 0.65625, 'learning_rate': 0.00015581625899403192, 'epoch': 0.22}\n",
      "{'loss': 1.2752, 'grad_norm': 0.53515625, 'learning_rate': 0.00015580489661656805, 'epoch': 0.22}\n",
      "{'loss': 1.0472, 'grad_norm': 0.56640625, 'learning_rate': 0.0001557935342391042, 'epoch': 0.22}\n",
      "{'loss': 1.3736, 'grad_norm': 0.43359375, 'learning_rate': 0.00015578217186164035, 'epoch': 0.22}\n",
      "{'loss': 1.3153, 'grad_norm': 0.60546875, 'learning_rate': 0.00015577080948417647, 'epoch': 0.22}\n",
      "{'loss': 1.3795, 'grad_norm': 0.421875, 'learning_rate': 0.00015575944710671263, 'epoch': 0.22}\n",
      "{'loss': 1.3075, 'grad_norm': 0.48828125, 'learning_rate': 0.00015574808472924875, 'epoch': 0.22}\n",
      "{'loss': 1.0666, 'grad_norm': 0.37109375, 'learning_rate': 0.0001557367223517849, 'epoch': 0.22}\n",
      "{'loss': 1.2435, 'grad_norm': 0.62890625, 'learning_rate': 0.00015572535997432103, 'epoch': 0.22}\n",
      "{'loss': 1.0984, 'grad_norm': 0.88671875, 'learning_rate': 0.00015571399759685718, 'epoch': 0.22}\n",
      "{'loss': 1.3026, 'grad_norm': 0.5, 'learning_rate': 0.00015570263521939333, 'epoch': 0.22}\n",
      "{'loss': 1.2755, 'grad_norm': 0.404296875, 'learning_rate': 0.00015569127284192945, 'epoch': 0.22}\n",
      "{'loss': 1.0735, 'grad_norm': 0.6875, 'learning_rate': 0.0001556799104644656, 'epoch': 0.22}\n",
      "{'loss': 1.3285, 'grad_norm': 0.42578125, 'learning_rate': 0.00015566854808700173, 'epoch': 0.22}\n",
      "{'loss': 1.2946, 'grad_norm': 1.0, 'learning_rate': 0.00015565718570953788, 'epoch': 0.22}\n",
      "{'loss': 1.1816, 'grad_norm': 0.451171875, 'learning_rate': 0.000155645823332074, 'epoch': 0.22}\n",
      "{'loss': 1.2468, 'grad_norm': 0.66015625, 'learning_rate': 0.00015563446095461013, 'epoch': 0.22}\n",
      "{'loss': 1.1566, 'grad_norm': 0.8046875, 'learning_rate': 0.0001556230985771463, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3398, 'grad_norm': 0.43359375, 'learning_rate': 0.00015561173619968243, 'epoch': 0.22}\n",
      "{'loss': 1.2401, 'grad_norm': 0.80859375, 'learning_rate': 0.00015560037382221858, 'epoch': 0.22}\n",
      "{'loss': 1.2168, 'grad_norm': 0.423828125, 'learning_rate': 0.0001555890114447547, 'epoch': 0.22}\n",
      "{'loss': 1.3299, 'grad_norm': 0.53515625, 'learning_rate': 0.00015557764906729083, 'epoch': 0.22}\n",
      "{'loss': 1.0693, 'grad_norm': 1.5, 'learning_rate': 0.00015556628668982698, 'epoch': 0.22}\n",
      "{'loss': 1.3079, 'grad_norm': 0.5546875, 'learning_rate': 0.00015555492431236313, 'epoch': 0.22}\n",
      "{'loss': 1.2274, 'grad_norm': 0.6875, 'learning_rate': 0.00015554356193489929, 'epoch': 0.22}\n",
      "{'loss': 1.2539, 'grad_norm': 0.37109375, 'learning_rate': 0.0001555321995574354, 'epoch': 0.22}\n",
      "{'loss': 1.2072, 'grad_norm': 0.57421875, 'learning_rate': 0.00015552083717997153, 'epoch': 0.22}\n",
      "{'loss': 1.1114, 'grad_norm': 0.82421875, 'learning_rate': 0.00015550947480250769, 'epoch': 0.22}\n",
      "{'loss': 1.2716, 'grad_norm': 0.625, 'learning_rate': 0.0001554981124250438, 'epoch': 0.22}\n",
      "{'loss': 1.2482, 'grad_norm': 0.73046875, 'learning_rate': 0.00015548675004757996, 'epoch': 0.22}\n",
      "{'loss': 1.2392, 'grad_norm': 0.44140625, 'learning_rate': 0.0001554753876701161, 'epoch': 0.22}\n",
      "{'loss': 1.2527, 'grad_norm': 0.625, 'learning_rate': 0.00015546402529265224, 'epoch': 0.22}\n",
      "{'loss': 1.1631, 'grad_norm': 0.625, 'learning_rate': 0.0001554526629151884, 'epoch': 0.22}\n",
      "{'loss': 1.2449, 'grad_norm': 0.46875, 'learning_rate': 0.0001554413005377245, 'epoch': 0.22}\n",
      "{'loss': 1.1923, 'grad_norm': 0.74609375, 'learning_rate': 0.00015542993816026066, 'epoch': 0.22}\n",
      "{'loss': 1.321, 'grad_norm': 0.376953125, 'learning_rate': 0.0001554185757827968, 'epoch': 0.22}\n",
      "{'loss': 1.2746, 'grad_norm': 0.66015625, 'learning_rate': 0.00015540721340533294, 'epoch': 0.22}\n",
      "{'loss': 1.1125, 'grad_norm': 0.84765625, 'learning_rate': 0.0001553958510278691, 'epoch': 0.22}\n",
      "{'loss': 1.4672, 'grad_norm': 0.5078125, 'learning_rate': 0.00015538448865040522, 'epoch': 0.22}\n",
      "{'loss': 1.166, 'grad_norm': 0.6015625, 'learning_rate': 0.00015537312627294137, 'epoch': 0.22}\n",
      "{'loss': 1.2889, 'grad_norm': 0.43359375, 'learning_rate': 0.0001553617638954775, 'epoch': 0.22}\n",
      "{'loss': 1.2141, 'grad_norm': 0.51171875, 'learning_rate': 0.00015535040151801364, 'epoch': 0.22}\n",
      "{'loss': 1.1105, 'grad_norm': 1.1796875, 'learning_rate': 0.00015533903914054977, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5333, 'grad_norm': 0.453125, 'learning_rate': 0.00015532767676308592, 'epoch': 0.22}\n",
      "{'loss': 1.1953, 'grad_norm': 0.80078125, 'learning_rate': 0.00015531631438562207, 'epoch': 0.22}\n",
      "{'loss': 1.1985, 'grad_norm': 0.46875, 'learning_rate': 0.0001553049520081582, 'epoch': 0.22}\n",
      "{'loss': 1.2899, 'grad_norm': 0.71875, 'learning_rate': 0.00015529358963069435, 'epoch': 0.22}\n",
      "{'loss': 1.1523, 'grad_norm': 1.15625, 'learning_rate': 0.00015528222725323047, 'epoch': 0.22}\n",
      "{'loss': 1.3227, 'grad_norm': 0.455078125, 'learning_rate': 0.00015527086487576662, 'epoch': 0.22}\n",
      "{'loss': 1.2808, 'grad_norm': 0.75, 'learning_rate': 0.00015525950249830275, 'epoch': 0.22}\n",
      "{'loss': 1.2674, 'grad_norm': 0.455078125, 'learning_rate': 0.00015524814012083887, 'epoch': 0.22}\n",
      "{'loss': 1.2439, 'grad_norm': 0.47265625, 'learning_rate': 0.00015523677774337505, 'epoch': 0.22}\n",
      "{'loss': 1.1094, 'grad_norm': 1.234375, 'learning_rate': 0.00015522541536591117, 'epoch': 0.22}\n",
      "{'loss': 1.4438, 'grad_norm': 0.5078125, 'learning_rate': 0.00015521405298844732, 'epoch': 0.22}\n",
      "{'loss': 1.1809, 'grad_norm': 0.68359375, 'learning_rate': 0.00015520269061098345, 'epoch': 0.22}\n",
      "{'loss': 1.1446, 'grad_norm': 0.4296875, 'learning_rate': 0.00015519132823351957, 'epoch': 0.22}\n",
      "{'loss': 1.1744, 'grad_norm': 0.447265625, 'learning_rate': 0.00015517996585605572, 'epoch': 0.22}\n",
      "{'loss': 1.1198, 'grad_norm': 0.8671875, 'learning_rate': 0.00015516860347859188, 'epoch': 0.22}\n",
      "{'loss': 1.388, 'grad_norm': 0.423828125, 'learning_rate': 0.00015515724110112803, 'epoch': 0.22}\n",
      "{'loss': 1.1687, 'grad_norm': 0.60546875, 'learning_rate': 0.00015514587872366415, 'epoch': 0.22}\n",
      "{'loss': 1.3423, 'grad_norm': 0.63671875, 'learning_rate': 0.00015513451634620028, 'epoch': 0.22}\n",
      "{'loss': 1.2088, 'grad_norm': 0.447265625, 'learning_rate': 0.00015512315396873643, 'epoch': 0.22}\n",
      "{'loss': 1.0608, 'grad_norm': 1.0, 'learning_rate': 0.00015511179159127255, 'epoch': 0.22}\n",
      "{'loss': 1.3988, 'grad_norm': 0.458984375, 'learning_rate': 0.0001551004292138087, 'epoch': 0.22}\n",
      "{'loss': 1.2513, 'grad_norm': 0.57421875, 'learning_rate': 0.00015508906683634485, 'epoch': 0.22}\n",
      "{'loss': 1.2076, 'grad_norm': 0.4921875, 'learning_rate': 0.00015507770445888098, 'epoch': 0.22}\n",
      "{'loss': 1.3375, 'grad_norm': 0.58984375, 'learning_rate': 0.00015506634208141713, 'epoch': 0.22}\n",
      "{'loss': 1.1612, 'grad_norm': 0.5078125, 'learning_rate': 0.00015505497970395325, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.331, 'grad_norm': 0.494140625, 'learning_rate': 0.0001550436173264894, 'epoch': 0.22}\n",
      "{'loss': 1.0456, 'grad_norm': 0.84765625, 'learning_rate': 0.00015503225494902553, 'epoch': 0.22}\n",
      "{'loss': 1.2912, 'grad_norm': 0.443359375, 'learning_rate': 0.00015502089257156168, 'epoch': 0.22}\n",
      "{'loss': 1.3235, 'grad_norm': 0.482421875, 'learning_rate': 0.00015500953019409783, 'epoch': 0.22}\n",
      "{'loss': 1.1574, 'grad_norm': 0.57421875, 'learning_rate': 0.00015499816781663396, 'epoch': 0.22}\n",
      "{'loss': 1.448, 'grad_norm': 0.5703125, 'learning_rate': 0.0001549868054391701, 'epoch': 0.22}\n",
      "{'loss': 1.2937, 'grad_norm': 0.62109375, 'learning_rate': 0.00015497544306170623, 'epoch': 0.22}\n",
      "{'loss': 1.2709, 'grad_norm': 0.435546875, 'learning_rate': 0.00015496408068424238, 'epoch': 0.22}\n",
      "{'loss': 1.3562, 'grad_norm': 0.5625, 'learning_rate': 0.0001549527183067785, 'epoch': 0.22}\n",
      "{'loss': 1.1411, 'grad_norm': 0.73828125, 'learning_rate': 0.00015494135592931466, 'epoch': 0.22}\n",
      "{'loss': 1.3266, 'grad_norm': 0.458984375, 'learning_rate': 0.0001549299935518508, 'epoch': 0.22}\n",
      "{'loss': 1.2121, 'grad_norm': 0.53515625, 'learning_rate': 0.00015491863117438694, 'epoch': 0.22}\n",
      "{'loss': 1.2646, 'grad_norm': 0.408203125, 'learning_rate': 0.0001549072687969231, 'epoch': 0.22}\n",
      "{'loss': 1.1665, 'grad_norm': 0.466796875, 'learning_rate': 0.0001548959064194592, 'epoch': 0.22}\n",
      "{'loss': 1.1447, 'grad_norm': 0.671875, 'learning_rate': 0.00015488454404199536, 'epoch': 0.22}\n",
      "{'loss': 1.3694, 'grad_norm': 0.439453125, 'learning_rate': 0.0001548731816645315, 'epoch': 0.22}\n",
      "{'loss': 1.2536, 'grad_norm': 0.443359375, 'learning_rate': 0.00015486181928706764, 'epoch': 0.22}\n",
      "{'loss': 1.2528, 'grad_norm': 0.46875, 'learning_rate': 0.0001548504569096038, 'epoch': 0.22}\n",
      "{'loss': 1.2753, 'grad_norm': 0.63671875, 'learning_rate': 0.00015483909453213991, 'epoch': 0.22}\n",
      "{'loss': 1.2214, 'grad_norm': 0.8046875, 'learning_rate': 0.00015482773215467607, 'epoch': 0.22}\n",
      "{'loss': 1.409, 'grad_norm': 0.5, 'learning_rate': 0.0001548163697772122, 'epoch': 0.22}\n",
      "{'loss': 1.1197, 'grad_norm': 0.515625, 'learning_rate': 0.00015480500739974831, 'epoch': 0.22}\n",
      "{'loss': 1.2615, 'grad_norm': 0.6484375, 'learning_rate': 0.00015479364502228447, 'epoch': 0.22}\n",
      "{'loss': 1.233, 'grad_norm': 0.53125, 'learning_rate': 0.00015478228264482062, 'epoch': 0.22}\n",
      "{'loss': 1.1746, 'grad_norm': 0.69140625, 'learning_rate': 0.00015477092026735677, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3796, 'grad_norm': 0.4375, 'learning_rate': 0.0001547595578898929, 'epoch': 0.22}\n",
      "{'loss': 1.2794, 'grad_norm': 0.71875, 'learning_rate': 0.00015474819551242902, 'epoch': 0.22}\n",
      "{'loss': 1.2164, 'grad_norm': 0.578125, 'learning_rate': 0.00015473683313496517, 'epoch': 0.22}\n",
      "{'loss': 1.2527, 'grad_norm': 0.8125, 'learning_rate': 0.0001547254707575013, 'epoch': 0.22}\n",
      "{'loss': 1.122, 'grad_norm': 0.91796875, 'learning_rate': 0.00015471410838003744, 'epoch': 0.22}\n",
      "{'loss': 1.5108, 'grad_norm': 0.52734375, 'learning_rate': 0.0001547027460025736, 'epoch': 0.22}\n",
      "{'loss': 1.3069, 'grad_norm': 0.57421875, 'learning_rate': 0.00015469138362510972, 'epoch': 0.22}\n",
      "{'loss': 1.2721, 'grad_norm': 0.52734375, 'learning_rate': 0.00015468002124764587, 'epoch': 0.22}\n",
      "{'loss': 1.1592, 'grad_norm': 0.60546875, 'learning_rate': 0.000154668658870182, 'epoch': 0.22}\n",
      "{'loss': 1.1433, 'grad_norm': 0.76171875, 'learning_rate': 0.00015465729649271815, 'epoch': 0.22}\n",
      "{'loss': 1.3011, 'grad_norm': 0.4140625, 'learning_rate': 0.00015464593411525427, 'epoch': 0.22}\n",
      "{'loss': 1.1642, 'grad_norm': 0.609375, 'learning_rate': 0.00015463457173779042, 'epoch': 0.23}\n",
      "{'loss': 1.2617, 'grad_norm': 0.5625, 'learning_rate': 0.00015462320936032657, 'epoch': 0.23}\n",
      "{'loss': 1.2734, 'grad_norm': 0.5, 'learning_rate': 0.0001546118469828627, 'epoch': 0.23}\n",
      "{'loss': 1.1384, 'grad_norm': 0.484375, 'learning_rate': 0.00015460048460539885, 'epoch': 0.23}\n",
      "{'loss': 1.3105, 'grad_norm': 0.515625, 'learning_rate': 0.00015458912222793497, 'epoch': 0.23}\n",
      "{'loss': 1.1497, 'grad_norm': 0.6953125, 'learning_rate': 0.00015457775985047113, 'epoch': 0.23}\n",
      "{'loss': 1.1492, 'grad_norm': 0.58984375, 'learning_rate': 0.00015456639747300725, 'epoch': 0.23}\n",
      "{'loss': 1.2956, 'grad_norm': 0.56640625, 'learning_rate': 0.0001545550350955434, 'epoch': 0.23}\n",
      "{'loss': 1.1685, 'grad_norm': 0.84375, 'learning_rate': 0.00015454367271807955, 'epoch': 0.23}\n",
      "{'loss': 1.3884, 'grad_norm': 0.478515625, 'learning_rate': 0.00015453231034061568, 'epoch': 0.23}\n",
      "{'loss': 1.3272, 'grad_norm': 0.515625, 'learning_rate': 0.00015452094796315183, 'epoch': 0.23}\n",
      "{'loss': 1.2766, 'grad_norm': 0.359375, 'learning_rate': 0.00015450958558568795, 'epoch': 0.23}\n",
      "{'loss': 1.1831, 'grad_norm': 0.55078125, 'learning_rate': 0.0001544982232082241, 'epoch': 0.23}\n",
      "{'loss': 1.1765, 'grad_norm': 0.8046875, 'learning_rate': 0.00015448686083076023, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3266, 'grad_norm': 0.478515625, 'learning_rate': 0.00015447549845329638, 'epoch': 0.23}\n",
      "{'loss': 1.0799, 'grad_norm': 0.6328125, 'learning_rate': 0.00015446413607583253, 'epoch': 0.23}\n",
      "{'loss': 1.2087, 'grad_norm': 0.38671875, 'learning_rate': 0.00015445277369836866, 'epoch': 0.23}\n",
      "{'loss': 1.3064, 'grad_norm': 0.48828125, 'learning_rate': 0.0001544414113209048, 'epoch': 0.23}\n",
      "{'loss': 1.0885, 'grad_norm': 0.609375, 'learning_rate': 0.00015443004894344093, 'epoch': 0.23}\n",
      "{'loss': 1.3152, 'grad_norm': 0.54296875, 'learning_rate': 0.00015441868656597706, 'epoch': 0.23}\n",
      "{'loss': 1.2492, 'grad_norm': 0.6640625, 'learning_rate': 0.0001544073241885132, 'epoch': 0.23}\n",
      "{'loss': 1.1121, 'grad_norm': 0.341796875, 'learning_rate': 0.00015439596181104936, 'epoch': 0.23}\n",
      "{'loss': 1.0956, 'grad_norm': 0.578125, 'learning_rate': 0.0001543845994335855, 'epoch': 0.23}\n",
      "{'loss': 1.189, 'grad_norm': 1.0625, 'learning_rate': 0.00015437323705612163, 'epoch': 0.23}\n",
      "{'loss': 1.31, 'grad_norm': 0.5234375, 'learning_rate': 0.00015436187467865776, 'epoch': 0.23}\n",
      "{'loss': 1.286, 'grad_norm': 0.8046875, 'learning_rate': 0.0001543505123011939, 'epoch': 0.23}\n",
      "{'loss': 1.3346, 'grad_norm': 0.48828125, 'learning_rate': 0.00015433914992373003, 'epoch': 0.23}\n",
      "{'loss': 1.3886, 'grad_norm': 0.55078125, 'learning_rate': 0.00015432778754626619, 'epoch': 0.23}\n",
      "{'loss': 0.9657, 'grad_norm': 0.65234375, 'learning_rate': 0.00015431642516880234, 'epoch': 0.23}\n",
      "{'loss': 1.4738, 'grad_norm': 0.427734375, 'learning_rate': 0.00015430506279133846, 'epoch': 0.23}\n",
      "{'loss': 1.2163, 'grad_norm': 0.58984375, 'learning_rate': 0.0001542937004138746, 'epoch': 0.23}\n",
      "{'loss': 1.0933, 'grad_norm': 0.373046875, 'learning_rate': 0.00015428233803641074, 'epoch': 0.23}\n",
      "{'loss': 1.1802, 'grad_norm': 0.44140625, 'learning_rate': 0.0001542709756589469, 'epoch': 0.23}\n",
      "{'loss': 1.2914, 'grad_norm': 0.8671875, 'learning_rate': 0.000154259613281483, 'epoch': 0.23}\n",
      "{'loss': 1.247, 'grad_norm': 0.7421875, 'learning_rate': 0.00015424825090401916, 'epoch': 0.23}\n",
      "{'loss': 1.209, 'grad_norm': 0.55859375, 'learning_rate': 0.00015423688852655532, 'epoch': 0.23}\n",
      "{'loss': 1.3529, 'grad_norm': 0.380859375, 'learning_rate': 0.00015422552614909144, 'epoch': 0.23}\n",
      "{'loss': 1.3355, 'grad_norm': 0.5390625, 'learning_rate': 0.0001542141637716276, 'epoch': 0.23}\n",
      "{'loss': 1.1294, 'grad_norm': 0.71875, 'learning_rate': 0.00015420280139416372, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.429, 'grad_norm': 0.49609375, 'learning_rate': 0.00015419143901669987, 'epoch': 0.23}\n",
      "{'loss': 1.2662, 'grad_norm': 0.6484375, 'learning_rate': 0.000154180076639236, 'epoch': 0.23}\n",
      "{'loss': 1.1891, 'grad_norm': 0.431640625, 'learning_rate': 0.00015416871426177214, 'epoch': 0.23}\n",
      "{'loss': 1.2541, 'grad_norm': 0.546875, 'learning_rate': 0.0001541573518843083, 'epoch': 0.23}\n",
      "{'loss': 1.2352, 'grad_norm': 0.65625, 'learning_rate': 0.00015414598950684442, 'epoch': 0.23}\n",
      "{'loss': 1.4191, 'grad_norm': 0.5, 'learning_rate': 0.00015413462712938057, 'epoch': 0.23}\n",
      "{'loss': 1.2035, 'grad_norm': 0.5859375, 'learning_rate': 0.0001541232647519167, 'epoch': 0.23}\n",
      "{'loss': 1.3266, 'grad_norm': 0.375, 'learning_rate': 0.00015411190237445285, 'epoch': 0.23}\n",
      "{'loss': 1.3062, 'grad_norm': 0.7734375, 'learning_rate': 0.00015410053999698897, 'epoch': 0.23}\n",
      "{'loss': 1.0941, 'grad_norm': 0.63671875, 'learning_rate': 0.00015408917761952512, 'epoch': 0.23}\n",
      "{'loss': 1.5387, 'grad_norm': 0.63671875, 'learning_rate': 0.00015407781524206127, 'epoch': 0.23}\n",
      "{'loss': 1.2158, 'grad_norm': 0.55859375, 'learning_rate': 0.0001540664528645974, 'epoch': 0.23}\n",
      "{'loss': 1.2126, 'grad_norm': 0.41015625, 'learning_rate': 0.00015405509048713355, 'epoch': 0.23}\n",
      "{'loss': 1.2543, 'grad_norm': 0.53125, 'learning_rate': 0.00015404372810966967, 'epoch': 0.23}\n",
      "{'loss': 1.0601, 'grad_norm': 0.470703125, 'learning_rate': 0.0001540323657322058, 'epoch': 0.23}\n",
      "{'loss': 1.4003, 'grad_norm': 0.5078125, 'learning_rate': 0.00015402100335474195, 'epoch': 0.23}\n",
      "{'loss': 1.354, 'grad_norm': 0.53515625, 'learning_rate': 0.0001540096409772781, 'epoch': 0.23}\n",
      "{'loss': 1.2377, 'grad_norm': 0.458984375, 'learning_rate': 0.00015399827859981425, 'epoch': 0.23}\n",
      "{'loss': 1.3301, 'grad_norm': 0.578125, 'learning_rate': 0.00015398691622235038, 'epoch': 0.23}\n",
      "{'loss': 1.0983, 'grad_norm': 0.8203125, 'learning_rate': 0.0001539755538448865, 'epoch': 0.23}\n",
      "{'loss': 1.2487, 'grad_norm': 0.435546875, 'learning_rate': 0.00015396419146742265, 'epoch': 0.23}\n",
      "{'loss': 1.1845, 'grad_norm': 0.64453125, 'learning_rate': 0.00015395282908995878, 'epoch': 0.23}\n",
      "{'loss': 1.2609, 'grad_norm': 0.3984375, 'learning_rate': 0.00015394146671249493, 'epoch': 0.23}\n",
      "{'loss': 1.2471, 'grad_norm': 0.53125, 'learning_rate': 0.00015393010433503108, 'epoch': 0.23}\n",
      "{'loss': 1.1323, 'grad_norm': 0.74609375, 'learning_rate': 0.0001539187419575672, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4594, 'grad_norm': 0.51953125, 'learning_rate': 0.00015390737958010335, 'epoch': 0.23}\n",
      "{'loss': 1.1492, 'grad_norm': 0.56640625, 'learning_rate': 0.00015389601720263948, 'epoch': 0.23}\n",
      "{'loss': 1.1771, 'grad_norm': 0.46875, 'learning_rate': 0.00015388465482517563, 'epoch': 0.23}\n",
      "{'loss': 1.2209, 'grad_norm': 0.458984375, 'learning_rate': 0.00015387329244771175, 'epoch': 0.23}\n",
      "{'loss': 1.0643, 'grad_norm': 0.84765625, 'learning_rate': 0.0001538619300702479, 'epoch': 0.23}\n",
      "{'loss': 1.6579, 'grad_norm': 0.4765625, 'learning_rate': 0.00015385056769278406, 'epoch': 0.23}\n",
      "{'loss': 1.2212, 'grad_norm': 0.5546875, 'learning_rate': 0.00015383920531532018, 'epoch': 0.23}\n",
      "{'loss': 1.3083, 'grad_norm': 0.349609375, 'learning_rate': 0.00015382784293785633, 'epoch': 0.23}\n",
      "{'loss': 1.2167, 'grad_norm': 0.66796875, 'learning_rate': 0.00015381648056039246, 'epoch': 0.23}\n",
      "{'loss': 1.2401, 'grad_norm': 1.0625, 'learning_rate': 0.0001538051181829286, 'epoch': 0.23}\n",
      "{'loss': 1.1777, 'grad_norm': 0.6484375, 'learning_rate': 0.00015379375580546473, 'epoch': 0.23}\n",
      "{'loss': 1.113, 'grad_norm': 0.64453125, 'learning_rate': 0.00015378239342800088, 'epoch': 0.23}\n",
      "{'loss': 1.1968, 'grad_norm': 0.36328125, 'learning_rate': 0.00015377103105053704, 'epoch': 0.23}\n",
      "{'loss': 1.263, 'grad_norm': 0.640625, 'learning_rate': 0.00015375966867307316, 'epoch': 0.23}\n",
      "{'loss': 1.1394, 'grad_norm': 0.49609375, 'learning_rate': 0.0001537483062956093, 'epoch': 0.23}\n",
      "{'loss': 1.3799, 'grad_norm': 0.466796875, 'learning_rate': 0.00015373694391814544, 'epoch': 0.23}\n",
      "{'loss': 1.1071, 'grad_norm': 0.6484375, 'learning_rate': 0.0001537255815406816, 'epoch': 0.23}\n",
      "{'loss': 1.2154, 'grad_norm': 0.57421875, 'learning_rate': 0.0001537142191632177, 'epoch': 0.23}\n",
      "{'loss': 1.2734, 'grad_norm': 0.494140625, 'learning_rate': 0.00015370285678575386, 'epoch': 0.23}\n",
      "{'loss': 1.0674, 'grad_norm': 0.62890625, 'learning_rate': 0.00015369149440829001, 'epoch': 0.23}\n",
      "{'loss': 1.4923, 'grad_norm': 0.46484375, 'learning_rate': 0.00015368013203082614, 'epoch': 0.23}\n",
      "{'loss': 1.2822, 'grad_norm': 1.0546875, 'learning_rate': 0.0001536687696533623, 'epoch': 0.23}\n",
      "{'loss': 1.2183, 'grad_norm': 0.37109375, 'learning_rate': 0.00015365740727589841, 'epoch': 0.23}\n",
      "{'loss': 1.227, 'grad_norm': 0.85546875, 'learning_rate': 0.00015364604489843454, 'epoch': 0.23}\n",
      "{'loss': 1.1877, 'grad_norm': 0.640625, 'learning_rate': 0.0001536346825209707, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4343, 'grad_norm': 0.470703125, 'learning_rate': 0.00015362332014350684, 'epoch': 0.23}\n",
      "{'loss': 1.1789, 'grad_norm': 0.59765625, 'learning_rate': 0.000153611957766043, 'epoch': 0.23}\n",
      "{'loss': 1.2164, 'grad_norm': 0.5, 'learning_rate': 0.00015360059538857912, 'epoch': 0.23}\n",
      "{'loss': 1.1618, 'grad_norm': 0.48828125, 'learning_rate': 0.00015358923301111524, 'epoch': 0.23}\n",
      "{'loss': 1.0583, 'grad_norm': 0.91015625, 'learning_rate': 0.0001535778706336514, 'epoch': 0.23}\n",
      "{'loss': 1.3078, 'grad_norm': 0.486328125, 'learning_rate': 0.00015356650825618752, 'epoch': 0.23}\n",
      "{'loss': 1.2561, 'grad_norm': 0.66796875, 'learning_rate': 0.00015355514587872367, 'epoch': 0.23}\n",
      "{'loss': 1.2602, 'grad_norm': 0.408203125, 'learning_rate': 0.00015354378350125982, 'epoch': 0.23}\n",
      "{'loss': 1.2971, 'grad_norm': 0.51953125, 'learning_rate': 0.00015353242112379594, 'epoch': 0.23}\n",
      "{'loss': 1.0832, 'grad_norm': 1.1640625, 'learning_rate': 0.0001535210587463321, 'epoch': 0.23}\n",
      "{'loss': 1.3183, 'grad_norm': 0.427734375, 'learning_rate': 0.00015350969636886822, 'epoch': 0.23}\n",
      "{'loss': 1.2127, 'grad_norm': 0.67578125, 'learning_rate': 0.00015349833399140437, 'epoch': 0.23}\n",
      "{'loss': 1.3102, 'grad_norm': 0.388671875, 'learning_rate': 0.0001534869716139405, 'epoch': 0.23}\n",
      "{'loss': 1.0667, 'grad_norm': 0.59765625, 'learning_rate': 0.00015347560923647665, 'epoch': 0.23}\n",
      "{'loss': 1.0885, 'grad_norm': 0.58203125, 'learning_rate': 0.0001534642468590128, 'epoch': 0.23}\n",
      "{'loss': 1.3753, 'grad_norm': 0.43359375, 'learning_rate': 0.00015345288448154892, 'epoch': 0.23}\n",
      "{'loss': 1.1601, 'grad_norm': 0.64453125, 'learning_rate': 0.00015344152210408507, 'epoch': 0.23}\n",
      "{'loss': 1.2036, 'grad_norm': 0.439453125, 'learning_rate': 0.0001534301597266212, 'epoch': 0.23}\n",
      "{'loss': 1.3296, 'grad_norm': 0.7421875, 'learning_rate': 0.00015341879734915735, 'epoch': 0.23}\n",
      "{'loss': 1.0567, 'grad_norm': 0.54296875, 'learning_rate': 0.00015340743497169347, 'epoch': 0.23}\n",
      "{'loss': 1.4334, 'grad_norm': 0.46484375, 'learning_rate': 0.00015339607259422962, 'epoch': 0.23}\n",
      "{'loss': 1.2484, 'grad_norm': 0.50390625, 'learning_rate': 0.00015338471021676578, 'epoch': 0.23}\n",
      "{'loss': 1.3225, 'grad_norm': 0.443359375, 'learning_rate': 0.0001533733478393019, 'epoch': 0.23}\n",
      "{'loss': 1.3992, 'grad_norm': 0.81640625, 'learning_rate': 0.00015336198546183805, 'epoch': 0.23}\n",
      "{'loss': 1.1185, 'grad_norm': 0.6171875, 'learning_rate': 0.00015335062308437418, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3159, 'grad_norm': 0.5234375, 'learning_rate': 0.00015333926070691033, 'epoch': 0.23}\n",
      "{'loss': 1.2665, 'grad_norm': 0.57421875, 'learning_rate': 0.00015332789832944645, 'epoch': 0.23}\n",
      "{'loss': 1.2882, 'grad_norm': 0.4765625, 'learning_rate': 0.0001533165359519826, 'epoch': 0.23}\n",
      "{'loss': 1.3576, 'grad_norm': 1.0078125, 'learning_rate': 0.00015330517357451875, 'epoch': 0.23}\n",
      "{'loss': 1.2058, 'grad_norm': 1.0, 'learning_rate': 0.00015329381119705488, 'epoch': 0.23}\n",
      "{'loss': 1.4366, 'grad_norm': 0.71875, 'learning_rate': 0.00015328244881959103, 'epoch': 0.23}\n",
      "{'loss': 1.177, 'grad_norm': 0.58984375, 'learning_rate': 0.00015327108644212715, 'epoch': 0.23}\n",
      "{'loss': 1.168, 'grad_norm': 0.59765625, 'learning_rate': 0.00015325972406466328, 'epoch': 0.23}\n",
      "{'loss': 1.3302, 'grad_norm': 0.482421875, 'learning_rate': 0.00015324836168719943, 'epoch': 0.23}\n",
      "{'loss': 1.1108, 'grad_norm': 0.70703125, 'learning_rate': 0.00015323699930973558, 'epoch': 0.23}\n",
      "{'loss': 1.2929, 'grad_norm': 0.4296875, 'learning_rate': 0.00015322563693227173, 'epoch': 0.23}\n",
      "{'loss': 1.2792, 'grad_norm': 0.5546875, 'learning_rate': 0.00015321427455480786, 'epoch': 0.23}\n",
      "{'loss': 1.2586, 'grad_norm': 0.353515625, 'learning_rate': 0.00015320291217734398, 'epoch': 0.23}\n",
      "{'loss': 1.3972, 'grad_norm': 0.47265625, 'learning_rate': 0.00015319154979988013, 'epoch': 0.23}\n",
      "{'loss': 1.1862, 'grad_norm': 0.671875, 'learning_rate': 0.00015318018742241626, 'epoch': 0.23}\n",
      "{'loss': 1.4202, 'grad_norm': 0.6328125, 'learning_rate': 0.00015316882504495244, 'epoch': 0.23}\n",
      "{'loss': 1.2279, 'grad_norm': 0.55859375, 'learning_rate': 0.00015315746266748856, 'epoch': 0.23}\n",
      "{'loss': 1.1898, 'grad_norm': 0.43359375, 'learning_rate': 0.00015314610029002468, 'epoch': 0.23}\n",
      "{'loss': 1.4783, 'grad_norm': 0.60546875, 'learning_rate': 0.00015313473791256084, 'epoch': 0.23}\n",
      "{'loss': 1.1394, 'grad_norm': 0.5546875, 'learning_rate': 0.00015312337553509696, 'epoch': 0.23}\n",
      "{'loss': 1.3833, 'grad_norm': 0.5078125, 'learning_rate': 0.0001531120131576331, 'epoch': 0.23}\n",
      "{'loss': 1.1903, 'grad_norm': 0.625, 'learning_rate': 0.00015310065078016924, 'epoch': 0.23}\n",
      "{'loss': 1.2788, 'grad_norm': 0.474609375, 'learning_rate': 0.0001530892884027054, 'epoch': 0.23}\n",
      "{'loss': 1.2761, 'grad_norm': 0.7265625, 'learning_rate': 0.00015307792602524154, 'epoch': 0.23}\n",
      "{'loss': 1.0725, 'grad_norm': 0.8984375, 'learning_rate': 0.00015306656364777766, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3258, 'grad_norm': 0.625, 'learning_rate': 0.00015305520127031381, 'epoch': 0.23}\n",
      "{'loss': 1.1565, 'grad_norm': 0.69140625, 'learning_rate': 0.00015304383889284994, 'epoch': 0.23}\n",
      "{'loss': 1.2815, 'grad_norm': 0.46875, 'learning_rate': 0.0001530324765153861, 'epoch': 0.23}\n",
      "{'loss': 1.3235, 'grad_norm': 0.625, 'learning_rate': 0.00015302111413792221, 'epoch': 0.23}\n",
      "{'loss': 1.1908, 'grad_norm': 0.67578125, 'learning_rate': 0.00015300975176045837, 'epoch': 0.23}\n",
      "{'loss': 1.2498, 'grad_norm': 0.51953125, 'learning_rate': 0.00015299838938299452, 'epoch': 0.23}\n",
      "{'loss': 1.2934, 'grad_norm': 0.57421875, 'learning_rate': 0.00015298702700553064, 'epoch': 0.23}\n",
      "{'loss': 1.2054, 'grad_norm': 0.4609375, 'learning_rate': 0.0001529756646280668, 'epoch': 0.23}\n",
      "{'loss': 1.2431, 'grad_norm': 0.57421875, 'learning_rate': 0.00015296430225060292, 'epoch': 0.23}\n",
      "{'loss': 0.9835, 'grad_norm': 0.36328125, 'learning_rate': 0.00015295293987313907, 'epoch': 0.23}\n",
      "{'loss': 1.3631, 'grad_norm': 0.71875, 'learning_rate': 0.0001529415774956752, 'epoch': 0.23}\n",
      "{'loss': 1.1624, 'grad_norm': 0.49609375, 'learning_rate': 0.00015293021511821134, 'epoch': 0.23}\n",
      "{'loss': 1.354, 'grad_norm': 0.4140625, 'learning_rate': 0.0001529188527407475, 'epoch': 0.23}\n",
      "{'loss': 1.2833, 'grad_norm': 0.515625, 'learning_rate': 0.00015290749036328362, 'epoch': 0.23}\n",
      "{'loss': 1.1927, 'grad_norm': 0.8828125, 'learning_rate': 0.00015289612798581977, 'epoch': 0.23}\n",
      "{'loss': 1.5174, 'grad_norm': 0.408203125, 'learning_rate': 0.0001528847656083559, 'epoch': 0.23}\n",
      "{'loss': 1.2181, 'grad_norm': 0.60546875, 'learning_rate': 0.00015287340323089202, 'epoch': 0.23}\n",
      "{'loss': 1.1182, 'grad_norm': 0.419921875, 'learning_rate': 0.00015286204085342817, 'epoch': 0.23}\n",
      "{'loss': 1.2927, 'grad_norm': 0.6015625, 'learning_rate': 0.00015285067847596432, 'epoch': 0.23}\n",
      "{'loss': 1.1479, 'grad_norm': 0.7734375, 'learning_rate': 0.00015283931609850047, 'epoch': 0.23}\n",
      "{'loss': 1.3129, 'grad_norm': 0.48046875, 'learning_rate': 0.0001528279537210366, 'epoch': 0.23}\n",
      "{'loss': 1.2868, 'grad_norm': 0.7109375, 'learning_rate': 0.00015281659134357272, 'epoch': 0.23}\n",
      "{'loss': 1.1461, 'grad_norm': 0.515625, 'learning_rate': 0.00015280522896610887, 'epoch': 0.23}\n",
      "{'loss': 1.1356, 'grad_norm': 0.482421875, 'learning_rate': 0.000152793866588645, 'epoch': 0.23}\n",
      "{'loss': 1.0374, 'grad_norm': 1.0078125, 'learning_rate': 0.00015278250421118118, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3474, 'grad_norm': 0.470703125, 'learning_rate': 0.0001527711418337173, 'epoch': 0.23}\n",
      "{'loss': 1.2896, 'grad_norm': 0.68359375, 'learning_rate': 0.00015275977945625343, 'epoch': 0.23}\n",
      "{'loss': 1.1274, 'grad_norm': 0.7265625, 'learning_rate': 0.00015274841707878958, 'epoch': 0.23}\n",
      "{'loss': 1.2745, 'grad_norm': 0.5, 'learning_rate': 0.0001527370547013257, 'epoch': 0.23}\n",
      "{'loss': 1.1048, 'grad_norm': 0.609375, 'learning_rate': 0.00015272569232386185, 'epoch': 0.23}\n",
      "{'loss': 1.2468, 'grad_norm': 0.458984375, 'learning_rate': 0.00015271432994639798, 'epoch': 0.23}\n",
      "{'loss': 1.2599, 'grad_norm': 0.51953125, 'learning_rate': 0.00015270296756893413, 'epoch': 0.23}\n",
      "{'loss': 1.2612, 'grad_norm': 0.40234375, 'learning_rate': 0.00015269160519147028, 'epoch': 0.23}\n",
      "{'loss': 1.2571, 'grad_norm': 0.66015625, 'learning_rate': 0.0001526802428140064, 'epoch': 0.23}\n",
      "{'loss': 1.1094, 'grad_norm': 0.67578125, 'learning_rate': 0.00015266888043654256, 'epoch': 0.23}\n",
      "{'loss': 1.3868, 'grad_norm': 0.62109375, 'learning_rate': 0.00015265751805907868, 'epoch': 0.23}\n",
      "{'loss': 1.1527, 'grad_norm': 0.84765625, 'learning_rate': 0.00015264615568161483, 'epoch': 0.23}\n",
      "{'loss': 1.1876, 'grad_norm': 0.44140625, 'learning_rate': 0.00015263479330415096, 'epoch': 0.23}\n",
      "{'loss': 1.3197, 'grad_norm': 0.486328125, 'learning_rate': 0.0001526234309266871, 'epoch': 0.23}\n",
      "{'loss': 1.2003, 'grad_norm': 1.109375, 'learning_rate': 0.00015261206854922326, 'epoch': 0.23}\n",
      "{'loss': 1.3655, 'grad_norm': 0.5078125, 'learning_rate': 0.00015260070617175938, 'epoch': 0.23}\n",
      "{'loss': 1.279, 'grad_norm': 0.734375, 'learning_rate': 0.00015258934379429553, 'epoch': 0.23}\n",
      "{'loss': 1.252, 'grad_norm': 0.400390625, 'learning_rate': 0.00015257798141683166, 'epoch': 0.23}\n",
      "{'loss': 1.2862, 'grad_norm': 0.494140625, 'learning_rate': 0.0001525666190393678, 'epoch': 0.23}\n",
      "{'loss': 1.0791, 'grad_norm': 1.125, 'learning_rate': 0.00015255525666190393, 'epoch': 0.23}\n",
      "{'loss': 1.429, 'grad_norm': 0.498046875, 'learning_rate': 0.00015254389428444009, 'epoch': 0.23}\n",
      "{'loss': 1.1912, 'grad_norm': 0.44140625, 'learning_rate': 0.00015253253190697624, 'epoch': 0.23}\n",
      "{'loss': 1.1379, 'grad_norm': 0.43359375, 'learning_rate': 0.00015252116952951236, 'epoch': 0.23}\n",
      "{'loss': 1.1595, 'grad_norm': 0.57421875, 'learning_rate': 0.0001525098071520485, 'epoch': 0.23}\n",
      "{'loss': 1.0162, 'grad_norm': 0.75, 'learning_rate': 0.00015249844477458464, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3722, 'grad_norm': 0.466796875, 'learning_rate': 0.00015248708239712076, 'epoch': 0.23}\n",
      "{'loss': 1.0704, 'grad_norm': 0.79296875, 'learning_rate': 0.00015247572001965694, 'epoch': 0.23}\n",
      "{'loss': 1.0844, 'grad_norm': 0.494140625, 'learning_rate': 0.00015246435764219306, 'epoch': 0.23}\n",
      "{'loss': 1.2589, 'grad_norm': 0.71875, 'learning_rate': 0.00015245299526472922, 'epoch': 0.23}\n",
      "{'loss': 1.2375, 'grad_norm': 0.515625, 'learning_rate': 0.00015244163288726534, 'epoch': 0.23}\n",
      "{'loss': 1.4066, 'grad_norm': 0.431640625, 'learning_rate': 0.00015243027050980146, 'epoch': 0.23}\n",
      "{'loss': 1.2973, 'grad_norm': 0.62109375, 'learning_rate': 0.00015241890813233762, 'epoch': 0.23}\n",
      "{'loss': 1.2905, 'grad_norm': 0.443359375, 'learning_rate': 0.00015240754575487374, 'epoch': 0.23}\n",
      "{'loss': 1.2211, 'grad_norm': 0.59375, 'learning_rate': 0.00015239618337740992, 'epoch': 0.23}\n",
      "{'loss': 1.0309, 'grad_norm': 0.2470703125, 'learning_rate': 0.00015238482099994604, 'epoch': 0.23}\n",
      "{'loss': 1.299, 'grad_norm': 0.45703125, 'learning_rate': 0.00015237345862248217, 'epoch': 0.23}\n",
      "{'loss': 1.0728, 'grad_norm': 0.8984375, 'learning_rate': 0.00015236209624501832, 'epoch': 0.23}\n",
      "{'loss': 1.3738, 'grad_norm': 0.458984375, 'learning_rate': 0.00015235073386755444, 'epoch': 0.23}\n",
      "{'loss': 1.2243, 'grad_norm': 0.50390625, 'learning_rate': 0.0001523393714900906, 'epoch': 0.24}\n",
      "{'loss': 1.0414, 'grad_norm': 0.77734375, 'learning_rate': 0.00015232800911262672, 'epoch': 0.24}\n",
      "{'loss': 1.4285, 'grad_norm': 0.41015625, 'learning_rate': 0.00015231664673516287, 'epoch': 0.24}\n",
      "{'loss': 1.2452, 'grad_norm': 0.640625, 'learning_rate': 0.00015230528435769902, 'epoch': 0.24}\n",
      "{'loss': 1.2794, 'grad_norm': 0.4296875, 'learning_rate': 0.00015229392198023515, 'epoch': 0.24}\n",
      "{'loss': 1.2754, 'grad_norm': 0.44140625, 'learning_rate': 0.0001522825596027713, 'epoch': 0.24}\n",
      "{'loss': 1.1174, 'grad_norm': 0.9375, 'learning_rate': 0.00015227119722530742, 'epoch': 0.24}\n",
      "{'loss': 1.298, 'grad_norm': 0.5390625, 'learning_rate': 0.00015225983484784357, 'epoch': 0.24}\n",
      "{'loss': 1.1585, 'grad_norm': 0.6796875, 'learning_rate': 0.0001522484724703797, 'epoch': 0.24}\n",
      "{'loss': 1.2422, 'grad_norm': 0.45703125, 'learning_rate': 0.00015223711009291585, 'epoch': 0.24}\n",
      "{'loss': 1.2869, 'grad_norm': 0.76171875, 'learning_rate': 0.000152225747715452, 'epoch': 0.24}\n",
      "{'loss': 1.0866, 'grad_norm': 0.91015625, 'learning_rate': 0.00015221438533798812, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3663, 'grad_norm': 0.4609375, 'learning_rate': 0.00015220302296052428, 'epoch': 0.24}\n",
      "{'loss': 1.2499, 'grad_norm': 0.8125, 'learning_rate': 0.0001521916605830604, 'epoch': 0.24}\n",
      "{'loss': 1.3923, 'grad_norm': 0.376953125, 'learning_rate': 0.00015218029820559655, 'epoch': 0.24}\n",
      "{'loss': 1.2183, 'grad_norm': 0.51953125, 'learning_rate': 0.00015216893582813268, 'epoch': 0.24}\n",
      "{'loss': 1.1211, 'grad_norm': 0.828125, 'learning_rate': 0.00015215757345066883, 'epoch': 0.24}\n",
      "{'loss': 1.3775, 'grad_norm': 0.4921875, 'learning_rate': 0.00015214621107320498, 'epoch': 0.24}\n",
      "{'loss': 1.2182, 'grad_norm': 0.52734375, 'learning_rate': 0.0001521348486957411, 'epoch': 0.24}\n",
      "{'loss': 1.1755, 'grad_norm': 0.458984375, 'learning_rate': 0.00015212348631827725, 'epoch': 0.24}\n",
      "{'loss': 1.1818, 'grad_norm': 0.466796875, 'learning_rate': 0.00015211212394081338, 'epoch': 0.24}\n",
      "{'loss': 1.1721, 'grad_norm': 0.75, 'learning_rate': 0.0001521007615633495, 'epoch': 0.24}\n",
      "{'loss': 1.3065, 'grad_norm': 0.6015625, 'learning_rate': 0.00015208939918588568, 'epoch': 0.24}\n",
      "{'loss': 1.227, 'grad_norm': 0.53515625, 'learning_rate': 0.0001520780368084218, 'epoch': 0.24}\n",
      "{'loss': 1.1315, 'grad_norm': 0.5625, 'learning_rate': 0.00015206667443095796, 'epoch': 0.24}\n",
      "{'loss': 1.2157, 'grad_norm': 0.61328125, 'learning_rate': 0.00015205531205349408, 'epoch': 0.24}\n",
      "{'loss': 1.1497, 'grad_norm': 1.25, 'learning_rate': 0.0001520439496760302, 'epoch': 0.24}\n",
      "{'loss': 1.2781, 'grad_norm': 0.494140625, 'learning_rate': 0.00015203258729856636, 'epoch': 0.24}\n",
      "{'loss': 1.2348, 'grad_norm': 0.94921875, 'learning_rate': 0.00015202122492110248, 'epoch': 0.24}\n",
      "{'loss': 1.2416, 'grad_norm': 0.4765625, 'learning_rate': 0.00015200986254363866, 'epoch': 0.24}\n",
      "{'loss': 1.413, 'grad_norm': 0.66796875, 'learning_rate': 0.00015199850016617478, 'epoch': 0.24}\n",
      "{'loss': 1.1788, 'grad_norm': 1.34375, 'learning_rate': 0.0001519871377887109, 'epoch': 0.24}\n",
      "{'loss': 1.4804, 'grad_norm': 0.47265625, 'learning_rate': 0.00015197577541124706, 'epoch': 0.24}\n",
      "{'loss': 1.2615, 'grad_norm': 0.76953125, 'learning_rate': 0.00015196441303378318, 'epoch': 0.24}\n",
      "{'loss': 1.1525, 'grad_norm': 0.3203125, 'learning_rate': 0.00015195305065631934, 'epoch': 0.24}\n",
      "{'loss': 1.301, 'grad_norm': 0.49609375, 'learning_rate': 0.00015194168827885546, 'epoch': 0.24}\n",
      "{'loss': 0.9696, 'grad_norm': 1.515625, 'learning_rate': 0.0001519303259013916, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4715, 'grad_norm': 0.43359375, 'learning_rate': 0.00015191896352392776, 'epoch': 0.24}\n",
      "{'loss': 1.2952, 'grad_norm': 0.546875, 'learning_rate': 0.0001519076011464639, 'epoch': 0.24}\n",
      "{'loss': 1.2885, 'grad_norm': 0.37890625, 'learning_rate': 0.00015189623876900004, 'epoch': 0.24}\n",
      "{'loss': 1.2902, 'grad_norm': 0.5625, 'learning_rate': 0.00015188487639153616, 'epoch': 0.24}\n",
      "{'loss': 1.0948, 'grad_norm': 0.92578125, 'learning_rate': 0.00015187351401407231, 'epoch': 0.24}\n",
      "{'loss': 1.3694, 'grad_norm': 0.447265625, 'learning_rate': 0.00015186215163660844, 'epoch': 0.24}\n",
      "{'loss': 1.1164, 'grad_norm': 0.5703125, 'learning_rate': 0.0001518507892591446, 'epoch': 0.24}\n",
      "{'loss': 1.2786, 'grad_norm': 0.37890625, 'learning_rate': 0.00015183942688168074, 'epoch': 0.24}\n",
      "{'loss': 1.2512, 'grad_norm': 0.494140625, 'learning_rate': 0.00015182806450421687, 'epoch': 0.24}\n",
      "{'loss': 1.0999, 'grad_norm': 0.91796875, 'learning_rate': 0.00015181670212675302, 'epoch': 0.24}\n",
      "{'loss': 1.3329, 'grad_norm': 0.494140625, 'learning_rate': 0.00015180533974928914, 'epoch': 0.24}\n",
      "{'loss': 1.1273, 'grad_norm': 0.9140625, 'learning_rate': 0.0001517939773718253, 'epoch': 0.24}\n",
      "{'loss': 1.1102, 'grad_norm': 0.361328125, 'learning_rate': 0.00015178261499436144, 'epoch': 0.24}\n",
      "{'loss': 1.281, 'grad_norm': 0.50390625, 'learning_rate': 0.00015177125261689757, 'epoch': 0.24}\n",
      "{'loss': 1.088, 'grad_norm': 0.875, 'learning_rate': 0.00015175989023943372, 'epoch': 0.24}\n",
      "{'loss': 1.4377, 'grad_norm': 0.455078125, 'learning_rate': 0.00015174852786196984, 'epoch': 0.24}\n",
      "{'loss': 1.1579, 'grad_norm': 0.73046875, 'learning_rate': 0.000151737165484506, 'epoch': 0.24}\n",
      "{'loss': 1.2839, 'grad_norm': 0.375, 'learning_rate': 0.00015172580310704212, 'epoch': 0.24}\n",
      "{'loss': 1.3746, 'grad_norm': 0.6328125, 'learning_rate': 0.00015171444072957824, 'epoch': 0.24}\n",
      "{'loss': 1.085, 'grad_norm': 0.7734375, 'learning_rate': 0.00015170307835211442, 'epoch': 0.24}\n",
      "{'loss': 1.303, 'grad_norm': 0.443359375, 'learning_rate': 0.00015169171597465055, 'epoch': 0.24}\n",
      "{'loss': 1.1692, 'grad_norm': 0.5234375, 'learning_rate': 0.0001516803535971867, 'epoch': 0.24}\n",
      "{'loss': 1.3147, 'grad_norm': 0.427734375, 'learning_rate': 0.00015166899121972282, 'epoch': 0.24}\n",
      "{'loss': 1.1355, 'grad_norm': 0.65625, 'learning_rate': 0.00015165762884225895, 'epoch': 0.24}\n",
      "{'loss': 1.1455, 'grad_norm': 0.486328125, 'learning_rate': 0.0001516462664647951, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3446, 'grad_norm': 0.4375, 'learning_rate': 0.00015163490408733122, 'epoch': 0.24}\n",
      "{'loss': 1.215, 'grad_norm': 0.384765625, 'learning_rate': 0.0001516235417098674, 'epoch': 0.24}\n",
      "{'loss': 1.1869, 'grad_norm': 0.404296875, 'learning_rate': 0.00015161217933240353, 'epoch': 0.24}\n",
      "{'loss': 1.2757, 'grad_norm': 0.578125, 'learning_rate': 0.00015160081695493965, 'epoch': 0.24}\n",
      "{'loss': 1.1473, 'grad_norm': 0.83203125, 'learning_rate': 0.0001515894545774758, 'epoch': 0.24}\n",
      "{'loss': 1.3649, 'grad_norm': 0.5078125, 'learning_rate': 0.00015157809220001193, 'epoch': 0.24}\n",
      "{'loss': 1.1073, 'grad_norm': 0.54296875, 'learning_rate': 0.00015156672982254808, 'epoch': 0.24}\n",
      "{'loss': 1.3201, 'grad_norm': 0.41796875, 'learning_rate': 0.0001515553674450842, 'epoch': 0.24}\n",
      "{'loss': 1.2372, 'grad_norm': 0.53125, 'learning_rate': 0.00015154400506762035, 'epoch': 0.24}\n",
      "{'loss': 1.3478, 'grad_norm': 0.96484375, 'learning_rate': 0.0001515326426901565, 'epoch': 0.24}\n",
      "{'loss': 1.3615, 'grad_norm': 0.6484375, 'learning_rate': 0.00015152128031269263, 'epoch': 0.24}\n",
      "{'loss': 1.1898, 'grad_norm': 0.578125, 'learning_rate': 0.00015150991793522878, 'epoch': 0.24}\n",
      "{'loss': 1.2824, 'grad_norm': 0.37109375, 'learning_rate': 0.0001514985555577649, 'epoch': 0.24}\n",
      "{'loss': 1.2901, 'grad_norm': 0.57421875, 'learning_rate': 0.00015148719318030106, 'epoch': 0.24}\n",
      "{'loss': 1.1958, 'grad_norm': 0.921875, 'learning_rate': 0.00015147583080283718, 'epoch': 0.24}\n",
      "{'loss': 1.2769, 'grad_norm': 0.482421875, 'learning_rate': 0.00015146446842537333, 'epoch': 0.24}\n",
      "{'loss': 1.1886, 'grad_norm': 0.54296875, 'learning_rate': 0.00015145310604790948, 'epoch': 0.24}\n",
      "{'loss': 1.3216, 'grad_norm': 0.4296875, 'learning_rate': 0.0001514417436704456, 'epoch': 0.24}\n",
      "{'loss': 1.2891, 'grad_norm': 0.51953125, 'learning_rate': 0.00015143038129298176, 'epoch': 0.24}\n",
      "{'loss': 1.0571, 'grad_norm': 0.703125, 'learning_rate': 0.00015141901891551788, 'epoch': 0.24}\n",
      "{'loss': 1.2459, 'grad_norm': 1.1484375, 'learning_rate': 0.00015140765653805403, 'epoch': 0.24}\n",
      "{'loss': 1.1629, 'grad_norm': 0.546875, 'learning_rate': 0.00015139629416059019, 'epoch': 0.24}\n",
      "{'loss': 1.2736, 'grad_norm': 0.46875, 'learning_rate': 0.0001513849317831263, 'epoch': 0.24}\n",
      "{'loss': 1.2274, 'grad_norm': 0.62109375, 'learning_rate': 0.00015137356940566246, 'epoch': 0.24}\n",
      "{'loss': 1.0515, 'grad_norm': 1.25, 'learning_rate': 0.00015136220702819859, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4397, 'grad_norm': 0.51171875, 'learning_rate': 0.00015135084465073474, 'epoch': 0.24}\n",
      "{'loss': 1.1965, 'grad_norm': 0.6171875, 'learning_rate': 0.00015133948227327086, 'epoch': 0.24}\n",
      "{'loss': 1.1629, 'grad_norm': 0.3828125, 'learning_rate': 0.00015132811989580699, 'epoch': 0.24}\n",
      "{'loss': 1.1509, 'grad_norm': 0.49609375, 'learning_rate': 0.00015131675751834316, 'epoch': 0.24}\n",
      "{'loss': 1.0531, 'grad_norm': 0.6640625, 'learning_rate': 0.0001513053951408793, 'epoch': 0.24}\n",
      "{'loss': 1.2875, 'grad_norm': 0.451171875, 'learning_rate': 0.00015129403276341544, 'epoch': 0.24}\n",
      "{'loss': 1.1643, 'grad_norm': 0.78125, 'learning_rate': 0.00015128267038595156, 'epoch': 0.24}\n",
      "{'loss': 1.3136, 'grad_norm': 0.408203125, 'learning_rate': 0.0001512713080084877, 'epoch': 0.24}\n",
      "{'loss': 1.2404, 'grad_norm': 0.796875, 'learning_rate': 0.00015125994563102384, 'epoch': 0.24}\n",
      "{'loss': 1.1426, 'grad_norm': 1.34375, 'learning_rate': 0.00015124858325355996, 'epoch': 0.24}\n",
      "{'loss': 1.3847, 'grad_norm': 0.486328125, 'learning_rate': 0.00015123722087609614, 'epoch': 0.24}\n",
      "{'loss': 1.3012, 'grad_norm': 0.64453125, 'learning_rate': 0.00015122585849863227, 'epoch': 0.24}\n",
      "{'loss': 1.1755, 'grad_norm': 0.546875, 'learning_rate': 0.0001512144961211684, 'epoch': 0.24}\n",
      "{'loss': 1.2527, 'grad_norm': 0.59375, 'learning_rate': 0.00015120313374370454, 'epoch': 0.24}\n",
      "{'loss': 1.1643, 'grad_norm': 0.86328125, 'learning_rate': 0.00015119177136624067, 'epoch': 0.24}\n",
      "{'loss': 1.3825, 'grad_norm': 0.69921875, 'learning_rate': 0.00015118040898877682, 'epoch': 0.24}\n",
      "{'loss': 1.2698, 'grad_norm': 0.5703125, 'learning_rate': 0.00015116904661131294, 'epoch': 0.24}\n",
      "{'loss': 1.1724, 'grad_norm': 0.423828125, 'learning_rate': 0.0001511576842338491, 'epoch': 0.24}\n",
      "{'loss': 1.2551, 'grad_norm': 0.43359375, 'learning_rate': 0.00015114632185638525, 'epoch': 0.24}\n",
      "{'loss': 1.1041, 'grad_norm': 0.90625, 'learning_rate': 0.00015113495947892137, 'epoch': 0.24}\n",
      "{'loss': 1.3812, 'grad_norm': 0.5, 'learning_rate': 0.00015112359710145752, 'epoch': 0.24}\n",
      "{'loss': 1.2083, 'grad_norm': 0.671875, 'learning_rate': 0.00015111223472399365, 'epoch': 0.24}\n",
      "{'loss': 1.2212, 'grad_norm': 0.412109375, 'learning_rate': 0.0001511008723465298, 'epoch': 0.24}\n",
      "{'loss': 1.2932, 'grad_norm': 0.5546875, 'learning_rate': 0.00015108950996906595, 'epoch': 0.24}\n",
      "{'loss': 0.9774, 'grad_norm': 0.25390625, 'learning_rate': 0.00015107814759160207, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4551, 'grad_norm': 0.62890625, 'learning_rate': 0.00015106678521413822, 'epoch': 0.24}\n",
      "{'loss': 1.2739, 'grad_norm': 0.5546875, 'learning_rate': 0.00015105542283667435, 'epoch': 0.24}\n",
      "{'loss': 1.2013, 'grad_norm': 0.384765625, 'learning_rate': 0.0001510440604592105, 'epoch': 0.24}\n",
      "{'loss': 1.2294, 'grad_norm': 1.015625, 'learning_rate': 0.00015103269808174662, 'epoch': 0.24}\n",
      "{'loss': 1.0632, 'grad_norm': 0.84375, 'learning_rate': 0.00015102133570428278, 'epoch': 0.24}\n",
      "{'loss': 1.272, 'grad_norm': 0.6328125, 'learning_rate': 0.00015100997332681893, 'epoch': 0.24}\n",
      "{'loss': 1.2855, 'grad_norm': 0.5078125, 'learning_rate': 0.00015099861094935505, 'epoch': 0.24}\n",
      "{'loss': 1.2484, 'grad_norm': 0.416015625, 'learning_rate': 0.0001509872485718912, 'epoch': 0.24}\n",
      "{'loss': 1.2256, 'grad_norm': 0.828125, 'learning_rate': 0.00015097588619442733, 'epoch': 0.24}\n",
      "{'loss': 1.1751, 'grad_norm': 0.92578125, 'learning_rate': 0.00015096452381696348, 'epoch': 0.24}\n",
      "{'loss': 1.3195, 'grad_norm': 0.39453125, 'learning_rate': 0.0001509531614394996, 'epoch': 0.24}\n",
      "{'loss': 1.2858, 'grad_norm': 0.66015625, 'learning_rate': 0.00015094179906203573, 'epoch': 0.24}\n",
      "{'loss': 1.2336, 'grad_norm': 0.37890625, 'learning_rate': 0.0001509304366845719, 'epoch': 0.24}\n",
      "{'loss': 1.3378, 'grad_norm': 0.42578125, 'learning_rate': 0.00015091907430710803, 'epoch': 0.24}\n",
      "{'loss': 1.0563, 'grad_norm': 1.1875, 'learning_rate': 0.00015090771192964418, 'epoch': 0.24}\n",
      "{'loss': 1.2606, 'grad_norm': 0.6171875, 'learning_rate': 0.0001508963495521803, 'epoch': 0.24}\n",
      "{'loss': 1.2257, 'grad_norm': 0.5703125, 'learning_rate': 0.00015088498717471643, 'epoch': 0.24}\n",
      "{'loss': 1.2651, 'grad_norm': 0.443359375, 'learning_rate': 0.00015087362479725258, 'epoch': 0.24}\n",
      "{'loss': 1.3369, 'grad_norm': 0.5859375, 'learning_rate': 0.0001508622624197887, 'epoch': 0.24}\n",
      "{'loss': 0.9962, 'grad_norm': 0.34765625, 'learning_rate': 0.00015085090004232488, 'epoch': 0.24}\n",
      "{'loss': 1.3589, 'grad_norm': 0.71484375, 'learning_rate': 0.000150839537664861, 'epoch': 0.24}\n",
      "{'loss': 1.1791, 'grad_norm': 0.51171875, 'learning_rate': 0.00015082817528739713, 'epoch': 0.24}\n",
      "{'loss': 1.4265, 'grad_norm': 0.3984375, 'learning_rate': 0.00015081681290993328, 'epoch': 0.24}\n",
      "{'loss': 1.2533, 'grad_norm': 0.62109375, 'learning_rate': 0.0001508054505324694, 'epoch': 0.24}\n",
      "{'loss': 1.0939, 'grad_norm': 0.93359375, 'learning_rate': 0.00015079408815500556, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2862, 'grad_norm': 0.44921875, 'learning_rate': 0.00015078272577754168, 'epoch': 0.24}\n",
      "{'loss': 1.0897, 'grad_norm': 0.515625, 'learning_rate': 0.00015077136340007784, 'epoch': 0.24}\n",
      "{'loss': 1.2625, 'grad_norm': 0.52734375, 'learning_rate': 0.000150760001022614, 'epoch': 0.24}\n",
      "{'loss': 1.3115, 'grad_norm': 0.90625, 'learning_rate': 0.0001507486386451501, 'epoch': 0.24}\n",
      "{'loss': 1.1108, 'grad_norm': 0.84375, 'learning_rate': 0.00015073727626768626, 'epoch': 0.24}\n",
      "{'loss': 1.3144, 'grad_norm': 0.5625, 'learning_rate': 0.0001507259138902224, 'epoch': 0.24}\n",
      "{'loss': 1.1363, 'grad_norm': 0.98828125, 'learning_rate': 0.00015071455151275854, 'epoch': 0.24}\n",
      "{'loss': 1.1696, 'grad_norm': 0.4921875, 'learning_rate': 0.0001507031891352947, 'epoch': 0.24}\n",
      "{'loss': 1.256, 'grad_norm': 0.578125, 'learning_rate': 0.00015069182675783081, 'epoch': 0.24}\n",
      "{'loss': 1.1643, 'grad_norm': 0.61328125, 'learning_rate': 0.00015068046438036697, 'epoch': 0.24}\n",
      "{'loss': 1.3421, 'grad_norm': 0.67578125, 'learning_rate': 0.0001506691020029031, 'epoch': 0.24}\n",
      "{'loss': 1.2393, 'grad_norm': 0.494140625, 'learning_rate': 0.00015065773962543924, 'epoch': 0.24}\n",
      "{'loss': 1.318, 'grad_norm': 0.66015625, 'learning_rate': 0.00015064637724797537, 'epoch': 0.24}\n",
      "{'loss': 1.2599, 'grad_norm': 0.73828125, 'learning_rate': 0.00015063501487051152, 'epoch': 0.24}\n",
      "{'loss': 1.1897, 'grad_norm': 1.21875, 'learning_rate': 0.00015062365249304767, 'epoch': 0.24}\n",
      "{'loss': 1.4653, 'grad_norm': 0.515625, 'learning_rate': 0.0001506122901155838, 'epoch': 0.24}\n",
      "{'loss': 1.1754, 'grad_norm': 0.65234375, 'learning_rate': 0.00015060092773811994, 'epoch': 0.24}\n",
      "{'loss': 1.255, 'grad_norm': 0.392578125, 'learning_rate': 0.00015058956536065607, 'epoch': 0.24}\n",
      "{'loss': 1.2264, 'grad_norm': 0.51953125, 'learning_rate': 0.00015057820298319222, 'epoch': 0.24}\n",
      "{'loss': 1.0079, 'grad_norm': 0.6796875, 'learning_rate': 0.00015056684060572834, 'epoch': 0.24}\n",
      "{'loss': 1.5072, 'grad_norm': 0.5703125, 'learning_rate': 0.00015055547822826447, 'epoch': 0.24}\n",
      "{'loss': 1.1618, 'grad_norm': 0.63671875, 'learning_rate': 0.00015054411585080065, 'epoch': 0.24}\n",
      "{'loss': 1.2328, 'grad_norm': 0.68359375, 'learning_rate': 0.00015053275347333677, 'epoch': 0.24}\n",
      "{'loss': 1.4185, 'grad_norm': 0.50390625, 'learning_rate': 0.00015052139109587292, 'epoch': 0.24}\n",
      "{'loss': 1.1766, 'grad_norm': 0.82421875, 'learning_rate': 0.00015051002871840905, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5021, 'grad_norm': 0.578125, 'learning_rate': 0.00015049866634094517, 'epoch': 0.24}\n",
      "{'loss': 1.1898, 'grad_norm': 0.6171875, 'learning_rate': 0.00015048730396348132, 'epoch': 0.24}\n",
      "{'loss': 1.3124, 'grad_norm': 0.5234375, 'learning_rate': 0.00015047594158601745, 'epoch': 0.24}\n",
      "{'loss': 1.204, 'grad_norm': 0.443359375, 'learning_rate': 0.00015046457920855363, 'epoch': 0.24}\n",
      "{'loss': 1.077, 'grad_norm': 0.56640625, 'learning_rate': 0.00015045321683108975, 'epoch': 0.24}\n",
      "{'loss': 1.2598, 'grad_norm': 0.455078125, 'learning_rate': 0.00015044185445362587, 'epoch': 0.24}\n",
      "{'loss': 1.2334, 'grad_norm': 0.6953125, 'learning_rate': 0.00015043049207616203, 'epoch': 0.24}\n",
      "{'loss': 1.255, 'grad_norm': 0.62109375, 'learning_rate': 0.00015041912969869815, 'epoch': 0.24}\n",
      "{'loss': 1.2228, 'grad_norm': 0.5234375, 'learning_rate': 0.0001504077673212343, 'epoch': 0.24}\n",
      "{'loss': 1.1827, 'grad_norm': 0.8203125, 'learning_rate': 0.00015039640494377045, 'epoch': 0.24}\n",
      "{'loss': 1.4488, 'grad_norm': 0.734375, 'learning_rate': 0.00015038504256630658, 'epoch': 0.24}\n",
      "{'loss': 1.0851, 'grad_norm': 0.6875, 'learning_rate': 0.00015037368018884273, 'epoch': 0.24}\n",
      "{'loss': 1.1224, 'grad_norm': 0.380859375, 'learning_rate': 0.00015036231781137885, 'epoch': 0.24}\n",
      "{'loss': 1.1744, 'grad_norm': 0.5390625, 'learning_rate': 0.000150350955433915, 'epoch': 0.24}\n",
      "{'loss': 1.1435, 'grad_norm': 1.234375, 'learning_rate': 0.00015033959305645113, 'epoch': 0.24}\n",
      "{'loss': 1.2374, 'grad_norm': 0.5, 'learning_rate': 0.00015032823067898728, 'epoch': 0.24}\n",
      "{'loss': 1.0424, 'grad_norm': 0.6015625, 'learning_rate': 0.00015031686830152343, 'epoch': 0.24}\n",
      "{'loss': 1.349, 'grad_norm': 0.625, 'learning_rate': 0.00015030550592405956, 'epoch': 0.24}\n",
      "{'loss': 1.2006, 'grad_norm': 0.83984375, 'learning_rate': 0.0001502941435465957, 'epoch': 0.24}\n",
      "{'loss': 1.1488, 'grad_norm': 0.7890625, 'learning_rate': 0.00015028278116913183, 'epoch': 0.24}\n",
      "{'loss': 1.4143, 'grad_norm': 0.58203125, 'learning_rate': 0.00015027141879166798, 'epoch': 0.24}\n",
      "{'loss': 1.189, 'grad_norm': 0.51953125, 'learning_rate': 0.0001502600564142041, 'epoch': 0.24}\n",
      "{'loss': 1.1851, 'grad_norm': 0.546875, 'learning_rate': 0.00015024869403674026, 'epoch': 0.24}\n",
      "{'loss': 1.2669, 'grad_norm': 0.73828125, 'learning_rate': 0.0001502373316592764, 'epoch': 0.24}\n",
      "{'loss': 1.1171, 'grad_norm': 0.734375, 'learning_rate': 0.00015022596928181253, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3466, 'grad_norm': 0.447265625, 'learning_rate': 0.00015021460690434868, 'epoch': 0.24}\n",
      "{'loss': 1.1706, 'grad_norm': 0.64453125, 'learning_rate': 0.0001502032445268848, 'epoch': 0.24}\n",
      "{'loss': 1.3304, 'grad_norm': 0.431640625, 'learning_rate': 0.00015019188214942096, 'epoch': 0.24}\n",
      "{'loss': 1.1749, 'grad_norm': 0.455078125, 'learning_rate': 0.00015018051977195709, 'epoch': 0.24}\n",
      "{'loss': 1.0733, 'grad_norm': 0.76171875, 'learning_rate': 0.0001501691573944932, 'epoch': 0.24}\n",
      "{'loss': 1.2793, 'grad_norm': 0.48046875, 'learning_rate': 0.0001501577950170294, 'epoch': 0.24}\n",
      "{'loss': 1.1952, 'grad_norm': 1.1015625, 'learning_rate': 0.0001501464326395655, 'epoch': 0.24}\n",
      "{'loss': 1.1788, 'grad_norm': 0.5078125, 'learning_rate': 0.00015013507026210166, 'epoch': 0.24}\n",
      "{'loss': 1.1662, 'grad_norm': 0.458984375, 'learning_rate': 0.0001501237078846378, 'epoch': 0.24}\n",
      "{'loss': 1.0497, 'grad_norm': 0.8515625, 'learning_rate': 0.0001501123455071739, 'epoch': 0.24}\n",
      "{'loss': 1.4017, 'grad_norm': 0.51953125, 'learning_rate': 0.00015010098312971006, 'epoch': 0.24}\n",
      "{'loss': 1.1413, 'grad_norm': 0.8828125, 'learning_rate': 0.0001500896207522462, 'epoch': 0.24}\n",
      "{'loss': 1.12, 'grad_norm': 0.373046875, 'learning_rate': 0.00015007825837478237, 'epoch': 0.24}\n",
      "{'loss': 1.3247, 'grad_norm': 0.5234375, 'learning_rate': 0.0001500668959973185, 'epoch': 0.24}\n",
      "{'loss': 1.1768, 'grad_norm': 1.2109375, 'learning_rate': 0.00015005553361985461, 'epoch': 0.25}\n",
      "{'loss': 1.4068, 'grad_norm': 0.4375, 'learning_rate': 0.00015004417124239077, 'epoch': 0.25}\n",
      "{'loss': 1.3471, 'grad_norm': 0.71484375, 'learning_rate': 0.0001500328088649269, 'epoch': 0.25}\n",
      "{'loss': 1.1585, 'grad_norm': 0.52734375, 'learning_rate': 0.00015002144648746304, 'epoch': 0.25}\n",
      "{'loss': 1.3069, 'grad_norm': 0.435546875, 'learning_rate': 0.0001500100841099992, 'epoch': 0.25}\n",
      "{'loss': 1.0395, 'grad_norm': 0.419921875, 'learning_rate': 0.00014999872173253532, 'epoch': 0.25}\n",
      "{'loss': 1.2658, 'grad_norm': 0.466796875, 'learning_rate': 0.00014998735935507147, 'epoch': 0.25}\n",
      "{'loss': 1.1674, 'grad_norm': 0.71875, 'learning_rate': 0.0001499759969776076, 'epoch': 0.25}\n",
      "{'loss': 1.1711, 'grad_norm': 0.37890625, 'learning_rate': 0.00014996463460014374, 'epoch': 0.25}\n",
      "{'loss': 1.2624, 'grad_norm': 0.5234375, 'learning_rate': 0.00014995327222267987, 'epoch': 0.25}\n",
      "{'loss': 1.0541, 'grad_norm': 0.50390625, 'learning_rate': 0.00014994190984521602, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4551, 'grad_norm': 0.62890625, 'learning_rate': 0.00014993054746775217, 'epoch': 0.25}\n",
      "{'loss': 1.2179, 'grad_norm': 0.78515625, 'learning_rate': 0.0001499191850902883, 'epoch': 0.25}\n",
      "{'loss': 1.2495, 'grad_norm': 0.53125, 'learning_rate': 0.00014990782271282445, 'epoch': 0.25}\n",
      "{'loss': 1.2471, 'grad_norm': 0.49609375, 'learning_rate': 0.00014989646033536057, 'epoch': 0.25}\n",
      "{'loss': 0.965, 'grad_norm': 0.53125, 'learning_rate': 0.00014988509795789672, 'epoch': 0.25}\n",
      "{'loss': 1.2563, 'grad_norm': 0.455078125, 'learning_rate': 0.00014987373558043285, 'epoch': 0.25}\n",
      "{'loss': 1.1893, 'grad_norm': 0.6875, 'learning_rate': 0.000149862373202969, 'epoch': 0.25}\n",
      "{'loss': 1.3251, 'grad_norm': 0.353515625, 'learning_rate': 0.00014985101082550515, 'epoch': 0.25}\n",
      "{'loss': 1.1881, 'grad_norm': 0.6484375, 'learning_rate': 0.00014983964844804127, 'epoch': 0.25}\n",
      "{'loss': 1.1179, 'grad_norm': 0.87109375, 'learning_rate': 0.00014982828607057743, 'epoch': 0.25}\n",
      "{'loss': 1.294, 'grad_norm': 0.408203125, 'learning_rate': 0.00014981692369311355, 'epoch': 0.25}\n",
      "{'loss': 1.1731, 'grad_norm': 1.1484375, 'learning_rate': 0.0001498055613156497, 'epoch': 0.25}\n",
      "{'loss': 1.3166, 'grad_norm': 0.5, 'learning_rate': 0.00014979419893818583, 'epoch': 0.25}\n",
      "{'loss': 1.1331, 'grad_norm': 0.5078125, 'learning_rate': 0.00014978283656072195, 'epoch': 0.25}\n",
      "{'loss': 1.0639, 'grad_norm': 0.67578125, 'learning_rate': 0.00014977147418325813, 'epoch': 0.25}\n",
      "{'loss': 1.2503, 'grad_norm': 0.53515625, 'learning_rate': 0.00014976011180579425, 'epoch': 0.25}\n",
      "{'loss': 1.1512, 'grad_norm': 0.65234375, 'learning_rate': 0.0001497487494283304, 'epoch': 0.25}\n",
      "{'loss': 1.0802, 'grad_norm': 0.388671875, 'learning_rate': 0.00014973738705086653, 'epoch': 0.25}\n",
      "{'loss': 1.2479, 'grad_norm': 0.796875, 'learning_rate': 0.00014972602467340265, 'epoch': 0.25}\n",
      "{'loss': 1.0505, 'grad_norm': 0.490234375, 'learning_rate': 0.0001497146622959388, 'epoch': 0.25}\n",
      "{'loss': 1.2308, 'grad_norm': 0.671875, 'learning_rate': 0.00014970329991847496, 'epoch': 0.25}\n",
      "{'loss': 1.1727, 'grad_norm': 0.5859375, 'learning_rate': 0.0001496919375410111, 'epoch': 0.25}\n",
      "{'loss': 1.3296, 'grad_norm': 0.3984375, 'learning_rate': 0.00014968057516354723, 'epoch': 0.25}\n",
      "{'loss': 1.2511, 'grad_norm': 0.57421875, 'learning_rate': 0.00014966921278608336, 'epoch': 0.25}\n",
      "{'loss': 1.0155, 'grad_norm': 0.640625, 'learning_rate': 0.0001496578504086195, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4544, 'grad_norm': 0.455078125, 'learning_rate': 0.00014964648803115563, 'epoch': 0.25}\n",
      "{'loss': 1.2149, 'grad_norm': 0.7890625, 'learning_rate': 0.00014963512565369178, 'epoch': 0.25}\n",
      "{'loss': 1.1653, 'grad_norm': 0.427734375, 'learning_rate': 0.00014962376327622793, 'epoch': 0.25}\n",
      "{'loss': 1.129, 'grad_norm': 0.6171875, 'learning_rate': 0.00014961240089876406, 'epoch': 0.25}\n",
      "{'loss': 1.1171, 'grad_norm': 0.74609375, 'learning_rate': 0.0001496010385213002, 'epoch': 0.25}\n",
      "{'loss': 1.3794, 'grad_norm': 0.4921875, 'learning_rate': 0.00014958967614383633, 'epoch': 0.25}\n",
      "{'loss': 1.227, 'grad_norm': 0.64453125, 'learning_rate': 0.00014957831376637249, 'epoch': 0.25}\n",
      "{'loss': 1.1004, 'grad_norm': 0.482421875, 'learning_rate': 0.0001495669513889086, 'epoch': 0.25}\n",
      "{'loss': 1.2917, 'grad_norm': 0.5859375, 'learning_rate': 0.00014955558901144476, 'epoch': 0.25}\n",
      "{'loss': 1.0575, 'grad_norm': 0.859375, 'learning_rate': 0.0001495442266339809, 'epoch': 0.25}\n",
      "{'loss': 1.379, 'grad_norm': 0.5859375, 'learning_rate': 0.00014953286425651704, 'epoch': 0.25}\n",
      "{'loss': 1.2464, 'grad_norm': 0.84765625, 'learning_rate': 0.0001495215018790532, 'epoch': 0.25}\n",
      "{'loss': 1.3032, 'grad_norm': 0.455078125, 'learning_rate': 0.0001495101395015893, 'epoch': 0.25}\n",
      "{'loss': 1.181, 'grad_norm': 0.53125, 'learning_rate': 0.00014949877712412546, 'epoch': 0.25}\n",
      "{'loss': 1.0278, 'grad_norm': 0.73046875, 'learning_rate': 0.0001494874147466616, 'epoch': 0.25}\n",
      "{'loss': 1.3545, 'grad_norm': 0.43359375, 'learning_rate': 0.00014947605236919774, 'epoch': 0.25}\n",
      "{'loss': 1.0914, 'grad_norm': 0.75, 'learning_rate': 0.0001494646899917339, 'epoch': 0.25}\n",
      "{'loss': 1.3328, 'grad_norm': 0.419921875, 'learning_rate': 0.00014945332761427002, 'epoch': 0.25}\n",
      "{'loss': 1.2271, 'grad_norm': 0.49609375, 'learning_rate': 0.00014944196523680617, 'epoch': 0.25}\n",
      "{'loss': 1.0851, 'grad_norm': 1.1796875, 'learning_rate': 0.0001494306028593423, 'epoch': 0.25}\n",
      "{'loss': 1.5391, 'grad_norm': 0.408203125, 'learning_rate': 0.00014941924048187844, 'epoch': 0.25}\n",
      "{'loss': 1.1281, 'grad_norm': 0.921875, 'learning_rate': 0.00014940787810441457, 'epoch': 0.25}\n",
      "{'loss': 1.2463, 'grad_norm': 0.5, 'learning_rate': 0.00014939651572695072, 'epoch': 0.25}\n",
      "{'loss': 1.2325, 'grad_norm': 0.58984375, 'learning_rate': 0.00014938515334948687, 'epoch': 0.25}\n",
      "{'loss': 0.9854, 'grad_norm': 0.5859375, 'learning_rate': 0.000149373790972023, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3995, 'grad_norm': 0.498046875, 'learning_rate': 0.00014936242859455915, 'epoch': 0.25}\n",
      "{'loss': 1.2155, 'grad_norm': 0.6953125, 'learning_rate': 0.00014935106621709527, 'epoch': 0.25}\n",
      "{'loss': 1.1145, 'grad_norm': 0.51171875, 'learning_rate': 0.0001493397038396314, 'epoch': 0.25}\n",
      "{'loss': 1.2414, 'grad_norm': 0.55859375, 'learning_rate': 0.00014932834146216755, 'epoch': 0.25}\n",
      "{'loss': 1.1983, 'grad_norm': 0.75, 'learning_rate': 0.0001493169790847037, 'epoch': 0.25}\n",
      "{'loss': 1.3237, 'grad_norm': 0.53125, 'learning_rate': 0.00014930561670723985, 'epoch': 0.25}\n",
      "{'loss': 1.1777, 'grad_norm': 0.458984375, 'learning_rate': 0.00014929425432977597, 'epoch': 0.25}\n",
      "{'loss': 1.2302, 'grad_norm': 0.443359375, 'learning_rate': 0.0001492828919523121, 'epoch': 0.25}\n",
      "{'loss': 1.1842, 'grad_norm': 0.490234375, 'learning_rate': 0.00014927152957484825, 'epoch': 0.25}\n",
      "{'loss': 1.1043, 'grad_norm': 0.5546875, 'learning_rate': 0.00014926016719738437, 'epoch': 0.25}\n",
      "{'loss': 1.4536, 'grad_norm': 0.60546875, 'learning_rate': 0.00014924880481992052, 'epoch': 0.25}\n",
      "{'loss': 1.1566, 'grad_norm': 0.48828125, 'learning_rate': 0.00014923744244245668, 'epoch': 0.25}\n",
      "{'loss': 1.4036, 'grad_norm': 0.46875, 'learning_rate': 0.0001492260800649928, 'epoch': 0.25}\n",
      "{'loss': 1.2356, 'grad_norm': 0.6171875, 'learning_rate': 0.00014921471768752895, 'epoch': 0.25}\n",
      "{'loss': 1.085, 'grad_norm': 0.5703125, 'learning_rate': 0.00014920335531006508, 'epoch': 0.25}\n",
      "{'loss': 1.3709, 'grad_norm': 0.625, 'learning_rate': 0.00014919199293260123, 'epoch': 0.25}\n",
      "{'loss': 1.2354, 'grad_norm': 0.640625, 'learning_rate': 0.00014918063055513735, 'epoch': 0.25}\n",
      "{'loss': 1.217, 'grad_norm': 0.419921875, 'learning_rate': 0.0001491692681776735, 'epoch': 0.25}\n",
      "{'loss': 1.4236, 'grad_norm': 0.74609375, 'learning_rate': 0.00014915790580020965, 'epoch': 0.25}\n",
      "{'loss': 1.0771, 'grad_norm': 0.7421875, 'learning_rate': 0.00014914654342274578, 'epoch': 0.25}\n",
      "{'loss': 1.3306, 'grad_norm': 0.5390625, 'learning_rate': 0.00014913518104528193, 'epoch': 0.25}\n",
      "{'loss': 1.2159, 'grad_norm': 0.765625, 'learning_rate': 0.00014912381866781805, 'epoch': 0.25}\n",
      "{'loss': 1.1965, 'grad_norm': 0.609375, 'learning_rate': 0.0001491124562903542, 'epoch': 0.25}\n",
      "{'loss': 1.2202, 'grad_norm': 0.609375, 'learning_rate': 0.00014910109391289033, 'epoch': 0.25}\n",
      "{'loss': 1.1229, 'grad_norm': 0.4765625, 'learning_rate': 0.00014908973153542648, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3499, 'grad_norm': 0.59375, 'learning_rate': 0.00014907836915796263, 'epoch': 0.25}\n",
      "{'loss': 1.3153, 'grad_norm': 0.515625, 'learning_rate': 0.00014906700678049876, 'epoch': 0.25}\n",
      "{'loss': 1.1563, 'grad_norm': 0.76171875, 'learning_rate': 0.0001490556444030349, 'epoch': 0.25}\n",
      "{'loss': 1.1933, 'grad_norm': 0.515625, 'learning_rate': 0.00014904428202557103, 'epoch': 0.25}\n",
      "{'loss': 1.1314, 'grad_norm': 0.6796875, 'learning_rate': 0.00014903291964810718, 'epoch': 0.25}\n",
      "{'loss': 1.3963, 'grad_norm': 0.734375, 'learning_rate': 0.0001490215572706433, 'epoch': 0.25}\n",
      "{'loss': 1.1502, 'grad_norm': 0.53515625, 'learning_rate': 0.00014901019489317946, 'epoch': 0.25}\n",
      "{'loss': 1.2312, 'grad_norm': 0.458984375, 'learning_rate': 0.0001489988325157156, 'epoch': 0.25}\n",
      "{'loss': 1.2566, 'grad_norm': 0.703125, 'learning_rate': 0.00014898747013825174, 'epoch': 0.25}\n",
      "{'loss': 0.9751, 'grad_norm': 0.609375, 'learning_rate': 0.0001489761077607879, 'epoch': 0.25}\n",
      "{'loss': 1.2968, 'grad_norm': 0.4765625, 'learning_rate': 0.000148964745383324, 'epoch': 0.25}\n",
      "{'loss': 1.1849, 'grad_norm': 0.578125, 'learning_rate': 0.00014895338300586014, 'epoch': 0.25}\n",
      "{'loss': 1.3784, 'grad_norm': 0.361328125, 'learning_rate': 0.0001489420206283963, 'epoch': 0.25}\n",
      "{'loss': 1.2286, 'grad_norm': 0.5234375, 'learning_rate': 0.00014893065825093244, 'epoch': 0.25}\n",
      "{'loss': 1.1461, 'grad_norm': 0.52734375, 'learning_rate': 0.0001489192958734686, 'epoch': 0.25}\n",
      "{'loss': 1.3363, 'grad_norm': 0.5390625, 'learning_rate': 0.00014890793349600471, 'epoch': 0.25}\n",
      "{'loss': 1.1513, 'grad_norm': 0.53515625, 'learning_rate': 0.00014889657111854084, 'epoch': 0.25}\n",
      "{'loss': 1.1987, 'grad_norm': 0.474609375, 'learning_rate': 0.000148885208741077, 'epoch': 0.25}\n",
      "{'loss': 1.2172, 'grad_norm': 0.5546875, 'learning_rate': 0.00014887384636361311, 'epoch': 0.25}\n",
      "{'loss': 1.1665, 'grad_norm': 0.82421875, 'learning_rate': 0.00014886248398614927, 'epoch': 0.25}\n",
      "{'loss': 1.3091, 'grad_norm': 0.57421875, 'learning_rate': 0.00014885112160868542, 'epoch': 0.25}\n",
      "{'loss': 1.0677, 'grad_norm': 0.60546875, 'learning_rate': 0.00014883975923122154, 'epoch': 0.25}\n",
      "{'loss': 1.2723, 'grad_norm': 0.328125, 'learning_rate': 0.0001488283968537577, 'epoch': 0.25}\n",
      "{'loss': 1.1873, 'grad_norm': 0.427734375, 'learning_rate': 0.00014881703447629382, 'epoch': 0.25}\n",
      "{'loss': 1.2126, 'grad_norm': 0.74609375, 'learning_rate': 0.00014880567209882997, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3941, 'grad_norm': 0.484375, 'learning_rate': 0.0001487943097213661, 'epoch': 0.25}\n",
      "{'loss': 1.2117, 'grad_norm': 0.60546875, 'learning_rate': 0.00014878294734390224, 'epoch': 0.25}\n",
      "{'loss': 1.3288, 'grad_norm': 0.48046875, 'learning_rate': 0.0001487715849664384, 'epoch': 0.25}\n",
      "{'loss': 1.2776, 'grad_norm': 0.7578125, 'learning_rate': 0.00014876022258897452, 'epoch': 0.25}\n",
      "{'loss': 0.9892, 'grad_norm': 1.0625, 'learning_rate': 0.00014874886021151067, 'epoch': 0.25}\n",
      "{'loss': 1.4115, 'grad_norm': 0.49609375, 'learning_rate': 0.0001487374978340468, 'epoch': 0.25}\n",
      "{'loss': 1.2532, 'grad_norm': 0.63671875, 'learning_rate': 0.00014872613545658295, 'epoch': 0.25}\n",
      "{'loss': 1.1776, 'grad_norm': 0.41015625, 'learning_rate': 0.00014871477307911907, 'epoch': 0.25}\n",
      "{'loss': 1.1583, 'grad_norm': 0.59375, 'learning_rate': 0.00014870341070165522, 'epoch': 0.25}\n",
      "{'loss': 1.0652, 'grad_norm': 0.9296875, 'learning_rate': 0.00014869204832419137, 'epoch': 0.25}\n",
      "{'loss': 1.3114, 'grad_norm': 0.462890625, 'learning_rate': 0.0001486806859467275, 'epoch': 0.25}\n",
      "{'loss': 1.1286, 'grad_norm': 1.0390625, 'learning_rate': 0.00014866932356926365, 'epoch': 0.25}\n",
      "{'loss': 1.273, 'grad_norm': 0.427734375, 'learning_rate': 0.00014865796119179977, 'epoch': 0.25}\n",
      "{'loss': 1.2842, 'grad_norm': 0.58984375, 'learning_rate': 0.00014864659881433593, 'epoch': 0.25}\n",
      "{'loss': 1.0334, 'grad_norm': 0.6796875, 'learning_rate': 0.00014863523643687205, 'epoch': 0.25}\n",
      "{'loss': 1.353, 'grad_norm': 0.53515625, 'learning_rate': 0.0001486238740594082, 'epoch': 0.25}\n",
      "{'loss': 1.2432, 'grad_norm': 0.58984375, 'learning_rate': 0.00014861251168194435, 'epoch': 0.25}\n",
      "{'loss': 1.2234, 'grad_norm': 0.4765625, 'learning_rate': 0.00014860114930448048, 'epoch': 0.25}\n",
      "{'loss': 1.1943, 'grad_norm': 0.494140625, 'learning_rate': 0.00014858978692701663, 'epoch': 0.25}\n",
      "{'loss': 1.062, 'grad_norm': 0.79296875, 'learning_rate': 0.00014857842454955275, 'epoch': 0.25}\n",
      "{'loss': 1.3632, 'grad_norm': 0.49609375, 'learning_rate': 0.00014856706217208888, 'epoch': 0.25}\n",
      "{'loss': 1.1602, 'grad_norm': 0.6328125, 'learning_rate': 0.00014855569979462503, 'epoch': 0.25}\n",
      "{'loss': 1.3122, 'grad_norm': 0.388671875, 'learning_rate': 0.00014854433741716118, 'epoch': 0.25}\n",
      "{'loss': 1.3569, 'grad_norm': 0.5703125, 'learning_rate': 0.00014853297503969733, 'epoch': 0.25}\n",
      "{'loss': 1.0295, 'grad_norm': 0.302734375, 'learning_rate': 0.00014852161266223346, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3416, 'grad_norm': 0.91015625, 'learning_rate': 0.00014851025028476958, 'epoch': 0.25}\n",
      "{'loss': 1.2233, 'grad_norm': 0.71484375, 'learning_rate': 0.00014849888790730573, 'epoch': 0.25}\n",
      "{'loss': 1.3681, 'grad_norm': 0.443359375, 'learning_rate': 0.00014848752552984186, 'epoch': 0.25}\n",
      "{'loss': 1.2214, 'grad_norm': 0.451171875, 'learning_rate': 0.000148476163152378, 'epoch': 0.25}\n",
      "{'loss': 1.1618, 'grad_norm': 0.7734375, 'learning_rate': 0.00014846480077491416, 'epoch': 0.25}\n",
      "{'loss': 1.3686, 'grad_norm': 0.640625, 'learning_rate': 0.00014845343839745028, 'epoch': 0.25}\n",
      "{'loss': 1.2393, 'grad_norm': 0.859375, 'learning_rate': 0.00014844207601998643, 'epoch': 0.25}\n",
      "{'loss': 1.3098, 'grad_norm': 0.486328125, 'learning_rate': 0.00014843071364252256, 'epoch': 0.25}\n",
      "{'loss': 1.2654, 'grad_norm': 0.51953125, 'learning_rate': 0.0001484193512650587, 'epoch': 0.25}\n",
      "{'loss': 1.1285, 'grad_norm': 0.984375, 'learning_rate': 0.00014840798888759483, 'epoch': 0.25}\n",
      "{'loss': 1.2957, 'grad_norm': 0.5390625, 'learning_rate': 0.00014839662651013099, 'epoch': 0.25}\n",
      "{'loss': 1.2182, 'grad_norm': 0.57421875, 'learning_rate': 0.00014838526413266714, 'epoch': 0.25}\n",
      "{'loss': 1.1866, 'grad_norm': 0.388671875, 'learning_rate': 0.00014837390175520326, 'epoch': 0.25}\n",
      "{'loss': 1.2642, 'grad_norm': 0.61328125, 'learning_rate': 0.0001483625393777394, 'epoch': 0.25}\n",
      "{'loss': 1.1562, 'grad_norm': 0.70703125, 'learning_rate': 0.00014835117700027554, 'epoch': 0.25}\n",
      "{'loss': 1.3524, 'grad_norm': 0.498046875, 'learning_rate': 0.0001483398146228117, 'epoch': 0.25}\n",
      "{'loss': 1.0896, 'grad_norm': 0.53125, 'learning_rate': 0.0001483284522453478, 'epoch': 0.25}\n",
      "{'loss': 1.2498, 'grad_norm': 0.384765625, 'learning_rate': 0.00014831708986788396, 'epoch': 0.25}\n",
      "{'loss': 1.2974, 'grad_norm': 0.52734375, 'learning_rate': 0.00014830572749042012, 'epoch': 0.25}\n",
      "{'loss': 1.0389, 'grad_norm': 0.53515625, 'learning_rate': 0.00014829436511295624, 'epoch': 0.25}\n",
      "{'loss': 1.48, 'grad_norm': 0.41015625, 'learning_rate': 0.0001482830027354924, 'epoch': 0.25}\n",
      "{'loss': 1.2328, 'grad_norm': 0.703125, 'learning_rate': 0.00014827164035802852, 'epoch': 0.25}\n",
      "{'loss': 1.3277, 'grad_norm': 0.40625, 'learning_rate': 0.00014826027798056467, 'epoch': 0.25}\n",
      "{'loss': 1.2795, 'grad_norm': 0.4375, 'learning_rate': 0.0001482489156031008, 'epoch': 0.25}\n",
      "{'loss': 0.9427, 'grad_norm': 0.421875, 'learning_rate': 0.00014823755322563694, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3805, 'grad_norm': 0.65625, 'learning_rate': 0.0001482261908481731, 'epoch': 0.25}\n",
      "{'loss': 1.1713, 'grad_norm': 0.95703125, 'learning_rate': 0.00014821482847070922, 'epoch': 0.25}\n",
      "{'loss': 1.2185, 'grad_norm': 0.41015625, 'learning_rate': 0.00014820346609324537, 'epoch': 0.25}\n",
      "{'loss': 1.3214, 'grad_norm': 0.53515625, 'learning_rate': 0.0001481921037157815, 'epoch': 0.25}\n",
      "{'loss': 1.1555, 'grad_norm': 0.69921875, 'learning_rate': 0.00014818074133831762, 'epoch': 0.25}\n",
      "{'loss': 1.4421, 'grad_norm': 0.52734375, 'learning_rate': 0.00014816937896085377, 'epoch': 0.25}\n",
      "{'loss': 1.0741, 'grad_norm': 0.4765625, 'learning_rate': 0.00014815801658338992, 'epoch': 0.25}\n",
      "{'loss': 1.222, 'grad_norm': 0.421875, 'learning_rate': 0.00014814665420592607, 'epoch': 0.25}\n",
      "{'loss': 1.3337, 'grad_norm': 0.609375, 'learning_rate': 0.0001481352918284622, 'epoch': 0.25}\n",
      "{'loss': 1.1892, 'grad_norm': 0.765625, 'learning_rate': 0.00014812392945099832, 'epoch': 0.25}\n",
      "{'loss': 1.3188, 'grad_norm': 0.455078125, 'learning_rate': 0.00014811256707353447, 'epoch': 0.25}\n",
      "{'loss': 1.1547, 'grad_norm': 0.6328125, 'learning_rate': 0.0001481012046960706, 'epoch': 0.25}\n",
      "{'loss': 1.2469, 'grad_norm': 0.4296875, 'learning_rate': 0.00014808984231860675, 'epoch': 0.25}\n",
      "{'loss': 1.2494, 'grad_norm': 0.6484375, 'learning_rate': 0.0001480784799411429, 'epoch': 0.25}\n",
      "{'loss': 1.0477, 'grad_norm': 0.6328125, 'learning_rate': 0.00014806711756367902, 'epoch': 0.25}\n",
      "{'loss': 1.3139, 'grad_norm': 0.447265625, 'learning_rate': 0.00014805575518621518, 'epoch': 0.25}\n",
      "{'loss': 1.2376, 'grad_norm': 0.48046875, 'learning_rate': 0.0001480443928087513, 'epoch': 0.25}\n",
      "{'loss': 1.3297, 'grad_norm': 0.412109375, 'learning_rate': 0.00014803303043128745, 'epoch': 0.25}\n",
      "{'loss': 1.2433, 'grad_norm': 0.70703125, 'learning_rate': 0.00014802166805382358, 'epoch': 0.25}\n",
      "{'loss': 1.0515, 'grad_norm': 0.546875, 'learning_rate': 0.00014801030567635973, 'epoch': 0.25}\n",
      "{'loss': 1.3619, 'grad_norm': 0.56640625, 'learning_rate': 0.00014799894329889588, 'epoch': 0.25}\n",
      "{'loss': 1.2606, 'grad_norm': 0.412109375, 'learning_rate': 0.000147987580921432, 'epoch': 0.25}\n",
      "{'loss': 1.2865, 'grad_norm': 0.416015625, 'learning_rate': 0.00014797621854396815, 'epoch': 0.25}\n",
      "{'loss': 1.2341, 'grad_norm': 0.578125, 'learning_rate': 0.00014796485616650428, 'epoch': 0.25}\n",
      "{'loss': 1.1601, 'grad_norm': 0.859375, 'learning_rate': 0.00014795349378904043, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4661, 'grad_norm': 0.5625, 'learning_rate': 0.00014794213141157655, 'epoch': 0.25}\n",
      "{'loss': 1.151, 'grad_norm': 0.50390625, 'learning_rate': 0.0001479307690341127, 'epoch': 0.25}\n",
      "{'loss': 1.2604, 'grad_norm': 0.400390625, 'learning_rate': 0.00014791940665664886, 'epoch': 0.25}\n",
      "{'loss': 1.2549, 'grad_norm': 0.59375, 'learning_rate': 0.00014790804427918498, 'epoch': 0.25}\n",
      "{'loss': 1.0904, 'grad_norm': 1.125, 'learning_rate': 0.00014789668190172113, 'epoch': 0.25}\n",
      "{'loss': 1.3391, 'grad_norm': 0.74609375, 'learning_rate': 0.00014788531952425726, 'epoch': 0.25}\n",
      "{'loss': 1.176, 'grad_norm': 0.74609375, 'learning_rate': 0.0001478739571467934, 'epoch': 0.25}\n",
      "{'loss': 1.1616, 'grad_norm': 0.439453125, 'learning_rate': 0.00014786259476932953, 'epoch': 0.25}\n",
      "{'loss': 1.278, 'grad_norm': 0.5, 'learning_rate': 0.00014785123239186568, 'epoch': 0.25}\n",
      "{'loss': 0.9703, 'grad_norm': 0.88671875, 'learning_rate': 0.00014783987001440184, 'epoch': 0.25}\n",
      "{'loss': 1.3644, 'grad_norm': 0.400390625, 'learning_rate': 0.00014782850763693796, 'epoch': 0.25}\n",
      "{'loss': 1.1732, 'grad_norm': 0.79296875, 'learning_rate': 0.0001478171452594741, 'epoch': 0.25}\n",
      "{'loss': 1.2851, 'grad_norm': 0.53125, 'learning_rate': 0.00014780578288201024, 'epoch': 0.25}\n",
      "{'loss': 1.3135, 'grad_norm': 0.59375, 'learning_rate': 0.00014779442050454636, 'epoch': 0.25}\n",
      "{'loss': 1.1378, 'grad_norm': 0.76171875, 'learning_rate': 0.0001477830581270825, 'epoch': 0.25}\n",
      "{'loss': 1.3991, 'grad_norm': 0.482421875, 'learning_rate': 0.00014777169574961866, 'epoch': 0.25}\n",
      "{'loss': 1.0904, 'grad_norm': 0.55078125, 'learning_rate': 0.00014776033337215481, 'epoch': 0.26}\n",
      "{'loss': 1.2111, 'grad_norm': 0.427734375, 'learning_rate': 0.00014774897099469094, 'epoch': 0.26}\n",
      "{'loss': 1.3267, 'grad_norm': 0.58984375, 'learning_rate': 0.00014773760861722706, 'epoch': 0.26}\n",
      "{'loss': 1.1525, 'grad_norm': 0.6875, 'learning_rate': 0.00014772624623976321, 'epoch': 0.26}\n",
      "{'loss': 1.3834, 'grad_norm': 0.69140625, 'learning_rate': 0.00014771488386229934, 'epoch': 0.26}\n",
      "{'loss': 1.1032, 'grad_norm': 0.6015625, 'learning_rate': 0.0001477035214848355, 'epoch': 0.26}\n",
      "{'loss': 1.2485, 'grad_norm': 0.52734375, 'learning_rate': 0.00014769215910737164, 'epoch': 0.26}\n",
      "{'loss': 1.2255, 'grad_norm': 0.4921875, 'learning_rate': 0.00014768079672990777, 'epoch': 0.26}\n",
      "{'loss': 1.0149, 'grad_norm': 0.6796875, 'learning_rate': 0.00014766943435244392, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3184, 'grad_norm': 0.451171875, 'learning_rate': 0.00014765807197498004, 'epoch': 0.26}\n",
      "{'loss': 1.283, 'grad_norm': 0.6640625, 'learning_rate': 0.0001476467095975162, 'epoch': 0.26}\n",
      "{'loss': 1.326, 'grad_norm': 0.439453125, 'learning_rate': 0.00014763534722005232, 'epoch': 0.26}\n",
      "{'loss': 1.2803, 'grad_norm': 0.6171875, 'learning_rate': 0.00014762398484258847, 'epoch': 0.26}\n",
      "{'loss': 1.048, 'grad_norm': 0.87890625, 'learning_rate': 0.00014761262246512462, 'epoch': 0.26}\n",
      "{'loss': 1.4995, 'grad_norm': 0.55078125, 'learning_rate': 0.00014760126008766074, 'epoch': 0.26}\n",
      "{'loss': 1.3639, 'grad_norm': 0.65625, 'learning_rate': 0.0001475898977101969, 'epoch': 0.26}\n",
      "{'loss': 1.264, 'grad_norm': 0.54296875, 'learning_rate': 0.00014757853533273302, 'epoch': 0.26}\n",
      "{'loss': 1.2408, 'grad_norm': 0.515625, 'learning_rate': 0.00014756717295526917, 'epoch': 0.26}\n",
      "{'loss': 1.1352, 'grad_norm': 0.76171875, 'learning_rate': 0.0001475558105778053, 'epoch': 0.26}\n",
      "{'loss': 1.2899, 'grad_norm': 0.45703125, 'learning_rate': 0.00014754444820034145, 'epoch': 0.26}\n",
      "{'loss': 1.3088, 'grad_norm': 0.91796875, 'learning_rate': 0.0001475330858228776, 'epoch': 0.26}\n",
      "{'loss': 1.2598, 'grad_norm': 0.40234375, 'learning_rate': 0.00014752172344541372, 'epoch': 0.26}\n",
      "{'loss': 1.2565, 'grad_norm': 0.55078125, 'learning_rate': 0.00014751036106794987, 'epoch': 0.26}\n",
      "{'loss': 1.1619, 'grad_norm': 0.79296875, 'learning_rate': 0.000147498998690486, 'epoch': 0.26}\n",
      "{'loss': 1.3716, 'grad_norm': 0.46875, 'learning_rate': 0.00014748763631302215, 'epoch': 0.26}\n",
      "{'loss': 1.2735, 'grad_norm': 0.67578125, 'learning_rate': 0.00014747627393555827, 'epoch': 0.26}\n",
      "{'loss': 1.1583, 'grad_norm': 0.54296875, 'learning_rate': 0.00014746491155809443, 'epoch': 0.26}\n",
      "{'loss': 1.2958, 'grad_norm': 0.546875, 'learning_rate': 0.00014745354918063058, 'epoch': 0.26}\n",
      "{'loss': 1.143, 'grad_norm': 0.5390625, 'learning_rate': 0.0001474421868031667, 'epoch': 0.26}\n",
      "{'loss': 1.1942, 'grad_norm': 0.453125, 'learning_rate': 0.00014743082442570285, 'epoch': 0.26}\n",
      "{'loss': 1.1459, 'grad_norm': 0.66015625, 'learning_rate': 0.00014741946204823898, 'epoch': 0.26}\n",
      "{'loss': 1.2834, 'grad_norm': 0.55859375, 'learning_rate': 0.0001474080996707751, 'epoch': 0.26}\n",
      "{'loss': 1.3271, 'grad_norm': 0.484375, 'learning_rate': 0.00014739673729331125, 'epoch': 0.26}\n",
      "{'loss': 1.1533, 'grad_norm': 0.78515625, 'learning_rate': 0.0001473853749158474, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3969, 'grad_norm': 0.46875, 'learning_rate': 0.00014737401253838356, 'epoch': 0.26}\n",
      "{'loss': 1.1226, 'grad_norm': 0.52734375, 'learning_rate': 0.00014736265016091968, 'epoch': 0.26}\n",
      "{'loss': 1.2372, 'grad_norm': 0.4921875, 'learning_rate': 0.0001473512877834558, 'epoch': 0.26}\n",
      "{'loss': 1.2491, 'grad_norm': 0.5703125, 'learning_rate': 0.00014733992540599196, 'epoch': 0.26}\n",
      "{'loss': 1.0916, 'grad_norm': 0.62109375, 'learning_rate': 0.00014732856302852808, 'epoch': 0.26}\n",
      "{'loss': 1.4524, 'grad_norm': 0.482421875, 'learning_rate': 0.00014731720065106426, 'epoch': 0.26}\n",
      "{'loss': 1.243, 'grad_norm': 0.63671875, 'learning_rate': 0.00014730583827360038, 'epoch': 0.26}\n",
      "{'loss': 1.1956, 'grad_norm': 0.423828125, 'learning_rate': 0.0001472944758961365, 'epoch': 0.26}\n",
      "{'loss': 1.3182, 'grad_norm': 0.5078125, 'learning_rate': 0.00014728311351867266, 'epoch': 0.26}\n",
      "{'loss': 1.1371, 'grad_norm': 0.62890625, 'learning_rate': 0.00014727175114120878, 'epoch': 0.26}\n",
      "{'loss': 1.3001, 'grad_norm': 0.640625, 'learning_rate': 0.00014726038876374493, 'epoch': 0.26}\n",
      "{'loss': 1.1408, 'grad_norm': 0.6484375, 'learning_rate': 0.00014724902638628106, 'epoch': 0.26}\n",
      "{'loss': 1.2659, 'grad_norm': 0.443359375, 'learning_rate': 0.0001472376640088172, 'epoch': 0.26}\n",
      "{'loss': 1.3899, 'grad_norm': 0.5625, 'learning_rate': 0.00014722630163135336, 'epoch': 0.26}\n",
      "{'loss': 1.0979, 'grad_norm': 0.439453125, 'learning_rate': 0.00014721493925388949, 'epoch': 0.26}\n",
      "{'loss': 1.3021, 'grad_norm': 0.5546875, 'learning_rate': 0.00014720357687642564, 'epoch': 0.26}\n",
      "{'loss': 1.1348, 'grad_norm': 0.5625, 'learning_rate': 0.00014719221449896176, 'epoch': 0.26}\n",
      "{'loss': 1.2502, 'grad_norm': 0.51171875, 'learning_rate': 0.0001471808521214979, 'epoch': 0.26}\n",
      "{'loss': 1.2732, 'grad_norm': 0.8359375, 'learning_rate': 0.00014716948974403404, 'epoch': 0.26}\n",
      "{'loss': 1.0707, 'grad_norm': 0.271484375, 'learning_rate': 0.0001471581273665702, 'epoch': 0.26}\n",
      "{'loss': 1.4265, 'grad_norm': 0.453125, 'learning_rate': 0.00014714676498910634, 'epoch': 0.26}\n",
      "{'loss': 1.1721, 'grad_norm': 0.80859375, 'learning_rate': 0.00014713540261164246, 'epoch': 0.26}\n",
      "{'loss': 1.228, 'grad_norm': 0.4921875, 'learning_rate': 0.00014712404023417862, 'epoch': 0.26}\n",
      "{'loss': 1.371, 'grad_norm': 0.55859375, 'learning_rate': 0.00014711267785671474, 'epoch': 0.26}\n",
      "{'loss': 1.0951, 'grad_norm': 0.65625, 'learning_rate': 0.0001471013154792509, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2902, 'grad_norm': 0.66796875, 'learning_rate': 0.00014708995310178702, 'epoch': 0.26}\n",
      "{'loss': 1.1129, 'grad_norm': 0.74609375, 'learning_rate': 0.00014707859072432317, 'epoch': 0.26}\n",
      "{'loss': 1.1591, 'grad_norm': 0.4296875, 'learning_rate': 0.00014706722834685932, 'epoch': 0.26}\n",
      "{'loss': 1.2012, 'grad_norm': 0.5625, 'learning_rate': 0.00014705586596939544, 'epoch': 0.26}\n",
      "{'loss': 1.1086, 'grad_norm': 0.9609375, 'learning_rate': 0.0001470445035919316, 'epoch': 0.26}\n",
      "{'loss': 1.2807, 'grad_norm': 0.4921875, 'learning_rate': 0.00014703314121446772, 'epoch': 0.26}\n",
      "{'loss': 1.3037, 'grad_norm': 0.7890625, 'learning_rate': 0.00014702177883700384, 'epoch': 0.26}\n",
      "{'loss': 1.1402, 'grad_norm': 0.5390625, 'learning_rate': 0.00014701041645954, 'epoch': 0.26}\n",
      "{'loss': 1.1835, 'grad_norm': 0.53515625, 'learning_rate': 0.00014699905408207615, 'epoch': 0.26}\n",
      "{'loss': 0.8907, 'grad_norm': 0.5859375, 'learning_rate': 0.0001469876917046123, 'epoch': 0.26}\n",
      "{'loss': 1.4966, 'grad_norm': 0.482421875, 'learning_rate': 0.00014697632932714842, 'epoch': 0.26}\n",
      "{'loss': 1.2401, 'grad_norm': 0.5078125, 'learning_rate': 0.00014696496694968455, 'epoch': 0.26}\n",
      "{'loss': 1.2918, 'grad_norm': 0.51171875, 'learning_rate': 0.0001469536045722207, 'epoch': 0.26}\n",
      "{'loss': 1.267, 'grad_norm': 0.494140625, 'learning_rate': 0.00014694224219475682, 'epoch': 0.26}\n",
      "{'loss': 1.1583, 'grad_norm': 0.6953125, 'learning_rate': 0.000146930879817293, 'epoch': 0.26}\n",
      "{'loss': 1.3461, 'grad_norm': 0.546875, 'learning_rate': 0.00014691951743982912, 'epoch': 0.26}\n",
      "{'loss': 1.3282, 'grad_norm': 0.7578125, 'learning_rate': 0.00014690815506236525, 'epoch': 0.26}\n",
      "{'loss': 1.1675, 'grad_norm': 0.421875, 'learning_rate': 0.0001468967926849014, 'epoch': 0.26}\n",
      "{'loss': 1.407, 'grad_norm': 0.64453125, 'learning_rate': 0.00014688543030743752, 'epoch': 0.26}\n",
      "{'loss': 1.0066, 'grad_norm': 0.51171875, 'learning_rate': 0.00014687406792997367, 'epoch': 0.26}\n",
      "{'loss': 1.3392, 'grad_norm': 0.474609375, 'learning_rate': 0.0001468627055525098, 'epoch': 0.26}\n",
      "{'loss': 1.2401, 'grad_norm': 0.49609375, 'learning_rate': 0.00014685134317504595, 'epoch': 0.26}\n",
      "{'loss': 1.1564, 'grad_norm': 0.490234375, 'learning_rate': 0.0001468399807975821, 'epoch': 0.26}\n",
      "{'loss': 1.2827, 'grad_norm': 0.59765625, 'learning_rate': 0.00014682861842011823, 'epoch': 0.26}\n",
      "{'loss': 1.2366, 'grad_norm': 0.77734375, 'learning_rate': 0.00014681725604265438, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3413, 'grad_norm': 0.43359375, 'learning_rate': 0.0001468058936651905, 'epoch': 0.26}\n",
      "{'loss': 1.1736, 'grad_norm': 0.73046875, 'learning_rate': 0.00014679453128772665, 'epoch': 0.26}\n",
      "{'loss': 1.1202, 'grad_norm': 0.51953125, 'learning_rate': 0.00014678316891026278, 'epoch': 0.26}\n",
      "{'loss': 1.2189, 'grad_norm': 0.48046875, 'learning_rate': 0.00014677180653279893, 'epoch': 0.26}\n",
      "{'loss': 1.1498, 'grad_norm': 0.53125, 'learning_rate': 0.00014676044415533508, 'epoch': 0.26}\n",
      "{'loss': 1.313, 'grad_norm': 0.5234375, 'learning_rate': 0.0001467490817778712, 'epoch': 0.26}\n",
      "{'loss': 1.0595, 'grad_norm': 0.7421875, 'learning_rate': 0.00014673771940040736, 'epoch': 0.26}\n",
      "{'loss': 1.1918, 'grad_norm': 0.5, 'learning_rate': 0.00014672635702294348, 'epoch': 0.26}\n",
      "{'loss': 1.2048, 'grad_norm': 0.6015625, 'learning_rate': 0.00014671499464547963, 'epoch': 0.26}\n",
      "{'loss': 1.1136, 'grad_norm': 0.81640625, 'learning_rate': 0.00014670363226801576, 'epoch': 0.26}\n",
      "{'loss': 1.2835, 'grad_norm': 0.88671875, 'learning_rate': 0.0001466922698905519, 'epoch': 0.26}\n",
      "{'loss': 1.0317, 'grad_norm': 0.5390625, 'learning_rate': 0.00014668090751308806, 'epoch': 0.26}\n",
      "{'loss': 1.2232, 'grad_norm': 0.447265625, 'learning_rate': 0.00014666954513562418, 'epoch': 0.26}\n",
      "{'loss': 1.1874, 'grad_norm': 0.5078125, 'learning_rate': 0.00014665818275816033, 'epoch': 0.26}\n",
      "{'loss': 1.0387, 'grad_norm': 0.9453125, 'learning_rate': 0.00014664682038069646, 'epoch': 0.26}\n",
      "{'loss': 1.3982, 'grad_norm': 0.4296875, 'learning_rate': 0.00014663545800323258, 'epoch': 0.26}\n",
      "{'loss': 1.2391, 'grad_norm': 0.60546875, 'learning_rate': 0.00014662409562576876, 'epoch': 0.26}\n",
      "{'loss': 1.2645, 'grad_norm': 0.40625, 'learning_rate': 0.00014661273324830489, 'epoch': 0.26}\n",
      "{'loss': 1.4263, 'grad_norm': 1.0546875, 'learning_rate': 0.00014660137087084104, 'epoch': 0.26}\n",
      "{'loss': 1.1483, 'grad_norm': 0.8125, 'learning_rate': 0.00014659000849337716, 'epoch': 0.26}\n",
      "{'loss': 1.3267, 'grad_norm': 0.462890625, 'learning_rate': 0.00014657864611591329, 'epoch': 0.26}\n",
      "{'loss': 1.1263, 'grad_norm': 0.6484375, 'learning_rate': 0.00014656728373844944, 'epoch': 0.26}\n",
      "{'loss': 1.1781, 'grad_norm': 0.408203125, 'learning_rate': 0.00014655592136098556, 'epoch': 0.26}\n",
      "{'loss': 1.2646, 'grad_norm': 0.6015625, 'learning_rate': 0.00014654455898352174, 'epoch': 0.26}\n",
      "{'loss': 1.0811, 'grad_norm': 0.68359375, 'learning_rate': 0.00014653319660605786, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3031, 'grad_norm': 0.466796875, 'learning_rate': 0.000146521834228594, 'epoch': 0.26}\n",
      "{'loss': 1.2234, 'grad_norm': 0.703125, 'learning_rate': 0.00014651047185113014, 'epoch': 0.26}\n",
      "{'loss': 1.3208, 'grad_norm': 0.3984375, 'learning_rate': 0.00014649910947366626, 'epoch': 0.26}\n",
      "{'loss': 1.1949, 'grad_norm': 0.51953125, 'learning_rate': 0.00014648774709620242, 'epoch': 0.26}\n",
      "{'loss': 1.1822, 'grad_norm': 0.59375, 'learning_rate': 0.00014647638471873854, 'epoch': 0.26}\n",
      "{'loss': 1.4009, 'grad_norm': 0.68359375, 'learning_rate': 0.0001464650223412747, 'epoch': 0.26}\n",
      "{'loss': 1.1457, 'grad_norm': 0.482421875, 'learning_rate': 0.00014645365996381084, 'epoch': 0.26}\n",
      "{'loss': 1.1832, 'grad_norm': 0.400390625, 'learning_rate': 0.00014644229758634697, 'epoch': 0.26}\n",
      "{'loss': 1.1437, 'grad_norm': 0.7109375, 'learning_rate': 0.00014643093520888312, 'epoch': 0.26}\n",
      "{'loss': 1.0825, 'grad_norm': 0.6875, 'learning_rate': 0.00014641957283141924, 'epoch': 0.26}\n",
      "{'loss': 1.3505, 'grad_norm': 0.5546875, 'learning_rate': 0.0001464082104539554, 'epoch': 0.26}\n",
      "{'loss': 1.1669, 'grad_norm': 0.80078125, 'learning_rate': 0.00014639684807649152, 'epoch': 0.26}\n",
      "{'loss': 1.3004, 'grad_norm': 0.380859375, 'learning_rate': 0.00014638548569902767, 'epoch': 0.26}\n",
      "{'loss': 1.2949, 'grad_norm': 0.7890625, 'learning_rate': 0.00014637412332156382, 'epoch': 0.26}\n",
      "{'loss': 1.0506, 'grad_norm': 0.66796875, 'learning_rate': 0.00014636276094409995, 'epoch': 0.26}\n",
      "{'loss': 1.3599, 'grad_norm': 0.53515625, 'learning_rate': 0.0001463513985666361, 'epoch': 0.26}\n",
      "{'loss': 1.2587, 'grad_norm': 0.640625, 'learning_rate': 0.00014634003618917222, 'epoch': 0.26}\n",
      "{'loss': 1.2524, 'grad_norm': 0.423828125, 'learning_rate': 0.00014632867381170837, 'epoch': 0.26}\n",
      "{'loss': 1.3167, 'grad_norm': 0.7265625, 'learning_rate': 0.0001463173114342445, 'epoch': 0.26}\n",
      "{'loss': 1.1127, 'grad_norm': 0.71484375, 'learning_rate': 0.00014630594905678065, 'epoch': 0.26}\n",
      "{'loss': 1.3425, 'grad_norm': 0.625, 'learning_rate': 0.0001462945866793168, 'epoch': 0.26}\n",
      "{'loss': 1.1999, 'grad_norm': 0.69140625, 'learning_rate': 0.00014628322430185292, 'epoch': 0.26}\n",
      "{'loss': 1.329, 'grad_norm': 0.380859375, 'learning_rate': 0.00014627186192438908, 'epoch': 0.26}\n",
      "{'loss': 1.4923, 'grad_norm': 0.71484375, 'learning_rate': 0.0001462604995469252, 'epoch': 0.26}\n",
      "{'loss': 1.0588, 'grad_norm': 0.79296875, 'learning_rate': 0.00014624913716946132, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4047, 'grad_norm': 0.3984375, 'learning_rate': 0.0001462377747919975, 'epoch': 0.26}\n",
      "{'loss': 1.2595, 'grad_norm': 0.68359375, 'learning_rate': 0.00014622641241453363, 'epoch': 0.26}\n",
      "{'loss': 1.1563, 'grad_norm': 0.5390625, 'learning_rate': 0.00014621505003706978, 'epoch': 0.26}\n",
      "{'loss': 1.301, 'grad_norm': 0.55859375, 'learning_rate': 0.0001462036876596059, 'epoch': 0.26}\n",
      "{'loss': 1.0657, 'grad_norm': 0.5703125, 'learning_rate': 0.00014619232528214203, 'epoch': 0.26}\n",
      "{'loss': 1.415, 'grad_norm': 0.396484375, 'learning_rate': 0.00014618096290467818, 'epoch': 0.26}\n",
      "{'loss': 1.1516, 'grad_norm': 0.55859375, 'learning_rate': 0.0001461696005272143, 'epoch': 0.26}\n",
      "{'loss': 1.2135, 'grad_norm': 0.421875, 'learning_rate': 0.00014615823814975048, 'epoch': 0.26}\n",
      "{'loss': 1.3639, 'grad_norm': 0.61328125, 'learning_rate': 0.0001461468757722866, 'epoch': 0.26}\n",
      "{'loss': 1.1575, 'grad_norm': 0.9453125, 'learning_rate': 0.00014613551339482273, 'epoch': 0.26}\n",
      "{'loss': 1.4027, 'grad_norm': 0.453125, 'learning_rate': 0.00014612415101735888, 'epoch': 0.26}\n",
      "{'loss': 1.1826, 'grad_norm': 0.7109375, 'learning_rate': 0.000146112788639895, 'epoch': 0.26}\n",
      "{'loss': 1.2278, 'grad_norm': 0.408203125, 'learning_rate': 0.00014610142626243116, 'epoch': 0.26}\n",
      "{'loss': 1.3848, 'grad_norm': 0.478515625, 'learning_rate': 0.00014609006388496728, 'epoch': 0.26}\n",
      "{'loss': 1.0585, 'grad_norm': 0.8671875, 'learning_rate': 0.00014607870150750343, 'epoch': 0.26}\n",
      "{'loss': 1.2945, 'grad_norm': 0.50390625, 'learning_rate': 0.00014606733913003958, 'epoch': 0.26}\n",
      "{'loss': 1.3687, 'grad_norm': 0.59375, 'learning_rate': 0.0001460559767525757, 'epoch': 0.26}\n",
      "{'loss': 1.1151, 'grad_norm': 0.455078125, 'learning_rate': 0.00014604461437511186, 'epoch': 0.26}\n",
      "{'loss': 1.2807, 'grad_norm': 0.75390625, 'learning_rate': 0.00014603325199764798, 'epoch': 0.26}\n",
      "{'loss': 1.0877, 'grad_norm': 0.97265625, 'learning_rate': 0.00014602188962018414, 'epoch': 0.26}\n",
      "{'loss': 1.2334, 'grad_norm': 0.478515625, 'learning_rate': 0.00014601052724272026, 'epoch': 0.26}\n",
      "{'loss': 1.1511, 'grad_norm': 0.62109375, 'learning_rate': 0.0001459991648652564, 'epoch': 0.26}\n",
      "{'loss': 1.1926, 'grad_norm': 0.408203125, 'learning_rate': 0.00014598780248779256, 'epoch': 0.26}\n",
      "{'loss': 1.3389, 'grad_norm': 0.546875, 'learning_rate': 0.0001459764401103287, 'epoch': 0.26}\n",
      "{'loss': 1.0331, 'grad_norm': 0.73046875, 'learning_rate': 0.00014596507773286484, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.345, 'grad_norm': 0.470703125, 'learning_rate': 0.00014595371535540096, 'epoch': 0.26}\n",
      "{'loss': 1.1673, 'grad_norm': 0.74609375, 'learning_rate': 0.00014594235297793711, 'epoch': 0.26}\n",
      "{'loss': 1.1766, 'grad_norm': 0.462890625, 'learning_rate': 0.00014593099060047327, 'epoch': 0.26}\n",
      "{'loss': 1.263, 'grad_norm': 0.54296875, 'learning_rate': 0.0001459196282230094, 'epoch': 0.26}\n",
      "{'loss': 1.0991, 'grad_norm': 0.7265625, 'learning_rate': 0.00014590826584554554, 'epoch': 0.26}\n",
      "{'loss': 1.4142, 'grad_norm': 0.46484375, 'learning_rate': 0.00014589690346808167, 'epoch': 0.26}\n",
      "{'loss': 1.2804, 'grad_norm': 0.7578125, 'learning_rate': 0.00014588554109061782, 'epoch': 0.26}\n",
      "{'loss': 1.314, 'grad_norm': 0.53125, 'learning_rate': 0.00014587417871315394, 'epoch': 0.26}\n",
      "{'loss': 1.2442, 'grad_norm': 0.62890625, 'learning_rate': 0.00014586281633569007, 'epoch': 0.26}\n",
      "{'loss': 1.1926, 'grad_norm': 0.91796875, 'learning_rate': 0.00014585145395822624, 'epoch': 0.26}\n",
      "{'loss': 1.3317, 'grad_norm': 0.55859375, 'learning_rate': 0.00014584009158076237, 'epoch': 0.26}\n",
      "{'loss': 1.1526, 'grad_norm': 0.953125, 'learning_rate': 0.00014582872920329852, 'epoch': 0.26}\n",
      "{'loss': 1.1947, 'grad_norm': 0.37890625, 'learning_rate': 0.00014581736682583464, 'epoch': 0.26}\n",
      "{'loss': 1.2324, 'grad_norm': 0.55859375, 'learning_rate': 0.00014580600444837077, 'epoch': 0.26}\n",
      "{'loss': 1.2852, 'grad_norm': 1.125, 'learning_rate': 0.00014579464207090692, 'epoch': 0.26}\n",
      "{'loss': 1.3208, 'grad_norm': 0.5, 'learning_rate': 0.00014578327969344304, 'epoch': 0.26}\n",
      "{'loss': 1.2263, 'grad_norm': 0.64453125, 'learning_rate': 0.00014577191731597922, 'epoch': 0.26}\n",
      "{'loss': 1.1119, 'grad_norm': 0.5703125, 'learning_rate': 0.00014576055493851535, 'epoch': 0.26}\n",
      "{'loss': 1.2375, 'grad_norm': 0.6640625, 'learning_rate': 0.00014574919256105147, 'epoch': 0.26}\n",
      "{'loss': 1.0674, 'grad_norm': 0.435546875, 'learning_rate': 0.00014573783018358762, 'epoch': 0.26}\n",
      "{'loss': 1.2729, 'grad_norm': 0.439453125, 'learning_rate': 0.00014572646780612375, 'epoch': 0.26}\n",
      "{'loss': 1.1685, 'grad_norm': 1.0234375, 'learning_rate': 0.0001457151054286599, 'epoch': 0.26}\n",
      "{'loss': 1.2864, 'grad_norm': 0.4375, 'learning_rate': 0.00014570374305119602, 'epoch': 0.26}\n",
      "{'loss': 1.2118, 'grad_norm': 0.609375, 'learning_rate': 0.00014569238067373217, 'epoch': 0.26}\n",
      "{'loss': 1.1501, 'grad_norm': 0.359375, 'learning_rate': 0.00014568101829626833, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.252, 'grad_norm': 0.453125, 'learning_rate': 0.00014566965591880445, 'epoch': 0.26}\n",
      "{'loss': 1.2202, 'grad_norm': 0.515625, 'learning_rate': 0.0001456582935413406, 'epoch': 0.26}\n",
      "{'loss': 1.2845, 'grad_norm': 0.421875, 'learning_rate': 0.00014564693116387673, 'epoch': 0.26}\n",
      "{'loss': 1.1759, 'grad_norm': 0.60546875, 'learning_rate': 0.00014563556878641288, 'epoch': 0.26}\n",
      "{'loss': 1.1415, 'grad_norm': 0.4765625, 'learning_rate': 0.00014562420640894903, 'epoch': 0.26}\n",
      "{'loss': 1.4411, 'grad_norm': 0.58203125, 'learning_rate': 0.00014561284403148515, 'epoch': 0.26}\n",
      "{'loss': 1.1527, 'grad_norm': 0.58203125, 'learning_rate': 0.0001456014816540213, 'epoch': 0.26}\n",
      "{'loss': 1.2021, 'grad_norm': 0.51953125, 'learning_rate': 0.00014559011927655743, 'epoch': 0.26}\n",
      "{'loss': 1.2182, 'grad_norm': 0.64453125, 'learning_rate': 0.00014557875689909358, 'epoch': 0.26}\n",
      "{'loss': 1.0669, 'grad_norm': 1.046875, 'learning_rate': 0.0001455673945216297, 'epoch': 0.26}\n",
      "{'loss': 1.2376, 'grad_norm': 0.515625, 'learning_rate': 0.00014555603214416586, 'epoch': 0.26}\n",
      "{'loss': 1.2281, 'grad_norm': 0.7578125, 'learning_rate': 0.000145544669766702, 'epoch': 0.26}\n",
      "{'loss': 1.2973, 'grad_norm': 0.388671875, 'learning_rate': 0.00014553330738923813, 'epoch': 0.26}\n",
      "{'loss': 1.1174, 'grad_norm': 0.49609375, 'learning_rate': 0.00014552194501177428, 'epoch': 0.26}\n",
      "{'loss': 1.0595, 'grad_norm': 0.6171875, 'learning_rate': 0.0001455105826343104, 'epoch': 0.26}\n",
      "{'loss': 1.3514, 'grad_norm': 0.57421875, 'learning_rate': 0.00014549922025684656, 'epoch': 0.26}\n",
      "{'loss': 1.084, 'grad_norm': 0.9375, 'learning_rate': 0.00014548785787938268, 'epoch': 0.26}\n",
      "{'loss': 1.2525, 'grad_norm': 0.435546875, 'learning_rate': 0.0001454764955019188, 'epoch': 0.26}\n",
      "{'loss': 1.1855, 'grad_norm': 0.462890625, 'learning_rate': 0.00014546513312445499, 'epoch': 0.27}\n",
      "{'loss': 1.1266, 'grad_norm': 1.015625, 'learning_rate': 0.0001454537707469911, 'epoch': 0.27}\n",
      "{'loss': 1.3437, 'grad_norm': 0.5625, 'learning_rate': 0.00014544240836952726, 'epoch': 0.27}\n",
      "{'loss': 1.3051, 'grad_norm': 0.9609375, 'learning_rate': 0.00014543104599206339, 'epoch': 0.27}\n",
      "{'loss': 1.2061, 'grad_norm': 0.369140625, 'learning_rate': 0.0001454196836145995, 'epoch': 0.27}\n",
      "{'loss': 1.1421, 'grad_norm': 0.58984375, 'learning_rate': 0.00014540832123713566, 'epoch': 0.27}\n",
      "{'loss': 1.1329, 'grad_norm': 0.8046875, 'learning_rate': 0.00014539695885967179, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3882, 'grad_norm': 0.484375, 'learning_rate': 0.00014538559648220796, 'epoch': 0.27}\n",
      "{'loss': 1.1784, 'grad_norm': 0.828125, 'learning_rate': 0.0001453742341047441, 'epoch': 0.27}\n",
      "{'loss': 1.3063, 'grad_norm': 0.4609375, 'learning_rate': 0.0001453628717272802, 'epoch': 0.27}\n",
      "{'loss': 1.3628, 'grad_norm': 0.515625, 'learning_rate': 0.00014535150934981636, 'epoch': 0.27}\n",
      "{'loss': 1.1543, 'grad_norm': 0.94921875, 'learning_rate': 0.0001453401469723525, 'epoch': 0.27}\n",
      "{'loss': 1.3696, 'grad_norm': 0.427734375, 'learning_rate': 0.00014532878459488864, 'epoch': 0.27}\n",
      "{'loss': 1.302, 'grad_norm': 0.64453125, 'learning_rate': 0.00014531742221742476, 'epoch': 0.27}\n",
      "{'loss': 1.2417, 'grad_norm': 0.60546875, 'learning_rate': 0.00014530605983996092, 'epoch': 0.27}\n",
      "{'loss': 1.2024, 'grad_norm': 0.67578125, 'learning_rate': 0.00014529469746249707, 'epoch': 0.27}\n",
      "{'loss': 1.1249, 'grad_norm': 0.76171875, 'learning_rate': 0.0001452833350850332, 'epoch': 0.27}\n",
      "{'loss': 1.3515, 'grad_norm': 0.478515625, 'learning_rate': 0.00014527197270756934, 'epoch': 0.27}\n",
      "{'loss': 1.2625, 'grad_norm': 0.62890625, 'learning_rate': 0.00014526061033010547, 'epoch': 0.27}\n",
      "{'loss': 1.332, 'grad_norm': 0.5390625, 'learning_rate': 0.00014524924795264162, 'epoch': 0.27}\n",
      "{'loss': 1.28, 'grad_norm': 0.51171875, 'learning_rate': 0.00014523788557517777, 'epoch': 0.27}\n",
      "{'loss': 1.0837, 'grad_norm': 0.78125, 'learning_rate': 0.0001452265231977139, 'epoch': 0.27}\n",
      "{'loss': 1.3775, 'grad_norm': 0.5, 'learning_rate': 0.00014521516082025005, 'epoch': 0.27}\n",
      "{'loss': 1.2224, 'grad_norm': 0.62890625, 'learning_rate': 0.00014520379844278617, 'epoch': 0.27}\n",
      "{'loss': 1.1128, 'grad_norm': 0.515625, 'learning_rate': 0.00014519243606532232, 'epoch': 0.27}\n",
      "{'loss': 1.287, 'grad_norm': 0.83203125, 'learning_rate': 0.00014518107368785845, 'epoch': 0.27}\n",
      "{'loss': 1.1209, 'grad_norm': 0.78515625, 'learning_rate': 0.0001451697113103946, 'epoch': 0.27}\n",
      "{'loss': 1.495, 'grad_norm': 0.4921875, 'learning_rate': 0.00014515834893293075, 'epoch': 0.27}\n",
      "{'loss': 1.1923, 'grad_norm': 0.921875, 'learning_rate': 0.00014514698655546687, 'epoch': 0.27}\n",
      "{'loss': 1.2705, 'grad_norm': 0.515625, 'learning_rate': 0.00014513562417800302, 'epoch': 0.27}\n",
      "{'loss': 1.2902, 'grad_norm': 0.56640625, 'learning_rate': 0.00014512426180053915, 'epoch': 0.27}\n",
      "{'loss': 1.1349, 'grad_norm': 0.984375, 'learning_rate': 0.0001451128994230753, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4276, 'grad_norm': 0.50390625, 'learning_rate': 0.00014510153704561142, 'epoch': 0.27}\n",
      "{'loss': 1.2543, 'grad_norm': 0.78125, 'learning_rate': 0.00014509017466814755, 'epoch': 0.27}\n",
      "{'loss': 1.2587, 'grad_norm': 0.359375, 'learning_rate': 0.00014507881229068373, 'epoch': 0.27}\n",
      "{'loss': 1.4029, 'grad_norm': 0.63671875, 'learning_rate': 0.00014506744991321985, 'epoch': 0.27}\n",
      "{'loss': 1.1235, 'grad_norm': 0.83203125, 'learning_rate': 0.000145056087535756, 'epoch': 0.27}\n",
      "{'loss': 1.3804, 'grad_norm': 0.494140625, 'learning_rate': 0.00014504472515829213, 'epoch': 0.27}\n",
      "{'loss': 1.194, 'grad_norm': 0.5546875, 'learning_rate': 0.00014503336278082825, 'epoch': 0.27}\n",
      "{'loss': 1.1211, 'grad_norm': 0.48828125, 'learning_rate': 0.0001450220004033644, 'epoch': 0.27}\n",
      "{'loss': 1.2785, 'grad_norm': 0.55859375, 'learning_rate': 0.00014501063802590053, 'epoch': 0.27}\n",
      "{'loss': 1.1739, 'grad_norm': 0.92578125, 'learning_rate': 0.0001449992756484367, 'epoch': 0.27}\n",
      "{'loss': 1.3466, 'grad_norm': 0.6484375, 'learning_rate': 0.00014498791327097283, 'epoch': 0.27}\n",
      "{'loss': 1.1622, 'grad_norm': 0.66796875, 'learning_rate': 0.00014497655089350895, 'epoch': 0.27}\n",
      "{'loss': 1.2843, 'grad_norm': 0.390625, 'learning_rate': 0.0001449651885160451, 'epoch': 0.27}\n",
      "{'loss': 1.2256, 'grad_norm': 0.53515625, 'learning_rate': 0.00014495382613858123, 'epoch': 0.27}\n",
      "{'loss': 1.1269, 'grad_norm': 0.87109375, 'learning_rate': 0.00014494246376111738, 'epoch': 0.27}\n",
      "{'loss': 1.3518, 'grad_norm': 0.578125, 'learning_rate': 0.00014493110138365353, 'epoch': 0.27}\n",
      "{'loss': 1.1393, 'grad_norm': 0.54296875, 'learning_rate': 0.00014491973900618966, 'epoch': 0.27}\n",
      "{'loss': 1.0701, 'grad_norm': 0.427734375, 'learning_rate': 0.0001449083766287258, 'epoch': 0.27}\n",
      "{'loss': 1.2786, 'grad_norm': 0.4609375, 'learning_rate': 0.00014489701425126193, 'epoch': 0.27}\n",
      "{'loss': 1.1735, 'grad_norm': 0.59765625, 'learning_rate': 0.00014488565187379808, 'epoch': 0.27}\n",
      "{'loss': 1.2913, 'grad_norm': 0.65234375, 'learning_rate': 0.0001448742894963342, 'epoch': 0.27}\n",
      "{'loss': 1.0926, 'grad_norm': 0.65625, 'learning_rate': 0.00014486292711887036, 'epoch': 0.27}\n",
      "{'loss': 1.1319, 'grad_norm': 0.447265625, 'learning_rate': 0.0001448515647414065, 'epoch': 0.27}\n",
      "{'loss': 1.1603, 'grad_norm': 0.51171875, 'learning_rate': 0.00014484020236394264, 'epoch': 0.27}\n",
      "{'loss': 1.0957, 'grad_norm': 0.67578125, 'learning_rate': 0.0001448288399864788, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.293, 'grad_norm': 0.47265625, 'learning_rate': 0.0001448174776090149, 'epoch': 0.27}\n",
      "{'loss': 1.177, 'grad_norm': 0.59765625, 'learning_rate': 0.00014480611523155106, 'epoch': 0.27}\n",
      "{'loss': 1.3864, 'grad_norm': 0.390625, 'learning_rate': 0.0001447947528540872, 'epoch': 0.27}\n",
      "{'loss': 1.2665, 'grad_norm': 0.56640625, 'learning_rate': 0.00014478339047662334, 'epoch': 0.27}\n",
      "{'loss': 1.058, 'grad_norm': 0.59375, 'learning_rate': 0.0001447720280991595, 'epoch': 0.27}\n",
      "{'loss': 1.2073, 'grad_norm': 0.470703125, 'learning_rate': 0.00014476066572169561, 'epoch': 0.27}\n",
      "{'loss': 1.2311, 'grad_norm': 0.625, 'learning_rate': 0.00014474930334423177, 'epoch': 0.27}\n",
      "{'loss': 1.2659, 'grad_norm': 0.353515625, 'learning_rate': 0.0001447379409667679, 'epoch': 0.27}\n",
      "{'loss': 1.3417, 'grad_norm': 0.52734375, 'learning_rate': 0.00014472657858930404, 'epoch': 0.27}\n",
      "{'loss': 1.1856, 'grad_norm': 0.89453125, 'learning_rate': 0.00014471521621184017, 'epoch': 0.27}\n",
      "{'loss': 1.393, 'grad_norm': 0.44921875, 'learning_rate': 0.0001447038538343763, 'epoch': 0.27}\n",
      "{'loss': 1.1786, 'grad_norm': 0.7265625, 'learning_rate': 0.00014469249145691247, 'epoch': 0.27}\n",
      "{'loss': 1.3176, 'grad_norm': 0.365234375, 'learning_rate': 0.0001446811290794486, 'epoch': 0.27}\n",
      "{'loss': 1.1626, 'grad_norm': 0.65234375, 'learning_rate': 0.00014466976670198474, 'epoch': 0.27}\n",
      "{'loss': 1.119, 'grad_norm': 0.859375, 'learning_rate': 0.00014465840432452087, 'epoch': 0.27}\n",
      "{'loss': 1.2379, 'grad_norm': 0.490234375, 'learning_rate': 0.000144647041947057, 'epoch': 0.27}\n",
      "{'loss': 1.1532, 'grad_norm': 0.66015625, 'learning_rate': 0.00014463567956959314, 'epoch': 0.27}\n",
      "{'loss': 1.2333, 'grad_norm': 0.439453125, 'learning_rate': 0.00014462431719212927, 'epoch': 0.27}\n",
      "{'loss': 1.3027, 'grad_norm': 0.76171875, 'learning_rate': 0.00014461295481466545, 'epoch': 0.27}\n",
      "{'loss': 1.1846, 'grad_norm': 0.8359375, 'learning_rate': 0.00014460159243720157, 'epoch': 0.27}\n",
      "{'loss': 1.3648, 'grad_norm': 0.51171875, 'learning_rate': 0.0001445902300597377, 'epoch': 0.27}\n",
      "{'loss': 1.2538, 'grad_norm': 0.61328125, 'learning_rate': 0.00014457886768227385, 'epoch': 0.27}\n",
      "{'loss': 1.3815, 'grad_norm': 0.5078125, 'learning_rate': 0.00014456750530480997, 'epoch': 0.27}\n",
      "{'loss': 1.2531, 'grad_norm': 0.61328125, 'learning_rate': 0.00014455614292734612, 'epoch': 0.27}\n",
      "{'loss': 1.2144, 'grad_norm': 0.65234375, 'learning_rate': 0.00014454478054988227, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3537, 'grad_norm': 0.484375, 'learning_rate': 0.0001445334181724184, 'epoch': 0.27}\n",
      "{'loss': 1.2616, 'grad_norm': 0.76171875, 'learning_rate': 0.00014452205579495455, 'epoch': 0.27}\n",
      "{'loss': 1.2753, 'grad_norm': 0.47265625, 'learning_rate': 0.00014451069341749067, 'epoch': 0.27}\n",
      "{'loss': 1.2182, 'grad_norm': 0.56640625, 'learning_rate': 0.00014449933104002683, 'epoch': 0.27}\n",
      "{'loss': 1.0737, 'grad_norm': 1.09375, 'learning_rate': 0.00014448796866256295, 'epoch': 0.27}\n",
      "{'loss': 1.3062, 'grad_norm': 0.41796875, 'learning_rate': 0.0001444766062850991, 'epoch': 0.27}\n",
      "{'loss': 1.2123, 'grad_norm': 0.59375, 'learning_rate': 0.00014446524390763525, 'epoch': 0.27}\n",
      "{'loss': 1.3055, 'grad_norm': 0.486328125, 'learning_rate': 0.00014445388153017138, 'epoch': 0.27}\n",
      "{'loss': 1.2629, 'grad_norm': 0.53515625, 'learning_rate': 0.00014444251915270753, 'epoch': 0.27}\n",
      "{'loss': 1.2148, 'grad_norm': 0.75, 'learning_rate': 0.00014443115677524365, 'epoch': 0.27}\n",
      "{'loss': 1.5131, 'grad_norm': 0.48046875, 'learning_rate': 0.0001444197943977798, 'epoch': 0.27}\n",
      "{'loss': 1.2743, 'grad_norm': 0.72265625, 'learning_rate': 0.00014440843202031593, 'epoch': 0.27}\n",
      "{'loss': 1.2672, 'grad_norm': 0.443359375, 'learning_rate': 0.00014439706964285208, 'epoch': 0.27}\n",
      "{'loss': 1.2776, 'grad_norm': 0.482421875, 'learning_rate': 0.00014438570726538823, 'epoch': 0.27}\n",
      "{'loss': 1.0923, 'grad_norm': 0.52734375, 'learning_rate': 0.00014437434488792436, 'epoch': 0.27}\n",
      "{'loss': 1.3838, 'grad_norm': 0.4921875, 'learning_rate': 0.0001443629825104605, 'epoch': 0.27}\n",
      "{'loss': 1.0288, 'grad_norm': 0.67578125, 'learning_rate': 0.00014435162013299663, 'epoch': 0.27}\n",
      "{'loss': 1.1936, 'grad_norm': 0.4375, 'learning_rate': 0.00014434025775553278, 'epoch': 0.27}\n",
      "{'loss': 1.2786, 'grad_norm': 0.56640625, 'learning_rate': 0.0001443288953780689, 'epoch': 0.27}\n",
      "{'loss': 1.0315, 'grad_norm': 0.375, 'learning_rate': 0.00014431753300060503, 'epoch': 0.27}\n",
      "{'loss': 1.4304, 'grad_norm': 0.47265625, 'learning_rate': 0.0001443061706231412, 'epoch': 0.27}\n",
      "{'loss': 1.0861, 'grad_norm': 0.828125, 'learning_rate': 0.00014429480824567733, 'epoch': 0.27}\n",
      "{'loss': 1.1796, 'grad_norm': 0.44921875, 'learning_rate': 0.00014428344586821349, 'epoch': 0.27}\n",
      "{'loss': 1.3649, 'grad_norm': 0.57421875, 'learning_rate': 0.0001442720834907496, 'epoch': 0.27}\n",
      "{'loss': 1.1637, 'grad_norm': 0.80078125, 'learning_rate': 0.00014426072111328573, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4692, 'grad_norm': 0.474609375, 'learning_rate': 0.00014424935873582189, 'epoch': 0.27}\n",
      "{'loss': 1.2366, 'grad_norm': 0.66015625, 'learning_rate': 0.00014423799635835804, 'epoch': 0.27}\n",
      "{'loss': 1.1091, 'grad_norm': 0.447265625, 'learning_rate': 0.0001442266339808942, 'epoch': 0.27}\n",
      "{'loss': 1.3147, 'grad_norm': 0.546875, 'learning_rate': 0.0001442152716034303, 'epoch': 0.27}\n",
      "{'loss': 1.0878, 'grad_norm': 0.97265625, 'learning_rate': 0.00014420390922596644, 'epoch': 0.27}\n",
      "{'loss': 1.3561, 'grad_norm': 0.64453125, 'learning_rate': 0.0001441925468485026, 'epoch': 0.27}\n",
      "{'loss': 1.3444, 'grad_norm': 0.6171875, 'learning_rate': 0.0001441811844710387, 'epoch': 0.27}\n",
      "{'loss': 1.1497, 'grad_norm': 0.50390625, 'learning_rate': 0.00014416982209357486, 'epoch': 0.27}\n",
      "{'loss': 1.3379, 'grad_norm': 0.65625, 'learning_rate': 0.00014415845971611102, 'epoch': 0.27}\n",
      "{'loss': 1.1122, 'grad_norm': 1.28125, 'learning_rate': 0.00014414709733864714, 'epoch': 0.27}\n",
      "{'loss': 1.2707, 'grad_norm': 0.55078125, 'learning_rate': 0.0001441357349611833, 'epoch': 0.27}\n",
      "{'loss': 1.255, 'grad_norm': 0.6875, 'learning_rate': 0.00014412437258371942, 'epoch': 0.27}\n",
      "{'loss': 1.2565, 'grad_norm': 0.380859375, 'learning_rate': 0.00014411301020625557, 'epoch': 0.27}\n",
      "{'loss': 0.982, 'grad_norm': 0.494140625, 'learning_rate': 0.0001441016478287917, 'epoch': 0.27}\n",
      "{'loss': 1.16, 'grad_norm': 0.63671875, 'learning_rate': 0.00014409028545132784, 'epoch': 0.27}\n",
      "{'loss': 1.3922, 'grad_norm': 0.498046875, 'learning_rate': 0.000144078923073864, 'epoch': 0.27}\n",
      "{'loss': 1.2409, 'grad_norm': 0.7109375, 'learning_rate': 0.00014406756069640012, 'epoch': 0.27}\n",
      "{'loss': 1.2348, 'grad_norm': 0.4375, 'learning_rate': 0.00014405619831893627, 'epoch': 0.27}\n",
      "{'loss': 1.2792, 'grad_norm': 0.64453125, 'learning_rate': 0.0001440448359414724, 'epoch': 0.27}\n",
      "{'loss': 1.1309, 'grad_norm': 0.7109375, 'learning_rate': 0.00014403347356400855, 'epoch': 0.27}\n",
      "{'loss': 1.4582, 'grad_norm': 0.43359375, 'learning_rate': 0.00014402211118654467, 'epoch': 0.27}\n",
      "{'loss': 1.1838, 'grad_norm': 0.73828125, 'learning_rate': 0.00014401074880908082, 'epoch': 0.27}\n",
      "{'loss': 1.2103, 'grad_norm': 0.53125, 'learning_rate': 0.00014399938643161697, 'epoch': 0.27}\n",
      "{'loss': 1.2503, 'grad_norm': 0.56640625, 'learning_rate': 0.0001439880240541531, 'epoch': 0.27}\n",
      "{'loss': 1.0559, 'grad_norm': 0.4375, 'learning_rate': 0.00014397666167668925, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3515, 'grad_norm': 0.5625, 'learning_rate': 0.00014396529929922537, 'epoch': 0.27}\n",
      "{'loss': 1.3066, 'grad_norm': 0.7578125, 'learning_rate': 0.00014395393692176152, 'epoch': 0.27}\n",
      "{'loss': 1.3057, 'grad_norm': 0.609375, 'learning_rate': 0.00014394257454429765, 'epoch': 0.27}\n",
      "{'loss': 1.2341, 'grad_norm': 0.57421875, 'learning_rate': 0.00014393121216683377, 'epoch': 0.27}\n",
      "{'loss': 1.0584, 'grad_norm': 0.6796875, 'learning_rate': 0.00014391984978936995, 'epoch': 0.27}\n",
      "{'loss': 1.2529, 'grad_norm': 0.546875, 'learning_rate': 0.00014390848741190608, 'epoch': 0.27}\n",
      "{'loss': 1.1832, 'grad_norm': 0.86328125, 'learning_rate': 0.00014389712503444223, 'epoch': 0.27}\n",
      "{'loss': 1.25, 'grad_norm': 0.388671875, 'learning_rate': 0.00014388576265697835, 'epoch': 0.27}\n",
      "{'loss': 1.3365, 'grad_norm': 0.83203125, 'learning_rate': 0.00014387440027951448, 'epoch': 0.27}\n",
      "{'loss': 1.1464, 'grad_norm': 0.66796875, 'learning_rate': 0.00014386303790205063, 'epoch': 0.27}\n",
      "{'loss': 1.3287, 'grad_norm': 0.5078125, 'learning_rate': 0.00014385167552458678, 'epoch': 0.27}\n",
      "{'loss': 1.1317, 'grad_norm': 0.59375, 'learning_rate': 0.00014384031314712293, 'epoch': 0.27}\n",
      "{'loss': 1.1437, 'grad_norm': 0.419921875, 'learning_rate': 0.00014382895076965905, 'epoch': 0.27}\n",
      "{'loss': 1.237, 'grad_norm': 0.494140625, 'learning_rate': 0.00014381758839219518, 'epoch': 0.27}\n",
      "{'loss': 1.2313, 'grad_norm': 0.92578125, 'learning_rate': 0.00014380622601473133, 'epoch': 0.27}\n",
      "{'loss': 1.4342, 'grad_norm': 0.59375, 'learning_rate': 0.00014379486363726745, 'epoch': 0.27}\n",
      "{'loss': 1.3364, 'grad_norm': 1.1171875, 'learning_rate': 0.0001437835012598036, 'epoch': 0.27}\n",
      "{'loss': 1.2835, 'grad_norm': 0.490234375, 'learning_rate': 0.00014377213888233976, 'epoch': 0.27}\n",
      "{'loss': 1.1493, 'grad_norm': 0.54296875, 'learning_rate': 0.00014376077650487588, 'epoch': 0.27}\n",
      "{'loss': 1.1106, 'grad_norm': 0.6015625, 'learning_rate': 0.00014374941412741203, 'epoch': 0.27}\n",
      "{'loss': 1.3239, 'grad_norm': 0.392578125, 'learning_rate': 0.00014373805174994816, 'epoch': 0.27}\n",
      "{'loss': 1.2299, 'grad_norm': 0.59765625, 'learning_rate': 0.0001437266893724843, 'epoch': 0.27}\n",
      "{'loss': 1.2323, 'grad_norm': 0.435546875, 'learning_rate': 0.00014371532699502043, 'epoch': 0.27}\n",
      "{'loss': 1.2334, 'grad_norm': 0.58984375, 'learning_rate': 0.00014370396461755658, 'epoch': 0.27}\n",
      "{'loss': 1.1976, 'grad_norm': 0.7109375, 'learning_rate': 0.00014369260224009273, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3411, 'grad_norm': 0.484375, 'learning_rate': 0.00014368123986262886, 'epoch': 0.27}\n",
      "{'loss': 1.2135, 'grad_norm': 1.0703125, 'learning_rate': 0.000143669877485165, 'epoch': 0.27}\n",
      "{'loss': 1.2085, 'grad_norm': 0.474609375, 'learning_rate': 0.00014365851510770113, 'epoch': 0.27}\n",
      "{'loss': 1.1792, 'grad_norm': 0.5546875, 'learning_rate': 0.00014364715273023729, 'epoch': 0.27}\n",
      "{'loss': 1.0357, 'grad_norm': 0.99609375, 'learning_rate': 0.0001436357903527734, 'epoch': 0.27}\n",
      "{'loss': 1.261, 'grad_norm': 0.51953125, 'learning_rate': 0.00014362442797530956, 'epoch': 0.27}\n",
      "{'loss': 1.2041, 'grad_norm': 0.6015625, 'learning_rate': 0.0001436130655978457, 'epoch': 0.27}\n",
      "{'loss': 1.2766, 'grad_norm': 0.447265625, 'learning_rate': 0.00014360170322038184, 'epoch': 0.27}\n",
      "{'loss': 1.319, 'grad_norm': 0.515625, 'learning_rate': 0.000143590340842918, 'epoch': 0.27}\n",
      "{'loss': 1.167, 'grad_norm': 0.89453125, 'learning_rate': 0.0001435789784654541, 'epoch': 0.27}\n",
      "{'loss': 1.453, 'grad_norm': 0.640625, 'learning_rate': 0.00014356761608799026, 'epoch': 0.27}\n",
      "{'loss': 1.2123, 'grad_norm': 0.60546875, 'learning_rate': 0.0001435562537105264, 'epoch': 0.27}\n",
      "{'loss': 1.1849, 'grad_norm': 0.458984375, 'learning_rate': 0.00014354489133306254, 'epoch': 0.27}\n",
      "{'loss': 1.2652, 'grad_norm': 0.63671875, 'learning_rate': 0.0001435335289555987, 'epoch': 0.27}\n",
      "{'loss': 1.1159, 'grad_norm': 0.9609375, 'learning_rate': 0.00014352216657813482, 'epoch': 0.27}\n",
      "{'loss': 1.3167, 'grad_norm': 0.58984375, 'learning_rate': 0.00014351080420067097, 'epoch': 0.27}\n",
      "{'loss': 1.1896, 'grad_norm': 0.5625, 'learning_rate': 0.0001434994418232071, 'epoch': 0.27}\n",
      "{'loss': 1.2891, 'grad_norm': 0.486328125, 'learning_rate': 0.00014348807944574322, 'epoch': 0.27}\n",
      "{'loss': 1.2673, 'grad_norm': 0.6484375, 'learning_rate': 0.00014347671706827937, 'epoch': 0.27}\n",
      "{'loss': 1.1456, 'grad_norm': 1.703125, 'learning_rate': 0.00014346535469081552, 'epoch': 0.27}\n",
      "{'loss': 1.4236, 'grad_norm': 0.58984375, 'learning_rate': 0.00014345399231335167, 'epoch': 0.27}\n",
      "{'loss': 1.137, 'grad_norm': 0.98046875, 'learning_rate': 0.0001434426299358878, 'epoch': 0.27}\n",
      "{'loss': 1.1861, 'grad_norm': 0.427734375, 'learning_rate': 0.00014343126755842392, 'epoch': 0.27}\n",
      "{'loss': 1.3364, 'grad_norm': 0.57421875, 'learning_rate': 0.00014341990518096007, 'epoch': 0.27}\n",
      "{'loss': 1.1653, 'grad_norm': 0.89453125, 'learning_rate': 0.0001434085428034962, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2963, 'grad_norm': 0.6640625, 'learning_rate': 0.00014339718042603235, 'epoch': 0.27}\n",
      "{'loss': 1.195, 'grad_norm': 0.80859375, 'learning_rate': 0.0001433858180485685, 'epoch': 0.27}\n",
      "{'loss': 1.1383, 'grad_norm': 0.3984375, 'learning_rate': 0.00014337445567110462, 'epoch': 0.27}\n",
      "{'loss': 1.2784, 'grad_norm': 0.466796875, 'learning_rate': 0.00014336309329364077, 'epoch': 0.27}\n",
      "{'loss': 0.9964, 'grad_norm': 1.1171875, 'learning_rate': 0.0001433517309161769, 'epoch': 0.27}\n",
      "{'loss': 1.3477, 'grad_norm': 0.55859375, 'learning_rate': 0.00014334036853871305, 'epoch': 0.27}\n",
      "{'loss': 1.1996, 'grad_norm': 0.75390625, 'learning_rate': 0.00014332900616124917, 'epoch': 0.27}\n",
      "{'loss': 1.2757, 'grad_norm': 0.462890625, 'learning_rate': 0.00014331764378378532, 'epoch': 0.27}\n",
      "{'loss': 1.3789, 'grad_norm': 0.5859375, 'learning_rate': 0.00014330628140632148, 'epoch': 0.27}\n",
      "{'loss': 1.1507, 'grad_norm': 0.84765625, 'learning_rate': 0.0001432949190288576, 'epoch': 0.27}\n",
      "{'loss': 1.3306, 'grad_norm': 0.5, 'learning_rate': 0.00014328355665139375, 'epoch': 0.27}\n",
      "{'loss': 1.2608, 'grad_norm': 0.490234375, 'learning_rate': 0.00014327219427392988, 'epoch': 0.27}\n",
      "{'loss': 1.3215, 'grad_norm': 0.47265625, 'learning_rate': 0.00014326083189646603, 'epoch': 0.27}\n",
      "{'loss': 1.2829, 'grad_norm': 0.54296875, 'learning_rate': 0.00014324946951900215, 'epoch': 0.27}\n",
      "{'loss': 1.0287, 'grad_norm': 1.03125, 'learning_rate': 0.0001432381071415383, 'epoch': 0.27}\n",
      "{'loss': 1.3556, 'grad_norm': 0.478515625, 'learning_rate': 0.00014322674476407445, 'epoch': 0.27}\n",
      "{'loss': 1.2051, 'grad_norm': 0.55078125, 'learning_rate': 0.00014321538238661058, 'epoch': 0.27}\n",
      "{'loss': 1.2541, 'grad_norm': 0.466796875, 'learning_rate': 0.00014320402000914673, 'epoch': 0.27}\n",
      "{'loss': 1.3527, 'grad_norm': 0.546875, 'learning_rate': 0.00014319265763168285, 'epoch': 0.27}\n",
      "{'loss': 1.2324, 'grad_norm': 0.83984375, 'learning_rate': 0.000143181295254219, 'epoch': 0.28}\n",
      "{'loss': 1.53, 'grad_norm': 0.431640625, 'learning_rate': 0.00014316993287675513, 'epoch': 0.28}\n",
      "{'loss': 1.2637, 'grad_norm': 0.62890625, 'learning_rate': 0.00014315857049929128, 'epoch': 0.28}\n",
      "{'loss': 1.1444, 'grad_norm': 0.41796875, 'learning_rate': 0.00014314720812182743, 'epoch': 0.28}\n",
      "{'loss': 1.1995, 'grad_norm': 0.490234375, 'learning_rate': 0.00014313584574436356, 'epoch': 0.28}\n",
      "{'loss': 1.0553, 'grad_norm': 0.7109375, 'learning_rate': 0.0001431244833668997, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3398, 'grad_norm': 0.4921875, 'learning_rate': 0.00014311312098943583, 'epoch': 0.28}\n",
      "{'loss': 1.3456, 'grad_norm': 0.65625, 'learning_rate': 0.00014310175861197196, 'epoch': 0.28}\n",
      "{'loss': 1.1674, 'grad_norm': 0.412109375, 'learning_rate': 0.0001430903962345081, 'epoch': 0.28}\n",
      "{'loss': 1.1917, 'grad_norm': 0.7109375, 'learning_rate': 0.00014307903385704426, 'epoch': 0.28}\n",
      "{'loss': 1.0514, 'grad_norm': 0.91796875, 'learning_rate': 0.0001430676714795804, 'epoch': 0.28}\n",
      "{'loss': 1.3661, 'grad_norm': 0.53515625, 'learning_rate': 0.00014305630910211654, 'epoch': 0.28}\n",
      "{'loss': 1.1899, 'grad_norm': 0.59765625, 'learning_rate': 0.00014304494672465266, 'epoch': 0.28}\n",
      "{'loss': 1.2454, 'grad_norm': 0.44921875, 'learning_rate': 0.0001430335843471888, 'epoch': 0.28}\n",
      "{'loss': 1.203, 'grad_norm': 0.58203125, 'learning_rate': 0.00014302222196972494, 'epoch': 0.28}\n",
      "{'loss': 1.0817, 'grad_norm': 1.046875, 'learning_rate': 0.0001430108595922611, 'epoch': 0.28}\n",
      "{'loss': 1.5726, 'grad_norm': 0.5625, 'learning_rate': 0.00014299949721479724, 'epoch': 0.28}\n",
      "{'loss': 1.1468, 'grad_norm': 0.88671875, 'learning_rate': 0.00014298813483733336, 'epoch': 0.28}\n",
      "{'loss': 1.1776, 'grad_norm': 0.439453125, 'learning_rate': 0.00014297677245986951, 'epoch': 0.28}\n",
      "{'loss': 1.1538, 'grad_norm': 0.58203125, 'learning_rate': 0.00014296541008240564, 'epoch': 0.28}\n",
      "{'loss': 1.1264, 'grad_norm': 1.3359375, 'learning_rate': 0.0001429540477049418, 'epoch': 0.28}\n",
      "{'loss': 1.1506, 'grad_norm': 0.60546875, 'learning_rate': 0.00014294268532747791, 'epoch': 0.28}\n",
      "{'loss': 1.2214, 'grad_norm': 0.5234375, 'learning_rate': 0.00014293132295001407, 'epoch': 0.28}\n",
      "{'loss': 1.2579, 'grad_norm': 0.48046875, 'learning_rate': 0.00014291996057255022, 'epoch': 0.28}\n",
      "{'loss': 1.1546, 'grad_norm': 0.796875, 'learning_rate': 0.00014290859819508634, 'epoch': 0.28}\n",
      "{'loss': 1.1624, 'grad_norm': 1.359375, 'learning_rate': 0.0001428972358176225, 'epoch': 0.28}\n",
      "{'loss': 1.4256, 'grad_norm': 0.453125, 'learning_rate': 0.00014288587344015862, 'epoch': 0.28}\n",
      "{'loss': 1.2571, 'grad_norm': 0.7109375, 'learning_rate': 0.00014287451106269477, 'epoch': 0.28}\n",
      "{'loss': 1.3286, 'grad_norm': 0.53125, 'learning_rate': 0.0001428631486852309, 'epoch': 0.28}\n",
      "{'loss': 1.333, 'grad_norm': 0.5390625, 'learning_rate': 0.00014285178630776704, 'epoch': 0.28}\n",
      "{'loss': 1.109, 'grad_norm': 0.97265625, 'learning_rate': 0.0001428404239303032, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5477, 'grad_norm': 0.51953125, 'learning_rate': 0.00014282906155283932, 'epoch': 0.28}\n",
      "{'loss': 1.2386, 'grad_norm': 0.64453125, 'learning_rate': 0.00014281769917537547, 'epoch': 0.28}\n",
      "{'loss': 1.3231, 'grad_norm': 0.373046875, 'learning_rate': 0.0001428063367979116, 'epoch': 0.28}\n",
      "{'loss': 1.2383, 'grad_norm': 0.609375, 'learning_rate': 0.00014279497442044775, 'epoch': 0.28}\n",
      "{'loss': 1.122, 'grad_norm': 1.1640625, 'learning_rate': 0.00014278361204298387, 'epoch': 0.28}\n",
      "{'loss': 1.4638, 'grad_norm': 0.50390625, 'learning_rate': 0.00014277224966552002, 'epoch': 0.28}\n",
      "{'loss': 1.1976, 'grad_norm': 0.62890625, 'learning_rate': 0.00014276088728805617, 'epoch': 0.28}\n",
      "{'loss': 1.1822, 'grad_norm': 0.451171875, 'learning_rate': 0.0001427495249105923, 'epoch': 0.28}\n",
      "{'loss': 1.2146, 'grad_norm': 0.6328125, 'learning_rate': 0.00014273816253312845, 'epoch': 0.28}\n",
      "{'loss': 1.0129, 'grad_norm': 0.51171875, 'learning_rate': 0.00014272680015566457, 'epoch': 0.28}\n",
      "{'loss': 1.2874, 'grad_norm': 0.53125, 'learning_rate': 0.0001427154377782007, 'epoch': 0.28}\n",
      "{'loss': 1.2553, 'grad_norm': 0.703125, 'learning_rate': 0.00014270407540073685, 'epoch': 0.28}\n",
      "{'loss': 1.2294, 'grad_norm': 0.435546875, 'learning_rate': 0.000142692713023273, 'epoch': 0.28}\n",
      "{'loss': 1.37, 'grad_norm': 0.47265625, 'learning_rate': 0.00014268135064580915, 'epoch': 0.28}\n",
      "{'loss': 1.011, 'grad_norm': 0.61328125, 'learning_rate': 0.00014266998826834528, 'epoch': 0.28}\n",
      "{'loss': 1.3393, 'grad_norm': 0.46875, 'learning_rate': 0.0001426586258908814, 'epoch': 0.28}\n",
      "{'loss': 1.169, 'grad_norm': 0.73046875, 'learning_rate': 0.00014264726351341755, 'epoch': 0.28}\n",
      "{'loss': 1.1111, 'grad_norm': 0.462890625, 'learning_rate': 0.00014263590113595368, 'epoch': 0.28}\n",
      "{'loss': 1.2991, 'grad_norm': 0.470703125, 'learning_rate': 0.00014262453875848983, 'epoch': 0.28}\n",
      "{'loss': 1.1526, 'grad_norm': 0.90234375, 'learning_rate': 0.00014261317638102598, 'epoch': 0.28}\n",
      "{'loss': 1.2736, 'grad_norm': 0.71484375, 'learning_rate': 0.0001426018140035621, 'epoch': 0.28}\n",
      "{'loss': 1.285, 'grad_norm': 1.0078125, 'learning_rate': 0.00014259045162609826, 'epoch': 0.28}\n",
      "{'loss': 0.9842, 'grad_norm': 0.408203125, 'learning_rate': 0.00014257908924863438, 'epoch': 0.28}\n",
      "{'loss': 1.3466, 'grad_norm': 0.5, 'learning_rate': 0.00014256772687117053, 'epoch': 0.28}\n",
      "{'loss': 1.1092, 'grad_norm': 0.59765625, 'learning_rate': 0.00014255636449370666, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3123, 'grad_norm': 0.494140625, 'learning_rate': 0.0001425450021162428, 'epoch': 0.28}\n",
      "{'loss': 1.0732, 'grad_norm': 0.68359375, 'learning_rate': 0.00014253363973877896, 'epoch': 0.28}\n",
      "{'loss': 1.2157, 'grad_norm': 0.56640625, 'learning_rate': 0.00014252227736131508, 'epoch': 0.28}\n",
      "{'loss': 1.2943, 'grad_norm': 0.494140625, 'learning_rate': 0.00014251091498385123, 'epoch': 0.28}\n",
      "{'loss': 1.1288, 'grad_norm': 0.765625, 'learning_rate': 0.00014249955260638736, 'epoch': 0.28}\n",
      "{'loss': 1.3316, 'grad_norm': 0.52734375, 'learning_rate': 0.0001424881902289235, 'epoch': 0.28}\n",
      "{'loss': 1.1646, 'grad_norm': 0.765625, 'learning_rate': 0.00014247682785145963, 'epoch': 0.28}\n",
      "{'loss': 1.2451, 'grad_norm': 0.455078125, 'learning_rate': 0.00014246546547399579, 'epoch': 0.28}\n",
      "{'loss': 1.2639, 'grad_norm': 0.59375, 'learning_rate': 0.00014245410309653194, 'epoch': 0.28}\n",
      "{'loss': 1.1109, 'grad_norm': 0.7109375, 'learning_rate': 0.00014244274071906806, 'epoch': 0.28}\n",
      "{'loss': 1.4417, 'grad_norm': 0.7578125, 'learning_rate': 0.0001424313783416042, 'epoch': 0.28}\n",
      "{'loss': 1.162, 'grad_norm': 0.9921875, 'learning_rate': 0.00014242001596414034, 'epoch': 0.28}\n",
      "{'loss': 1.114, 'grad_norm': 0.365234375, 'learning_rate': 0.0001424086535866765, 'epoch': 0.28}\n",
      "{'loss': 1.3372, 'grad_norm': 0.51953125, 'learning_rate': 0.0001423972912092126, 'epoch': 0.28}\n",
      "{'loss': 1.1437, 'grad_norm': 0.60546875, 'learning_rate': 0.00014238592883174876, 'epoch': 0.28}\n",
      "{'loss': 1.4449, 'grad_norm': 0.423828125, 'learning_rate': 0.00014237456645428492, 'epoch': 0.28}\n",
      "{'loss': 1.1449, 'grad_norm': 0.515625, 'learning_rate': 0.00014236320407682104, 'epoch': 0.28}\n",
      "{'loss': 1.2883, 'grad_norm': 0.400390625, 'learning_rate': 0.0001423518416993572, 'epoch': 0.28}\n",
      "{'loss': 1.1339, 'grad_norm': 0.5078125, 'learning_rate': 0.00014234047932189332, 'epoch': 0.28}\n",
      "{'loss': 0.9998, 'grad_norm': 0.56640625, 'learning_rate': 0.00014232911694442944, 'epoch': 0.28}\n",
      "{'loss': 1.4336, 'grad_norm': 0.546875, 'learning_rate': 0.0001423177545669656, 'epoch': 0.28}\n",
      "{'loss': 1.155, 'grad_norm': 0.6875, 'learning_rate': 0.00014230639218950174, 'epoch': 0.28}\n",
      "{'loss': 1.1458, 'grad_norm': 0.470703125, 'learning_rate': 0.0001422950298120379, 'epoch': 0.28}\n",
      "{'loss': 1.2161, 'grad_norm': 0.70703125, 'learning_rate': 0.00014228366743457402, 'epoch': 0.28}\n",
      "{'loss': 1.1812, 'grad_norm': 0.6328125, 'learning_rate': 0.00014227230505711014, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2995, 'grad_norm': 0.47265625, 'learning_rate': 0.0001422609426796463, 'epoch': 0.28}\n",
      "{'loss': 1.2362, 'grad_norm': 0.70703125, 'learning_rate': 0.00014224958030218242, 'epoch': 0.28}\n",
      "{'loss': 1.1848, 'grad_norm': 0.41796875, 'learning_rate': 0.00014223821792471857, 'epoch': 0.28}\n",
      "{'loss': 1.3562, 'grad_norm': 0.58984375, 'learning_rate': 0.00014222685554725472, 'epoch': 0.28}\n",
      "{'loss': 1.0612, 'grad_norm': 0.287109375, 'learning_rate': 0.00014221549316979085, 'epoch': 0.28}\n",
      "{'loss': 1.316, 'grad_norm': 0.51953125, 'learning_rate': 0.000142204130792327, 'epoch': 0.28}\n",
      "{'loss': 1.1934, 'grad_norm': 0.486328125, 'learning_rate': 0.00014219276841486312, 'epoch': 0.28}\n",
      "{'loss': 1.2309, 'grad_norm': 0.42578125, 'learning_rate': 0.00014218140603739927, 'epoch': 0.28}\n",
      "{'loss': 1.2926, 'grad_norm': 0.47265625, 'learning_rate': 0.0001421700436599354, 'epoch': 0.28}\n",
      "{'loss': 1.1607, 'grad_norm': 0.9375, 'learning_rate': 0.00014215868128247155, 'epoch': 0.28}\n",
      "{'loss': 1.3424, 'grad_norm': 0.462890625, 'learning_rate': 0.0001421473189050077, 'epoch': 0.28}\n",
      "{'loss': 1.1948, 'grad_norm': 0.625, 'learning_rate': 0.00014213595652754382, 'epoch': 0.28}\n",
      "{'loss': 1.2871, 'grad_norm': 0.51953125, 'learning_rate': 0.00014212459415007998, 'epoch': 0.28}\n",
      "{'loss': 1.2667, 'grad_norm': 0.490234375, 'learning_rate': 0.0001421132317726161, 'epoch': 0.28}\n",
      "{'loss': 0.9585, 'grad_norm': 1.171875, 'learning_rate': 0.00014210186939515225, 'epoch': 0.28}\n",
      "{'loss': 1.2254, 'grad_norm': 0.416015625, 'learning_rate': 0.00014209050701768838, 'epoch': 0.28}\n",
      "{'loss': 1.2534, 'grad_norm': 0.72265625, 'learning_rate': 0.00014207914464022453, 'epoch': 0.28}\n",
      "{'loss': 1.2087, 'grad_norm': 0.5078125, 'learning_rate': 0.00014206778226276068, 'epoch': 0.28}\n",
      "{'loss': 1.327, 'grad_norm': 0.578125, 'learning_rate': 0.0001420564198852968, 'epoch': 0.28}\n",
      "{'loss': 1.1385, 'grad_norm': 0.63671875, 'learning_rate': 0.00014204505750783295, 'epoch': 0.28}\n",
      "{'loss': 1.446, 'grad_norm': 0.60546875, 'learning_rate': 0.00014203369513036908, 'epoch': 0.28}\n",
      "{'loss': 1.2176, 'grad_norm': 0.52734375, 'learning_rate': 0.00014202233275290523, 'epoch': 0.28}\n",
      "{'loss': 1.1591, 'grad_norm': 0.43359375, 'learning_rate': 0.00014201097037544135, 'epoch': 0.28}\n",
      "{'loss': 1.1135, 'grad_norm': 0.625, 'learning_rate': 0.0001419996079979775, 'epoch': 0.28}\n",
      "{'loss': 1.047, 'grad_norm': 0.4609375, 'learning_rate': 0.00014198824562051366, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3759, 'grad_norm': 0.470703125, 'learning_rate': 0.00014197688324304978, 'epoch': 0.28}\n",
      "{'loss': 1.2846, 'grad_norm': 1.0078125, 'learning_rate': 0.00014196552086558593, 'epoch': 0.28}\n",
      "{'loss': 1.2326, 'grad_norm': 0.416015625, 'learning_rate': 0.00014195415848812206, 'epoch': 0.28}\n",
      "{'loss': 1.2679, 'grad_norm': 0.6875, 'learning_rate': 0.00014194279611065818, 'epoch': 0.28}\n",
      "{'loss': 1.0611, 'grad_norm': 0.5703125, 'learning_rate': 0.00014193143373319433, 'epoch': 0.28}\n",
      "{'loss': 1.2751, 'grad_norm': 0.79296875, 'learning_rate': 0.00014192007135573048, 'epoch': 0.28}\n",
      "{'loss': 1.2632, 'grad_norm': 0.67578125, 'learning_rate': 0.00014190870897826664, 'epoch': 0.28}\n",
      "{'loss': 1.1515, 'grad_norm': 0.388671875, 'learning_rate': 0.00014189734660080276, 'epoch': 0.28}\n",
      "{'loss': 1.2288, 'grad_norm': 0.84375, 'learning_rate': 0.00014188598422333888, 'epoch': 0.28}\n",
      "{'loss': 0.9731, 'grad_norm': 1.1015625, 'learning_rate': 0.00014187462184587504, 'epoch': 0.28}\n",
      "{'loss': 1.4848, 'grad_norm': 0.455078125, 'learning_rate': 0.00014186325946841116, 'epoch': 0.28}\n",
      "{'loss': 1.231, 'grad_norm': 0.80078125, 'learning_rate': 0.00014185189709094734, 'epoch': 0.28}\n",
      "{'loss': 1.2151, 'grad_norm': 0.48828125, 'learning_rate': 0.00014184053471348346, 'epoch': 0.28}\n",
      "{'loss': 1.151, 'grad_norm': 0.51953125, 'learning_rate': 0.0001418291723360196, 'epoch': 0.28}\n",
      "{'loss': 1.0855, 'grad_norm': 0.72265625, 'learning_rate': 0.00014181780995855574, 'epoch': 0.28}\n",
      "{'loss': 1.2444, 'grad_norm': 0.65625, 'learning_rate': 0.00014180644758109186, 'epoch': 0.28}\n",
      "{'loss': 1.2854, 'grad_norm': 0.66796875, 'learning_rate': 0.00014179508520362801, 'epoch': 0.28}\n",
      "{'loss': 1.1816, 'grad_norm': 0.361328125, 'learning_rate': 0.00014178372282616414, 'epoch': 0.28}\n",
      "{'loss': 1.1887, 'grad_norm': 0.5390625, 'learning_rate': 0.0001417723604487003, 'epoch': 0.28}\n",
      "{'loss': 1.0577, 'grad_norm': 0.63671875, 'learning_rate': 0.00014176099807123644, 'epoch': 0.28}\n",
      "{'loss': 1.3864, 'grad_norm': 1.015625, 'learning_rate': 0.00014174963569377257, 'epoch': 0.28}\n",
      "{'loss': 1.1339, 'grad_norm': 0.6640625, 'learning_rate': 0.00014173827331630872, 'epoch': 0.28}\n",
      "{'loss': 1.234, 'grad_norm': 0.435546875, 'learning_rate': 0.00014172691093884484, 'epoch': 0.28}\n",
      "{'loss': 1.1119, 'grad_norm': 0.48828125, 'learning_rate': 0.000141715548561381, 'epoch': 0.28}\n",
      "{'loss': 1.1252, 'grad_norm': 1.453125, 'learning_rate': 0.00014170418618391712, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3243, 'grad_norm': 0.546875, 'learning_rate': 0.00014169282380645327, 'epoch': 0.28}\n",
      "{'loss': 1.2295, 'grad_norm': 0.65625, 'learning_rate': 0.00014168146142898942, 'epoch': 0.28}\n",
      "{'loss': 1.363, 'grad_norm': 0.451171875, 'learning_rate': 0.00014167009905152554, 'epoch': 0.28}\n",
      "{'loss': 1.2395, 'grad_norm': 0.46875, 'learning_rate': 0.0001416587366740617, 'epoch': 0.28}\n",
      "{'loss': 1.0045, 'grad_norm': 0.609375, 'learning_rate': 0.00014164737429659782, 'epoch': 0.28}\n",
      "{'loss': 1.4104, 'grad_norm': 0.421875, 'learning_rate': 0.00014163601191913397, 'epoch': 0.28}\n",
      "{'loss': 1.2139, 'grad_norm': 0.6953125, 'learning_rate': 0.0001416246495416701, 'epoch': 0.28}\n",
      "{'loss': 1.2367, 'grad_norm': 0.61328125, 'learning_rate': 0.00014161328716420625, 'epoch': 0.28}\n",
      "{'loss': 1.3574, 'grad_norm': 0.447265625, 'learning_rate': 0.0001416019247867424, 'epoch': 0.28}\n",
      "{'loss': 0.985, 'grad_norm': 0.70703125, 'learning_rate': 0.00014159056240927852, 'epoch': 0.28}\n",
      "{'loss': 1.4028, 'grad_norm': 0.4765625, 'learning_rate': 0.00014157920003181467, 'epoch': 0.28}\n",
      "{'loss': 1.1252, 'grad_norm': 0.88671875, 'learning_rate': 0.0001415678376543508, 'epoch': 0.28}\n",
      "{'loss': 1.1888, 'grad_norm': 0.38671875, 'learning_rate': 0.00014155647527688692, 'epoch': 0.28}\n",
      "{'loss': 1.3066, 'grad_norm': 0.515625, 'learning_rate': 0.00014154511289942307, 'epoch': 0.28}\n",
      "{'loss': 1.1109, 'grad_norm': 0.4453125, 'learning_rate': 0.00014153375052195923, 'epoch': 0.28}\n",
      "{'loss': 1.4327, 'grad_norm': 0.71484375, 'learning_rate': 0.00014152238814449538, 'epoch': 0.28}\n",
      "{'loss': 1.1885, 'grad_norm': 1.0078125, 'learning_rate': 0.0001415110257670315, 'epoch': 0.28}\n",
      "{'loss': 1.2026, 'grad_norm': 0.388671875, 'learning_rate': 0.00014149966338956763, 'epoch': 0.28}\n",
      "{'loss': 1.3685, 'grad_norm': 0.5859375, 'learning_rate': 0.00014148830101210378, 'epoch': 0.28}\n",
      "{'loss': 1.1637, 'grad_norm': 0.7734375, 'learning_rate': 0.0001414769386346399, 'epoch': 0.28}\n",
      "{'loss': 1.4669, 'grad_norm': 0.44140625, 'learning_rate': 0.00014146557625717608, 'epoch': 0.28}\n",
      "{'loss': 1.2163, 'grad_norm': 0.76953125, 'learning_rate': 0.0001414542138797122, 'epoch': 0.28}\n",
      "{'loss': 1.2065, 'grad_norm': 0.435546875, 'learning_rate': 0.00014144285150224833, 'epoch': 0.28}\n",
      "{'loss': 1.2679, 'grad_norm': 0.57421875, 'learning_rate': 0.00014143148912478448, 'epoch': 0.28}\n",
      "{'loss': 1.0201, 'grad_norm': 0.82421875, 'learning_rate': 0.0001414201267473206, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3753, 'grad_norm': 0.4765625, 'learning_rate': 0.00014140876436985676, 'epoch': 0.28}\n",
      "{'loss': 1.2578, 'grad_norm': 1.0078125, 'learning_rate': 0.00014139740199239288, 'epoch': 0.28}\n",
      "{'loss': 1.1802, 'grad_norm': 0.6171875, 'learning_rate': 0.00014138603961492903, 'epoch': 0.28}\n",
      "{'loss': 1.3187, 'grad_norm': 0.5234375, 'learning_rate': 0.00014137467723746518, 'epoch': 0.28}\n",
      "{'loss': 1.0822, 'grad_norm': 0.67578125, 'learning_rate': 0.0001413633148600013, 'epoch': 0.28}\n",
      "{'loss': 1.3891, 'grad_norm': 0.5390625, 'learning_rate': 0.00014135195248253746, 'epoch': 0.28}\n",
      "{'loss': 1.1062, 'grad_norm': 0.6484375, 'learning_rate': 0.00014134059010507358, 'epoch': 0.28}\n",
      "{'loss': 1.4129, 'grad_norm': 0.421875, 'learning_rate': 0.00014132922772760973, 'epoch': 0.28}\n",
      "{'loss': 1.3341, 'grad_norm': 0.51171875, 'learning_rate': 0.00014131786535014586, 'epoch': 0.28}\n",
      "{'loss': 1.0635, 'grad_norm': 0.94140625, 'learning_rate': 0.000141306502972682, 'epoch': 0.28}\n",
      "{'loss': 1.3288, 'grad_norm': 0.44140625, 'learning_rate': 0.00014129514059521816, 'epoch': 0.28}\n",
      "{'loss': 1.2524, 'grad_norm': 0.62109375, 'learning_rate': 0.00014128377821775429, 'epoch': 0.28}\n",
      "{'loss': 1.4034, 'grad_norm': 0.466796875, 'learning_rate': 0.00014127241584029044, 'epoch': 0.28}\n",
      "{'loss': 1.3369, 'grad_norm': 0.68359375, 'learning_rate': 0.00014126105346282656, 'epoch': 0.28}\n",
      "{'loss': 1.0819, 'grad_norm': 0.421875, 'learning_rate': 0.0001412496910853627, 'epoch': 0.28}\n",
      "{'loss': 1.3129, 'grad_norm': 0.458984375, 'learning_rate': 0.00014123832870789884, 'epoch': 0.28}\n",
      "{'loss': 1.1615, 'grad_norm': 0.7421875, 'learning_rate': 0.000141226966330435, 'epoch': 0.28}\n",
      "{'loss': 1.3098, 'grad_norm': 0.65234375, 'learning_rate': 0.00014121560395297114, 'epoch': 0.28}\n",
      "{'loss': 1.4168, 'grad_norm': 0.57421875, 'learning_rate': 0.00014120424157550726, 'epoch': 0.28}\n",
      "{'loss': 1.1017, 'grad_norm': 1.015625, 'learning_rate': 0.00014119287919804342, 'epoch': 0.28}\n",
      "{'loss': 1.3304, 'grad_norm': 0.412109375, 'learning_rate': 0.00014118151682057954, 'epoch': 0.28}\n",
      "{'loss': 1.0771, 'grad_norm': 0.447265625, 'learning_rate': 0.00014117015444311566, 'epoch': 0.28}\n",
      "{'loss': 1.2353, 'grad_norm': 0.423828125, 'learning_rate': 0.00014115879206565184, 'epoch': 0.28}\n",
      "{'loss': 1.1166, 'grad_norm': 0.609375, 'learning_rate': 0.00014114742968818797, 'epoch': 0.28}\n",
      "{'loss': 1.0488, 'grad_norm': 0.31640625, 'learning_rate': 0.00014113606731072412, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2706, 'grad_norm': 0.43359375, 'learning_rate': 0.00014112470493326024, 'epoch': 0.28}\n",
      "{'loss': 1.1569, 'grad_norm': 0.80078125, 'learning_rate': 0.00014111334255579637, 'epoch': 0.28}\n",
      "{'loss': 1.2666, 'grad_norm': 0.408203125, 'learning_rate': 0.00014110198017833252, 'epoch': 0.28}\n",
      "{'loss': 1.2201, 'grad_norm': 1.0390625, 'learning_rate': 0.00014109061780086864, 'epoch': 0.28}\n",
      "{'loss': 1.1225, 'grad_norm': 0.68359375, 'learning_rate': 0.00014107925542340482, 'epoch': 0.28}\n",
      "{'loss': 1.346, 'grad_norm': 0.6640625, 'learning_rate': 0.00014106789304594095, 'epoch': 0.28}\n",
      "{'loss': 1.0869, 'grad_norm': 0.76171875, 'learning_rate': 0.00014105653066847707, 'epoch': 0.28}\n",
      "{'loss': 1.0632, 'grad_norm': 0.4765625, 'learning_rate': 0.00014104516829101322, 'epoch': 0.28}\n",
      "{'loss': 1.1447, 'grad_norm': 0.51953125, 'learning_rate': 0.00014103380591354935, 'epoch': 0.28}\n",
      "{'loss': 1.1171, 'grad_norm': 0.78515625, 'learning_rate': 0.0001410224435360855, 'epoch': 0.28}\n",
      "{'loss': 1.3585, 'grad_norm': 0.51171875, 'learning_rate': 0.00014101108115862162, 'epoch': 0.28}\n",
      "{'loss': 1.1495, 'grad_norm': 0.60546875, 'learning_rate': 0.00014099971878115777, 'epoch': 0.28}\n",
      "{'loss': 1.2176, 'grad_norm': 0.462890625, 'learning_rate': 0.00014098835640369392, 'epoch': 0.28}\n",
      "{'loss': 1.1506, 'grad_norm': 0.82421875, 'learning_rate': 0.00014097699402623005, 'epoch': 0.28}\n",
      "{'loss': 1.1416, 'grad_norm': 0.6640625, 'learning_rate': 0.0001409656316487662, 'epoch': 0.28}\n",
      "{'loss': 1.3866, 'grad_norm': 0.609375, 'learning_rate': 0.00014095426927130232, 'epoch': 0.28}\n",
      "{'loss': 1.1923, 'grad_norm': 0.83203125, 'learning_rate': 0.00014094290689383848, 'epoch': 0.28}\n",
      "{'loss': 1.2663, 'grad_norm': 0.45703125, 'learning_rate': 0.0001409315445163746, 'epoch': 0.28}\n",
      "{'loss': 1.2291, 'grad_norm': 0.54296875, 'learning_rate': 0.00014092018213891075, 'epoch': 0.28}\n",
      "{'loss': 1.0072, 'grad_norm': 0.6953125, 'learning_rate': 0.0001409088197614469, 'epoch': 0.28}\n",
      "{'loss': 1.4615, 'grad_norm': 0.443359375, 'learning_rate': 0.00014089745738398303, 'epoch': 0.28}\n",
      "{'loss': 1.124, 'grad_norm': 0.765625, 'learning_rate': 0.00014088609500651918, 'epoch': 0.29}\n",
      "{'loss': 1.2363, 'grad_norm': 0.40234375, 'learning_rate': 0.0001408747326290553, 'epoch': 0.29}\n",
      "{'loss': 1.2436, 'grad_norm': 0.63671875, 'learning_rate': 0.00014086337025159145, 'epoch': 0.29}\n",
      "{'loss': 1.343, 'grad_norm': 1.109375, 'learning_rate': 0.00014085200787412758, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3237, 'grad_norm': 0.69140625, 'learning_rate': 0.00014084064549666373, 'epoch': 0.29}\n",
      "{'loss': 1.1935, 'grad_norm': 0.58203125, 'learning_rate': 0.00014082928311919988, 'epoch': 0.29}\n",
      "{'loss': 1.1912, 'grad_norm': 0.40625, 'learning_rate': 0.000140817920741736, 'epoch': 0.29}\n",
      "{'loss': 1.2903, 'grad_norm': 0.5390625, 'learning_rate': 0.00014080655836427216, 'epoch': 0.29}\n",
      "{'loss': 1.1726, 'grad_norm': 0.75, 'learning_rate': 0.00014079519598680828, 'epoch': 0.29}\n",
      "{'loss': 1.2774, 'grad_norm': 0.64453125, 'learning_rate': 0.0001407838336093444, 'epoch': 0.29}\n",
      "{'loss': 1.0904, 'grad_norm': 0.76171875, 'learning_rate': 0.00014077247123188058, 'epoch': 0.29}\n",
      "{'loss': 1.1802, 'grad_norm': 0.3828125, 'learning_rate': 0.0001407611088544167, 'epoch': 0.29}\n",
      "{'loss': 1.2044, 'grad_norm': 0.5859375, 'learning_rate': 0.00014074974647695286, 'epoch': 0.29}\n",
      "{'loss': 1.1521, 'grad_norm': 0.63671875, 'learning_rate': 0.00014073838409948898, 'epoch': 0.29}\n",
      "{'loss': 1.34, 'grad_norm': 0.486328125, 'learning_rate': 0.0001407270217220251, 'epoch': 0.29}\n",
      "{'loss': 1.0749, 'grad_norm': 0.55078125, 'learning_rate': 0.00014071565934456126, 'epoch': 0.29}\n",
      "{'loss': 1.1507, 'grad_norm': 0.48046875, 'learning_rate': 0.00014070429696709738, 'epoch': 0.29}\n",
      "{'loss': 1.2879, 'grad_norm': 0.5078125, 'learning_rate': 0.00014069293458963356, 'epoch': 0.29}\n",
      "{'loss': 1.1623, 'grad_norm': 0.8984375, 'learning_rate': 0.0001406815722121697, 'epoch': 0.29}\n",
      "{'loss': 1.2922, 'grad_norm': 0.62109375, 'learning_rate': 0.0001406702098347058, 'epoch': 0.29}\n",
      "{'loss': 1.2119, 'grad_norm': 0.734375, 'learning_rate': 0.00014065884745724196, 'epoch': 0.29}\n",
      "{'loss': 1.1281, 'grad_norm': 0.62109375, 'learning_rate': 0.0001406474850797781, 'epoch': 0.29}\n",
      "{'loss': 1.2205, 'grad_norm': 0.55859375, 'learning_rate': 0.00014063612270231424, 'epoch': 0.29}\n",
      "{'loss': 1.0984, 'grad_norm': 1.03125, 'learning_rate': 0.00014062476032485036, 'epoch': 0.29}\n",
      "{'loss': 1.3022, 'grad_norm': 0.54296875, 'learning_rate': 0.00014061339794738651, 'epoch': 0.29}\n",
      "{'loss': 1.3829, 'grad_norm': 0.60546875, 'learning_rate': 0.00014060203556992267, 'epoch': 0.29}\n",
      "{'loss': 1.277, 'grad_norm': 0.39453125, 'learning_rate': 0.0001405906731924588, 'epoch': 0.29}\n",
      "{'loss': 1.2585, 'grad_norm': 0.59765625, 'learning_rate': 0.00014057931081499494, 'epoch': 0.29}\n",
      "{'loss': 1.2046, 'grad_norm': 0.7734375, 'learning_rate': 0.00014056794843753107, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4481, 'grad_norm': 0.45703125, 'learning_rate': 0.00014055658606006722, 'epoch': 0.29}\n",
      "{'loss': 1.0893, 'grad_norm': 0.78515625, 'learning_rate': 0.00014054522368260334, 'epoch': 0.29}\n",
      "{'loss': 1.308, 'grad_norm': 0.546875, 'learning_rate': 0.0001405338613051395, 'epoch': 0.29}\n",
      "{'loss': 1.2407, 'grad_norm': 0.671875, 'learning_rate': 0.00014052249892767564, 'epoch': 0.29}\n",
      "{'loss': 1.1437, 'grad_norm': 0.72265625, 'learning_rate': 0.00014051113655021177, 'epoch': 0.29}\n",
      "{'loss': 1.4751, 'grad_norm': 0.458984375, 'learning_rate': 0.00014049977417274792, 'epoch': 0.29}\n",
      "{'loss': 1.2151, 'grad_norm': 0.5, 'learning_rate': 0.00014048841179528404, 'epoch': 0.29}\n",
      "{'loss': 1.0905, 'grad_norm': 0.41796875, 'learning_rate': 0.0001404770494178202, 'epoch': 0.29}\n",
      "{'loss': 1.2348, 'grad_norm': 0.640625, 'learning_rate': 0.00014046568704035635, 'epoch': 0.29}\n",
      "{'loss': 1.0137, 'grad_norm': 0.462890625, 'learning_rate': 0.00014045432466289247, 'epoch': 0.29}\n",
      "{'loss': 1.4426, 'grad_norm': 0.45703125, 'learning_rate': 0.00014044296228542862, 'epoch': 0.29}\n",
      "{'loss': 1.2661, 'grad_norm': 0.7265625, 'learning_rate': 0.00014043159990796475, 'epoch': 0.29}\n",
      "{'loss': 1.2374, 'grad_norm': 0.419921875, 'learning_rate': 0.0001404202375305009, 'epoch': 0.29}\n",
      "{'loss': 1.3773, 'grad_norm': 0.5078125, 'learning_rate': 0.00014040887515303702, 'epoch': 0.29}\n",
      "{'loss': 1.1814, 'grad_norm': 0.74609375, 'learning_rate': 0.00014039751277557315, 'epoch': 0.29}\n",
      "{'loss': 1.3391, 'grad_norm': 0.400390625, 'learning_rate': 0.00014038615039810932, 'epoch': 0.29}\n",
      "{'loss': 1.2615, 'grad_norm': 0.5234375, 'learning_rate': 0.00014037478802064545, 'epoch': 0.29}\n",
      "{'loss': 1.2441, 'grad_norm': 0.40625, 'learning_rate': 0.0001403634256431816, 'epoch': 0.29}\n",
      "{'loss': 1.2175, 'grad_norm': 0.56640625, 'learning_rate': 0.00014035206326571772, 'epoch': 0.29}\n",
      "{'loss': 1.1379, 'grad_norm': 0.921875, 'learning_rate': 0.00014034070088825385, 'epoch': 0.29}\n",
      "{'loss': 1.4271, 'grad_norm': 0.5, 'learning_rate': 0.00014032933851079, 'epoch': 0.29}\n",
      "{'loss': 1.2072, 'grad_norm': 0.55859375, 'learning_rate': 0.00014031797613332612, 'epoch': 0.29}\n",
      "{'loss': 1.2377, 'grad_norm': 0.396484375, 'learning_rate': 0.0001403066137558623, 'epoch': 0.29}\n",
      "{'loss': 1.3667, 'grad_norm': 0.50390625, 'learning_rate': 0.00014029525137839843, 'epoch': 0.29}\n",
      "{'loss': 1.2523, 'grad_norm': 0.66015625, 'learning_rate': 0.00014028388900093455, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3971, 'grad_norm': 0.443359375, 'learning_rate': 0.0001402725266234707, 'epoch': 0.29}\n",
      "{'loss': 1.2572, 'grad_norm': 0.65234375, 'learning_rate': 0.00014026116424600683, 'epoch': 0.29}\n",
      "{'loss': 1.3152, 'grad_norm': 0.462890625, 'learning_rate': 0.00014024980186854298, 'epoch': 0.29}\n",
      "{'loss': 1.3406, 'grad_norm': 0.59765625, 'learning_rate': 0.0001402384394910791, 'epoch': 0.29}\n",
      "{'loss': 1.045, 'grad_norm': 0.8125, 'learning_rate': 0.00014022707711361525, 'epoch': 0.29}\n",
      "{'loss': 1.3784, 'grad_norm': 0.5078125, 'learning_rate': 0.0001402157147361514, 'epoch': 0.29}\n",
      "{'loss': 1.2285, 'grad_norm': 0.8203125, 'learning_rate': 0.00014020435235868753, 'epoch': 0.29}\n",
      "{'loss': 1.2422, 'grad_norm': 0.62109375, 'learning_rate': 0.00014019298998122368, 'epoch': 0.29}\n",
      "{'loss': 1.2532, 'grad_norm': 0.7265625, 'learning_rate': 0.0001401816276037598, 'epoch': 0.29}\n",
      "{'loss': 1.028, 'grad_norm': 0.89453125, 'learning_rate': 0.00014017026522629596, 'epoch': 0.29}\n",
      "{'loss': 1.2597, 'grad_norm': 0.494140625, 'learning_rate': 0.00014015890284883208, 'epoch': 0.29}\n",
      "{'loss': 1.04, 'grad_norm': 0.640625, 'learning_rate': 0.00014014754047136823, 'epoch': 0.29}\n",
      "{'loss': 1.2622, 'grad_norm': 0.44140625, 'learning_rate': 0.00014013617809390438, 'epoch': 0.29}\n",
      "{'loss': 1.3357, 'grad_norm': 0.64453125, 'learning_rate': 0.0001401248157164405, 'epoch': 0.29}\n",
      "{'loss': 1.1436, 'grad_norm': 0.703125, 'learning_rate': 0.00014011345333897666, 'epoch': 0.29}\n",
      "{'loss': 1.4024, 'grad_norm': 0.4453125, 'learning_rate': 0.00014010209096151278, 'epoch': 0.29}\n",
      "{'loss': 1.2307, 'grad_norm': 0.65234375, 'learning_rate': 0.00014009072858404894, 'epoch': 0.29}\n",
      "{'loss': 1.1003, 'grad_norm': 0.447265625, 'learning_rate': 0.0001400793662065851, 'epoch': 0.29}\n",
      "{'loss': 1.3108, 'grad_norm': 0.609375, 'learning_rate': 0.0001400680038291212, 'epoch': 0.29}\n",
      "{'loss': 1.1187, 'grad_norm': 0.70703125, 'learning_rate': 0.00014005664145165736, 'epoch': 0.29}\n",
      "{'loss': 1.543, 'grad_norm': 0.625, 'learning_rate': 0.0001400452790741935, 'epoch': 0.29}\n",
      "{'loss': 1.2477, 'grad_norm': 0.7265625, 'learning_rate': 0.00014003391669672964, 'epoch': 0.29}\n",
      "{'loss': 1.2278, 'grad_norm': 0.44140625, 'learning_rate': 0.00014002255431926576, 'epoch': 0.29}\n",
      "{'loss': 1.3817, 'grad_norm': 0.4453125, 'learning_rate': 0.0001400111919418019, 'epoch': 0.29}\n",
      "{'loss': 1.0683, 'grad_norm': 0.703125, 'learning_rate': 0.00013999982956433807, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.317, 'grad_norm': 0.431640625, 'learning_rate': 0.0001399884671868742, 'epoch': 0.29}\n",
      "{'loss': 1.1669, 'grad_norm': 0.609375, 'learning_rate': 0.00013997710480941034, 'epoch': 0.29}\n",
      "{'loss': 1.0697, 'grad_norm': 0.71875, 'learning_rate': 0.00013996574243194647, 'epoch': 0.29}\n",
      "{'loss': 1.2716, 'grad_norm': 0.5546875, 'learning_rate': 0.0001399543800544826, 'epoch': 0.29}\n",
      "{'loss': 1.1376, 'grad_norm': 1.25, 'learning_rate': 0.00013994301767701874, 'epoch': 0.29}\n",
      "{'loss': 1.4435, 'grad_norm': 0.44140625, 'learning_rate': 0.00013993165529955487, 'epoch': 0.29}\n",
      "{'loss': 1.2856, 'grad_norm': 0.62109375, 'learning_rate': 0.00013992029292209104, 'epoch': 0.29}\n",
      "{'loss': 1.147, 'grad_norm': 0.4296875, 'learning_rate': 0.00013990893054462717, 'epoch': 0.29}\n",
      "{'loss': 1.3042, 'grad_norm': 0.5625, 'learning_rate': 0.0001398975681671633, 'epoch': 0.29}\n",
      "{'loss': 1.1385, 'grad_norm': 0.58984375, 'learning_rate': 0.00013988620578969944, 'epoch': 0.29}\n",
      "{'loss': 1.3551, 'grad_norm': 0.640625, 'learning_rate': 0.00013987484341223557, 'epoch': 0.29}\n",
      "{'loss': 1.2238, 'grad_norm': 0.74609375, 'learning_rate': 0.00013986348103477172, 'epoch': 0.29}\n",
      "{'loss': 1.2245, 'grad_norm': 0.6328125, 'learning_rate': 0.00013985211865730784, 'epoch': 0.29}\n",
      "{'loss': 1.1724, 'grad_norm': 0.54296875, 'learning_rate': 0.000139840756279844, 'epoch': 0.29}\n",
      "{'loss': 1.2066, 'grad_norm': 0.56640625, 'learning_rate': 0.00013982939390238015, 'epoch': 0.29}\n",
      "{'loss': 1.4289, 'grad_norm': 0.640625, 'learning_rate': 0.00013981803152491627, 'epoch': 0.29}\n",
      "{'loss': 1.2722, 'grad_norm': 0.63671875, 'learning_rate': 0.00013980666914745242, 'epoch': 0.29}\n",
      "{'loss': 1.1586, 'grad_norm': 0.431640625, 'learning_rate': 0.00013979530676998855, 'epoch': 0.29}\n",
      "{'loss': 1.1788, 'grad_norm': 0.5390625, 'learning_rate': 0.0001397839443925247, 'epoch': 0.29}\n",
      "{'loss': 1.0597, 'grad_norm': 1.0390625, 'learning_rate': 0.00013977258201506085, 'epoch': 0.29}\n",
      "{'loss': 1.3175, 'grad_norm': 0.88671875, 'learning_rate': 0.00013976121963759697, 'epoch': 0.29}\n",
      "{'loss': 1.139, 'grad_norm': 0.55859375, 'learning_rate': 0.00013974985726013313, 'epoch': 0.29}\n",
      "{'loss': 1.1752, 'grad_norm': 0.37890625, 'learning_rate': 0.00013973849488266925, 'epoch': 0.29}\n",
      "{'loss': 1.1557, 'grad_norm': 0.75, 'learning_rate': 0.0001397271325052054, 'epoch': 0.29}\n",
      "{'loss': 1.1511, 'grad_norm': 0.7421875, 'learning_rate': 0.00013971577012774153, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3671, 'grad_norm': 0.5078125, 'learning_rate': 0.00013970440775027768, 'epoch': 0.29}\n",
      "{'loss': 1.181, 'grad_norm': 0.609375, 'learning_rate': 0.00013969304537281383, 'epoch': 0.29}\n",
      "{'loss': 1.1959, 'grad_norm': 0.35546875, 'learning_rate': 0.00013968168299534995, 'epoch': 0.29}\n",
      "{'loss': 1.2646, 'grad_norm': 0.58984375, 'learning_rate': 0.0001396703206178861, 'epoch': 0.29}\n",
      "{'loss': 1.2577, 'grad_norm': 0.84765625, 'learning_rate': 0.00013965895824042223, 'epoch': 0.29}\n",
      "{'loss': 1.2637, 'grad_norm': 0.5234375, 'learning_rate': 0.00013964759586295838, 'epoch': 0.29}\n",
      "{'loss': 1.0014, 'grad_norm': 0.9140625, 'learning_rate': 0.0001396362334854945, 'epoch': 0.29}\n",
      "{'loss': 1.3388, 'grad_norm': 0.44921875, 'learning_rate': 0.00013962487110803063, 'epoch': 0.29}\n",
      "{'loss': 1.1174, 'grad_norm': 0.5703125, 'learning_rate': 0.0001396135087305668, 'epoch': 0.29}\n",
      "{'loss': 1.125, 'grad_norm': 1.078125, 'learning_rate': 0.00013960214635310293, 'epoch': 0.29}\n",
      "{'loss': 1.3121, 'grad_norm': 0.5234375, 'learning_rate': 0.00013959078397563908, 'epoch': 0.29}\n",
      "{'loss': 1.3007, 'grad_norm': 0.64453125, 'learning_rate': 0.0001395794215981752, 'epoch': 0.29}\n",
      "{'loss': 1.2513, 'grad_norm': 0.44921875, 'learning_rate': 0.00013956805922071133, 'epoch': 0.29}\n",
      "{'loss': 1.1439, 'grad_norm': 0.515625, 'learning_rate': 0.00013955669684324748, 'epoch': 0.29}\n",
      "{'loss': 1.1972, 'grad_norm': 0.96484375, 'learning_rate': 0.0001395453344657836, 'epoch': 0.29}\n",
      "{'loss': 1.4195, 'grad_norm': 0.46484375, 'learning_rate': 0.00013953397208831979, 'epoch': 0.29}\n",
      "{'loss': 1.271, 'grad_norm': 0.68359375, 'learning_rate': 0.0001395226097108559, 'epoch': 0.29}\n",
      "{'loss': 1.3096, 'grad_norm': 0.44921875, 'learning_rate': 0.00013951124733339203, 'epoch': 0.29}\n",
      "{'loss': 1.2657, 'grad_norm': 0.7421875, 'learning_rate': 0.00013949988495592819, 'epoch': 0.29}\n",
      "{'loss': 1.109, 'grad_norm': 0.85546875, 'learning_rate': 0.0001394885225784643, 'epoch': 0.29}\n",
      "{'loss': 1.3308, 'grad_norm': 0.65234375, 'learning_rate': 0.00013947716020100046, 'epoch': 0.29}\n",
      "{'loss': 1.2135, 'grad_norm': 0.71875, 'learning_rate': 0.00013946579782353659, 'epoch': 0.29}\n",
      "{'loss': 1.3613, 'grad_norm': 0.400390625, 'learning_rate': 0.00013945443544607274, 'epoch': 0.29}\n",
      "{'loss': 1.2396, 'grad_norm': 0.482421875, 'learning_rate': 0.0001394430730686089, 'epoch': 0.29}\n",
      "{'loss': 1.13, 'grad_norm': 0.35546875, 'learning_rate': 0.000139431710691145, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3183, 'grad_norm': 0.455078125, 'learning_rate': 0.00013942034831368116, 'epoch': 0.29}\n",
      "{'loss': 1.2254, 'grad_norm': 0.6015625, 'learning_rate': 0.0001394089859362173, 'epoch': 0.29}\n",
      "{'loss': 1.2086, 'grad_norm': 0.443359375, 'learning_rate': 0.00013939762355875344, 'epoch': 0.29}\n",
      "{'loss': 1.3399, 'grad_norm': 0.455078125, 'learning_rate': 0.0001393862611812896, 'epoch': 0.29}\n",
      "{'loss': 1.0567, 'grad_norm': 0.8125, 'learning_rate': 0.00013937489880382572, 'epoch': 0.29}\n",
      "{'loss': 1.3834, 'grad_norm': 0.478515625, 'learning_rate': 0.00013936353642636187, 'epoch': 0.29}\n",
      "{'loss': 1.143, 'grad_norm': 0.5859375, 'learning_rate': 0.000139352174048898, 'epoch': 0.29}\n",
      "{'loss': 1.2323, 'grad_norm': 0.455078125, 'learning_rate': 0.00013934081167143414, 'epoch': 0.29}\n",
      "{'loss': 1.255, 'grad_norm': 0.51171875, 'learning_rate': 0.00013932944929397027, 'epoch': 0.29}\n",
      "{'loss': 1.1208, 'grad_norm': 0.81640625, 'learning_rate': 0.00013931808691650642, 'epoch': 0.29}\n",
      "{'loss': 1.305, 'grad_norm': 0.51953125, 'learning_rate': 0.00013930672453904257, 'epoch': 0.29}\n",
      "{'loss': 1.1132, 'grad_norm': 0.76953125, 'learning_rate': 0.0001392953621615787, 'epoch': 0.29}\n",
      "{'loss': 1.1934, 'grad_norm': 0.439453125, 'learning_rate': 0.00013928399978411485, 'epoch': 0.29}\n",
      "{'loss': 1.2356, 'grad_norm': 0.7109375, 'learning_rate': 0.00013927263740665097, 'epoch': 0.29}\n",
      "{'loss': 1.2069, 'grad_norm': 0.87109375, 'learning_rate': 0.00013926127502918712, 'epoch': 0.29}\n",
      "{'loss': 1.3769, 'grad_norm': 0.4609375, 'learning_rate': 0.00013924991265172325, 'epoch': 0.29}\n",
      "{'loss': 1.2454, 'grad_norm': 0.69921875, 'learning_rate': 0.00013923855027425937, 'epoch': 0.29}\n",
      "{'loss': 1.3189, 'grad_norm': 0.39453125, 'learning_rate': 0.00013922718789679555, 'epoch': 0.29}\n",
      "{'loss': 1.3475, 'grad_norm': 0.77734375, 'learning_rate': 0.00013921582551933167, 'epoch': 0.29}\n",
      "{'loss': 1.1078, 'grad_norm': 0.75390625, 'learning_rate': 0.00013920446314186782, 'epoch': 0.29}\n",
      "{'loss': 1.4745, 'grad_norm': 0.52734375, 'learning_rate': 0.00013919310076440395, 'epoch': 0.29}\n",
      "{'loss': 1.2131, 'grad_norm': 0.6796875, 'learning_rate': 0.00013918173838694007, 'epoch': 0.29}\n",
      "{'loss': 1.167, 'grad_norm': 1.2578125, 'learning_rate': 0.00013917037600947622, 'epoch': 0.29}\n",
      "{'loss': 1.2902, 'grad_norm': 0.6171875, 'learning_rate': 0.00013915901363201235, 'epoch': 0.29}\n",
      "{'loss': 1.0635, 'grad_norm': 0.5703125, 'learning_rate': 0.00013914765125454853, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3241, 'grad_norm': 0.46875, 'learning_rate': 0.00013913628887708465, 'epoch': 0.29}\n",
      "{'loss': 1.1538, 'grad_norm': 0.60546875, 'learning_rate': 0.00013912492649962078, 'epoch': 0.29}\n",
      "{'loss': 1.321, 'grad_norm': 0.6875, 'learning_rate': 0.00013911356412215693, 'epoch': 0.29}\n",
      "{'loss': 1.2339, 'grad_norm': 0.625, 'learning_rate': 0.00013910220174469305, 'epoch': 0.29}\n",
      "{'loss': 1.0265, 'grad_norm': 1.0390625, 'learning_rate': 0.0001390908393672292, 'epoch': 0.29}\n",
      "{'loss': 1.2514, 'grad_norm': 0.53125, 'learning_rate': 0.00013907947698976535, 'epoch': 0.29}\n",
      "{'loss': 1.161, 'grad_norm': 0.7421875, 'learning_rate': 0.00013906811461230148, 'epoch': 0.29}\n",
      "{'loss': 1.2274, 'grad_norm': 0.49609375, 'learning_rate': 0.00013905675223483763, 'epoch': 0.29}\n",
      "{'loss': 1.2894, 'grad_norm': 0.5859375, 'learning_rate': 0.00013904538985737375, 'epoch': 0.29}\n",
      "{'loss': 1.0203, 'grad_norm': 0.82421875, 'learning_rate': 0.0001390340274799099, 'epoch': 0.29}\n",
      "{'loss': 1.4621, 'grad_norm': 0.4765625, 'learning_rate': 0.00013902266510244603, 'epoch': 0.29}\n",
      "{'loss': 1.257, 'grad_norm': 0.5, 'learning_rate': 0.00013901130272498218, 'epoch': 0.29}\n",
      "{'loss': 1.1201, 'grad_norm': 0.4296875, 'learning_rate': 0.00013899994034751833, 'epoch': 0.29}\n",
      "{'loss': 1.3475, 'grad_norm': 0.49609375, 'learning_rate': 0.00013898857797005446, 'epoch': 0.29}\n",
      "{'loss': 1.1754, 'grad_norm': 0.546875, 'learning_rate': 0.0001389772155925906, 'epoch': 0.29}\n",
      "{'loss': 1.2712, 'grad_norm': 0.5625, 'learning_rate': 0.00013896585321512673, 'epoch': 0.29}\n",
      "{'loss': 1.1715, 'grad_norm': 0.75390625, 'learning_rate': 0.00013895449083766288, 'epoch': 0.29}\n",
      "{'loss': 1.2463, 'grad_norm': 0.439453125, 'learning_rate': 0.000138943128460199, 'epoch': 0.29}\n",
      "{'loss': 1.1284, 'grad_norm': 0.578125, 'learning_rate': 0.00013893176608273516, 'epoch': 0.29}\n",
      "{'loss': 0.9277, 'grad_norm': 1.1484375, 'learning_rate': 0.0001389204037052713, 'epoch': 0.29}\n",
      "{'loss': 1.3083, 'grad_norm': 0.4453125, 'learning_rate': 0.00013890904132780744, 'epoch': 0.29}\n",
      "{'loss': 1.1694, 'grad_norm': 0.65234375, 'learning_rate': 0.0001388976789503436, 'epoch': 0.29}\n",
      "{'loss': 1.1648, 'grad_norm': 0.4453125, 'learning_rate': 0.0001388863165728797, 'epoch': 0.29}\n",
      "{'loss': 1.353, 'grad_norm': 0.546875, 'learning_rate': 0.00013887495419541586, 'epoch': 0.29}\n",
      "{'loss': 1.0627, 'grad_norm': 0.91796875, 'learning_rate': 0.000138863591817952, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3724, 'grad_norm': 0.55859375, 'learning_rate': 0.0001388522294404881, 'epoch': 0.29}\n",
      "{'loss': 1.0912, 'grad_norm': 1.140625, 'learning_rate': 0.0001388408670630243, 'epoch': 0.29}\n",
      "{'loss': 1.3368, 'grad_norm': 0.48828125, 'learning_rate': 0.00013882950468556041, 'epoch': 0.29}\n",
      "{'loss': 1.1761, 'grad_norm': 0.57421875, 'learning_rate': 0.00013881814230809657, 'epoch': 0.29}\n",
      "{'loss': 1.0487, 'grad_norm': 1.234375, 'learning_rate': 0.0001388067799306327, 'epoch': 0.29}\n",
      "{'loss': 1.4263, 'grad_norm': 0.50390625, 'learning_rate': 0.00013879541755316881, 'epoch': 0.29}\n",
      "{'loss': 1.2477, 'grad_norm': 0.60546875, 'learning_rate': 0.00013878405517570497, 'epoch': 0.29}\n",
      "{'loss': 1.1992, 'grad_norm': 0.4765625, 'learning_rate': 0.0001387726927982411, 'epoch': 0.29}\n",
      "{'loss': 1.2906, 'grad_norm': 0.5234375, 'learning_rate': 0.00013876133042077727, 'epoch': 0.29}\n",
      "{'loss': 0.9946, 'grad_norm': 0.79296875, 'learning_rate': 0.0001387499680433134, 'epoch': 0.29}\n",
      "{'loss': 1.4664, 'grad_norm': 0.5, 'learning_rate': 0.00013873860566584952, 'epoch': 0.29}\n",
      "{'loss': 1.1469, 'grad_norm': 0.5703125, 'learning_rate': 0.00013872724328838567, 'epoch': 0.29}\n",
      "{'loss': 1.3101, 'grad_norm': 0.4296875, 'learning_rate': 0.0001387158809109218, 'epoch': 0.29}\n",
      "{'loss': 1.463, 'grad_norm': 0.57421875, 'learning_rate': 0.00013870451853345794, 'epoch': 0.29}\n",
      "{'loss': 1.2245, 'grad_norm': 0.60546875, 'learning_rate': 0.0001386931561559941, 'epoch': 0.29}\n",
      "{'loss': 1.4297, 'grad_norm': 0.455078125, 'learning_rate': 0.00013868179377853022, 'epoch': 0.29}\n",
      "{'loss': 1.192, 'grad_norm': 0.6484375, 'learning_rate': 0.00013867043140106637, 'epoch': 0.29}\n",
      "{'loss': 1.0391, 'grad_norm': 0.39453125, 'learning_rate': 0.0001386590690236025, 'epoch': 0.29}\n",
      "{'loss': 1.254, 'grad_norm': 0.55078125, 'learning_rate': 0.00013864770664613865, 'epoch': 0.29}\n",
      "{'loss': 1.1086, 'grad_norm': 0.6640625, 'learning_rate': 0.00013863634426867477, 'epoch': 0.29}\n",
      "{'loss': 1.3964, 'grad_norm': 0.490234375, 'learning_rate': 0.00013862498189121092, 'epoch': 0.29}\n",
      "{'loss': 1.1332, 'grad_norm': 1.0234375, 'learning_rate': 0.00013861361951374707, 'epoch': 0.29}\n",
      "{'loss': 1.1772, 'grad_norm': 0.458984375, 'learning_rate': 0.0001386022571362832, 'epoch': 0.3}\n",
      "{'loss': 1.2969, 'grad_norm': 0.6015625, 'learning_rate': 0.00013859089475881935, 'epoch': 0.3}\n",
      "{'loss': 1.0281, 'grad_norm': 1.0234375, 'learning_rate': 0.00013857953238135547, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.408, 'grad_norm': 0.6171875, 'learning_rate': 0.00013856817000389163, 'epoch': 0.3}\n",
      "{'loss': 1.1829, 'grad_norm': 0.5390625, 'learning_rate': 0.00013855680762642775, 'epoch': 0.3}\n",
      "{'loss': 1.2844, 'grad_norm': 0.5859375, 'learning_rate': 0.0001385454452489639, 'epoch': 0.3}\n",
      "{'loss': 1.2709, 'grad_norm': 0.53515625, 'learning_rate': 0.00013853408287150005, 'epoch': 0.3}\n",
      "{'loss': 1.0954, 'grad_norm': 0.5703125, 'learning_rate': 0.00013852272049403618, 'epoch': 0.3}\n",
      "{'loss': 1.4405, 'grad_norm': 0.5, 'learning_rate': 0.00013851135811657233, 'epoch': 0.3}\n",
      "{'loss': 1.4105, 'grad_norm': 0.82421875, 'learning_rate': 0.00013849999573910845, 'epoch': 0.3}\n",
      "{'loss': 1.1626, 'grad_norm': 0.63671875, 'learning_rate': 0.0001384886333616446, 'epoch': 0.3}\n",
      "{'loss': 1.2983, 'grad_norm': 0.546875, 'learning_rate': 0.00013847727098418073, 'epoch': 0.3}\n",
      "{'loss': 1.1579, 'grad_norm': 0.56640625, 'learning_rate': 0.00013846590860671685, 'epoch': 0.3}\n",
      "{'loss': 1.4625, 'grad_norm': 0.50390625, 'learning_rate': 0.00013845454622925303, 'epoch': 0.3}\n",
      "{'loss': 1.1755, 'grad_norm': 0.57421875, 'learning_rate': 0.00013844318385178916, 'epoch': 0.3}\n",
      "{'loss': 1.2851, 'grad_norm': 0.54296875, 'learning_rate': 0.0001384318214743253, 'epoch': 0.3}\n",
      "{'loss': 1.1829, 'grad_norm': 0.55078125, 'learning_rate': 0.00013842045909686143, 'epoch': 0.3}\n",
      "{'loss': 1.115, 'grad_norm': 0.37890625, 'learning_rate': 0.00013840909671939756, 'epoch': 0.3}\n",
      "{'loss': 1.318, 'grad_norm': 1.3828125, 'learning_rate': 0.0001383977343419337, 'epoch': 0.3}\n",
      "{'loss': 1.1536, 'grad_norm': 0.66796875, 'learning_rate': 0.00013838637196446986, 'epoch': 0.3}\n",
      "{'loss': 1.2318, 'grad_norm': 0.44921875, 'learning_rate': 0.000138375009587006, 'epoch': 0.3}\n",
      "{'loss': 1.1308, 'grad_norm': 0.4765625, 'learning_rate': 0.00013836364720954213, 'epoch': 0.3}\n",
      "{'loss': 1.088, 'grad_norm': 0.62890625, 'learning_rate': 0.00013835228483207826, 'epoch': 0.3}\n",
      "{'loss': 1.4288, 'grad_norm': 0.58203125, 'learning_rate': 0.0001383409224546144, 'epoch': 0.3}\n",
      "{'loss': 1.228, 'grad_norm': 0.75, 'learning_rate': 0.00013832956007715053, 'epoch': 0.3}\n",
      "{'loss': 1.3572, 'grad_norm': 0.392578125, 'learning_rate': 0.00013831819769968669, 'epoch': 0.3}\n",
      "{'loss': 1.2517, 'grad_norm': 0.46875, 'learning_rate': 0.00013830683532222284, 'epoch': 0.3}\n",
      "{'loss': 1.0441, 'grad_norm': 0.74609375, 'learning_rate': 0.000138295472944759, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.28, 'grad_norm': 0.6875, 'learning_rate': 0.0001382841105672951, 'epoch': 0.3}\n",
      "{'loss': 1.0739, 'grad_norm': 0.625, 'learning_rate': 0.00013827274818983124, 'epoch': 0.3}\n",
      "{'loss': 1.2294, 'grad_norm': 0.40625, 'learning_rate': 0.0001382613858123674, 'epoch': 0.3}\n",
      "{'loss': 1.3, 'grad_norm': 0.40234375, 'learning_rate': 0.0001382500234349035, 'epoch': 0.3}\n",
      "{'loss': 1.154, 'grad_norm': 0.71484375, 'learning_rate': 0.00013823866105743966, 'epoch': 0.3}\n",
      "{'loss': 1.3485, 'grad_norm': 0.72265625, 'learning_rate': 0.00013822729867997582, 'epoch': 0.3}\n",
      "{'loss': 1.1697, 'grad_norm': 0.640625, 'learning_rate': 0.00013821593630251194, 'epoch': 0.3}\n",
      "{'loss': 1.3097, 'grad_norm': 0.54296875, 'learning_rate': 0.0001382045739250481, 'epoch': 0.3}\n",
      "{'loss': 1.2822, 'grad_norm': 0.57421875, 'learning_rate': 0.00013819321154758422, 'epoch': 0.3}\n",
      "{'loss': 1.0991, 'grad_norm': 0.69921875, 'learning_rate': 0.00013818184917012037, 'epoch': 0.3}\n",
      "{'loss': 1.331, 'grad_norm': 0.486328125, 'learning_rate': 0.0001381704867926565, 'epoch': 0.3}\n",
      "{'loss': 1.1837, 'grad_norm': 0.6796875, 'learning_rate': 0.00013815912441519264, 'epoch': 0.3}\n",
      "{'loss': 1.1538, 'grad_norm': 0.421875, 'learning_rate': 0.0001381477620377288, 'epoch': 0.3}\n",
      "{'loss': 1.2703, 'grad_norm': 0.6015625, 'learning_rate': 0.00013813639966026492, 'epoch': 0.3}\n",
      "{'loss': 0.9807, 'grad_norm': 0.578125, 'learning_rate': 0.00013812503728280107, 'epoch': 0.3}\n",
      "{'loss': 1.4921, 'grad_norm': 0.4609375, 'learning_rate': 0.0001381136749053372, 'epoch': 0.3}\n",
      "{'loss': 1.1509, 'grad_norm': 0.74609375, 'learning_rate': 0.00013810231252787335, 'epoch': 0.3}\n",
      "{'loss': 1.2853, 'grad_norm': 0.515625, 'learning_rate': 0.00013809095015040947, 'epoch': 0.3}\n",
      "{'loss': 1.1857, 'grad_norm': 0.4375, 'learning_rate': 0.0001380795877729456, 'epoch': 0.3}\n",
      "{'loss': 1.0493, 'grad_norm': 0.8203125, 'learning_rate': 0.00013806822539548177, 'epoch': 0.3}\n",
      "{'loss': 1.3648, 'grad_norm': 0.69921875, 'learning_rate': 0.0001380568630180179, 'epoch': 0.3}\n",
      "{'loss': 1.1177, 'grad_norm': 0.69921875, 'learning_rate': 0.00013804550064055405, 'epoch': 0.3}\n",
      "{'loss': 1.3728, 'grad_norm': 0.408203125, 'learning_rate': 0.00013803413826309017, 'epoch': 0.3}\n",
      "{'loss': 1.2902, 'grad_norm': 0.69921875, 'learning_rate': 0.0001380227758856263, 'epoch': 0.3}\n",
      "{'loss': 1.1873, 'grad_norm': 0.53515625, 'learning_rate': 0.00013801141350816245, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.338, 'grad_norm': 0.49609375, 'learning_rate': 0.0001380000511306986, 'epoch': 0.3}\n",
      "{'loss': 1.1859, 'grad_norm': 1.046875, 'learning_rate': 0.00013798868875323475, 'epoch': 0.3}\n",
      "{'loss': 1.1048, 'grad_norm': 0.376953125, 'learning_rate': 0.00013797732637577088, 'epoch': 0.3}\n",
      "{'loss': 1.3003, 'grad_norm': 0.4921875, 'learning_rate': 0.000137965963998307, 'epoch': 0.3}\n",
      "{'loss': 1.0671, 'grad_norm': 0.62109375, 'learning_rate': 0.00013795460162084315, 'epoch': 0.3}\n",
      "{'loss': 1.4684, 'grad_norm': 0.455078125, 'learning_rate': 0.00013794323924337928, 'epoch': 0.3}\n",
      "{'loss': 1.0884, 'grad_norm': 0.5859375, 'learning_rate': 0.00013793187686591543, 'epoch': 0.3}\n",
      "{'loss': 1.1295, 'grad_norm': 0.5546875, 'learning_rate': 0.00013792051448845158, 'epoch': 0.3}\n",
      "{'loss': 1.2803, 'grad_norm': 0.57421875, 'learning_rate': 0.00013790915211098773, 'epoch': 0.3}\n",
      "{'loss': 1.0982, 'grad_norm': 0.91796875, 'learning_rate': 0.00013789778973352385, 'epoch': 0.3}\n",
      "{'loss': 1.2454, 'grad_norm': 0.515625, 'learning_rate': 0.00013788642735605998, 'epoch': 0.3}\n",
      "{'loss': 1.1969, 'grad_norm': 0.69921875, 'learning_rate': 0.00013787506497859613, 'epoch': 0.3}\n",
      "{'loss': 1.1838, 'grad_norm': 0.52734375, 'learning_rate': 0.00013786370260113225, 'epoch': 0.3}\n",
      "{'loss': 1.3154, 'grad_norm': 0.51953125, 'learning_rate': 0.0001378523402236684, 'epoch': 0.3}\n",
      "{'loss': 1.0913, 'grad_norm': 0.65625, 'learning_rate': 0.00013784097784620456, 'epoch': 0.3}\n",
      "{'loss': 1.331, 'grad_norm': 0.466796875, 'learning_rate': 0.00013782961546874068, 'epoch': 0.3}\n",
      "{'loss': 1.2411, 'grad_norm': 0.5625, 'learning_rate': 0.00013781825309127683, 'epoch': 0.3}\n",
      "{'loss': 1.1935, 'grad_norm': 0.396484375, 'learning_rate': 0.00013780689071381296, 'epoch': 0.3}\n",
      "{'loss': 1.2136, 'grad_norm': 0.53125, 'learning_rate': 0.0001377955283363491, 'epoch': 0.3}\n",
      "{'loss': 1.0875, 'grad_norm': 0.8046875, 'learning_rate': 0.00013778416595888523, 'epoch': 0.3}\n",
      "{'loss': 1.3867, 'grad_norm': 0.60546875, 'learning_rate': 0.00013777280358142138, 'epoch': 0.3}\n",
      "{'loss': 1.1868, 'grad_norm': 0.494140625, 'learning_rate': 0.00013776144120395754, 'epoch': 0.3}\n",
      "{'loss': 1.3117, 'grad_norm': 0.392578125, 'learning_rate': 0.00013775007882649366, 'epoch': 0.3}\n",
      "{'loss': 1.167, 'grad_norm': 0.6484375, 'learning_rate': 0.0001377387164490298, 'epoch': 0.3}\n",
      "{'loss': 1.0653, 'grad_norm': 0.78125, 'learning_rate': 0.00013772735407156594, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1928, 'grad_norm': 0.62109375, 'learning_rate': 0.0001377159916941021, 'epoch': 0.3}\n",
      "{'loss': 1.2262, 'grad_norm': 0.6953125, 'learning_rate': 0.0001377046293166382, 'epoch': 0.3}\n",
      "{'loss': 1.2396, 'grad_norm': 0.416015625, 'learning_rate': 0.00013769326693917436, 'epoch': 0.3}\n",
      "{'loss': 1.2169, 'grad_norm': 0.5, 'learning_rate': 0.00013768190456171051, 'epoch': 0.3}\n",
      "{'loss': 0.9945, 'grad_norm': 0.66796875, 'learning_rate': 0.00013767054218424664, 'epoch': 0.3}\n",
      "{'loss': 1.2543, 'grad_norm': 0.5078125, 'learning_rate': 0.0001376591798067828, 'epoch': 0.3}\n",
      "{'loss': 1.2233, 'grad_norm': 1.15625, 'learning_rate': 0.00013764781742931891, 'epoch': 0.3}\n",
      "{'loss': 1.2059, 'grad_norm': 0.625, 'learning_rate': 0.00013763645505185504, 'epoch': 0.3}\n",
      "{'loss': 1.3335, 'grad_norm': 0.50390625, 'learning_rate': 0.0001376250926743912, 'epoch': 0.3}\n",
      "{'loss': 1.0574, 'grad_norm': 0.484375, 'learning_rate': 0.00013761373029692734, 'epoch': 0.3}\n",
      "{'loss': 1.3356, 'grad_norm': 0.408203125, 'learning_rate': 0.0001376023679194635, 'epoch': 0.3}\n",
      "{'loss': 1.2792, 'grad_norm': 0.59765625, 'learning_rate': 0.00013759100554199962, 'epoch': 0.3}\n",
      "{'loss': 1.2603, 'grad_norm': 0.453125, 'learning_rate': 0.00013757964316453574, 'epoch': 0.3}\n",
      "{'loss': 1.2855, 'grad_norm': 0.8125, 'learning_rate': 0.0001375682807870719, 'epoch': 0.3}\n",
      "{'loss': 1.0807, 'grad_norm': 1.5390625, 'learning_rate': 0.00013755691840960802, 'epoch': 0.3}\n",
      "{'loss': 1.438, 'grad_norm': 0.56640625, 'learning_rate': 0.00013754555603214417, 'epoch': 0.3}\n",
      "{'loss': 1.2172, 'grad_norm': 0.8203125, 'learning_rate': 0.00013753419365468032, 'epoch': 0.3}\n",
      "{'loss': 1.2198, 'grad_norm': 0.61328125, 'learning_rate': 0.00013752283127721647, 'epoch': 0.3}\n",
      "{'loss': 1.2972, 'grad_norm': 0.54296875, 'learning_rate': 0.0001375114688997526, 'epoch': 0.3}\n",
      "{'loss': 1.037, 'grad_norm': 0.80078125, 'learning_rate': 0.00013750010652228872, 'epoch': 0.3}\n",
      "{'loss': 1.3288, 'grad_norm': 0.78125, 'learning_rate': 0.00013748874414482487, 'epoch': 0.3}\n",
      "{'loss': 1.2229, 'grad_norm': 0.57421875, 'learning_rate': 0.000137477381767361, 'epoch': 0.3}\n",
      "{'loss': 1.2989, 'grad_norm': 0.353515625, 'learning_rate': 0.00013746601938989715, 'epoch': 0.3}\n",
      "{'loss': 1.2015, 'grad_norm': 0.69921875, 'learning_rate': 0.0001374546570124333, 'epoch': 0.3}\n",
      "{'loss': 1.1523, 'grad_norm': 0.6953125, 'learning_rate': 0.00013744329463496942, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2915, 'grad_norm': 0.5390625, 'learning_rate': 0.00013743193225750557, 'epoch': 0.3}\n",
      "{'loss': 1.3022, 'grad_norm': 1.1796875, 'learning_rate': 0.0001374205698800417, 'epoch': 0.3}\n",
      "{'loss': 1.2096, 'grad_norm': 0.51171875, 'learning_rate': 0.00013740920750257785, 'epoch': 0.3}\n",
      "{'loss': 1.2585, 'grad_norm': 0.61328125, 'learning_rate': 0.00013739784512511397, 'epoch': 0.3}\n",
      "{'loss': 1.0475, 'grad_norm': 0.94921875, 'learning_rate': 0.00013738648274765013, 'epoch': 0.3}\n",
      "{'loss': 1.3534, 'grad_norm': 0.4375, 'learning_rate': 0.00013737512037018628, 'epoch': 0.3}\n",
      "{'loss': 1.1471, 'grad_norm': 0.5625, 'learning_rate': 0.0001373637579927224, 'epoch': 0.3}\n",
      "{'loss': 1.2583, 'grad_norm': 0.44140625, 'learning_rate': 0.00013735239561525855, 'epoch': 0.3}\n",
      "{'loss': 1.2686, 'grad_norm': 0.6015625, 'learning_rate': 0.00013734103323779468, 'epoch': 0.3}\n",
      "{'loss': 1.0437, 'grad_norm': 0.9296875, 'learning_rate': 0.00013732967086033083, 'epoch': 0.3}\n",
      "{'loss': 1.41, 'grad_norm': 0.486328125, 'learning_rate': 0.00013731830848286695, 'epoch': 0.3}\n",
      "{'loss': 1.2604, 'grad_norm': 0.94921875, 'learning_rate': 0.0001373069461054031, 'epoch': 0.3}\n",
      "{'loss': 1.202, 'grad_norm': 0.484375, 'learning_rate': 0.00013729558372793926, 'epoch': 0.3}\n",
      "{'loss': 1.1098, 'grad_norm': 0.84375, 'learning_rate': 0.00013728422135047538, 'epoch': 0.3}\n",
      "{'loss': 1.1787, 'grad_norm': 0.578125, 'learning_rate': 0.00013727285897301153, 'epoch': 0.3}\n",
      "{'loss': 1.306, 'grad_norm': 0.5, 'learning_rate': 0.00013726149659554766, 'epoch': 0.3}\n",
      "{'loss': 1.1855, 'grad_norm': 0.53125, 'learning_rate': 0.00013725013421808378, 'epoch': 0.3}\n",
      "{'loss': 1.2764, 'grad_norm': 0.69921875, 'learning_rate': 0.00013723877184061993, 'epoch': 0.3}\n",
      "{'loss': 1.2271, 'grad_norm': 0.5625, 'learning_rate': 0.00013722740946315608, 'epoch': 0.3}\n",
      "{'loss': 1.1265, 'grad_norm': 0.6875, 'learning_rate': 0.00013721604708569223, 'epoch': 0.3}\n",
      "{'loss': 1.357, 'grad_norm': 0.609375, 'learning_rate': 0.00013720468470822836, 'epoch': 0.3}\n",
      "{'loss': 1.3218, 'grad_norm': 0.625, 'learning_rate': 0.00013719332233076448, 'epoch': 0.3}\n",
      "{'loss': 1.3618, 'grad_norm': 0.45703125, 'learning_rate': 0.00013718195995330063, 'epoch': 0.3}\n",
      "{'loss': 1.2445, 'grad_norm': 0.6953125, 'learning_rate': 0.00013717059757583676, 'epoch': 0.3}\n",
      "{'loss': 1.0969, 'grad_norm': 0.66015625, 'learning_rate': 0.0001371592351983729, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4805, 'grad_norm': 0.474609375, 'learning_rate': 0.00013714787282090906, 'epoch': 0.3}\n",
      "{'loss': 1.2438, 'grad_norm': 0.60546875, 'learning_rate': 0.0001371365104434452, 'epoch': 0.3}\n",
      "{'loss': 1.1754, 'grad_norm': 0.416015625, 'learning_rate': 0.00013712514806598134, 'epoch': 0.3}\n",
      "{'loss': 1.2983, 'grad_norm': 0.515625, 'learning_rate': 0.00013711378568851746, 'epoch': 0.3}\n",
      "{'loss': 1.136, 'grad_norm': 0.7578125, 'learning_rate': 0.0001371024233110536, 'epoch': 0.3}\n",
      "{'loss': 1.2764, 'grad_norm': 0.89453125, 'learning_rate': 0.00013709106093358974, 'epoch': 0.3}\n",
      "{'loss': 1.1201, 'grad_norm': 0.71875, 'learning_rate': 0.0001370796985561259, 'epoch': 0.3}\n",
      "{'loss': 1.2403, 'grad_norm': 0.458984375, 'learning_rate': 0.00013706833617866204, 'epoch': 0.3}\n",
      "{'loss': 1.188, 'grad_norm': 0.5703125, 'learning_rate': 0.00013705697380119816, 'epoch': 0.3}\n",
      "{'loss': 1.0622, 'grad_norm': 0.89453125, 'learning_rate': 0.00013704561142373431, 'epoch': 0.3}\n",
      "{'loss': 1.2627, 'grad_norm': 0.482421875, 'learning_rate': 0.00013703424904627044, 'epoch': 0.3}\n",
      "{'loss': 1.2677, 'grad_norm': 0.67578125, 'learning_rate': 0.0001370228866688066, 'epoch': 0.3}\n",
      "{'loss': 1.1814, 'grad_norm': 0.57421875, 'learning_rate': 0.00013701152429134271, 'epoch': 0.3}\n",
      "{'loss': 1.3174, 'grad_norm': 0.50390625, 'learning_rate': 0.00013700016191387887, 'epoch': 0.3}\n",
      "{'loss': 1.0721, 'grad_norm': 0.63671875, 'learning_rate': 0.00013698879953641502, 'epoch': 0.3}\n",
      "{'loss': 1.2321, 'grad_norm': 0.4765625, 'learning_rate': 0.00013697743715895114, 'epoch': 0.3}\n",
      "{'loss': 1.1704, 'grad_norm': 0.58984375, 'learning_rate': 0.0001369660747814873, 'epoch': 0.3}\n",
      "{'loss': 1.2519, 'grad_norm': 0.37109375, 'learning_rate': 0.00013695471240402342, 'epoch': 0.3}\n",
      "{'loss': 1.2408, 'grad_norm': 0.515625, 'learning_rate': 0.00013694335002655957, 'epoch': 0.3}\n",
      "{'loss': 1.1412, 'grad_norm': 0.6328125, 'learning_rate': 0.0001369319876490957, 'epoch': 0.3}\n",
      "{'loss': 1.2866, 'grad_norm': 0.478515625, 'learning_rate': 0.00013692062527163184, 'epoch': 0.3}\n",
      "{'loss': 1.147, 'grad_norm': 0.87109375, 'learning_rate': 0.000136909262894168, 'epoch': 0.3}\n",
      "{'loss': 1.1641, 'grad_norm': 0.40625, 'learning_rate': 0.00013689790051670412, 'epoch': 0.3}\n",
      "{'loss': 1.2265, 'grad_norm': 0.58203125, 'learning_rate': 0.00013688653813924027, 'epoch': 0.3}\n",
      "{'loss': 1.1131, 'grad_norm': 0.8828125, 'learning_rate': 0.0001368751757617764, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4684, 'grad_norm': 0.77734375, 'learning_rate': 0.00013686381338431252, 'epoch': 0.3}\n",
      "{'loss': 1.1985, 'grad_norm': 0.52734375, 'learning_rate': 0.00013685245100684867, 'epoch': 0.3}\n",
      "{'loss': 1.2711, 'grad_norm': 0.408203125, 'learning_rate': 0.00013684108862938482, 'epoch': 0.3}\n",
      "{'loss': 1.3313, 'grad_norm': 1.1328125, 'learning_rate': 0.00013682972625192097, 'epoch': 0.3}\n",
      "{'loss': 1.0635, 'grad_norm': 0.671875, 'learning_rate': 0.0001368183638744571, 'epoch': 0.3}\n",
      "{'loss': 1.3014, 'grad_norm': 0.396484375, 'learning_rate': 0.00013680700149699322, 'epoch': 0.3}\n",
      "{'loss': 1.1283, 'grad_norm': 0.75, 'learning_rate': 0.00013679563911952937, 'epoch': 0.3}\n",
      "{'loss': 1.2426, 'grad_norm': 0.42578125, 'learning_rate': 0.0001367842767420655, 'epoch': 0.3}\n",
      "{'loss': 1.3011, 'grad_norm': 0.8671875, 'learning_rate': 0.00013677291436460165, 'epoch': 0.3}\n",
      "{'loss': 1.1583, 'grad_norm': 0.7109375, 'learning_rate': 0.0001367615519871378, 'epoch': 0.3}\n",
      "{'loss': 1.3508, 'grad_norm': 0.54296875, 'learning_rate': 0.00013675018960967395, 'epoch': 0.3}\n",
      "{'loss': 1.285, 'grad_norm': 0.50390625, 'learning_rate': 0.00013673882723221008, 'epoch': 0.3}\n",
      "{'loss': 1.2306, 'grad_norm': 0.498046875, 'learning_rate': 0.0001367274648547462, 'epoch': 0.3}\n",
      "{'loss': 1.2269, 'grad_norm': 0.53515625, 'learning_rate': 0.00013671610247728235, 'epoch': 0.3}\n",
      "{'loss': 1.1472, 'grad_norm': 1.078125, 'learning_rate': 0.00013670474009981848, 'epoch': 0.3}\n",
      "{'loss': 1.547, 'grad_norm': 0.59375, 'learning_rate': 0.00013669337772235466, 'epoch': 0.3}\n",
      "{'loss': 1.2718, 'grad_norm': 0.5, 'learning_rate': 0.00013668201534489078, 'epoch': 0.3}\n",
      "{'loss': 1.2276, 'grad_norm': 0.451171875, 'learning_rate': 0.0001366706529674269, 'epoch': 0.3}\n",
      "{'loss': 1.2829, 'grad_norm': 0.83203125, 'learning_rate': 0.00013665929058996306, 'epoch': 0.3}\n",
      "{'loss': 1.1391, 'grad_norm': 0.59765625, 'learning_rate': 0.00013664792821249918, 'epoch': 0.3}\n",
      "{'loss': 1.4176, 'grad_norm': 0.578125, 'learning_rate': 0.00013663656583503533, 'epoch': 0.3}\n",
      "{'loss': 1.164, 'grad_norm': 0.6953125, 'learning_rate': 0.00013662520345757146, 'epoch': 0.3}\n",
      "{'loss': 1.2025, 'grad_norm': 0.4921875, 'learning_rate': 0.0001366138410801076, 'epoch': 0.3}\n",
      "{'loss': 1.1186, 'grad_norm': 0.625, 'learning_rate': 0.00013660247870264376, 'epoch': 0.3}\n",
      "{'loss': 0.9997, 'grad_norm': 0.875, 'learning_rate': 0.00013659111632517988, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3653, 'grad_norm': 0.46875, 'learning_rate': 0.00013657975394771603, 'epoch': 0.3}\n",
      "{'loss': 1.1844, 'grad_norm': 0.6171875, 'learning_rate': 0.00013656839157025216, 'epoch': 0.3}\n",
      "{'loss': 1.1436, 'grad_norm': 0.39453125, 'learning_rate': 0.0001365570291927883, 'epoch': 0.3}\n",
      "{'loss': 1.2228, 'grad_norm': 0.5546875, 'learning_rate': 0.00013654566681532443, 'epoch': 0.3}\n",
      "{'loss': 0.9752, 'grad_norm': 0.703125, 'learning_rate': 0.00013653430443786059, 'epoch': 0.3}\n",
      "{'loss': 1.3876, 'grad_norm': 0.3984375, 'learning_rate': 0.00013652294206039674, 'epoch': 0.3}\n",
      "{'loss': 1.1623, 'grad_norm': 0.7734375, 'learning_rate': 0.00013651157968293286, 'epoch': 0.3}\n",
      "{'loss': 1.1825, 'grad_norm': 0.4453125, 'learning_rate': 0.000136500217305469, 'epoch': 0.3}\n",
      "{'loss': 1.4012, 'grad_norm': 0.443359375, 'learning_rate': 0.00013648885492800514, 'epoch': 0.3}\n",
      "{'loss': 1.0638, 'grad_norm': 0.640625, 'learning_rate': 0.00013647749255054126, 'epoch': 0.3}\n",
      "{'loss': 1.4423, 'grad_norm': 0.60546875, 'learning_rate': 0.0001364661301730774, 'epoch': 0.3}\n",
      "{'loss': 1.1944, 'grad_norm': 0.51953125, 'learning_rate': 0.00013645476779561356, 'epoch': 0.3}\n",
      "{'loss': 1.1773, 'grad_norm': 0.416015625, 'learning_rate': 0.00013644340541814972, 'epoch': 0.3}\n",
      "{'loss': 1.2323, 'grad_norm': 0.62890625, 'learning_rate': 0.00013643204304068584, 'epoch': 0.3}\n",
      "{'loss': 1.0107, 'grad_norm': 0.9609375, 'learning_rate': 0.00013642068066322196, 'epoch': 0.3}\n",
      "{'loss': 1.4487, 'grad_norm': 0.515625, 'learning_rate': 0.00013640931828575812, 'epoch': 0.3}\n",
      "{'loss': 1.2284, 'grad_norm': 0.5859375, 'learning_rate': 0.00013639795590829424, 'epoch': 0.3}\n",
      "{'loss': 1.2451, 'grad_norm': 0.4296875, 'learning_rate': 0.0001363865935308304, 'epoch': 0.3}\n",
      "{'loss': 1.2678, 'grad_norm': 0.546875, 'learning_rate': 0.00013637523115336654, 'epoch': 0.3}\n",
      "{'loss': 1.1187, 'grad_norm': 0.890625, 'learning_rate': 0.0001363638687759027, 'epoch': 0.3}\n",
      "{'loss': 1.3599, 'grad_norm': 0.4609375, 'learning_rate': 0.00013635250639843882, 'epoch': 0.3}\n",
      "{'loss': 1.1976, 'grad_norm': 0.62109375, 'learning_rate': 0.00013634114402097494, 'epoch': 0.3}\n",
      "{'loss': 1.2466, 'grad_norm': 0.408203125, 'learning_rate': 0.0001363297816435111, 'epoch': 0.3}\n",
      "{'loss': 1.2054, 'grad_norm': 0.5859375, 'learning_rate': 0.00013631841926604722, 'epoch': 0.3}\n",
      "{'loss': 1.0455, 'grad_norm': 0.734375, 'learning_rate': 0.0001363070568885834, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4749, 'grad_norm': 0.388671875, 'learning_rate': 0.00013629569451111952, 'epoch': 0.31}\n",
      "{'loss': 1.218, 'grad_norm': 0.5546875, 'learning_rate': 0.00013628433213365565, 'epoch': 0.31}\n",
      "{'loss': 1.109, 'grad_norm': 0.400390625, 'learning_rate': 0.0001362729697561918, 'epoch': 0.31}\n",
      "{'loss': 1.2377, 'grad_norm': 0.81640625, 'learning_rate': 0.00013626160737872792, 'epoch': 0.31}\n",
      "{'loss': 0.9893, 'grad_norm': 0.78515625, 'learning_rate': 0.00013625024500126407, 'epoch': 0.31}\n",
      "{'loss': 1.3314, 'grad_norm': 0.44921875, 'learning_rate': 0.0001362388826238002, 'epoch': 0.31}\n",
      "{'loss': 1.1044, 'grad_norm': 0.53125, 'learning_rate': 0.00013622752024633635, 'epoch': 0.31}\n",
      "{'loss': 1.2322, 'grad_norm': 0.486328125, 'learning_rate': 0.0001362161578688725, 'epoch': 0.31}\n",
      "{'loss': 1.3506, 'grad_norm': 0.64453125, 'learning_rate': 0.00013620479549140862, 'epoch': 0.31}\n",
      "{'loss': 1.1789, 'grad_norm': 0.9453125, 'learning_rate': 0.00013619343311394478, 'epoch': 0.31}\n",
      "{'loss': 1.4265, 'grad_norm': 0.4296875, 'learning_rate': 0.0001361820707364809, 'epoch': 0.31}\n",
      "{'loss': 1.2975, 'grad_norm': 0.59375, 'learning_rate': 0.00013617070835901705, 'epoch': 0.31}\n",
      "{'loss': 1.2023, 'grad_norm': 0.482421875, 'learning_rate': 0.00013615934598155318, 'epoch': 0.31}\n",
      "{'loss': 1.3286, 'grad_norm': 0.48828125, 'learning_rate': 0.00013614798360408933, 'epoch': 0.31}\n",
      "{'loss': 1.167, 'grad_norm': 0.85546875, 'learning_rate': 0.00013613662122662548, 'epoch': 0.31}\n",
      "{'loss': 1.3619, 'grad_norm': 0.5546875, 'learning_rate': 0.0001361252588491616, 'epoch': 0.31}\n",
      "{'loss': 1.1439, 'grad_norm': 0.66015625, 'learning_rate': 0.00013611389647169775, 'epoch': 0.31}\n",
      "{'loss': 1.2392, 'grad_norm': 0.447265625, 'learning_rate': 0.00013610253409423388, 'epoch': 0.31}\n",
      "{'loss': 1.268, 'grad_norm': 0.55078125, 'learning_rate': 0.00013609117171677, 'epoch': 0.31}\n",
      "{'loss': 1.112, 'grad_norm': 0.57421875, 'learning_rate': 0.00013607980933930615, 'epoch': 0.31}\n",
      "{'loss': 1.3302, 'grad_norm': 0.4375, 'learning_rate': 0.0001360684469618423, 'epoch': 0.31}\n",
      "{'loss': 1.2072, 'grad_norm': 0.6171875, 'learning_rate': 0.00013605708458437846, 'epoch': 0.31}\n",
      "{'loss': 1.2694, 'grad_norm': 0.3984375, 'learning_rate': 0.00013604572220691458, 'epoch': 0.31}\n",
      "{'loss': 1.3037, 'grad_norm': 0.56640625, 'learning_rate': 0.0001360343598294507, 'epoch': 0.31}\n",
      "{'loss': 1.0905, 'grad_norm': 0.55078125, 'learning_rate': 0.00013602299745198686, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3207, 'grad_norm': 0.458984375, 'learning_rate': 0.00013601163507452298, 'epoch': 0.31}\n",
      "{'loss': 1.1523, 'grad_norm': 0.74609375, 'learning_rate': 0.00013600027269705916, 'epoch': 0.31}\n",
      "{'loss': 1.2335, 'grad_norm': 0.67578125, 'learning_rate': 0.00013598891031959528, 'epoch': 0.31}\n",
      "{'loss': 1.2271, 'grad_norm': 0.56640625, 'learning_rate': 0.00013597754794213144, 'epoch': 0.31}\n",
      "{'loss': 1.1437, 'grad_norm': 0.734375, 'learning_rate': 0.00013596618556466756, 'epoch': 0.31}\n",
      "{'loss': 1.3596, 'grad_norm': 0.703125, 'learning_rate': 0.00013595482318720368, 'epoch': 0.31}\n",
      "{'loss': 1.1601, 'grad_norm': 0.83984375, 'learning_rate': 0.00013594346080973984, 'epoch': 0.31}\n",
      "{'loss': 1.2029, 'grad_norm': 0.5234375, 'learning_rate': 0.00013593209843227596, 'epoch': 0.31}\n",
      "{'loss': 1.2394, 'grad_norm': 0.55859375, 'learning_rate': 0.00013592073605481214, 'epoch': 0.31}\n",
      "{'loss': 1.1349, 'grad_norm': 0.73046875, 'learning_rate': 0.00013590937367734826, 'epoch': 0.31}\n",
      "{'loss': 1.2719, 'grad_norm': 0.369140625, 'learning_rate': 0.0001358980112998844, 'epoch': 0.31}\n",
      "{'loss': 1.2283, 'grad_norm': 0.392578125, 'learning_rate': 0.00013588664892242054, 'epoch': 0.31}\n",
      "{'loss': 1.2118, 'grad_norm': 0.458984375, 'learning_rate': 0.00013587528654495666, 'epoch': 0.31}\n",
      "{'loss': 1.2438, 'grad_norm': 0.5625, 'learning_rate': 0.00013586392416749281, 'epoch': 0.31}\n",
      "{'loss': 1.1629, 'grad_norm': 1.3125, 'learning_rate': 0.00013585256179002894, 'epoch': 0.31}\n",
      "{'loss': 1.408, 'grad_norm': 0.51953125, 'learning_rate': 0.0001358411994125651, 'epoch': 0.31}\n",
      "{'loss': 1.3143, 'grad_norm': 0.73828125, 'learning_rate': 0.00013582983703510124, 'epoch': 0.31}\n",
      "{'loss': 1.3114, 'grad_norm': 0.478515625, 'learning_rate': 0.00013581847465763737, 'epoch': 0.31}\n",
      "{'loss': 1.3863, 'grad_norm': 0.498046875, 'learning_rate': 0.00013580711228017352, 'epoch': 0.31}\n",
      "{'loss': 1.002, 'grad_norm': 0.5859375, 'learning_rate': 0.00013579574990270964, 'epoch': 0.31}\n",
      "{'loss': 1.2963, 'grad_norm': 0.55859375, 'learning_rate': 0.0001357843875252458, 'epoch': 0.31}\n",
      "{'loss': 1.2832, 'grad_norm': 0.69140625, 'learning_rate': 0.00013577302514778192, 'epoch': 0.31}\n",
      "{'loss': 1.3494, 'grad_norm': 0.37109375, 'learning_rate': 0.00013576166277031807, 'epoch': 0.31}\n",
      "{'loss': 1.309, 'grad_norm': 0.4921875, 'learning_rate': 0.00013575030039285422, 'epoch': 0.31}\n",
      "{'loss': 1.1654, 'grad_norm': 1.15625, 'learning_rate': 0.00013573893801539034, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4083, 'grad_norm': 0.7265625, 'learning_rate': 0.0001357275756379265, 'epoch': 0.31}\n",
      "{'loss': 1.1724, 'grad_norm': 0.50390625, 'learning_rate': 0.00013571621326046262, 'epoch': 0.31}\n",
      "{'loss': 1.1723, 'grad_norm': 0.5078125, 'learning_rate': 0.00013570485088299874, 'epoch': 0.31}\n",
      "{'loss': 1.2717, 'grad_norm': 0.83203125, 'learning_rate': 0.0001356934885055349, 'epoch': 0.31}\n",
      "{'loss': 1.124, 'grad_norm': 0.546875, 'learning_rate': 0.00013568212612807105, 'epoch': 0.31}\n",
      "{'loss': 1.4453, 'grad_norm': 0.640625, 'learning_rate': 0.0001356707637506072, 'epoch': 0.31}\n",
      "{'loss': 1.2876, 'grad_norm': 0.640625, 'learning_rate': 0.00013565940137314332, 'epoch': 0.31}\n",
      "{'loss': 1.1659, 'grad_norm': 0.396484375, 'learning_rate': 0.00013564803899567945, 'epoch': 0.31}\n",
      "{'loss': 1.2344, 'grad_norm': 0.515625, 'learning_rate': 0.0001356366766182156, 'epoch': 0.31}\n",
      "{'loss': 1.1624, 'grad_norm': 0.421875, 'learning_rate': 0.00013562531424075172, 'epoch': 0.31}\n",
      "{'loss': 1.3488, 'grad_norm': 0.43359375, 'learning_rate': 0.0001356139518632879, 'epoch': 0.31}\n",
      "{'loss': 1.3063, 'grad_norm': 1.0390625, 'learning_rate': 0.00013560258948582403, 'epoch': 0.31}\n",
      "{'loss': 1.2647, 'grad_norm': 0.65234375, 'learning_rate': 0.00013559122710836018, 'epoch': 0.31}\n",
      "{'loss': 1.2716, 'grad_norm': 0.578125, 'learning_rate': 0.0001355798647308963, 'epoch': 0.31}\n",
      "{'loss': 1.0767, 'grad_norm': 0.5234375, 'learning_rate': 0.00013556850235343243, 'epoch': 0.31}\n",
      "{'loss': 1.3533, 'grad_norm': 0.462890625, 'learning_rate': 0.00013555713997596858, 'epoch': 0.31}\n",
      "{'loss': 1.2411, 'grad_norm': 0.6796875, 'learning_rate': 0.0001355457775985047, 'epoch': 0.31}\n",
      "{'loss': 1.1502, 'grad_norm': 0.4375, 'learning_rate': 0.00013553441522104088, 'epoch': 0.31}\n",
      "{'loss': 1.2104, 'grad_norm': 0.486328125, 'learning_rate': 0.000135523052843577, 'epoch': 0.31}\n",
      "{'loss': 1.1679, 'grad_norm': 0.69921875, 'learning_rate': 0.00013551169046611313, 'epoch': 0.31}\n",
      "{'loss': 1.3064, 'grad_norm': 0.4765625, 'learning_rate': 0.00013550032808864928, 'epoch': 0.31}\n",
      "{'loss': 1.175, 'grad_norm': 0.578125, 'learning_rate': 0.0001354889657111854, 'epoch': 0.31}\n",
      "{'loss': 1.2568, 'grad_norm': 0.388671875, 'learning_rate': 0.00013547760333372156, 'epoch': 0.31}\n",
      "{'loss': 1.2731, 'grad_norm': 1.046875, 'learning_rate': 0.00013546624095625768, 'epoch': 0.31}\n",
      "{'loss': 1.133, 'grad_norm': 0.67578125, 'learning_rate': 0.00013545487857879383, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3881, 'grad_norm': 0.5234375, 'learning_rate': 0.00013544351620132998, 'epoch': 0.31}\n",
      "{'loss': 1.1933, 'grad_norm': 0.8046875, 'learning_rate': 0.0001354321538238661, 'epoch': 0.31}\n",
      "{'loss': 1.1779, 'grad_norm': 0.6015625, 'learning_rate': 0.00013542079144640226, 'epoch': 0.31}\n",
      "{'loss': 1.2216, 'grad_norm': 0.5546875, 'learning_rate': 0.00013540942906893838, 'epoch': 0.31}\n",
      "{'loss': 1.071, 'grad_norm': 0.7734375, 'learning_rate': 0.00013539806669147453, 'epoch': 0.31}\n",
      "{'loss': 1.347, 'grad_norm': 0.609375, 'learning_rate': 0.00013538670431401066, 'epoch': 0.31}\n",
      "{'loss': 1.1754, 'grad_norm': 0.78515625, 'learning_rate': 0.0001353753419365468, 'epoch': 0.31}\n",
      "{'loss': 1.3325, 'grad_norm': 0.421875, 'learning_rate': 0.00013536397955908296, 'epoch': 0.31}\n",
      "{'loss': 1.2432, 'grad_norm': 0.58984375, 'learning_rate': 0.00013535261718161909, 'epoch': 0.31}\n",
      "{'loss': 1.1334, 'grad_norm': 0.80859375, 'learning_rate': 0.00013534125480415524, 'epoch': 0.31}\n",
      "{'loss': 1.3012, 'grad_norm': 0.60546875, 'learning_rate': 0.00013532989242669136, 'epoch': 0.31}\n",
      "{'loss': 1.1923, 'grad_norm': 0.59375, 'learning_rate': 0.00013531853004922749, 'epoch': 0.31}\n",
      "{'loss': 1.1755, 'grad_norm': 0.5078125, 'learning_rate': 0.00013530716767176366, 'epoch': 0.31}\n",
      "{'loss': 1.2761, 'grad_norm': 0.466796875, 'learning_rate': 0.0001352958052942998, 'epoch': 0.31}\n",
      "{'loss': 1.0682, 'grad_norm': 0.828125, 'learning_rate': 0.00013528444291683594, 'epoch': 0.31}\n",
      "{'loss': 1.3848, 'grad_norm': 0.6015625, 'learning_rate': 0.00013527308053937206, 'epoch': 0.31}\n",
      "{'loss': 1.1067, 'grad_norm': 0.515625, 'learning_rate': 0.0001352617181619082, 'epoch': 0.31}\n",
      "{'loss': 1.3212, 'grad_norm': 0.44140625, 'learning_rate': 0.00013525035578444434, 'epoch': 0.31}\n",
      "{'loss': 1.2152, 'grad_norm': 0.546875, 'learning_rate': 0.00013523899340698046, 'epoch': 0.31}\n",
      "{'loss': 1.0387, 'grad_norm': 1.03125, 'learning_rate': 0.00013522763102951664, 'epoch': 0.31}\n",
      "{'loss': 1.395, 'grad_norm': 0.51953125, 'learning_rate': 0.00013521626865205277, 'epoch': 0.31}\n",
      "{'loss': 1.1908, 'grad_norm': 0.59765625, 'learning_rate': 0.00013520490627458892, 'epoch': 0.31}\n",
      "{'loss': 1.2651, 'grad_norm': 0.6796875, 'learning_rate': 0.00013519354389712504, 'epoch': 0.31}\n",
      "{'loss': 1.2026, 'grad_norm': 0.62890625, 'learning_rate': 0.00013518218151966117, 'epoch': 0.31}\n",
      "{'loss': 1.2621, 'grad_norm': 1.09375, 'learning_rate': 0.00013517081914219732, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2866, 'grad_norm': 0.546875, 'learning_rate': 0.00013515945676473344, 'epoch': 0.31}\n",
      "{'loss': 1.0589, 'grad_norm': 0.50390625, 'learning_rate': 0.00013514809438726962, 'epoch': 0.31}\n",
      "{'loss': 1.2516, 'grad_norm': 0.400390625, 'learning_rate': 0.00013513673200980575, 'epoch': 0.31}\n",
      "{'loss': 1.2756, 'grad_norm': 0.515625, 'learning_rate': 0.00013512536963234187, 'epoch': 0.31}\n",
      "{'loss': 1.1498, 'grad_norm': 0.828125, 'learning_rate': 0.00013511400725487802, 'epoch': 0.31}\n",
      "{'loss': 1.3438, 'grad_norm': 0.48046875, 'learning_rate': 0.00013510264487741415, 'epoch': 0.31}\n",
      "{'loss': 1.2684, 'grad_norm': 0.7265625, 'learning_rate': 0.0001350912824999503, 'epoch': 0.31}\n",
      "{'loss': 1.1173, 'grad_norm': 0.41015625, 'learning_rate': 0.00013507992012248642, 'epoch': 0.31}\n",
      "{'loss': 1.3077, 'grad_norm': 0.455078125, 'learning_rate': 0.00013506855774502257, 'epoch': 0.31}\n",
      "{'loss': 0.9965, 'grad_norm': 0.93359375, 'learning_rate': 0.00013505719536755872, 'epoch': 0.31}\n",
      "{'loss': 1.4141, 'grad_norm': 0.55078125, 'learning_rate': 0.00013504583299009485, 'epoch': 0.31}\n",
      "{'loss': 1.2582, 'grad_norm': 1.0234375, 'learning_rate': 0.000135034470612631, 'epoch': 0.31}\n",
      "{'loss': 1.2448, 'grad_norm': 0.5703125, 'learning_rate': 0.00013502310823516712, 'epoch': 0.31}\n",
      "{'loss': 1.185, 'grad_norm': 0.56640625, 'learning_rate': 0.00013501174585770328, 'epoch': 0.31}\n",
      "{'loss': 1.0768, 'grad_norm': 0.76171875, 'learning_rate': 0.0001350003834802394, 'epoch': 0.31}\n",
      "{'loss': 1.3358, 'grad_norm': 0.46875, 'learning_rate': 0.00013498902110277555, 'epoch': 0.31}\n",
      "{'loss': 1.2829, 'grad_norm': 0.63671875, 'learning_rate': 0.0001349776587253117, 'epoch': 0.31}\n",
      "{'loss': 1.0908, 'grad_norm': 0.451171875, 'learning_rate': 0.00013496629634784783, 'epoch': 0.31}\n",
      "{'loss': 1.2787, 'grad_norm': 0.55859375, 'learning_rate': 0.00013495493397038398, 'epoch': 0.31}\n",
      "{'loss': 1.1537, 'grad_norm': 1.046875, 'learning_rate': 0.0001349435715929201, 'epoch': 0.31}\n",
      "{'loss': 1.352, 'grad_norm': 0.51953125, 'learning_rate': 0.00013493220921545623, 'epoch': 0.31}\n",
      "{'loss': 1.1451, 'grad_norm': 0.78125, 'learning_rate': 0.0001349208468379924, 'epoch': 0.31}\n",
      "{'loss': 1.2505, 'grad_norm': 0.455078125, 'learning_rate': 0.00013490948446052853, 'epoch': 0.31}\n",
      "{'loss': 1.1924, 'grad_norm': 0.6953125, 'learning_rate': 0.00013489812208306468, 'epoch': 0.31}\n",
      "{'loss': 1.0397, 'grad_norm': 0.349609375, 'learning_rate': 0.0001348867597056008, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.475, 'grad_norm': 0.44140625, 'learning_rate': 0.00013487539732813693, 'epoch': 0.31}\n",
      "{'loss': 1.1799, 'grad_norm': 0.65234375, 'learning_rate': 0.00013486403495067308, 'epoch': 0.31}\n",
      "{'loss': 1.316, 'grad_norm': 0.46484375, 'learning_rate': 0.0001348526725732092, 'epoch': 0.31}\n",
      "{'loss': 1.2804, 'grad_norm': 0.56640625, 'learning_rate': 0.00013484131019574538, 'epoch': 0.31}\n",
      "{'loss': 1.0025, 'grad_norm': 0.58203125, 'learning_rate': 0.0001348299478182815, 'epoch': 0.31}\n",
      "{'loss': 1.3843, 'grad_norm': 0.70703125, 'learning_rate': 0.00013481858544081766, 'epoch': 0.31}\n",
      "{'loss': 1.2303, 'grad_norm': 0.7421875, 'learning_rate': 0.00013480722306335378, 'epoch': 0.31}\n",
      "{'loss': 1.2717, 'grad_norm': 0.5234375, 'learning_rate': 0.0001347958606858899, 'epoch': 0.31}\n",
      "{'loss': 1.3032, 'grad_norm': 0.65625, 'learning_rate': 0.00013478449830842606, 'epoch': 0.31}\n",
      "{'loss': 1.1126, 'grad_norm': 0.43359375, 'learning_rate': 0.00013477313593096218, 'epoch': 0.31}\n",
      "{'loss': 1.5063, 'grad_norm': 0.51171875, 'learning_rate': 0.00013476177355349836, 'epoch': 0.31}\n",
      "{'loss': 1.1276, 'grad_norm': 0.578125, 'learning_rate': 0.0001347504111760345, 'epoch': 0.31}\n",
      "{'loss': 1.3071, 'grad_norm': 0.4140625, 'learning_rate': 0.0001347390487985706, 'epoch': 0.31}\n",
      "{'loss': 1.2543, 'grad_norm': 0.53125, 'learning_rate': 0.00013472768642110676, 'epoch': 0.31}\n",
      "{'loss': 0.9939, 'grad_norm': 0.6328125, 'learning_rate': 0.0001347163240436429, 'epoch': 0.31}\n",
      "{'loss': 1.3085, 'grad_norm': 0.85546875, 'learning_rate': 0.00013470496166617904, 'epoch': 0.31}\n",
      "{'loss': 1.201, 'grad_norm': 0.7578125, 'learning_rate': 0.00013469359928871516, 'epoch': 0.31}\n",
      "{'loss': 1.1952, 'grad_norm': 0.462890625, 'learning_rate': 0.00013468223691125131, 'epoch': 0.31}\n",
      "{'loss': 1.1727, 'grad_norm': 0.64453125, 'learning_rate': 0.00013467087453378747, 'epoch': 0.31}\n",
      "{'loss': 1.1587, 'grad_norm': 0.59375, 'learning_rate': 0.0001346595121563236, 'epoch': 0.31}\n",
      "{'loss': 1.3448, 'grad_norm': 0.5625, 'learning_rate': 0.00013464814977885974, 'epoch': 0.31}\n",
      "{'loss': 1.219, 'grad_norm': 0.51171875, 'learning_rate': 0.00013463678740139587, 'epoch': 0.31}\n",
      "{'loss': 1.1667, 'grad_norm': 0.4140625, 'learning_rate': 0.00013462542502393202, 'epoch': 0.31}\n",
      "{'loss': 1.1837, 'grad_norm': 0.494140625, 'learning_rate': 0.00013461406264646817, 'epoch': 0.31}\n",
      "{'loss': 1.2254, 'grad_norm': 0.69921875, 'learning_rate': 0.0001346027002690043, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3702, 'grad_norm': 0.447265625, 'learning_rate': 0.00013459133789154044, 'epoch': 0.31}\n",
      "{'loss': 1.1512, 'grad_norm': 0.72265625, 'learning_rate': 0.00013457997551407657, 'epoch': 0.31}\n",
      "{'loss': 1.1504, 'grad_norm': 0.498046875, 'learning_rate': 0.00013456861313661272, 'epoch': 0.31}\n",
      "{'loss': 1.3325, 'grad_norm': 0.5078125, 'learning_rate': 0.00013455725075914884, 'epoch': 0.31}\n",
      "{'loss': 1.171, 'grad_norm': 0.69140625, 'learning_rate': 0.00013454588838168497, 'epoch': 0.31}\n",
      "{'loss': 1.3793, 'grad_norm': 0.451171875, 'learning_rate': 0.00013453452600422115, 'epoch': 0.31}\n",
      "{'loss': 1.3068, 'grad_norm': 0.94140625, 'learning_rate': 0.00013452316362675727, 'epoch': 0.31}\n",
      "{'loss': 1.1709, 'grad_norm': 0.52734375, 'learning_rate': 0.00013451180124929342, 'epoch': 0.31}\n",
      "{'loss': 1.2398, 'grad_norm': 0.5234375, 'learning_rate': 0.00013450043887182955, 'epoch': 0.31}\n",
      "{'loss': 1.1685, 'grad_norm': 0.70703125, 'learning_rate': 0.00013448907649436567, 'epoch': 0.31}\n",
      "{'loss': 1.3528, 'grad_norm': 0.470703125, 'learning_rate': 0.00013447771411690182, 'epoch': 0.31}\n",
      "{'loss': 1.1996, 'grad_norm': 0.6171875, 'learning_rate': 0.00013446635173943795, 'epoch': 0.31}\n",
      "{'loss': 1.3729, 'grad_norm': 0.408203125, 'learning_rate': 0.00013445498936197413, 'epoch': 0.31}\n",
      "{'loss': 1.1586, 'grad_norm': 0.81640625, 'learning_rate': 0.00013444362698451025, 'epoch': 0.31}\n",
      "{'loss': 1.1053, 'grad_norm': 0.953125, 'learning_rate': 0.0001344322646070464, 'epoch': 0.31}\n",
      "{'loss': 1.4505, 'grad_norm': 0.734375, 'learning_rate': 0.00013442090222958253, 'epoch': 0.31}\n",
      "{'loss': 1.2055, 'grad_norm': 0.64453125, 'learning_rate': 0.00013440953985211865, 'epoch': 0.31}\n",
      "{'loss': 1.1091, 'grad_norm': 0.48828125, 'learning_rate': 0.0001343981774746548, 'epoch': 0.31}\n",
      "{'loss': 1.1453, 'grad_norm': 0.71875, 'learning_rate': 0.00013438681509719093, 'epoch': 0.31}\n",
      "{'loss': 1.145, 'grad_norm': 0.73828125, 'learning_rate': 0.0001343754527197271, 'epoch': 0.31}\n",
      "{'loss': 1.344, 'grad_norm': 0.7734375, 'learning_rate': 0.00013436409034226323, 'epoch': 0.31}\n",
      "{'loss': 1.2033, 'grad_norm': 0.57421875, 'learning_rate': 0.00013435272796479935, 'epoch': 0.31}\n",
      "{'loss': 1.1971, 'grad_norm': 0.388671875, 'learning_rate': 0.0001343413655873355, 'epoch': 0.31}\n",
      "{'loss': 1.3082, 'grad_norm': 0.65234375, 'learning_rate': 0.00013433000320987163, 'epoch': 0.31}\n",
      "{'loss': 1.1854, 'grad_norm': 0.92578125, 'learning_rate': 0.00013431864083240778, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4092, 'grad_norm': 0.734375, 'learning_rate': 0.0001343072784549439, 'epoch': 0.31}\n",
      "{'loss': 1.2032, 'grad_norm': 0.6171875, 'learning_rate': 0.00013429591607748006, 'epoch': 0.31}\n",
      "{'loss': 1.1922, 'grad_norm': 0.50390625, 'learning_rate': 0.0001342845537000162, 'epoch': 0.31}\n",
      "{'loss': 1.21, 'grad_norm': 0.51953125, 'learning_rate': 0.00013427319132255233, 'epoch': 0.31}\n",
      "{'loss': 1.0667, 'grad_norm': 0.58203125, 'learning_rate': 0.00013426182894508848, 'epoch': 0.31}\n",
      "{'loss': 1.446, 'grad_norm': 0.5859375, 'learning_rate': 0.0001342504665676246, 'epoch': 0.31}\n",
      "{'loss': 1.2652, 'grad_norm': 0.61328125, 'learning_rate': 0.00013423910419016076, 'epoch': 0.31}\n",
      "{'loss': 1.2198, 'grad_norm': 0.443359375, 'learning_rate': 0.0001342277418126969, 'epoch': 0.31}\n",
      "{'loss': 1.1721, 'grad_norm': 0.515625, 'learning_rate': 0.00013421637943523303, 'epoch': 0.31}\n",
      "{'loss': 1.0057, 'grad_norm': 0.703125, 'learning_rate': 0.00013420501705776919, 'epoch': 0.31}\n",
      "{'loss': 1.4876, 'grad_norm': 0.58203125, 'learning_rate': 0.0001341936546803053, 'epoch': 0.31}\n",
      "{'loss': 1.2203, 'grad_norm': 0.625, 'learning_rate': 0.00013418229230284146, 'epoch': 0.31}\n",
      "{'loss': 1.2065, 'grad_norm': 0.359375, 'learning_rate': 0.00013417092992537759, 'epoch': 0.31}\n",
      "{'loss': 1.2517, 'grad_norm': 0.58984375, 'learning_rate': 0.0001341595675479137, 'epoch': 0.31}\n",
      "{'loss': 1.0048, 'grad_norm': 0.67578125, 'learning_rate': 0.0001341482051704499, 'epoch': 0.31}\n",
      "{'loss': 1.486, 'grad_norm': 0.44140625, 'learning_rate': 0.000134136842792986, 'epoch': 0.31}\n",
      "{'loss': 1.1763, 'grad_norm': 0.57421875, 'learning_rate': 0.00013412548041552216, 'epoch': 0.31}\n",
      "{'loss': 1.211, 'grad_norm': 0.4609375, 'learning_rate': 0.0001341141180380583, 'epoch': 0.31}\n",
      "{'loss': 1.3383, 'grad_norm': 0.609375, 'learning_rate': 0.0001341027556605944, 'epoch': 0.31}\n",
      "{'loss': 1.1218, 'grad_norm': 1.0078125, 'learning_rate': 0.00013409139328313056, 'epoch': 0.31}\n",
      "{'loss': 1.2182, 'grad_norm': 0.55078125, 'learning_rate': 0.0001340800309056667, 'epoch': 0.31}\n",
      "{'loss': 1.1872, 'grad_norm': 0.88671875, 'learning_rate': 0.00013406866852820287, 'epoch': 0.31}\n",
      "{'loss': 1.0738, 'grad_norm': 0.55078125, 'learning_rate': 0.000134057306150739, 'epoch': 0.31}\n",
      "{'loss': 1.2569, 'grad_norm': 0.6171875, 'learning_rate': 0.00013404594377327514, 'epoch': 0.31}\n",
      "{'loss': 1.0439, 'grad_norm': 0.96875, 'learning_rate': 0.00013403458139581127, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3788, 'grad_norm': 0.51953125, 'learning_rate': 0.0001340232190183474, 'epoch': 0.31}\n",
      "{'loss': 1.1361, 'grad_norm': 0.6796875, 'learning_rate': 0.00013401185664088354, 'epoch': 0.32}\n",
      "{'loss': 1.0951, 'grad_norm': 0.37109375, 'learning_rate': 0.00013400049426341967, 'epoch': 0.32}\n",
      "{'loss': 1.2606, 'grad_norm': 0.74609375, 'learning_rate': 0.00013398913188595584, 'epoch': 0.32}\n",
      "{'loss': 1.1653, 'grad_norm': 0.71875, 'learning_rate': 0.00013397776950849197, 'epoch': 0.32}\n",
      "{'loss': 1.3199, 'grad_norm': 0.455078125, 'learning_rate': 0.0001339664071310281, 'epoch': 0.32}\n",
      "{'loss': 1.301, 'grad_norm': 0.796875, 'learning_rate': 0.00013395504475356424, 'epoch': 0.32}\n",
      "{'loss': 1.343, 'grad_norm': 0.47265625, 'learning_rate': 0.00013394368237610037, 'epoch': 0.32}\n",
      "{'loss': 1.322, 'grad_norm': 0.5078125, 'learning_rate': 0.00013393231999863652, 'epoch': 0.32}\n",
      "{'loss': 1.0582, 'grad_norm': 0.302734375, 'learning_rate': 0.00013392095762117267, 'epoch': 0.32}\n",
      "{'loss': 1.3483, 'grad_norm': 0.46875, 'learning_rate': 0.0001339095952437088, 'epoch': 0.32}\n",
      "{'loss': 1.3262, 'grad_norm': 0.56640625, 'learning_rate': 0.00013389823286624495, 'epoch': 0.32}\n",
      "{'loss': 1.0857, 'grad_norm': 0.4765625, 'learning_rate': 0.00013388687048878107, 'epoch': 0.32}\n",
      "{'loss': 1.1517, 'grad_norm': 0.5546875, 'learning_rate': 0.00013387550811131722, 'epoch': 0.32}\n",
      "{'loss': 1.0324, 'grad_norm': 0.6328125, 'learning_rate': 0.00013386414573385335, 'epoch': 0.32}\n",
      "{'loss': 1.3734, 'grad_norm': 0.5078125, 'learning_rate': 0.0001338527833563895, 'epoch': 0.32}\n",
      "{'loss': 1.1111, 'grad_norm': 0.75, 'learning_rate': 0.00013384142097892565, 'epoch': 0.32}\n",
      "{'loss': 1.3134, 'grad_norm': 0.5, 'learning_rate': 0.00013383005860146177, 'epoch': 0.32}\n",
      "{'loss': 1.182, 'grad_norm': 0.54296875, 'learning_rate': 0.00013381869622399793, 'epoch': 0.32}\n",
      "{'loss': 1.0117, 'grad_norm': 0.5703125, 'learning_rate': 0.00013380733384653405, 'epoch': 0.32}\n",
      "{'loss': 1.3703, 'grad_norm': 0.48828125, 'learning_rate': 0.0001337959714690702, 'epoch': 0.32}\n",
      "{'loss': 1.3029, 'grad_norm': 0.7578125, 'learning_rate': 0.00013378460909160633, 'epoch': 0.32}\n",
      "{'loss': 1.2321, 'grad_norm': 0.6015625, 'learning_rate': 0.00013377324671414245, 'epoch': 0.32}\n",
      "{'loss': 1.2712, 'grad_norm': 0.5546875, 'learning_rate': 0.00013376188433667863, 'epoch': 0.32}\n",
      "{'loss': 1.122, 'grad_norm': 0.515625, 'learning_rate': 0.00013375052195921475, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3001, 'grad_norm': 0.453125, 'learning_rate': 0.0001337391595817509, 'epoch': 0.32}\n",
      "{'loss': 1.1681, 'grad_norm': 0.53125, 'learning_rate': 0.00013372779720428703, 'epoch': 0.32}\n",
      "{'loss': 1.2554, 'grad_norm': 0.55078125, 'learning_rate': 0.00013371643482682318, 'epoch': 0.32}\n",
      "{'loss': 1.3013, 'grad_norm': 0.546875, 'learning_rate': 0.0001337050724493593, 'epoch': 0.32}\n",
      "{'loss': 1.0144, 'grad_norm': 0.66015625, 'learning_rate': 0.00013369371007189543, 'epoch': 0.32}\n",
      "{'loss': 1.3918, 'grad_norm': 0.51953125, 'learning_rate': 0.0001336823476944316, 'epoch': 0.32}\n",
      "{'loss': 1.1901, 'grad_norm': 0.79296875, 'learning_rate': 0.00013367098531696773, 'epoch': 0.32}\n",
      "{'loss': 1.1319, 'grad_norm': 0.478515625, 'learning_rate': 0.00013365962293950388, 'epoch': 0.32}\n",
      "{'loss': 1.4106, 'grad_norm': 0.47265625, 'learning_rate': 0.00013364826056204, 'epoch': 0.32}\n",
      "{'loss': 1.0231, 'grad_norm': 1.1171875, 'learning_rate': 0.00013363689818457613, 'epoch': 0.32}\n",
      "{'loss': 1.4316, 'grad_norm': 0.396484375, 'learning_rate': 0.00013362553580711228, 'epoch': 0.32}\n",
      "{'loss': 1.2403, 'grad_norm': 0.703125, 'learning_rate': 0.00013361417342964843, 'epoch': 0.32}\n",
      "{'loss': 1.3476, 'grad_norm': 0.484375, 'learning_rate': 0.00013360281105218459, 'epoch': 0.32}\n",
      "{'loss': 1.2799, 'grad_norm': 0.69921875, 'learning_rate': 0.0001335914486747207, 'epoch': 0.32}\n",
      "{'loss': 1.0373, 'grad_norm': 0.9921875, 'learning_rate': 0.00013358008629725683, 'epoch': 0.32}\n",
      "{'loss': 1.3663, 'grad_norm': 0.81640625, 'learning_rate': 0.00013356872391979299, 'epoch': 0.32}\n",
      "{'loss': 1.0975, 'grad_norm': 0.53125, 'learning_rate': 0.0001335573615423291, 'epoch': 0.32}\n",
      "{'loss': 1.2773, 'grad_norm': 0.390625, 'learning_rate': 0.00013354599916486526, 'epoch': 0.32}\n",
      "{'loss': 1.2667, 'grad_norm': 0.53515625, 'learning_rate': 0.0001335346367874014, 'epoch': 0.32}\n",
      "{'loss': 1.2165, 'grad_norm': 0.96484375, 'learning_rate': 0.00013352327440993754, 'epoch': 0.32}\n",
      "{'loss': 1.2863, 'grad_norm': 0.50390625, 'learning_rate': 0.0001335119120324737, 'epoch': 0.32}\n",
      "{'loss': 1.2464, 'grad_norm': 0.921875, 'learning_rate': 0.0001335005496550098, 'epoch': 0.32}\n",
      "{'loss': 1.241, 'grad_norm': 0.46875, 'learning_rate': 0.00013348918727754596, 'epoch': 0.32}\n",
      "{'loss': 1.2499, 'grad_norm': 0.47265625, 'learning_rate': 0.0001334778249000821, 'epoch': 0.32}\n",
      "{'loss': 1.0555, 'grad_norm': 0.48046875, 'learning_rate': 0.00013346646252261824, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2358, 'grad_norm': 0.671875, 'learning_rate': 0.0001334551001451544, 'epoch': 0.32}\n",
      "{'loss': 1.1858, 'grad_norm': 0.578125, 'learning_rate': 0.00013344373776769052, 'epoch': 0.32}\n",
      "{'loss': 1.1755, 'grad_norm': 0.4296875, 'learning_rate': 0.00013343237539022667, 'epoch': 0.32}\n",
      "{'loss': 1.3793, 'grad_norm': 0.515625, 'learning_rate': 0.0001334210130127628, 'epoch': 0.32}\n",
      "{'loss': 1.1321, 'grad_norm': 1.296875, 'learning_rate': 0.00013340965063529894, 'epoch': 0.32}\n",
      "{'loss': 1.2964, 'grad_norm': 0.57421875, 'learning_rate': 0.00013339828825783507, 'epoch': 0.32}\n",
      "{'loss': 1.199, 'grad_norm': 0.84765625, 'learning_rate': 0.0001333869258803712, 'epoch': 0.32}\n",
      "{'loss': 1.2961, 'grad_norm': 0.375, 'learning_rate': 0.00013337556350290737, 'epoch': 0.32}\n",
      "{'loss': 1.2782, 'grad_norm': 0.56640625, 'learning_rate': 0.0001333642011254435, 'epoch': 0.32}\n",
      "{'loss': 1.1173, 'grad_norm': 0.953125, 'learning_rate': 0.00013335283874797965, 'epoch': 0.32}\n",
      "{'loss': 1.3643, 'grad_norm': 0.57421875, 'learning_rate': 0.00013334147637051577, 'epoch': 0.32}\n",
      "{'loss': 1.0827, 'grad_norm': 0.82421875, 'learning_rate': 0.00013333011399305192, 'epoch': 0.32}\n",
      "{'loss': 1.1122, 'grad_norm': 0.404296875, 'learning_rate': 0.00013331875161558805, 'epoch': 0.32}\n",
      "{'loss': 1.3803, 'grad_norm': 0.7734375, 'learning_rate': 0.00013330738923812417, 'epoch': 0.32}\n",
      "{'loss': 1.1336, 'grad_norm': 0.7421875, 'learning_rate': 0.00013329602686066035, 'epoch': 0.32}\n",
      "{'loss': 1.2742, 'grad_norm': 0.5546875, 'learning_rate': 0.00013328466448319647, 'epoch': 0.32}\n",
      "{'loss': 1.1301, 'grad_norm': 0.62109375, 'learning_rate': 0.00013327330210573262, 'epoch': 0.32}\n",
      "{'loss': 1.2972, 'grad_norm': 0.515625, 'learning_rate': 0.00013326193972826875, 'epoch': 0.32}\n",
      "{'loss': 1.3168, 'grad_norm': 0.515625, 'learning_rate': 0.00013325057735080487, 'epoch': 0.32}\n",
      "{'loss': 1.0765, 'grad_norm': 0.8984375, 'learning_rate': 0.00013323921497334102, 'epoch': 0.32}\n",
      "{'loss': 1.3885, 'grad_norm': 0.76171875, 'learning_rate': 0.00013322785259587718, 'epoch': 0.32}\n",
      "{'loss': 1.3515, 'grad_norm': 1.09375, 'learning_rate': 0.00013321649021841333, 'epoch': 0.32}\n",
      "{'loss': 1.1869, 'grad_norm': 0.51171875, 'learning_rate': 0.00013320512784094945, 'epoch': 0.32}\n",
      "{'loss': 1.3321, 'grad_norm': 0.5703125, 'learning_rate': 0.00013319376546348558, 'epoch': 0.32}\n",
      "{'loss': 1.0584, 'grad_norm': 0.8125, 'learning_rate': 0.00013318240308602173, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3504, 'grad_norm': 0.52734375, 'learning_rate': 0.00013317104070855785, 'epoch': 0.32}\n",
      "{'loss': 1.2054, 'grad_norm': 0.796875, 'learning_rate': 0.000133159678331094, 'epoch': 0.32}\n",
      "{'loss': 1.2733, 'grad_norm': 0.435546875, 'learning_rate': 0.00013314831595363015, 'epoch': 0.32}\n",
      "{'loss': 1.1535, 'grad_norm': 0.71875, 'learning_rate': 0.00013313695357616628, 'epoch': 0.32}\n",
      "{'loss': 1.1921, 'grad_norm': 0.703125, 'learning_rate': 0.00013312559119870243, 'epoch': 0.32}\n",
      "{'loss': 1.3084, 'grad_norm': 0.66796875, 'learning_rate': 0.00013311422882123855, 'epoch': 0.32}\n",
      "{'loss': 1.1845, 'grad_norm': 0.51953125, 'learning_rate': 0.0001331028664437747, 'epoch': 0.32}\n",
      "{'loss': 1.3343, 'grad_norm': 0.38671875, 'learning_rate': 0.00013309150406631083, 'epoch': 0.32}\n",
      "{'loss': 1.3001, 'grad_norm': 1.171875, 'learning_rate': 0.00013308014168884698, 'epoch': 0.32}\n",
      "{'loss': 1.0786, 'grad_norm': 0.4765625, 'learning_rate': 0.00013306877931138313, 'epoch': 0.32}\n",
      "{'loss': 1.2686, 'grad_norm': 0.421875, 'learning_rate': 0.00013305741693391926, 'epoch': 0.32}\n",
      "{'loss': 1.1694, 'grad_norm': 0.63671875, 'learning_rate': 0.0001330460545564554, 'epoch': 0.32}\n",
      "{'loss': 1.155, 'grad_norm': 0.455078125, 'learning_rate': 0.00013303469217899153, 'epoch': 0.32}\n",
      "{'loss': 1.2782, 'grad_norm': 0.515625, 'learning_rate': 0.00013302332980152768, 'epoch': 0.32}\n",
      "{'loss': 1.1315, 'grad_norm': 1.03125, 'learning_rate': 0.0001330119674240638, 'epoch': 0.32}\n",
      "{'loss': 1.388, 'grad_norm': 0.439453125, 'learning_rate': 0.00013300060504659993, 'epoch': 0.32}\n",
      "{'loss': 1.1891, 'grad_norm': 0.6328125, 'learning_rate': 0.0001329892426691361, 'epoch': 0.32}\n",
      "{'loss': 1.2905, 'grad_norm': 0.57421875, 'learning_rate': 0.00013297788029167224, 'epoch': 0.32}\n",
      "{'loss': 1.3549, 'grad_norm': 0.56640625, 'learning_rate': 0.0001329665179142084, 'epoch': 0.32}\n",
      "{'loss': 1.0176, 'grad_norm': 0.9140625, 'learning_rate': 0.0001329551555367445, 'epoch': 0.32}\n",
      "{'loss': 1.4807, 'grad_norm': 0.55078125, 'learning_rate': 0.00013294379315928066, 'epoch': 0.32}\n",
      "{'loss': 1.1928, 'grad_norm': 0.78515625, 'learning_rate': 0.0001329324307818168, 'epoch': 0.32}\n",
      "{'loss': 1.113, 'grad_norm': 0.53515625, 'learning_rate': 0.00013292106840435294, 'epoch': 0.32}\n",
      "{'loss': 1.1994, 'grad_norm': 0.67578125, 'learning_rate': 0.0001329097060268891, 'epoch': 0.32}\n",
      "{'loss': 1.0937, 'grad_norm': 0.412109375, 'learning_rate': 0.00013289834364942521, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4412, 'grad_norm': 0.64453125, 'learning_rate': 0.00013288698127196137, 'epoch': 0.32}\n",
      "{'loss': 1.1448, 'grad_norm': 0.66796875, 'learning_rate': 0.0001328756188944975, 'epoch': 0.32}\n",
      "{'loss': 1.2236, 'grad_norm': 0.5234375, 'learning_rate': 0.00013286425651703361, 'epoch': 0.32}\n",
      "{'loss': 1.2957, 'grad_norm': 0.5234375, 'learning_rate': 0.00013285289413956977, 'epoch': 0.32}\n",
      "{'loss': 1.1041, 'grad_norm': 0.99609375, 'learning_rate': 0.00013284153176210592, 'epoch': 0.32}\n",
      "{'loss': 1.3236, 'grad_norm': 0.51953125, 'learning_rate': 0.00013283016938464207, 'epoch': 0.32}\n",
      "{'loss': 1.1604, 'grad_norm': 0.796875, 'learning_rate': 0.0001328188070071782, 'epoch': 0.32}\n",
      "{'loss': 1.1817, 'grad_norm': 0.482421875, 'learning_rate': 0.00013280744462971432, 'epoch': 0.32}\n",
      "{'loss': 1.3122, 'grad_norm': 0.546875, 'learning_rate': 0.00013279608225225047, 'epoch': 0.32}\n",
      "{'loss': 1.0104, 'grad_norm': 0.8046875, 'learning_rate': 0.0001327847198747866, 'epoch': 0.32}\n",
      "{'loss': 1.4464, 'grad_norm': 0.578125, 'learning_rate': 0.00013277335749732274, 'epoch': 0.32}\n",
      "{'loss': 1.1689, 'grad_norm': 0.578125, 'learning_rate': 0.0001327619951198589, 'epoch': 0.32}\n",
      "{'loss': 1.1174, 'grad_norm': 0.419921875, 'learning_rate': 0.00013275063274239502, 'epoch': 0.32}\n",
      "{'loss': 1.2945, 'grad_norm': 0.5859375, 'learning_rate': 0.00013273927036493117, 'epoch': 0.32}\n",
      "{'loss': 1.0736, 'grad_norm': 0.953125, 'learning_rate': 0.0001327279079874673, 'epoch': 0.32}\n",
      "{'loss': 1.2918, 'grad_norm': 0.52734375, 'learning_rate': 0.00013271654561000345, 'epoch': 0.32}\n",
      "{'loss': 1.2286, 'grad_norm': 0.66796875, 'learning_rate': 0.00013270518323253957, 'epoch': 0.32}\n",
      "{'loss': 1.2756, 'grad_norm': 0.48828125, 'learning_rate': 0.00013269382085507572, 'epoch': 0.32}\n",
      "{'loss': 1.2733, 'grad_norm': 0.61328125, 'learning_rate': 0.00013268245847761187, 'epoch': 0.32}\n",
      "{'loss': 1.0351, 'grad_norm': 0.41015625, 'learning_rate': 0.000132671096100148, 'epoch': 0.32}\n",
      "{'loss': 1.3209, 'grad_norm': 0.65234375, 'learning_rate': 0.00013265973372268415, 'epoch': 0.32}\n",
      "{'loss': 1.2406, 'grad_norm': 0.59765625, 'learning_rate': 0.00013264837134522027, 'epoch': 0.32}\n",
      "{'loss': 1.1698, 'grad_norm': 0.439453125, 'learning_rate': 0.00013263700896775643, 'epoch': 0.32}\n",
      "{'loss': 1.3391, 'grad_norm': 0.640625, 'learning_rate': 0.00013262564659029255, 'epoch': 0.32}\n",
      "{'loss': 1.002, 'grad_norm': 0.7109375, 'learning_rate': 0.00013261428421282867, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.356, 'grad_norm': 0.65234375, 'learning_rate': 0.00013260292183536485, 'epoch': 0.32}\n",
      "{'loss': 1.2399, 'grad_norm': 0.9375, 'learning_rate': 0.00013259155945790098, 'epoch': 0.32}\n",
      "{'loss': 1.2718, 'grad_norm': 0.421875, 'learning_rate': 0.00013258019708043713, 'epoch': 0.32}\n",
      "{'loss': 1.2791, 'grad_norm': 0.54296875, 'learning_rate': 0.00013256883470297325, 'epoch': 0.32}\n",
      "{'loss': 1.1112, 'grad_norm': 0.43359375, 'learning_rate': 0.0001325574723255094, 'epoch': 0.32}\n",
      "{'loss': 1.3766, 'grad_norm': 0.50390625, 'learning_rate': 0.00013254610994804553, 'epoch': 0.32}\n",
      "{'loss': 1.1701, 'grad_norm': 0.62109375, 'learning_rate': 0.00013253474757058168, 'epoch': 0.32}\n",
      "{'loss': 1.23, 'grad_norm': 0.43359375, 'learning_rate': 0.00013252338519311783, 'epoch': 0.32}\n",
      "{'loss': 1.2384, 'grad_norm': 0.65625, 'learning_rate': 0.00013251202281565396, 'epoch': 0.32}\n",
      "{'loss': 1.0775, 'grad_norm': 1.1484375, 'learning_rate': 0.0001325006604381901, 'epoch': 0.32}\n",
      "{'loss': 1.3561, 'grad_norm': 0.494140625, 'learning_rate': 0.00013248929806072623, 'epoch': 0.32}\n",
      "{'loss': 1.2277, 'grad_norm': 0.640625, 'learning_rate': 0.00013247793568326236, 'epoch': 0.32}\n",
      "{'loss': 1.1214, 'grad_norm': 0.53125, 'learning_rate': 0.0001324665733057985, 'epoch': 0.32}\n",
      "{'loss': 1.2465, 'grad_norm': 0.56640625, 'learning_rate': 0.00013245521092833466, 'epoch': 0.32}\n",
      "{'loss': 1.0336, 'grad_norm': 0.578125, 'learning_rate': 0.0001324438485508708, 'epoch': 0.32}\n",
      "{'loss': 1.3422, 'grad_norm': 0.53125, 'learning_rate': 0.00013243248617340693, 'epoch': 0.32}\n",
      "{'loss': 1.0822, 'grad_norm': 0.53515625, 'learning_rate': 0.00013242112379594306, 'epoch': 0.32}\n",
      "{'loss': 1.2072, 'grad_norm': 0.443359375, 'learning_rate': 0.0001324097614184792, 'epoch': 0.32}\n",
      "{'loss': 1.3092, 'grad_norm': 0.60546875, 'learning_rate': 0.00013239839904101533, 'epoch': 0.32}\n",
      "{'loss': 1.0673, 'grad_norm': 0.93359375, 'learning_rate': 0.00013238703666355149, 'epoch': 0.32}\n",
      "{'loss': 1.2464, 'grad_norm': 0.56640625, 'learning_rate': 0.00013237567428608764, 'epoch': 0.32}\n",
      "{'loss': 1.1441, 'grad_norm': 0.67578125, 'learning_rate': 0.00013236431190862376, 'epoch': 0.32}\n",
      "{'loss': 1.1671, 'grad_norm': 0.43359375, 'learning_rate': 0.0001323529495311599, 'epoch': 0.32}\n",
      "{'loss': 1.202, 'grad_norm': 0.53125, 'learning_rate': 0.00013234158715369604, 'epoch': 0.32}\n",
      "{'loss': 1.152, 'grad_norm': 1.3828125, 'learning_rate': 0.0001323302247762322, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3414, 'grad_norm': 0.48046875, 'learning_rate': 0.0001323188623987683, 'epoch': 0.32}\n",
      "{'loss': 1.1026, 'grad_norm': 0.5234375, 'learning_rate': 0.00013230750002130446, 'epoch': 0.32}\n",
      "{'loss': 1.1842, 'grad_norm': 0.400390625, 'learning_rate': 0.00013229613764384062, 'epoch': 0.32}\n",
      "{'loss': 1.2538, 'grad_norm': 0.6484375, 'learning_rate': 0.00013228477526637674, 'epoch': 0.32}\n",
      "{'loss': 1.1452, 'grad_norm': 1.1015625, 'learning_rate': 0.0001322734128889129, 'epoch': 0.32}\n",
      "{'loss': 1.328, 'grad_norm': 0.50390625, 'learning_rate': 0.00013226205051144902, 'epoch': 0.32}\n",
      "{'loss': 1.189, 'grad_norm': 0.6640625, 'learning_rate': 0.00013225068813398517, 'epoch': 0.32}\n",
      "{'loss': 1.2578, 'grad_norm': 0.451171875, 'learning_rate': 0.0001322393257565213, 'epoch': 0.32}\n",
      "{'loss': 1.2282, 'grad_norm': 0.6328125, 'learning_rate': 0.00013222796337905744, 'epoch': 0.32}\n",
      "{'loss': 1.1206, 'grad_norm': 0.82421875, 'learning_rate': 0.0001322166010015936, 'epoch': 0.32}\n",
      "{'loss': 1.3721, 'grad_norm': 0.5390625, 'learning_rate': 0.00013220523862412972, 'epoch': 0.32}\n",
      "{'loss': 1.1398, 'grad_norm': 0.80078125, 'learning_rate': 0.00013219387624666587, 'epoch': 0.32}\n",
      "{'loss': 1.2543, 'grad_norm': 0.455078125, 'learning_rate': 0.000132182513869202, 'epoch': 0.32}\n",
      "{'loss': 1.2371, 'grad_norm': 0.6015625, 'learning_rate': 0.00013217115149173815, 'epoch': 0.32}\n",
      "{'loss': 1.112, 'grad_norm': 0.88671875, 'learning_rate': 0.00013215978911427427, 'epoch': 0.32}\n",
      "{'loss': 1.3078, 'grad_norm': 0.546875, 'learning_rate': 0.00013214842673681042, 'epoch': 0.32}\n",
      "{'loss': 1.154, 'grad_norm': 0.671875, 'learning_rate': 0.00013213706435934657, 'epoch': 0.32}\n",
      "{'loss': 1.203, 'grad_norm': 0.58984375, 'learning_rate': 0.0001321257019818827, 'epoch': 0.32}\n",
      "{'loss': 1.1453, 'grad_norm': 0.63671875, 'learning_rate': 0.00013211433960441885, 'epoch': 0.32}\n",
      "{'loss': 1.0593, 'grad_norm': 0.76171875, 'learning_rate': 0.00013210297722695497, 'epoch': 0.32}\n",
      "{'loss': 1.1954, 'grad_norm': 0.474609375, 'learning_rate': 0.0001320916148494911, 'epoch': 0.32}\n",
      "{'loss': 1.1461, 'grad_norm': 0.53125, 'learning_rate': 0.00013208025247202725, 'epoch': 0.32}\n",
      "{'loss': 1.1524, 'grad_norm': 0.45703125, 'learning_rate': 0.0001320688900945634, 'epoch': 0.32}\n",
      "{'loss': 1.1751, 'grad_norm': 0.58984375, 'learning_rate': 0.00013205752771709955, 'epoch': 0.32}\n",
      "{'loss': 1.028, 'grad_norm': 0.671875, 'learning_rate': 0.00013204616533963568, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3376, 'grad_norm': 0.57421875, 'learning_rate': 0.0001320348029621718, 'epoch': 0.32}\n",
      "{'loss': 1.2303, 'grad_norm': 0.78125, 'learning_rate': 0.00013202344058470795, 'epoch': 0.32}\n",
      "{'loss': 1.3132, 'grad_norm': 0.4765625, 'learning_rate': 0.00013201207820724408, 'epoch': 0.32}\n",
      "{'loss': 1.1661, 'grad_norm': 0.578125, 'learning_rate': 0.00013200071582978023, 'epoch': 0.32}\n",
      "{'loss': 1.0609, 'grad_norm': 0.76953125, 'learning_rate': 0.00013198935345231638, 'epoch': 0.32}\n",
      "{'loss': 1.3965, 'grad_norm': 0.515625, 'learning_rate': 0.0001319779910748525, 'epoch': 0.32}\n",
      "{'loss': 1.1766, 'grad_norm': 0.6640625, 'learning_rate': 0.00013196662869738865, 'epoch': 0.32}\n",
      "{'loss': 1.2946, 'grad_norm': 0.62109375, 'learning_rate': 0.00013195526631992478, 'epoch': 0.32}\n",
      "{'loss': 1.2704, 'grad_norm': 0.6015625, 'learning_rate': 0.00013194390394246093, 'epoch': 0.32}\n",
      "{'loss': 1.1324, 'grad_norm': 1.3671875, 'learning_rate': 0.00013193254156499705, 'epoch': 0.32}\n",
      "{'loss': 1.241, 'grad_norm': 0.494140625, 'learning_rate': 0.0001319211791875332, 'epoch': 0.32}\n",
      "{'loss': 1.2186, 'grad_norm': 0.7421875, 'learning_rate': 0.00013190981681006936, 'epoch': 0.32}\n",
      "{'loss': 1.3161, 'grad_norm': 0.49609375, 'learning_rate': 0.00013189845443260548, 'epoch': 0.32}\n",
      "{'loss': 1.2506, 'grad_norm': 0.5390625, 'learning_rate': 0.00013188709205514163, 'epoch': 0.32}\n",
      "{'loss': 1.1684, 'grad_norm': 0.83203125, 'learning_rate': 0.00013187572967767776, 'epoch': 0.32}\n",
      "{'loss': 1.205, 'grad_norm': 0.44140625, 'learning_rate': 0.0001318643673002139, 'epoch': 0.32}\n",
      "{'loss': 1.1186, 'grad_norm': 0.7578125, 'learning_rate': 0.00013185300492275003, 'epoch': 0.32}\n",
      "{'loss': 1.2647, 'grad_norm': 0.46484375, 'learning_rate': 0.00013184164254528618, 'epoch': 0.32}\n",
      "{'loss': 1.2356, 'grad_norm': 0.7109375, 'learning_rate': 0.00013183028016782234, 'epoch': 0.32}\n",
      "{'loss': 1.1919, 'grad_norm': 0.4296875, 'learning_rate': 0.00013181891779035846, 'epoch': 0.32}\n",
      "{'loss': 1.3303, 'grad_norm': 0.421875, 'learning_rate': 0.0001318075554128946, 'epoch': 0.32}\n",
      "{'loss': 1.1486, 'grad_norm': 0.6328125, 'learning_rate': 0.00013179619303543074, 'epoch': 0.32}\n",
      "{'loss': 1.1521, 'grad_norm': 0.443359375, 'learning_rate': 0.0001317848306579669, 'epoch': 0.32}\n",
      "{'loss': 1.2532, 'grad_norm': 0.6171875, 'learning_rate': 0.000131773468280503, 'epoch': 0.32}\n",
      "{'loss': 1.1116, 'grad_norm': 0.9765625, 'learning_rate': 0.00013176210590303916, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2274, 'grad_norm': 0.6171875, 'learning_rate': 0.00013175074352557531, 'epoch': 0.32}\n",
      "{'loss': 1.1565, 'grad_norm': 0.70703125, 'learning_rate': 0.00013173938114811144, 'epoch': 0.32}\n",
      "{'loss': 1.2668, 'grad_norm': 0.50390625, 'learning_rate': 0.0001317280187706476, 'epoch': 0.33}\n",
      "{'loss': 1.3121, 'grad_norm': 0.81640625, 'learning_rate': 0.00013171665639318371, 'epoch': 0.33}\n",
      "{'loss': 1.0173, 'grad_norm': 1.21875, 'learning_rate': 0.00013170529401571984, 'epoch': 0.33}\n",
      "{'loss': 1.3579, 'grad_norm': 0.48828125, 'learning_rate': 0.000131693931638256, 'epoch': 0.33}\n",
      "{'loss': 1.2302, 'grad_norm': 0.75, 'learning_rate': 0.00013168256926079214, 'epoch': 0.33}\n",
      "{'loss': 1.2464, 'grad_norm': 0.41796875, 'learning_rate': 0.0001316712068833283, 'epoch': 0.33}\n",
      "{'loss': 1.1361, 'grad_norm': 0.55078125, 'learning_rate': 0.00013165984450586442, 'epoch': 0.33}\n",
      "{'loss': 1.0968, 'grad_norm': 2.0, 'learning_rate': 0.00013164848212840054, 'epoch': 0.33}\n",
      "{'loss': 1.3377, 'grad_norm': 0.4140625, 'learning_rate': 0.0001316371197509367, 'epoch': 0.33}\n",
      "{'loss': 1.1529, 'grad_norm': 0.69140625, 'learning_rate': 0.00013162575737347282, 'epoch': 0.33}\n",
      "{'loss': 1.1579, 'grad_norm': 0.455078125, 'learning_rate': 0.00013161439499600897, 'epoch': 0.33}\n",
      "{'loss': 1.2182, 'grad_norm': 0.6328125, 'learning_rate': 0.00013160303261854512, 'epoch': 0.33}\n",
      "{'loss': 1.1162, 'grad_norm': 0.52734375, 'learning_rate': 0.00013159167024108124, 'epoch': 0.33}\n",
      "{'loss': 1.3633, 'grad_norm': 0.484375, 'learning_rate': 0.0001315803078636174, 'epoch': 0.33}\n",
      "{'loss': 1.0555, 'grad_norm': 0.51171875, 'learning_rate': 0.00013156894548615352, 'epoch': 0.33}\n",
      "{'loss': 1.2455, 'grad_norm': 0.380859375, 'learning_rate': 0.00013155758310868967, 'epoch': 0.33}\n",
      "{'loss': 1.2477, 'grad_norm': 0.69140625, 'learning_rate': 0.0001315462207312258, 'epoch': 0.33}\n",
      "{'loss': 1.0088, 'grad_norm': 0.35546875, 'learning_rate': 0.00013153485835376195, 'epoch': 0.33}\n",
      "{'loss': 1.3795, 'grad_norm': 0.64453125, 'learning_rate': 0.0001315234959762981, 'epoch': 0.33}\n",
      "{'loss': 1.3281, 'grad_norm': 0.80078125, 'learning_rate': 0.00013151213359883422, 'epoch': 0.33}\n",
      "{'loss': 1.1983, 'grad_norm': 0.59375, 'learning_rate': 0.00013150077122137037, 'epoch': 0.33}\n",
      "{'loss': 1.1719, 'grad_norm': 0.5078125, 'learning_rate': 0.0001314894088439065, 'epoch': 0.33}\n",
      "{'loss': 1.1178, 'grad_norm': 1.296875, 'learning_rate': 0.00013147804646644265, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3775, 'grad_norm': 0.515625, 'learning_rate': 0.00013146668408897877, 'epoch': 0.33}\n",
      "{'loss': 1.1674, 'grad_norm': 0.73828125, 'learning_rate': 0.00013145532171151493, 'epoch': 0.33}\n",
      "{'loss': 1.2318, 'grad_norm': 0.376953125, 'learning_rate': 0.00013144395933405108, 'epoch': 0.33}\n",
      "{'loss': 1.1813, 'grad_norm': 0.78515625, 'learning_rate': 0.0001314325969565872, 'epoch': 0.33}\n",
      "{'loss': 1.2277, 'grad_norm': 1.1171875, 'learning_rate': 0.00013142123457912335, 'epoch': 0.33}\n",
      "{'loss': 1.3032, 'grad_norm': 0.6171875, 'learning_rate': 0.00013140987220165948, 'epoch': 0.33}\n",
      "{'loss': 1.1657, 'grad_norm': 0.671875, 'learning_rate': 0.00013139850982419563, 'epoch': 0.33}\n",
      "{'loss': 1.1508, 'grad_norm': 0.41015625, 'learning_rate': 0.00013138714744673175, 'epoch': 0.33}\n",
      "{'loss': 1.2186, 'grad_norm': 0.53125, 'learning_rate': 0.0001313757850692679, 'epoch': 0.33}\n",
      "{'loss': 0.9971, 'grad_norm': 0.828125, 'learning_rate': 0.00013136442269180406, 'epoch': 0.33}\n",
      "{'loss': 1.2972, 'grad_norm': 0.546875, 'learning_rate': 0.00013135306031434018, 'epoch': 0.33}\n",
      "{'loss': 1.2316, 'grad_norm': 0.5, 'learning_rate': 0.00013134169793687633, 'epoch': 0.33}\n",
      "{'loss': 1.116, 'grad_norm': 0.474609375, 'learning_rate': 0.00013133033555941246, 'epoch': 0.33}\n",
      "{'loss': 1.2833, 'grad_norm': 0.54296875, 'learning_rate': 0.00013131897318194858, 'epoch': 0.33}\n",
      "{'loss': 1.0773, 'grad_norm': 1.0078125, 'learning_rate': 0.00013130761080448473, 'epoch': 0.33}\n",
      "{'loss': 1.2455, 'grad_norm': 0.6328125, 'learning_rate': 0.00013129624842702088, 'epoch': 0.33}\n",
      "{'loss': 1.2415, 'grad_norm': 0.796875, 'learning_rate': 0.00013128488604955703, 'epoch': 0.33}\n",
      "{'loss': 1.2783, 'grad_norm': 0.52734375, 'learning_rate': 0.00013127352367209316, 'epoch': 0.33}\n",
      "{'loss': 1.1577, 'grad_norm': 0.5625, 'learning_rate': 0.00013126216129462928, 'epoch': 0.33}\n",
      "{'loss': 1.1009, 'grad_norm': 0.412109375, 'learning_rate': 0.00013125079891716543, 'epoch': 0.33}\n",
      "{'loss': 1.3103, 'grad_norm': 0.59765625, 'learning_rate': 0.00013123943653970156, 'epoch': 0.33}\n",
      "{'loss': 1.2135, 'grad_norm': 0.71875, 'learning_rate': 0.0001312280741622377, 'epoch': 0.33}\n",
      "{'loss': 1.1695, 'grad_norm': 0.4296875, 'learning_rate': 0.00013121671178477386, 'epoch': 0.33}\n",
      "{'loss': 1.4042, 'grad_norm': 0.609375, 'learning_rate': 0.00013120534940730999, 'epoch': 0.33}\n",
      "{'loss': 1.238, 'grad_norm': 1.3984375, 'learning_rate': 0.00013119398702984614, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3706, 'grad_norm': 0.55859375, 'learning_rate': 0.00013118262465238226, 'epoch': 0.33}\n",
      "{'loss': 1.3102, 'grad_norm': 0.640625, 'learning_rate': 0.0001311712622749184, 'epoch': 0.33}\n",
      "{'loss': 1.333, 'grad_norm': 0.43359375, 'learning_rate': 0.00013115989989745454, 'epoch': 0.33}\n",
      "{'loss': 1.2157, 'grad_norm': 0.6953125, 'learning_rate': 0.0001311485375199907, 'epoch': 0.33}\n",
      "{'loss': 1.1809, 'grad_norm': 0.90234375, 'learning_rate': 0.00013113717514252684, 'epoch': 0.33}\n",
      "{'loss': 1.2153, 'grad_norm': 0.8203125, 'learning_rate': 0.00013112581276506296, 'epoch': 0.33}\n",
      "{'loss': 1.1787, 'grad_norm': 0.6953125, 'learning_rate': 0.00013111445038759912, 'epoch': 0.33}\n",
      "{'loss': 1.1647, 'grad_norm': 0.51953125, 'learning_rate': 0.00013110308801013524, 'epoch': 0.33}\n",
      "{'loss': 1.205, 'grad_norm': 0.7265625, 'learning_rate': 0.0001310917256326714, 'epoch': 0.33}\n",
      "{'loss': 0.9776, 'grad_norm': 0.41796875, 'learning_rate': 0.00013108036325520752, 'epoch': 0.33}\n",
      "{'loss': 1.4955, 'grad_norm': 0.46875, 'learning_rate': 0.00013106900087774367, 'epoch': 0.33}\n",
      "{'loss': 1.1147, 'grad_norm': 0.72265625, 'learning_rate': 0.00013105763850027982, 'epoch': 0.33}\n",
      "{'loss': 1.3045, 'grad_norm': 0.44921875, 'learning_rate': 0.00013104627612281594, 'epoch': 0.33}\n",
      "{'loss': 1.3218, 'grad_norm': 0.5859375, 'learning_rate': 0.0001310349137453521, 'epoch': 0.33}\n",
      "{'loss': 1.0152, 'grad_norm': 0.890625, 'learning_rate': 0.00013102355136788822, 'epoch': 0.33}\n",
      "{'loss': 1.4939, 'grad_norm': 0.478515625, 'learning_rate': 0.00013101218899042437, 'epoch': 0.33}\n",
      "{'loss': 1.2702, 'grad_norm': 0.66796875, 'learning_rate': 0.0001310008266129605, 'epoch': 0.33}\n",
      "{'loss': 1.2367, 'grad_norm': 0.54296875, 'learning_rate': 0.00013098946423549665, 'epoch': 0.33}\n",
      "{'loss': 1.2563, 'grad_norm': 0.546875, 'learning_rate': 0.0001309781018580328, 'epoch': 0.33}\n",
      "{'loss': 1.2187, 'grad_norm': 0.80078125, 'learning_rate': 0.00013096673948056892, 'epoch': 0.33}\n",
      "{'loss': 1.3098, 'grad_norm': 0.453125, 'learning_rate': 0.00013095537710310507, 'epoch': 0.33}\n",
      "{'loss': 1.2792, 'grad_norm': 0.7109375, 'learning_rate': 0.0001309440147256412, 'epoch': 0.33}\n",
      "{'loss': 1.2262, 'grad_norm': 0.55859375, 'learning_rate': 0.00013093265234817732, 'epoch': 0.33}\n",
      "{'loss': 1.3273, 'grad_norm': 0.671875, 'learning_rate': 0.00013092128997071347, 'epoch': 0.33}\n",
      "{'loss': 1.1821, 'grad_norm': 0.83203125, 'learning_rate': 0.00013090992759324962, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3268, 'grad_norm': 0.57421875, 'learning_rate': 0.00013089856521578578, 'epoch': 0.33}\n",
      "{'loss': 1.125, 'grad_norm': 0.498046875, 'learning_rate': 0.0001308872028383219, 'epoch': 0.33}\n",
      "{'loss': 1.2843, 'grad_norm': 0.45703125, 'learning_rate': 0.00013087584046085802, 'epoch': 0.33}\n",
      "{'loss': 1.2502, 'grad_norm': 0.5546875, 'learning_rate': 0.00013086447808339418, 'epoch': 0.33}\n",
      "{'loss': 1.0354, 'grad_norm': 0.62109375, 'learning_rate': 0.0001308531157059303, 'epoch': 0.33}\n",
      "{'loss': 1.2869, 'grad_norm': 0.498046875, 'learning_rate': 0.00013084175332846648, 'epoch': 0.33}\n",
      "{'loss': 1.2414, 'grad_norm': 0.44921875, 'learning_rate': 0.0001308303909510026, 'epoch': 0.33}\n",
      "{'loss': 1.3017, 'grad_norm': 0.44140625, 'learning_rate': 0.00013081902857353873, 'epoch': 0.33}\n",
      "{'loss': 1.2653, 'grad_norm': 0.55078125, 'learning_rate': 0.00013080766619607488, 'epoch': 0.33}\n",
      "{'loss': 1.13, 'grad_norm': 0.9375, 'learning_rate': 0.000130796303818611, 'epoch': 0.33}\n",
      "{'loss': 1.4039, 'grad_norm': 0.5625, 'learning_rate': 0.00013078494144114715, 'epoch': 0.33}\n",
      "{'loss': 1.3395, 'grad_norm': 0.66015625, 'learning_rate': 0.00013077357906368328, 'epoch': 0.33}\n",
      "{'loss': 1.3275, 'grad_norm': 0.51171875, 'learning_rate': 0.00013076221668621943, 'epoch': 0.33}\n",
      "{'loss': 1.3823, 'grad_norm': 0.59375, 'learning_rate': 0.00013075085430875558, 'epoch': 0.33}\n",
      "{'loss': 1.111, 'grad_norm': 0.81640625, 'learning_rate': 0.0001307394919312917, 'epoch': 0.33}\n",
      "{'loss': 1.4258, 'grad_norm': 0.59375, 'learning_rate': 0.00013072812955382786, 'epoch': 0.33}\n",
      "{'loss': 1.2429, 'grad_norm': 0.6328125, 'learning_rate': 0.00013071676717636398, 'epoch': 0.33}\n",
      "{'loss': 1.0285, 'grad_norm': 0.6328125, 'learning_rate': 0.00013070540479890013, 'epoch': 0.33}\n",
      "{'loss': 1.2737, 'grad_norm': 1.0078125, 'learning_rate': 0.00013069404242143626, 'epoch': 0.33}\n",
      "{'loss': 0.9434, 'grad_norm': 1.1015625, 'learning_rate': 0.0001306826800439724, 'epoch': 0.33}\n",
      "{'loss': 1.3318, 'grad_norm': 0.56640625, 'learning_rate': 0.00013067131766650856, 'epoch': 0.33}\n",
      "{'loss': 1.1641, 'grad_norm': 0.69921875, 'learning_rate': 0.00013065995528904468, 'epoch': 0.33}\n",
      "{'loss': 1.2296, 'grad_norm': 0.427734375, 'learning_rate': 0.00013064859291158083, 'epoch': 0.33}\n",
      "{'loss': 1.3502, 'grad_norm': 0.55078125, 'learning_rate': 0.00013063723053411696, 'epoch': 0.33}\n",
      "{'loss': 1.0725, 'grad_norm': 0.83203125, 'learning_rate': 0.0001306258681566531, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3251, 'grad_norm': 0.55078125, 'learning_rate': 0.00013061450577918923, 'epoch': 0.33}\n",
      "{'loss': 1.1159, 'grad_norm': 0.7109375, 'learning_rate': 0.00013060314340172539, 'epoch': 0.33}\n",
      "{'loss': 1.2347, 'grad_norm': 0.458984375, 'learning_rate': 0.00013059178102426154, 'epoch': 0.33}\n",
      "{'loss': 1.3, 'grad_norm': 0.55859375, 'learning_rate': 0.00013058041864679766, 'epoch': 0.33}\n",
      "{'loss': 1.1, 'grad_norm': 1.1875, 'learning_rate': 0.0001305690562693338, 'epoch': 0.33}\n",
      "{'loss': 1.2565, 'grad_norm': 0.45703125, 'learning_rate': 0.00013055769389186994, 'epoch': 0.33}\n",
      "{'loss': 1.2677, 'grad_norm': 0.59375, 'learning_rate': 0.00013054633151440606, 'epoch': 0.33}\n",
      "{'loss': 1.2122, 'grad_norm': 0.4296875, 'learning_rate': 0.0001305349691369422, 'epoch': 0.33}\n",
      "{'loss': 1.18, 'grad_norm': 0.51171875, 'learning_rate': 0.00013052360675947836, 'epoch': 0.33}\n",
      "{'loss': 1.0395, 'grad_norm': 0.6171875, 'learning_rate': 0.00013051224438201452, 'epoch': 0.33}\n",
      "{'loss': 1.3898, 'grad_norm': 0.58203125, 'learning_rate': 0.00013050088200455064, 'epoch': 0.33}\n",
      "{'loss': 1.2602, 'grad_norm': 0.6796875, 'learning_rate': 0.00013048951962708676, 'epoch': 0.33}\n",
      "{'loss': 1.1656, 'grad_norm': 0.49609375, 'learning_rate': 0.00013047815724962292, 'epoch': 0.33}\n",
      "{'loss': 1.3956, 'grad_norm': 0.54296875, 'learning_rate': 0.00013046679487215904, 'epoch': 0.33}\n",
      "{'loss': 1.1565, 'grad_norm': 0.78125, 'learning_rate': 0.00013045543249469522, 'epoch': 0.33}\n",
      "{'loss': 1.2981, 'grad_norm': 0.5234375, 'learning_rate': 0.00013044407011723134, 'epoch': 0.33}\n",
      "{'loss': 1.2099, 'grad_norm': 0.80859375, 'learning_rate': 0.00013043270773976747, 'epoch': 0.33}\n",
      "{'loss': 1.2232, 'grad_norm': 0.421875, 'learning_rate': 0.00013042134536230362, 'epoch': 0.33}\n",
      "{'loss': 1.2398, 'grad_norm': 0.5625, 'learning_rate': 0.00013040998298483974, 'epoch': 0.33}\n",
      "{'loss': 1.0285, 'grad_norm': 0.55078125, 'learning_rate': 0.0001303986206073759, 'epoch': 0.33}\n",
      "{'loss': 1.3384, 'grad_norm': 0.45703125, 'learning_rate': 0.00013038725822991202, 'epoch': 0.33}\n",
      "{'loss': 1.2127, 'grad_norm': 0.6875, 'learning_rate': 0.00013037589585244817, 'epoch': 0.33}\n",
      "{'loss': 1.3009, 'grad_norm': 0.419921875, 'learning_rate': 0.00013036453347498432, 'epoch': 0.33}\n",
      "{'loss': 1.2322, 'grad_norm': 0.55859375, 'learning_rate': 0.00013035317109752045, 'epoch': 0.33}\n",
      "{'loss': 1.0827, 'grad_norm': 0.63671875, 'learning_rate': 0.0001303418087200566, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3475, 'grad_norm': 0.87890625, 'learning_rate': 0.00013033044634259272, 'epoch': 0.33}\n",
      "{'loss': 1.0902, 'grad_norm': 0.55859375, 'learning_rate': 0.00013031908396512887, 'epoch': 0.33}\n",
      "{'loss': 1.2977, 'grad_norm': 0.484375, 'learning_rate': 0.000130307721587665, 'epoch': 0.33}\n",
      "{'loss': 1.3682, 'grad_norm': 0.7109375, 'learning_rate': 0.00013029635921020115, 'epoch': 0.33}\n",
      "{'loss': 1.0416, 'grad_norm': 1.03125, 'learning_rate': 0.0001302849968327373, 'epoch': 0.33}\n",
      "{'loss': 1.3061, 'grad_norm': 0.490234375, 'learning_rate': 0.00013027363445527342, 'epoch': 0.33}\n",
      "{'loss': 1.2547, 'grad_norm': 0.6484375, 'learning_rate': 0.00013026227207780958, 'epoch': 0.33}\n",
      "{'loss': 1.2215, 'grad_norm': 0.46875, 'learning_rate': 0.0001302509097003457, 'epoch': 0.33}\n",
      "{'loss': 1.2837, 'grad_norm': 0.6328125, 'learning_rate': 0.00013023954732288185, 'epoch': 0.33}\n",
      "{'loss': 1.1437, 'grad_norm': 0.8671875, 'learning_rate': 0.00013022818494541798, 'epoch': 0.33}\n",
      "{'loss': 1.3295, 'grad_norm': 0.51953125, 'learning_rate': 0.00013021682256795413, 'epoch': 0.33}\n",
      "{'loss': 1.1443, 'grad_norm': 0.64453125, 'learning_rate': 0.00013020546019049028, 'epoch': 0.33}\n",
      "{'loss': 1.2255, 'grad_norm': 0.42578125, 'learning_rate': 0.0001301940978130264, 'epoch': 0.33}\n",
      "{'loss': 1.3386, 'grad_norm': 0.6015625, 'learning_rate': 0.00013018273543556255, 'epoch': 0.33}\n",
      "{'loss': 1.1356, 'grad_norm': 0.310546875, 'learning_rate': 0.00013017137305809868, 'epoch': 0.33}\n",
      "{'loss': 1.2152, 'grad_norm': 0.609375, 'learning_rate': 0.0001301600106806348, 'epoch': 0.33}\n",
      "{'loss': 1.2442, 'grad_norm': 1.7109375, 'learning_rate': 0.00013014864830317098, 'epoch': 0.33}\n",
      "{'loss': 1.1142, 'grad_norm': 0.65234375, 'learning_rate': 0.0001301372859257071, 'epoch': 0.33}\n",
      "{'loss': 1.3662, 'grad_norm': 0.79296875, 'learning_rate': 0.00013012592354824326, 'epoch': 0.33}\n",
      "{'loss': 1.1205, 'grad_norm': 0.80078125, 'learning_rate': 0.00013011456117077938, 'epoch': 0.33}\n",
      "{'loss': 1.2672, 'grad_norm': 0.44921875, 'learning_rate': 0.0001301031987933155, 'epoch': 0.33}\n",
      "{'loss': 1.2737, 'grad_norm': 0.58984375, 'learning_rate': 0.00013009183641585166, 'epoch': 0.33}\n",
      "{'loss': 1.3499, 'grad_norm': 0.412109375, 'learning_rate': 0.00013008047403838778, 'epoch': 0.33}\n",
      "{'loss': 1.2762, 'grad_norm': 0.625, 'learning_rate': 0.00013006911166092396, 'epoch': 0.33}\n",
      "{'loss': 1.1385, 'grad_norm': 0.90234375, 'learning_rate': 0.00013005774928346008, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2492, 'grad_norm': 0.44140625, 'learning_rate': 0.0001300463869059962, 'epoch': 0.33}\n",
      "{'loss': 1.0863, 'grad_norm': 0.66015625, 'learning_rate': 0.00013003502452853236, 'epoch': 0.33}\n",
      "{'loss': 1.1296, 'grad_norm': 0.443359375, 'learning_rate': 0.00013002366215106848, 'epoch': 0.33}\n",
      "{'loss': 1.2507, 'grad_norm': 0.6875, 'learning_rate': 0.00013001229977360464, 'epoch': 0.33}\n",
      "{'loss': 1.1081, 'grad_norm': 0.82421875, 'learning_rate': 0.00013000093739614076, 'epoch': 0.33}\n",
      "{'loss': 1.3633, 'grad_norm': 0.609375, 'learning_rate': 0.0001299895750186769, 'epoch': 0.33}\n",
      "{'loss': 1.1993, 'grad_norm': 0.62890625, 'learning_rate': 0.00012997821264121306, 'epoch': 0.33}\n",
      "{'loss': 1.2318, 'grad_norm': 0.44921875, 'learning_rate': 0.0001299668502637492, 'epoch': 0.33}\n",
      "{'loss': 1.204, 'grad_norm': 0.5234375, 'learning_rate': 0.00012995548788628534, 'epoch': 0.33}\n",
      "{'loss': 1.1892, 'grad_norm': 1.109375, 'learning_rate': 0.00012994412550882146, 'epoch': 0.33}\n",
      "{'loss': 1.2749, 'grad_norm': 0.6328125, 'learning_rate': 0.00012993276313135761, 'epoch': 0.33}\n",
      "{'loss': 1.1121, 'grad_norm': 0.9140625, 'learning_rate': 0.00012992140075389374, 'epoch': 0.33}\n",
      "{'loss': 1.2572, 'grad_norm': 0.412109375, 'learning_rate': 0.0001299100383764299, 'epoch': 0.33}\n",
      "{'loss': 1.1356, 'grad_norm': 0.73828125, 'learning_rate': 0.00012989867599896604, 'epoch': 0.33}\n",
      "{'loss': 1.1526, 'grad_norm': 0.95703125, 'learning_rate': 0.00012988731362150217, 'epoch': 0.33}\n",
      "{'loss': 1.3199, 'grad_norm': 0.546875, 'learning_rate': 0.00012987595124403832, 'epoch': 0.33}\n",
      "{'loss': 1.1188, 'grad_norm': 0.5546875, 'learning_rate': 0.00012986458886657444, 'epoch': 0.33}\n",
      "{'loss': 1.2093, 'grad_norm': 0.54296875, 'learning_rate': 0.0001298532264891106, 'epoch': 0.33}\n",
      "{'loss': 1.0807, 'grad_norm': 0.474609375, 'learning_rate': 0.00012984186411164674, 'epoch': 0.33}\n",
      "{'loss': 1.1619, 'grad_norm': 1.140625, 'learning_rate': 0.00012983050173418287, 'epoch': 0.33}\n",
      "{'loss': 1.4212, 'grad_norm': 0.55859375, 'learning_rate': 0.00012981913935671902, 'epoch': 0.33}\n",
      "{'loss': 1.309, 'grad_norm': 0.80078125, 'learning_rate': 0.00012980777697925514, 'epoch': 0.33}\n",
      "{'loss': 1.2229, 'grad_norm': 0.400390625, 'learning_rate': 0.0001297964146017913, 'epoch': 0.33}\n",
      "{'loss': 1.3543, 'grad_norm': 0.55859375, 'learning_rate': 0.00012978505222432742, 'epoch': 0.33}\n",
      "{'loss': 1.0981, 'grad_norm': 0.376953125, 'learning_rate': 0.00012977368984686354, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3279, 'grad_norm': 0.412109375, 'learning_rate': 0.00012976232746939972, 'epoch': 0.33}\n",
      "{'loss': 1.1902, 'grad_norm': 0.78125, 'learning_rate': 0.00012975096509193585, 'epoch': 0.33}\n",
      "{'loss': 1.2124, 'grad_norm': 0.51953125, 'learning_rate': 0.000129739602714472, 'epoch': 0.33}\n",
      "{'loss': 1.1643, 'grad_norm': 0.8046875, 'learning_rate': 0.00012972824033700812, 'epoch': 0.33}\n",
      "{'loss': 1.1211, 'grad_norm': 0.83984375, 'learning_rate': 0.00012971687795954425, 'epoch': 0.33}\n",
      "{'loss': 1.353, 'grad_norm': 0.51953125, 'learning_rate': 0.0001297055155820804, 'epoch': 0.33}\n",
      "{'loss': 1.1341, 'grad_norm': 0.8828125, 'learning_rate': 0.00012969415320461652, 'epoch': 0.33}\n",
      "{'loss': 1.1815, 'grad_norm': 0.451171875, 'learning_rate': 0.0001296827908271527, 'epoch': 0.33}\n",
      "{'loss': 1.2204, 'grad_norm': 0.64453125, 'learning_rate': 0.00012967142844968883, 'epoch': 0.33}\n",
      "{'loss': 0.966, 'grad_norm': 0.5546875, 'learning_rate': 0.00012966006607222495, 'epoch': 0.33}\n",
      "{'loss': 1.263, 'grad_norm': 0.50390625, 'learning_rate': 0.0001296487036947611, 'epoch': 0.33}\n",
      "{'loss': 1.0717, 'grad_norm': 0.9296875, 'learning_rate': 0.00012963734131729723, 'epoch': 0.33}\n",
      "{'loss': 1.1603, 'grad_norm': 0.423828125, 'learning_rate': 0.00012962597893983338, 'epoch': 0.33}\n",
      "{'loss': 1.3986, 'grad_norm': 0.474609375, 'learning_rate': 0.0001296146165623695, 'epoch': 0.33}\n",
      "{'loss': 1.0946, 'grad_norm': 0.8203125, 'learning_rate': 0.00012960325418490565, 'epoch': 0.33}\n",
      "{'loss': 1.3203, 'grad_norm': 0.474609375, 'learning_rate': 0.0001295918918074418, 'epoch': 0.33}\n",
      "{'loss': 1.1743, 'grad_norm': 1.03125, 'learning_rate': 0.00012958052942997793, 'epoch': 0.33}\n",
      "{'loss': 1.2479, 'grad_norm': 0.5078125, 'learning_rate': 0.00012956916705251408, 'epoch': 0.33}\n",
      "{'loss': 1.3091, 'grad_norm': 0.68359375, 'learning_rate': 0.0001295578046750502, 'epoch': 0.33}\n",
      "{'loss': 1.1318, 'grad_norm': 0.56640625, 'learning_rate': 0.00012954644229758636, 'epoch': 0.33}\n",
      "{'loss': 1.3243, 'grad_norm': 0.5, 'learning_rate': 0.00012953507992012248, 'epoch': 0.33}\n",
      "{'loss': 1.129, 'grad_norm': 0.671875, 'learning_rate': 0.00012952371754265863, 'epoch': 0.33}\n",
      "{'loss': 1.2802, 'grad_norm': 0.384765625, 'learning_rate': 0.00012951235516519478, 'epoch': 0.33}\n",
      "{'loss': 1.2221, 'grad_norm': 0.54296875, 'learning_rate': 0.0001295009927877309, 'epoch': 0.33}\n",
      "{'loss': 1.0362, 'grad_norm': 0.671875, 'learning_rate': 0.00012948963041026706, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3754, 'grad_norm': 0.5078125, 'learning_rate': 0.00012947826803280318, 'epoch': 0.33}\n",
      "{'loss': 1.2189, 'grad_norm': 0.6328125, 'learning_rate': 0.00012946690565533933, 'epoch': 0.33}\n",
      "{'loss': 1.1597, 'grad_norm': 0.66015625, 'learning_rate': 0.00012945554327787549, 'epoch': 0.33}\n",
      "{'loss': 1.1975, 'grad_norm': 0.59375, 'learning_rate': 0.0001294441809004116, 'epoch': 0.33}\n",
      "{'loss': 1.0416, 'grad_norm': 0.80078125, 'learning_rate': 0.00012943281852294776, 'epoch': 0.34}\n",
      "{'loss': 1.3504, 'grad_norm': 0.482421875, 'learning_rate': 0.00012942145614548389, 'epoch': 0.34}\n",
      "{'loss': 1.2575, 'grad_norm': 0.6953125, 'learning_rate': 0.00012941009376802004, 'epoch': 0.34}\n",
      "{'loss': 1.1745, 'grad_norm': 0.421875, 'learning_rate': 0.00012939873139055616, 'epoch': 0.34}\n",
      "{'loss': 1.2705, 'grad_norm': 0.4921875, 'learning_rate': 0.00012938736901309229, 'epoch': 0.34}\n",
      "{'loss': 1.1978, 'grad_norm': 0.64453125, 'learning_rate': 0.00012937600663562846, 'epoch': 0.34}\n",
      "{'loss': 1.3386, 'grad_norm': 0.458984375, 'learning_rate': 0.0001293646442581646, 'epoch': 0.34}\n",
      "{'loss': 1.2519, 'grad_norm': 0.8984375, 'learning_rate': 0.00012935328188070074, 'epoch': 0.34}\n",
      "{'loss': 1.2324, 'grad_norm': 0.546875, 'learning_rate': 0.00012934191950323686, 'epoch': 0.34}\n",
      "{'loss': 1.1683, 'grad_norm': 0.47265625, 'learning_rate': 0.000129330557125773, 'epoch': 0.34}\n",
      "{'loss': 1.1637, 'grad_norm': 0.86328125, 'learning_rate': 0.00012931919474830914, 'epoch': 0.34}\n",
      "{'loss': 1.2291, 'grad_norm': 0.482421875, 'learning_rate': 0.00012930783237084526, 'epoch': 0.34}\n",
      "{'loss': 1.1898, 'grad_norm': 1.0234375, 'learning_rate': 0.00012929646999338144, 'epoch': 0.34}\n",
      "{'loss': 1.2043, 'grad_norm': 0.439453125, 'learning_rate': 0.00012928510761591757, 'epoch': 0.34}\n",
      "{'loss': 1.2464, 'grad_norm': 0.72265625, 'learning_rate': 0.0001292737452384537, 'epoch': 0.34}\n",
      "{'loss': 1.074, 'grad_norm': 0.59765625, 'learning_rate': 0.00012926238286098984, 'epoch': 0.34}\n",
      "{'loss': 1.347, 'grad_norm': 0.48046875, 'learning_rate': 0.00012925102048352597, 'epoch': 0.34}\n",
      "{'loss': 1.208, 'grad_norm': 0.69140625, 'learning_rate': 0.00012923965810606212, 'epoch': 0.34}\n",
      "{'loss': 1.1907, 'grad_norm': 0.62109375, 'learning_rate': 0.00012922829572859824, 'epoch': 0.34}\n",
      "{'loss': 1.3173, 'grad_norm': 0.6484375, 'learning_rate': 0.0001292169333511344, 'epoch': 0.34}\n",
      "{'loss': 1.1959, 'grad_norm': 0.63671875, 'learning_rate': 0.00012920557097367055, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2573, 'grad_norm': 0.5078125, 'learning_rate': 0.00012919420859620667, 'epoch': 0.34}\n",
      "{'loss': 1.225, 'grad_norm': 0.6796875, 'learning_rate': 0.00012918284621874282, 'epoch': 0.34}\n",
      "{'loss': 1.2613, 'grad_norm': 0.46875, 'learning_rate': 0.00012917148384127895, 'epoch': 0.34}\n",
      "{'loss': 1.2447, 'grad_norm': 0.6640625, 'learning_rate': 0.0001291601214638151, 'epoch': 0.34}\n",
      "{'loss': 1.1103, 'grad_norm': 1.2734375, 'learning_rate': 0.00012914875908635125, 'epoch': 0.34}\n",
      "{'loss': 1.4779, 'grad_norm': 0.56640625, 'learning_rate': 0.00012913739670888737, 'epoch': 0.34}\n",
      "{'loss': 1.0615, 'grad_norm': 0.77734375, 'learning_rate': 0.00012912603433142352, 'epoch': 0.34}\n",
      "{'loss': 1.1128, 'grad_norm': 0.515625, 'learning_rate': 0.00012911467195395965, 'epoch': 0.34}\n",
      "{'loss': 1.3183, 'grad_norm': 0.69140625, 'learning_rate': 0.0001291033095764958, 'epoch': 0.34}\n",
      "{'loss': 1.1047, 'grad_norm': 0.6484375, 'learning_rate': 0.00012909194719903192, 'epoch': 0.34}\n",
      "{'loss': 1.3174, 'grad_norm': 0.431640625, 'learning_rate': 0.00012908058482156808, 'epoch': 0.34}\n",
      "{'loss': 1.1268, 'grad_norm': 0.81640625, 'learning_rate': 0.00012906922244410423, 'epoch': 0.34}\n",
      "{'loss': 1.1414, 'grad_norm': 0.70703125, 'learning_rate': 0.00012905786006664035, 'epoch': 0.34}\n",
      "{'loss': 1.1866, 'grad_norm': 0.6796875, 'learning_rate': 0.0001290464976891765, 'epoch': 0.34}\n",
      "{'loss': 1.0185, 'grad_norm': 0.8359375, 'learning_rate': 0.00012903513531171263, 'epoch': 0.34}\n",
      "{'loss': 1.3296, 'grad_norm': 0.51171875, 'learning_rate': 0.00012902377293424878, 'epoch': 0.34}\n",
      "{'loss': 1.2724, 'grad_norm': 0.8671875, 'learning_rate': 0.0001290124105567849, 'epoch': 0.34}\n",
      "{'loss': 1.2436, 'grad_norm': 0.44921875, 'learning_rate': 0.00012900104817932103, 'epoch': 0.34}\n",
      "{'loss': 1.2186, 'grad_norm': 0.5390625, 'learning_rate': 0.0001289896858018572, 'epoch': 0.34}\n",
      "{'loss': 1.0663, 'grad_norm': 0.71875, 'learning_rate': 0.00012897832342439333, 'epoch': 0.34}\n",
      "{'loss': 1.2563, 'grad_norm': 0.50390625, 'learning_rate': 0.00012896696104692948, 'epoch': 0.34}\n",
      "{'loss': 1.1762, 'grad_norm': 0.671875, 'learning_rate': 0.0001289555986694656, 'epoch': 0.34}\n",
      "{'loss': 1.143, 'grad_norm': 0.38671875, 'learning_rate': 0.00012894423629200173, 'epoch': 0.34}\n",
      "{'loss': 1.2552, 'grad_norm': 0.470703125, 'learning_rate': 0.00012893287391453788, 'epoch': 0.34}\n",
      "{'loss': 1.0358, 'grad_norm': 0.69921875, 'learning_rate': 0.000128921511537074, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4646, 'grad_norm': 0.7421875, 'learning_rate': 0.00012891014915961018, 'epoch': 0.34}\n",
      "{'loss': 1.2155, 'grad_norm': 0.6875, 'learning_rate': 0.0001288987867821463, 'epoch': 0.34}\n",
      "{'loss': 1.1868, 'grad_norm': 0.486328125, 'learning_rate': 0.00012888742440468243, 'epoch': 0.34}\n",
      "{'loss': 1.2522, 'grad_norm': 0.65234375, 'learning_rate': 0.00012887606202721858, 'epoch': 0.34}\n",
      "{'loss': 1.163, 'grad_norm': 0.86328125, 'learning_rate': 0.0001288646996497547, 'epoch': 0.34}\n",
      "{'loss': 1.2984, 'grad_norm': 0.55078125, 'learning_rate': 0.00012885333727229086, 'epoch': 0.34}\n",
      "{'loss': 1.1866, 'grad_norm': 0.85546875, 'learning_rate': 0.00012884197489482698, 'epoch': 0.34}\n",
      "{'loss': 1.0641, 'grad_norm': 0.48046875, 'learning_rate': 0.00012883061251736314, 'epoch': 0.34}\n",
      "{'loss': 1.1806, 'grad_norm': 0.5703125, 'learning_rate': 0.0001288192501398993, 'epoch': 0.34}\n",
      "{'loss': 1.0135, 'grad_norm': 0.5703125, 'learning_rate': 0.0001288078877624354, 'epoch': 0.34}\n",
      "{'loss': 1.3072, 'grad_norm': 0.765625, 'learning_rate': 0.00012879652538497156, 'epoch': 0.34}\n",
      "{'loss': 1.1426, 'grad_norm': 1.0234375, 'learning_rate': 0.0001287851630075077, 'epoch': 0.34}\n",
      "{'loss': 1.4363, 'grad_norm': 0.48828125, 'learning_rate': 0.00012877380063004384, 'epoch': 0.34}\n",
      "{'loss': 1.3859, 'grad_norm': 0.60546875, 'learning_rate': 0.00012876243825258, 'epoch': 0.34}\n",
      "{'loss': 1.0798, 'grad_norm': 0.7890625, 'learning_rate': 0.00012875107587511611, 'epoch': 0.34}\n",
      "{'loss': 1.5124, 'grad_norm': 0.486328125, 'learning_rate': 0.00012873971349765227, 'epoch': 0.34}\n",
      "{'loss': 1.1433, 'grad_norm': 0.6875, 'learning_rate': 0.0001287283511201884, 'epoch': 0.34}\n",
      "{'loss': 1.234, 'grad_norm': 0.427734375, 'learning_rate': 0.00012871698874272454, 'epoch': 0.34}\n",
      "{'loss': 1.1341, 'grad_norm': 0.484375, 'learning_rate': 0.00012870562636526067, 'epoch': 0.34}\n",
      "{'loss': 0.9611, 'grad_norm': 0.765625, 'learning_rate': 0.00012869426398779682, 'epoch': 0.34}\n",
      "{'loss': 1.359, 'grad_norm': 0.8828125, 'learning_rate': 0.00012868290161033297, 'epoch': 0.34}\n",
      "{'loss': 1.1802, 'grad_norm': 0.6875, 'learning_rate': 0.0001286715392328691, 'epoch': 0.34}\n",
      "{'loss': 1.3541, 'grad_norm': 0.41015625, 'learning_rate': 0.00012866017685540524, 'epoch': 0.34}\n",
      "{'loss': 1.3261, 'grad_norm': 0.98828125, 'learning_rate': 0.00012864881447794137, 'epoch': 0.34}\n",
      "{'loss': 1.0205, 'grad_norm': 1.0703125, 'learning_rate': 0.00012863745210047752, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2672, 'grad_norm': 0.44921875, 'learning_rate': 0.00012862608972301364, 'epoch': 0.34}\n",
      "{'loss': 1.1313, 'grad_norm': 0.6484375, 'learning_rate': 0.00012861472734554977, 'epoch': 0.34}\n",
      "{'loss': 1.1179, 'grad_norm': 0.482421875, 'learning_rate': 0.00012860336496808595, 'epoch': 0.34}\n",
      "{'loss': 1.3188, 'grad_norm': 0.5078125, 'learning_rate': 0.00012859200259062207, 'epoch': 0.34}\n",
      "{'loss': 1.1316, 'grad_norm': 0.96875, 'learning_rate': 0.00012858064021315822, 'epoch': 0.34}\n",
      "{'loss': 1.4657, 'grad_norm': 0.47265625, 'learning_rate': 0.00012856927783569435, 'epoch': 0.34}\n",
      "{'loss': 1.2632, 'grad_norm': 0.56640625, 'learning_rate': 0.00012855791545823047, 'epoch': 0.34}\n",
      "{'loss': 1.1539, 'grad_norm': 0.44140625, 'learning_rate': 0.00012854655308076662, 'epoch': 0.34}\n",
      "{'loss': 1.2593, 'grad_norm': 0.52734375, 'learning_rate': 0.00012853519070330275, 'epoch': 0.34}\n",
      "{'loss': 1.0412, 'grad_norm': 1.0078125, 'learning_rate': 0.00012852382832583893, 'epoch': 0.34}\n",
      "{'loss': 1.3155, 'grad_norm': 0.5078125, 'learning_rate': 0.00012851246594837505, 'epoch': 0.34}\n",
      "{'loss': 1.173, 'grad_norm': 1.0703125, 'learning_rate': 0.00012850110357091117, 'epoch': 0.34}\n",
      "{'loss': 1.2422, 'grad_norm': 0.4453125, 'learning_rate': 0.00012848974119344733, 'epoch': 0.34}\n",
      "{'loss': 1.2102, 'grad_norm': 0.482421875, 'learning_rate': 0.00012847837881598345, 'epoch': 0.34}\n",
      "{'loss': 1.2208, 'grad_norm': 1.0234375, 'learning_rate': 0.0001284670164385196, 'epoch': 0.34}\n",
      "{'loss': 1.3123, 'grad_norm': 0.482421875, 'learning_rate': 0.00012845565406105575, 'epoch': 0.34}\n",
      "{'loss': 1.1952, 'grad_norm': 0.76171875, 'learning_rate': 0.00012844429168359188, 'epoch': 0.34}\n",
      "{'loss': 1.2043, 'grad_norm': 0.37890625, 'learning_rate': 0.00012843292930612803, 'epoch': 0.34}\n",
      "{'loss': 1.4308, 'grad_norm': 0.64453125, 'learning_rate': 0.00012842156692866415, 'epoch': 0.34}\n",
      "{'loss': 1.0736, 'grad_norm': 0.482421875, 'learning_rate': 0.0001284102045512003, 'epoch': 0.34}\n",
      "{'loss': 1.3395, 'grad_norm': 0.5390625, 'learning_rate': 0.00012839884217373643, 'epoch': 0.34}\n",
      "{'loss': 1.1777, 'grad_norm': 0.59765625, 'learning_rate': 0.00012838747979627258, 'epoch': 0.34}\n",
      "{'loss': 1.2297, 'grad_norm': 0.455078125, 'learning_rate': 0.00012837611741880873, 'epoch': 0.34}\n",
      "{'loss': 1.1275, 'grad_norm': 0.59765625, 'learning_rate': 0.00012836475504134486, 'epoch': 0.34}\n",
      "{'loss': 1.0307, 'grad_norm': 0.6796875, 'learning_rate': 0.000128353392663881, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3719, 'grad_norm': 0.5546875, 'learning_rate': 0.00012834203028641713, 'epoch': 0.34}\n",
      "{'loss': 1.1458, 'grad_norm': 0.9453125, 'learning_rate': 0.00012833066790895328, 'epoch': 0.34}\n",
      "{'loss': 1.1919, 'grad_norm': 0.431640625, 'learning_rate': 0.0001283193055314894, 'epoch': 0.34}\n",
      "{'loss': 1.3307, 'grad_norm': 0.4921875, 'learning_rate': 0.00012830794315402556, 'epoch': 0.34}\n",
      "{'loss': 1.0752, 'grad_norm': 0.57421875, 'learning_rate': 0.0001282965807765617, 'epoch': 0.34}\n",
      "{'loss': 1.4101, 'grad_norm': 0.546875, 'learning_rate': 0.00012828521839909783, 'epoch': 0.34}\n",
      "{'loss': 1.251, 'grad_norm': 0.921875, 'learning_rate': 0.00012827385602163399, 'epoch': 0.34}\n",
      "{'loss': 1.1607, 'grad_norm': 0.58984375, 'learning_rate': 0.0001282624936441701, 'epoch': 0.34}\n",
      "{'loss': 1.2488, 'grad_norm': 0.546875, 'learning_rate': 0.00012825113126670626, 'epoch': 0.34}\n",
      "{'loss': 1.0967, 'grad_norm': 1.2109375, 'learning_rate': 0.00012823976888924239, 'epoch': 0.34}\n",
      "{'loss': 1.3308, 'grad_norm': 0.4609375, 'learning_rate': 0.0001282284065117785, 'epoch': 0.34}\n",
      "{'loss': 1.2101, 'grad_norm': 0.61328125, 'learning_rate': 0.0001282170441343147, 'epoch': 0.34}\n",
      "{'loss': 1.3341, 'grad_norm': 0.458984375, 'learning_rate': 0.0001282056817568508, 'epoch': 0.34}\n",
      "{'loss': 1.3162, 'grad_norm': 0.60546875, 'learning_rate': 0.00012819431937938696, 'epoch': 0.34}\n",
      "{'loss': 1.1146, 'grad_norm': 0.71875, 'learning_rate': 0.0001281829570019231, 'epoch': 0.34}\n",
      "{'loss': 1.277, 'grad_norm': 0.58203125, 'learning_rate': 0.0001281715946244592, 'epoch': 0.34}\n",
      "{'loss': 1.1697, 'grad_norm': 0.91015625, 'learning_rate': 0.00012816023224699536, 'epoch': 0.34}\n",
      "{'loss': 1.29, 'grad_norm': 0.48828125, 'learning_rate': 0.0001281488698695315, 'epoch': 0.34}\n",
      "{'loss': 1.2378, 'grad_norm': 0.7890625, 'learning_rate': 0.00012813750749206767, 'epoch': 0.34}\n",
      "{'loss': 1.2234, 'grad_norm': 0.68359375, 'learning_rate': 0.0001281261451146038, 'epoch': 0.34}\n",
      "{'loss': 1.3161, 'grad_norm': 0.515625, 'learning_rate': 0.00012811478273713992, 'epoch': 0.34}\n",
      "{'loss': 1.2093, 'grad_norm': 0.51171875, 'learning_rate': 0.00012810342035967607, 'epoch': 0.34}\n",
      "{'loss': 1.1848, 'grad_norm': 0.44140625, 'learning_rate': 0.0001280920579822122, 'epoch': 0.34}\n",
      "{'loss': 1.3168, 'grad_norm': 0.65625, 'learning_rate': 0.00012808069560474834, 'epoch': 0.34}\n",
      "{'loss': 1.0555, 'grad_norm': 1.390625, 'learning_rate': 0.0001280693332272845, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2587, 'grad_norm': 0.498046875, 'learning_rate': 0.00012805797084982062, 'epoch': 0.34}\n",
      "{'loss': 1.1367, 'grad_norm': 1.046875, 'learning_rate': 0.00012804660847235677, 'epoch': 0.34}\n",
      "{'loss': 1.2425, 'grad_norm': 0.38671875, 'learning_rate': 0.0001280352460948929, 'epoch': 0.34}\n",
      "{'loss': 1.2383, 'grad_norm': 0.56640625, 'learning_rate': 0.00012802388371742905, 'epoch': 0.34}\n",
      "{'loss': 1.1224, 'grad_norm': 0.99609375, 'learning_rate': 0.00012801252133996517, 'epoch': 0.34}\n",
      "{'loss': 1.4284, 'grad_norm': 0.61328125, 'learning_rate': 0.00012800115896250132, 'epoch': 0.34}\n",
      "{'loss': 1.0869, 'grad_norm': 0.61328125, 'learning_rate': 0.00012798979658503747, 'epoch': 0.34}\n",
      "{'loss': 1.2513, 'grad_norm': 0.55859375, 'learning_rate': 0.0001279784342075736, 'epoch': 0.34}\n",
      "{'loss': 1.2441, 'grad_norm': 0.66796875, 'learning_rate': 0.00012796707183010975, 'epoch': 0.34}\n",
      "{'loss': 1.2332, 'grad_norm': 0.61328125, 'learning_rate': 0.00012795570945264587, 'epoch': 0.34}\n",
      "{'loss': 1.4164, 'grad_norm': 0.466796875, 'learning_rate': 0.00012794434707518202, 'epoch': 0.34}\n",
      "{'loss': 1.1452, 'grad_norm': 1.1484375, 'learning_rate': 0.00012793298469771815, 'epoch': 0.34}\n",
      "{'loss': 1.2005, 'grad_norm': 0.396484375, 'learning_rate': 0.0001279216223202543, 'epoch': 0.34}\n",
      "{'loss': 1.2972, 'grad_norm': 0.61328125, 'learning_rate': 0.00012791025994279045, 'epoch': 0.34}\n",
      "{'loss': 1.1684, 'grad_norm': 0.828125, 'learning_rate': 0.00012789889756532658, 'epoch': 0.34}\n",
      "{'loss': 1.3628, 'grad_norm': 0.39453125, 'learning_rate': 0.00012788753518786273, 'epoch': 0.34}\n",
      "{'loss': 1.1589, 'grad_norm': 0.58984375, 'learning_rate': 0.00012787617281039885, 'epoch': 0.34}\n",
      "{'loss': 1.226, 'grad_norm': 0.62109375, 'learning_rate': 0.000127864810432935, 'epoch': 0.34}\n",
      "{'loss': 1.2863, 'grad_norm': 0.546875, 'learning_rate': 0.00012785344805547113, 'epoch': 0.34}\n",
      "{'loss': 1.0527, 'grad_norm': 0.875, 'learning_rate': 0.00012784208567800725, 'epoch': 0.34}\n",
      "{'loss': 1.4065, 'grad_norm': 0.54296875, 'learning_rate': 0.00012783072330054343, 'epoch': 0.34}\n",
      "{'loss': 1.059, 'grad_norm': 0.62890625, 'learning_rate': 0.00012781936092307955, 'epoch': 0.34}\n",
      "{'loss': 1.3152, 'grad_norm': 0.435546875, 'learning_rate': 0.0001278079985456157, 'epoch': 0.34}\n",
      "{'loss': 1.2627, 'grad_norm': 0.6328125, 'learning_rate': 0.00012779663616815183, 'epoch': 0.34}\n",
      "{'loss': 1.0011, 'grad_norm': 0.7421875, 'learning_rate': 0.00012778527379068795, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3173, 'grad_norm': 0.51171875, 'learning_rate': 0.0001277739114132241, 'epoch': 0.34}\n",
      "{'loss': 1.0676, 'grad_norm': 0.63671875, 'learning_rate': 0.00012776254903576026, 'epoch': 0.34}\n",
      "{'loss': 1.2931, 'grad_norm': 0.443359375, 'learning_rate': 0.0001277511866582964, 'epoch': 0.34}\n",
      "{'loss': 1.1932, 'grad_norm': 0.61328125, 'learning_rate': 0.00012773982428083253, 'epoch': 0.34}\n",
      "{'loss': 1.0929, 'grad_norm': 0.71875, 'learning_rate': 0.00012772846190336866, 'epoch': 0.34}\n",
      "{'loss': 1.3465, 'grad_norm': 0.46484375, 'learning_rate': 0.0001277170995259048, 'epoch': 0.34}\n",
      "{'loss': 1.2116, 'grad_norm': 0.73046875, 'learning_rate': 0.00012770573714844093, 'epoch': 0.34}\n",
      "{'loss': 1.0936, 'grad_norm': 0.416015625, 'learning_rate': 0.00012769437477097708, 'epoch': 0.34}\n",
      "{'loss': 1.1799, 'grad_norm': 0.5859375, 'learning_rate': 0.00012768301239351324, 'epoch': 0.34}\n",
      "{'loss': 1.0855, 'grad_norm': 0.470703125, 'learning_rate': 0.00012767165001604936, 'epoch': 0.34}\n",
      "{'loss': 1.3534, 'grad_norm': 0.423828125, 'learning_rate': 0.0001276602876385855, 'epoch': 0.34}\n",
      "{'loss': 1.2631, 'grad_norm': 0.9375, 'learning_rate': 0.00012764892526112164, 'epoch': 0.34}\n",
      "{'loss': 1.3498, 'grad_norm': 0.474609375, 'learning_rate': 0.0001276375628836578, 'epoch': 0.34}\n",
      "{'loss': 1.319, 'grad_norm': 0.466796875, 'learning_rate': 0.0001276262005061939, 'epoch': 0.34}\n",
      "{'loss': 1.1593, 'grad_norm': 1.25, 'learning_rate': 0.00012761483812873006, 'epoch': 0.34}\n",
      "{'loss': 1.3327, 'grad_norm': 0.51953125, 'learning_rate': 0.00012760347575126621, 'epoch': 0.34}\n",
      "{'loss': 1.251, 'grad_norm': 0.65234375, 'learning_rate': 0.00012759211337380234, 'epoch': 0.34}\n",
      "{'loss': 1.2652, 'grad_norm': 0.50390625, 'learning_rate': 0.0001275807509963385, 'epoch': 0.34}\n",
      "{'loss': 1.3692, 'grad_norm': 0.50390625, 'learning_rate': 0.00012756938861887461, 'epoch': 0.34}\n",
      "{'loss': 1.1235, 'grad_norm': 0.43359375, 'learning_rate': 0.00012755802624141076, 'epoch': 0.34}\n",
      "{'loss': 1.3397, 'grad_norm': 0.55859375, 'learning_rate': 0.0001275466638639469, 'epoch': 0.34}\n",
      "{'loss': 1.1926, 'grad_norm': 0.5859375, 'learning_rate': 0.00012753530148648304, 'epoch': 0.34}\n",
      "{'loss': 1.243, 'grad_norm': 0.474609375, 'learning_rate': 0.0001275239391090192, 'epoch': 0.34}\n",
      "{'loss': 1.1762, 'grad_norm': 0.78515625, 'learning_rate': 0.00012751257673155532, 'epoch': 0.34}\n",
      "{'loss': 0.9491, 'grad_norm': 0.4609375, 'learning_rate': 0.00012750121435409147, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.365, 'grad_norm': 0.45703125, 'learning_rate': 0.0001274898519766276, 'epoch': 0.34}\n",
      "{'loss': 1.2002, 'grad_norm': 0.60546875, 'learning_rate': 0.00012747848959916374, 'epoch': 0.34}\n",
      "{'loss': 1.1314, 'grad_norm': 0.4140625, 'learning_rate': 0.00012746712722169987, 'epoch': 0.34}\n",
      "{'loss': 1.2042, 'grad_norm': 0.54296875, 'learning_rate': 0.000127455764844236, 'epoch': 0.34}\n",
      "{'loss': 1.1072, 'grad_norm': 0.6796875, 'learning_rate': 0.00012744440246677217, 'epoch': 0.34}\n",
      "{'loss': 1.4103, 'grad_norm': 0.57421875, 'learning_rate': 0.0001274330400893083, 'epoch': 0.34}\n",
      "{'loss': 1.1825, 'grad_norm': 0.478515625, 'learning_rate': 0.00012742167771184445, 'epoch': 0.34}\n",
      "{'loss': 1.1735, 'grad_norm': 0.462890625, 'learning_rate': 0.00012741031533438057, 'epoch': 0.34}\n",
      "{'loss': 1.249, 'grad_norm': 0.5390625, 'learning_rate': 0.0001273989529569167, 'epoch': 0.34}\n",
      "{'loss': 0.9412, 'grad_norm': 0.3203125, 'learning_rate': 0.00012738759057945285, 'epoch': 0.34}\n",
      "{'loss': 1.369, 'grad_norm': 0.427734375, 'learning_rate': 0.000127376228201989, 'epoch': 0.34}\n",
      "{'loss': 1.1011, 'grad_norm': 0.6015625, 'learning_rate': 0.00012736486582452515, 'epoch': 0.34}\n",
      "{'loss': 1.2317, 'grad_norm': 0.462890625, 'learning_rate': 0.00012735350344706127, 'epoch': 0.34}\n",
      "{'loss': 1.1721, 'grad_norm': 0.47265625, 'learning_rate': 0.0001273421410695974, 'epoch': 0.34}\n",
      "{'loss': 1.1198, 'grad_norm': 1.1015625, 'learning_rate': 0.00012733077869213355, 'epoch': 0.34}\n",
      "{'loss': 1.3064, 'grad_norm': 0.56640625, 'learning_rate': 0.00012731941631466967, 'epoch': 0.34}\n",
      "{'loss': 1.2029, 'grad_norm': 0.7734375, 'learning_rate': 0.00012730805393720582, 'epoch': 0.34}\n",
      "{'loss': 1.1954, 'grad_norm': 0.40234375, 'learning_rate': 0.00012729669155974198, 'epoch': 0.34}\n",
      "{'loss': 1.2317, 'grad_norm': 0.52734375, 'learning_rate': 0.0001272853291822781, 'epoch': 0.34}\n",
      "{'loss': 1.0725, 'grad_norm': 1.1171875, 'learning_rate': 0.00012727396680481425, 'epoch': 0.34}\n",
      "{'loss': 1.428, 'grad_norm': 0.52734375, 'learning_rate': 0.00012726260442735038, 'epoch': 0.34}\n",
      "{'loss': 1.2335, 'grad_norm': 0.625, 'learning_rate': 0.00012725124204988653, 'epoch': 0.34}\n",
      "{'loss': 1.2751, 'grad_norm': 0.52734375, 'learning_rate': 0.00012723987967242265, 'epoch': 0.34}\n",
      "{'loss': 1.198, 'grad_norm': 0.62109375, 'learning_rate': 0.0001272285172949588, 'epoch': 0.34}\n",
      "{'loss': 1.0751, 'grad_norm': 0.9453125, 'learning_rate': 0.00012721715491749495, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4548, 'grad_norm': 0.4765625, 'learning_rate': 0.00012720579254003108, 'epoch': 0.34}\n",
      "{'loss': 1.3444, 'grad_norm': 0.82421875, 'learning_rate': 0.00012719443016256723, 'epoch': 0.34}\n",
      "{'loss': 1.1655, 'grad_norm': 0.53515625, 'learning_rate': 0.00012718306778510335, 'epoch': 0.34}\n",
      "{'loss': 1.2133, 'grad_norm': 0.58984375, 'learning_rate': 0.0001271717054076395, 'epoch': 0.34}\n",
      "{'loss': 0.9408, 'grad_norm': 1.5703125, 'learning_rate': 0.00012716034303017563, 'epoch': 0.34}\n",
      "{'loss': 1.3858, 'grad_norm': 0.5, 'learning_rate': 0.00012714898065271178, 'epoch': 0.34}\n",
      "{'loss': 1.2343, 'grad_norm': 1.234375, 'learning_rate': 0.00012713761827524793, 'epoch': 0.35}\n",
      "{'loss': 1.2434, 'grad_norm': 0.396484375, 'learning_rate': 0.00012712625589778406, 'epoch': 0.35}\n",
      "{'loss': 1.2067, 'grad_norm': 0.5546875, 'learning_rate': 0.0001271148935203202, 'epoch': 0.35}\n",
      "{'loss': 1.0712, 'grad_norm': 0.64453125, 'learning_rate': 0.00012710353114285633, 'epoch': 0.35}\n",
      "{'loss': 1.2975, 'grad_norm': 0.52734375, 'learning_rate': 0.00012709216876539248, 'epoch': 0.35}\n",
      "{'loss': 1.1726, 'grad_norm': 0.8984375, 'learning_rate': 0.0001270808063879286, 'epoch': 0.35}\n",
      "{'loss': 1.0921, 'grad_norm': 0.48828125, 'learning_rate': 0.00012706944401046476, 'epoch': 0.35}\n",
      "{'loss': 1.2421, 'grad_norm': 0.62890625, 'learning_rate': 0.0001270580816330009, 'epoch': 0.35}\n",
      "{'loss': 1.0727, 'grad_norm': 0.7578125, 'learning_rate': 0.00012704671925553704, 'epoch': 0.35}\n",
      "{'loss': 1.3371, 'grad_norm': 0.51171875, 'learning_rate': 0.0001270353568780732, 'epoch': 0.35}\n",
      "{'loss': 1.0591, 'grad_norm': 0.81640625, 'learning_rate': 0.0001270239945006093, 'epoch': 0.35}\n",
      "{'loss': 1.2105, 'grad_norm': 0.408203125, 'learning_rate': 0.00012701263212314544, 'epoch': 0.35}\n",
      "{'loss': 1.2583, 'grad_norm': 0.58984375, 'learning_rate': 0.0001270012697456816, 'epoch': 0.35}\n",
      "{'loss': 1.1462, 'grad_norm': 0.7265625, 'learning_rate': 0.00012698990736821774, 'epoch': 0.35}\n",
      "{'loss': 1.378, 'grad_norm': 0.89453125, 'learning_rate': 0.0001269785449907539, 'epoch': 0.35}\n",
      "{'loss': 1.0608, 'grad_norm': 0.80078125, 'learning_rate': 0.00012696718261329001, 'epoch': 0.35}\n",
      "{'loss': 1.1355, 'grad_norm': 0.515625, 'learning_rate': 0.00012695582023582614, 'epoch': 0.35}\n",
      "{'loss': 1.3067, 'grad_norm': 0.69921875, 'learning_rate': 0.0001269444578583623, 'epoch': 0.35}\n",
      "{'loss': 1.067, 'grad_norm': 0.55859375, 'learning_rate': 0.00012693309548089841, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4222, 'grad_norm': 0.48828125, 'learning_rate': 0.00012692173310343457, 'epoch': 0.35}\n",
      "{'loss': 1.1439, 'grad_norm': 1.1015625, 'learning_rate': 0.00012691037072597072, 'epoch': 0.35}\n",
      "{'loss': 1.2004, 'grad_norm': 0.55078125, 'learning_rate': 0.00012689900834850684, 'epoch': 0.35}\n",
      "{'loss': 1.2262, 'grad_norm': 0.482421875, 'learning_rate': 0.000126887645971043, 'epoch': 0.35}\n",
      "{'loss': 1.0554, 'grad_norm': 0.64453125, 'learning_rate': 0.00012687628359357912, 'epoch': 0.35}\n",
      "{'loss': 1.3177, 'grad_norm': 0.51171875, 'learning_rate': 0.00012686492121611527, 'epoch': 0.35}\n",
      "{'loss': 1.1286, 'grad_norm': 0.828125, 'learning_rate': 0.0001268535588386514, 'epoch': 0.35}\n",
      "{'loss': 1.3205, 'grad_norm': 0.470703125, 'learning_rate': 0.00012684219646118754, 'epoch': 0.35}\n",
      "{'loss': 1.2941, 'grad_norm': 0.515625, 'learning_rate': 0.0001268308340837237, 'epoch': 0.35}\n",
      "{'loss': 1.1345, 'grad_norm': 0.703125, 'learning_rate': 0.00012681947170625982, 'epoch': 0.35}\n",
      "{'loss': 1.4006, 'grad_norm': 0.5546875, 'learning_rate': 0.00012680810932879597, 'epoch': 0.35}\n",
      "{'loss': 1.2479, 'grad_norm': 0.921875, 'learning_rate': 0.0001267967469513321, 'epoch': 0.35}\n",
      "{'loss': 1.1633, 'grad_norm': 0.4453125, 'learning_rate': 0.00012678538457386825, 'epoch': 0.35}\n",
      "{'loss': 1.2686, 'grad_norm': 0.7578125, 'learning_rate': 0.00012677402219640437, 'epoch': 0.35}\n",
      "{'loss': 1.0849, 'grad_norm': 0.84765625, 'learning_rate': 0.00012676265981894052, 'epoch': 0.35}\n",
      "{'loss': 1.3322, 'grad_norm': 0.66796875, 'learning_rate': 0.00012675129744147667, 'epoch': 0.35}\n",
      "{'loss': 1.2866, 'grad_norm': 0.55078125, 'learning_rate': 0.0001267399350640128, 'epoch': 0.35}\n",
      "{'loss': 1.2478, 'grad_norm': 0.474609375, 'learning_rate': 0.00012672857268654895, 'epoch': 0.35}\n",
      "{'loss': 1.2334, 'grad_norm': 0.7734375, 'learning_rate': 0.00012671721030908507, 'epoch': 0.35}\n",
      "{'loss': 1.052, 'grad_norm': 0.48046875, 'learning_rate': 0.00012670584793162123, 'epoch': 0.35}\n",
      "{'loss': 1.4791, 'grad_norm': 0.50390625, 'learning_rate': 0.00012669448555415735, 'epoch': 0.35}\n",
      "{'loss': 1.2164, 'grad_norm': 0.5859375, 'learning_rate': 0.0001266831231766935, 'epoch': 0.35}\n",
      "{'loss': 1.2327, 'grad_norm': 0.490234375, 'learning_rate': 0.00012667176079922965, 'epoch': 0.35}\n",
      "{'loss': 1.2495, 'grad_norm': 0.55859375, 'learning_rate': 0.00012666039842176578, 'epoch': 0.35}\n",
      "{'loss': 1.1177, 'grad_norm': 0.84375, 'learning_rate': 0.00012664903604430193, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2837, 'grad_norm': 0.453125, 'learning_rate': 0.00012663767366683805, 'epoch': 0.35}\n",
      "{'loss': 1.202, 'grad_norm': 0.8359375, 'learning_rate': 0.00012662631128937418, 'epoch': 0.35}\n",
      "{'loss': 1.203, 'grad_norm': 0.5703125, 'learning_rate': 0.00012661494891191033, 'epoch': 0.35}\n",
      "{'loss': 1.2216, 'grad_norm': 0.6171875, 'learning_rate': 0.00012660358653444648, 'epoch': 0.35}\n",
      "{'loss': 1.0261, 'grad_norm': 0.578125, 'learning_rate': 0.00012659222415698263, 'epoch': 0.35}\n",
      "{'loss': 1.2912, 'grad_norm': 0.59765625, 'learning_rate': 0.00012658086177951876, 'epoch': 0.35}\n",
      "{'loss': 1.1843, 'grad_norm': 0.67578125, 'learning_rate': 0.00012656949940205488, 'epoch': 0.35}\n",
      "{'loss': 1.3222, 'grad_norm': 0.431640625, 'learning_rate': 0.00012655813702459103, 'epoch': 0.35}\n",
      "{'loss': 1.3234, 'grad_norm': 0.6875, 'learning_rate': 0.00012654677464712716, 'epoch': 0.35}\n",
      "{'loss': 1.0396, 'grad_norm': 0.330078125, 'learning_rate': 0.0001265354122696633, 'epoch': 0.35}\n",
      "{'loss': 1.2729, 'grad_norm': 0.640625, 'learning_rate': 0.00012652404989219946, 'epoch': 0.35}\n",
      "{'loss': 1.2832, 'grad_norm': 0.69140625, 'learning_rate': 0.00012651268751473558, 'epoch': 0.35}\n",
      "{'loss': 1.2531, 'grad_norm': 0.62109375, 'learning_rate': 0.00012650132513727173, 'epoch': 0.35}\n",
      "{'loss': 1.4221, 'grad_norm': 0.60546875, 'learning_rate': 0.00012648996275980786, 'epoch': 0.35}\n",
      "{'loss': 1.1159, 'grad_norm': 0.58203125, 'learning_rate': 0.000126478600382344, 'epoch': 0.35}\n",
      "{'loss': 1.3357, 'grad_norm': 0.4921875, 'learning_rate': 0.00012646723800488013, 'epoch': 0.35}\n",
      "{'loss': 1.1478, 'grad_norm': 0.765625, 'learning_rate': 0.00012645587562741629, 'epoch': 0.35}\n",
      "{'loss': 1.3203, 'grad_norm': 0.55859375, 'learning_rate': 0.00012644451324995244, 'epoch': 0.35}\n",
      "{'loss': 1.1968, 'grad_norm': 0.484375, 'learning_rate': 0.00012643315087248856, 'epoch': 0.35}\n",
      "{'loss': 1.1271, 'grad_norm': 0.73046875, 'learning_rate': 0.0001264217884950247, 'epoch': 0.35}\n",
      "{'loss': 1.2551, 'grad_norm': 0.5, 'learning_rate': 0.00012641042611756084, 'epoch': 0.35}\n",
      "{'loss': 1.2144, 'grad_norm': 0.6640625, 'learning_rate': 0.000126399063740097, 'epoch': 0.35}\n",
      "{'loss': 1.0397, 'grad_norm': 0.51953125, 'learning_rate': 0.0001263877013626331, 'epoch': 0.35}\n",
      "{'loss': 1.239, 'grad_norm': 0.84765625, 'learning_rate': 0.00012637633898516926, 'epoch': 0.35}\n",
      "{'loss': 1.1185, 'grad_norm': 0.7265625, 'learning_rate': 0.00012636497660770542, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4407, 'grad_norm': 0.5, 'learning_rate': 0.00012635361423024154, 'epoch': 0.35}\n",
      "{'loss': 1.1508, 'grad_norm': 0.9140625, 'learning_rate': 0.0001263422518527777, 'epoch': 0.35}\n",
      "{'loss': 1.2246, 'grad_norm': 0.578125, 'learning_rate': 0.00012633088947531382, 'epoch': 0.35}\n",
      "{'loss': 1.2462, 'grad_norm': 0.7109375, 'learning_rate': 0.00012631952709784997, 'epoch': 0.35}\n",
      "{'loss': 1.1302, 'grad_norm': 0.57421875, 'learning_rate': 0.0001263081647203861, 'epoch': 0.35}\n",
      "{'loss': 1.3122, 'grad_norm': 0.52734375, 'learning_rate': 0.00012629680234292224, 'epoch': 0.35}\n",
      "{'loss': 1.1894, 'grad_norm': 0.80078125, 'learning_rate': 0.0001262854399654584, 'epoch': 0.35}\n",
      "{'loss': 1.3451, 'grad_norm': 0.421875, 'learning_rate': 0.00012627407758799452, 'epoch': 0.35}\n",
      "{'loss': 1.1006, 'grad_norm': 0.59375, 'learning_rate': 0.00012626271521053067, 'epoch': 0.35}\n",
      "{'loss': 1.1581, 'grad_norm': 0.91796875, 'learning_rate': 0.0001262513528330668, 'epoch': 0.35}\n",
      "{'loss': 1.2978, 'grad_norm': 0.53515625, 'learning_rate': 0.00012623999045560292, 'epoch': 0.35}\n",
      "{'loss': 1.1915, 'grad_norm': 0.5703125, 'learning_rate': 0.00012622862807813907, 'epoch': 0.35}\n",
      "{'loss': 1.0973, 'grad_norm': 0.44140625, 'learning_rate': 0.00012621726570067522, 'epoch': 0.35}\n",
      "{'loss': 1.2227, 'grad_norm': 0.60546875, 'learning_rate': 0.00012620590332321137, 'epoch': 0.35}\n",
      "{'loss': 1.0025, 'grad_norm': 0.3046875, 'learning_rate': 0.0001261945409457475, 'epoch': 0.35}\n",
      "{'loss': 1.2903, 'grad_norm': 0.45703125, 'learning_rate': 0.00012618317856828362, 'epoch': 0.35}\n",
      "{'loss': 1.1651, 'grad_norm': 0.5859375, 'learning_rate': 0.00012617181619081977, 'epoch': 0.35}\n",
      "{'loss': 1.1369, 'grad_norm': 0.68359375, 'learning_rate': 0.0001261604538133559, 'epoch': 0.35}\n",
      "{'loss': 1.2432, 'grad_norm': 0.6796875, 'learning_rate': 0.00012614909143589205, 'epoch': 0.35}\n",
      "{'loss': 1.2043, 'grad_norm': 0.66015625, 'learning_rate': 0.0001261377290584282, 'epoch': 0.35}\n",
      "{'loss': 1.21, 'grad_norm': 0.65625, 'learning_rate': 0.00012612636668096432, 'epoch': 0.35}\n",
      "{'loss': 1.1882, 'grad_norm': 0.5859375, 'learning_rate': 0.00012611500430350048, 'epoch': 0.35}\n",
      "{'loss': 1.2499, 'grad_norm': 0.466796875, 'learning_rate': 0.0001261036419260366, 'epoch': 0.35}\n",
      "{'loss': 1.276, 'grad_norm': 0.6015625, 'learning_rate': 0.00012609227954857275, 'epoch': 0.35}\n",
      "{'loss': 1.2189, 'grad_norm': 1.140625, 'learning_rate': 0.00012608091717110888, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4335, 'grad_norm': 0.51171875, 'learning_rate': 0.00012606955479364503, 'epoch': 0.35}\n",
      "{'loss': 1.1768, 'grad_norm': 0.5859375, 'learning_rate': 0.00012605819241618118, 'epoch': 0.35}\n",
      "{'loss': 1.2197, 'grad_norm': 0.48046875, 'learning_rate': 0.0001260468300387173, 'epoch': 0.35}\n",
      "{'loss': 1.2815, 'grad_norm': 0.55078125, 'learning_rate': 0.00012603546766125345, 'epoch': 0.35}\n",
      "{'loss': 1.0134, 'grad_norm': 1.0, 'learning_rate': 0.00012602410528378958, 'epoch': 0.35}\n",
      "{'loss': 1.3133, 'grad_norm': 0.49609375, 'learning_rate': 0.00012601274290632573, 'epoch': 0.35}\n",
      "{'loss': 1.2197, 'grad_norm': 0.8046875, 'learning_rate': 0.00012600138052886185, 'epoch': 0.35}\n",
      "{'loss': 1.2035, 'grad_norm': 0.4921875, 'learning_rate': 0.000125990018151398, 'epoch': 0.35}\n",
      "{'loss': 1.4076, 'grad_norm': 0.52734375, 'learning_rate': 0.00012597865577393416, 'epoch': 0.35}\n",
      "{'loss': 1.0915, 'grad_norm': 1.3125, 'learning_rate': 0.00012596729339647028, 'epoch': 0.35}\n",
      "{'loss': 1.3971, 'grad_norm': 0.451171875, 'learning_rate': 0.00012595593101900643, 'epoch': 0.35}\n",
      "{'loss': 1.1622, 'grad_norm': 0.6484375, 'learning_rate': 0.00012594456864154256, 'epoch': 0.35}\n",
      "{'loss': 1.1873, 'grad_norm': 0.58984375, 'learning_rate': 0.0001259332062640787, 'epoch': 0.35}\n",
      "{'loss': 1.2068, 'grad_norm': 0.66796875, 'learning_rate': 0.00012592184388661483, 'epoch': 0.35}\n",
      "{'loss': 1.1228, 'grad_norm': 0.71875, 'learning_rate': 0.00012591048150915098, 'epoch': 0.35}\n",
      "{'loss': 1.3607, 'grad_norm': 0.455078125, 'learning_rate': 0.00012589911913168714, 'epoch': 0.35}\n",
      "{'loss': 1.1364, 'grad_norm': 0.55078125, 'learning_rate': 0.00012588775675422326, 'epoch': 0.35}\n",
      "{'loss': 1.2387, 'grad_norm': 0.43359375, 'learning_rate': 0.0001258763943767594, 'epoch': 0.35}\n",
      "{'loss': 1.1946, 'grad_norm': 0.609375, 'learning_rate': 0.00012586503199929554, 'epoch': 0.35}\n",
      "{'loss': 1.116, 'grad_norm': 0.81640625, 'learning_rate': 0.00012585366962183166, 'epoch': 0.35}\n",
      "{'loss': 1.3442, 'grad_norm': 0.5, 'learning_rate': 0.0001258423072443678, 'epoch': 0.35}\n",
      "{'loss': 1.1441, 'grad_norm': 0.62890625, 'learning_rate': 0.00012583094486690396, 'epoch': 0.35}\n",
      "{'loss': 1.1164, 'grad_norm': 0.419921875, 'learning_rate': 0.00012581958248944011, 'epoch': 0.35}\n",
      "{'loss': 1.3814, 'grad_norm': 0.75, 'learning_rate': 0.00012580822011197624, 'epoch': 0.35}\n",
      "{'loss': 0.9786, 'grad_norm': 0.30859375, 'learning_rate': 0.00012579685773451236, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.271, 'grad_norm': 0.60546875, 'learning_rate': 0.00012578549535704851, 'epoch': 0.35}\n",
      "{'loss': 1.1645, 'grad_norm': 0.71875, 'learning_rate': 0.00012577413297958464, 'epoch': 0.35}\n",
      "{'loss': 1.3127, 'grad_norm': 0.546875, 'learning_rate': 0.0001257627706021208, 'epoch': 0.35}\n",
      "{'loss': 1.2364, 'grad_norm': 0.75, 'learning_rate': 0.00012575140822465694, 'epoch': 0.35}\n",
      "{'loss': 1.1469, 'grad_norm': 0.72265625, 'learning_rate': 0.00012574004584719307, 'epoch': 0.35}\n",
      "{'loss': 1.2854, 'grad_norm': 0.5859375, 'learning_rate': 0.00012572868346972922, 'epoch': 0.35}\n",
      "{'loss': 1.1541, 'grad_norm': 0.796875, 'learning_rate': 0.00012571732109226534, 'epoch': 0.35}\n",
      "{'loss': 1.1824, 'grad_norm': 0.51171875, 'learning_rate': 0.0001257059587148015, 'epoch': 0.35}\n",
      "{'loss': 1.3056, 'grad_norm': 0.54296875, 'learning_rate': 0.00012569459633733762, 'epoch': 0.35}\n",
      "{'loss': 1.1382, 'grad_norm': 1.0546875, 'learning_rate': 0.00012568323395987377, 'epoch': 0.35}\n",
      "{'loss': 1.2783, 'grad_norm': 0.53515625, 'learning_rate': 0.00012567187158240992, 'epoch': 0.35}\n",
      "{'loss': 1.1585, 'grad_norm': 0.8984375, 'learning_rate': 0.00012566050920494604, 'epoch': 0.35}\n",
      "{'loss': 1.2463, 'grad_norm': 0.4296875, 'learning_rate': 0.0001256491468274822, 'epoch': 0.35}\n",
      "{'loss': 1.3513, 'grad_norm': 0.51171875, 'learning_rate': 0.00012563778445001832, 'epoch': 0.35}\n",
      "{'loss': 1.1017, 'grad_norm': 0.9453125, 'learning_rate': 0.00012562642207255447, 'epoch': 0.35}\n",
      "{'loss': 1.3261, 'grad_norm': 0.5859375, 'learning_rate': 0.0001256150596950906, 'epoch': 0.35}\n",
      "{'loss': 1.1824, 'grad_norm': 0.609375, 'learning_rate': 0.00012560369731762675, 'epoch': 0.35}\n",
      "{'loss': 1.2442, 'grad_norm': 0.390625, 'learning_rate': 0.0001255923349401629, 'epoch': 0.35}\n",
      "{'loss': 1.2783, 'grad_norm': 0.73828125, 'learning_rate': 0.00012558097256269902, 'epoch': 0.35}\n",
      "{'loss': 1.0725, 'grad_norm': 0.9296875, 'learning_rate': 0.00012556961018523517, 'epoch': 0.35}\n",
      "{'loss': 1.3595, 'grad_norm': 0.470703125, 'learning_rate': 0.0001255582478077713, 'epoch': 0.35}\n",
      "{'loss': 1.2527, 'grad_norm': 0.63671875, 'learning_rate': 0.00012554688543030745, 'epoch': 0.35}\n",
      "{'loss': 1.1368, 'grad_norm': 0.498046875, 'learning_rate': 0.00012553552305284357, 'epoch': 0.35}\n",
      "{'loss': 1.2683, 'grad_norm': 0.6953125, 'learning_rate': 0.00012552416067537973, 'epoch': 0.35}\n",
      "{'loss': 1.1173, 'grad_norm': 0.65625, 'learning_rate': 0.00012551279829791588, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4016, 'grad_norm': 0.4296875, 'learning_rate': 0.000125501435920452, 'epoch': 0.35}\n",
      "{'loss': 1.2314, 'grad_norm': 0.73828125, 'learning_rate': 0.00012549007354298815, 'epoch': 0.35}\n",
      "{'loss': 1.1986, 'grad_norm': 0.46875, 'learning_rate': 0.00012547871116552428, 'epoch': 0.35}\n",
      "{'loss': 1.2432, 'grad_norm': 0.73828125, 'learning_rate': 0.0001254673487880604, 'epoch': 0.35}\n",
      "{'loss': 1.0313, 'grad_norm': 1.0859375, 'learning_rate': 0.00012545598641059655, 'epoch': 0.35}\n",
      "{'loss': 1.4518, 'grad_norm': 0.53125, 'learning_rate': 0.0001254446240331327, 'epoch': 0.35}\n",
      "{'loss': 1.2111, 'grad_norm': 1.125, 'learning_rate': 0.00012543326165566886, 'epoch': 0.35}\n",
      "{'loss': 1.1813, 'grad_norm': 0.828125, 'learning_rate': 0.00012542189927820498, 'epoch': 0.35}\n",
      "{'loss': 1.2074, 'grad_norm': 0.62890625, 'learning_rate': 0.0001254105369007411, 'epoch': 0.35}\n",
      "{'loss': 1.1101, 'grad_norm': 0.71875, 'learning_rate': 0.00012539917452327726, 'epoch': 0.35}\n",
      "{'loss': 1.3937, 'grad_norm': 0.546875, 'learning_rate': 0.00012538781214581338, 'epoch': 0.35}\n",
      "{'loss': 1.226, 'grad_norm': 0.57421875, 'learning_rate': 0.00012537644976834956, 'epoch': 0.35}\n",
      "{'loss': 1.2465, 'grad_norm': 0.4765625, 'learning_rate': 0.00012536508739088568, 'epoch': 0.35}\n",
      "{'loss': 1.1642, 'grad_norm': 0.470703125, 'learning_rate': 0.0001253537250134218, 'epoch': 0.35}\n",
      "{'loss': 1.1057, 'grad_norm': 1.2734375, 'learning_rate': 0.00012534236263595796, 'epoch': 0.35}\n",
      "{'loss': 1.4483, 'grad_norm': 0.51171875, 'learning_rate': 0.00012533100025849408, 'epoch': 0.35}\n",
      "{'loss': 1.2531, 'grad_norm': 0.6796875, 'learning_rate': 0.00012531963788103023, 'epoch': 0.35}\n",
      "{'loss': 1.2049, 'grad_norm': 0.396484375, 'learning_rate': 0.00012530827550356636, 'epoch': 0.35}\n",
      "{'loss': 1.2564, 'grad_norm': 0.64453125, 'learning_rate': 0.0001252969131261025, 'epoch': 0.35}\n",
      "{'loss': 1.0656, 'grad_norm': 0.9765625, 'learning_rate': 0.00012528555074863866, 'epoch': 0.35}\n",
      "{'loss': 1.2693, 'grad_norm': 0.7421875, 'learning_rate': 0.00012527418837117479, 'epoch': 0.35}\n",
      "{'loss': 1.2543, 'grad_norm': 0.703125, 'learning_rate': 0.00012526282599371094, 'epoch': 0.35}\n",
      "{'loss': 1.3312, 'grad_norm': 0.423828125, 'learning_rate': 0.00012525146361624706, 'epoch': 0.35}\n",
      "{'loss': 1.2911, 'grad_norm': 0.6640625, 'learning_rate': 0.0001252401012387832, 'epoch': 0.35}\n",
      "{'loss': 1.1719, 'grad_norm': 0.671875, 'learning_rate': 0.00012522873886131934, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3497, 'grad_norm': 0.72265625, 'learning_rate': 0.0001252173764838555, 'epoch': 0.35}\n",
      "{'loss': 1.1209, 'grad_norm': 0.4921875, 'learning_rate': 0.00012520601410639164, 'epoch': 0.35}\n",
      "{'loss': 1.3119, 'grad_norm': 0.5078125, 'learning_rate': 0.00012519465172892776, 'epoch': 0.35}\n",
      "{'loss': 1.265, 'grad_norm': 0.6796875, 'learning_rate': 0.00012518328935146392, 'epoch': 0.35}\n",
      "{'loss': 1.1663, 'grad_norm': 0.796875, 'learning_rate': 0.00012517192697400004, 'epoch': 0.35}\n",
      "{'loss': 1.4253, 'grad_norm': 0.451171875, 'learning_rate': 0.0001251605645965362, 'epoch': 0.35}\n",
      "{'loss': 1.1761, 'grad_norm': 0.79296875, 'learning_rate': 0.00012514920221907232, 'epoch': 0.35}\n",
      "{'loss': 1.1956, 'grad_norm': 0.59375, 'learning_rate': 0.00012513783984160847, 'epoch': 0.35}\n",
      "{'loss': 1.3636, 'grad_norm': 0.56640625, 'learning_rate': 0.00012512647746414462, 'epoch': 0.35}\n",
      "{'loss': 1.0196, 'grad_norm': 0.71875, 'learning_rate': 0.00012511511508668074, 'epoch': 0.35}\n",
      "{'loss': 1.2645, 'grad_norm': 0.474609375, 'learning_rate': 0.0001251037527092169, 'epoch': 0.35}\n",
      "{'loss': 1.1426, 'grad_norm': 0.73046875, 'learning_rate': 0.00012509239033175302, 'epoch': 0.35}\n",
      "{'loss': 1.1532, 'grad_norm': 0.373046875, 'learning_rate': 0.00012508102795428914, 'epoch': 0.35}\n",
      "{'loss': 1.2771, 'grad_norm': 0.56640625, 'learning_rate': 0.0001250696655768253, 'epoch': 0.35}\n",
      "{'loss': 1.074, 'grad_norm': 0.66015625, 'learning_rate': 0.00012505830319936145, 'epoch': 0.35}\n",
      "{'loss': 1.4396, 'grad_norm': 0.55859375, 'learning_rate': 0.0001250469408218976, 'epoch': 0.35}\n",
      "{'loss': 1.0354, 'grad_norm': 0.546875, 'learning_rate': 0.00012503557844443372, 'epoch': 0.35}\n",
      "{'loss': 1.3297, 'grad_norm': 0.56640625, 'learning_rate': 0.00012502421606696985, 'epoch': 0.35}\n",
      "{'loss': 1.2947, 'grad_norm': 0.60546875, 'learning_rate': 0.000125012853689506, 'epoch': 0.35}\n",
      "{'loss': 1.0739, 'grad_norm': 0.83984375, 'learning_rate': 0.00012500149131204212, 'epoch': 0.35}\n",
      "{'loss': 1.3324, 'grad_norm': 0.5703125, 'learning_rate': 0.0001249901289345783, 'epoch': 0.35}\n",
      "{'loss': 1.1912, 'grad_norm': 0.59375, 'learning_rate': 0.00012497876655711442, 'epoch': 0.35}\n",
      "{'loss': 1.1682, 'grad_norm': 0.47265625, 'learning_rate': 0.00012496740417965055, 'epoch': 0.35}\n",
      "{'loss': 1.3269, 'grad_norm': 0.515625, 'learning_rate': 0.0001249560418021867, 'epoch': 0.35}\n",
      "{'loss': 1.0242, 'grad_norm': 0.8984375, 'learning_rate': 0.00012494467942472282, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.343, 'grad_norm': 0.5078125, 'learning_rate': 0.00012493331704725898, 'epoch': 0.35}\n",
      "{'loss': 1.2459, 'grad_norm': 0.83203125, 'learning_rate': 0.0001249219546697951, 'epoch': 0.35}\n",
      "{'loss': 1.2479, 'grad_norm': 0.484375, 'learning_rate': 0.00012491059229233125, 'epoch': 0.35}\n",
      "{'loss': 1.2031, 'grad_norm': 0.400390625, 'learning_rate': 0.0001248992299148674, 'epoch': 0.35}\n",
      "{'loss': 1.142, 'grad_norm': 0.84375, 'learning_rate': 0.00012488786753740353, 'epoch': 0.35}\n",
      "{'loss': 1.3999, 'grad_norm': 0.498046875, 'learning_rate': 0.00012487650515993968, 'epoch': 0.35}\n",
      "{'loss': 1.1433, 'grad_norm': 0.74609375, 'learning_rate': 0.0001248651427824758, 'epoch': 0.35}\n",
      "{'loss': 1.1116, 'grad_norm': 0.47265625, 'learning_rate': 0.00012485378040501195, 'epoch': 0.36}\n",
      "{'loss': 1.1783, 'grad_norm': 0.5625, 'learning_rate': 0.00012484241802754808, 'epoch': 0.36}\n",
      "{'loss': 1.076, 'grad_norm': 0.78125, 'learning_rate': 0.00012483105565008423, 'epoch': 0.36}\n",
      "{'loss': 1.4631, 'grad_norm': 0.58203125, 'learning_rate': 0.00012481969327262038, 'epoch': 0.36}\n",
      "{'loss': 1.1065, 'grad_norm': 0.890625, 'learning_rate': 0.0001248083308951565, 'epoch': 0.36}\n",
      "{'loss': 1.2664, 'grad_norm': 0.412109375, 'learning_rate': 0.00012479696851769266, 'epoch': 0.36}\n",
      "{'loss': 1.1334, 'grad_norm': 0.82421875, 'learning_rate': 0.00012478560614022878, 'epoch': 0.36}\n",
      "{'loss': 1.05, 'grad_norm': 1.7265625, 'learning_rate': 0.00012477424376276493, 'epoch': 0.36}\n",
      "{'loss': 1.3318, 'grad_norm': 0.71484375, 'learning_rate': 0.00012476288138530106, 'epoch': 0.36}\n",
      "{'loss': 1.1952, 'grad_norm': 0.6015625, 'learning_rate': 0.0001247515190078372, 'epoch': 0.36}\n",
      "{'loss': 1.2746, 'grad_norm': 0.4609375, 'learning_rate': 0.00012474015663037336, 'epoch': 0.36}\n",
      "{'loss': 1.3213, 'grad_norm': 0.4921875, 'learning_rate': 0.00012472879425290948, 'epoch': 0.36}\n",
      "{'loss': 1.1385, 'grad_norm': 0.83203125, 'learning_rate': 0.00012471743187544564, 'epoch': 0.36}\n",
      "{'loss': 1.3037, 'grad_norm': 0.51171875, 'learning_rate': 0.00012470606949798176, 'epoch': 0.36}\n",
      "{'loss': 1.3133, 'grad_norm': 0.80859375, 'learning_rate': 0.00012469470712051788, 'epoch': 0.36}\n",
      "{'loss': 1.3418, 'grad_norm': 0.3984375, 'learning_rate': 0.00012468334474305406, 'epoch': 0.36}\n",
      "{'loss': 1.2011, 'grad_norm': 0.5546875, 'learning_rate': 0.0001246719823655902, 'epoch': 0.36}\n",
      "{'loss': 1.0429, 'grad_norm': 0.55078125, 'learning_rate': 0.00012466061998812634, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2713, 'grad_norm': 0.5546875, 'learning_rate': 0.00012464925761066246, 'epoch': 0.36}\n",
      "{'loss': 1.0972, 'grad_norm': 0.5703125, 'learning_rate': 0.0001246378952331986, 'epoch': 0.36}\n",
      "{'loss': 1.2041, 'grad_norm': 0.392578125, 'learning_rate': 0.00012462653285573474, 'epoch': 0.36}\n",
      "{'loss': 1.2822, 'grad_norm': 0.59375, 'learning_rate': 0.00012461517047827086, 'epoch': 0.36}\n",
      "{'loss': 1.0404, 'grad_norm': 0.640625, 'learning_rate': 0.00012460380810080704, 'epoch': 0.36}\n",
      "{'loss': 1.4476, 'grad_norm': 0.609375, 'learning_rate': 0.00012459244572334317, 'epoch': 0.36}\n",
      "{'loss': 1.2322, 'grad_norm': 0.6875, 'learning_rate': 0.0001245810833458793, 'epoch': 0.36}\n",
      "{'loss': 1.2925, 'grad_norm': 0.474609375, 'learning_rate': 0.00012456972096841544, 'epoch': 0.36}\n",
      "{'loss': 1.2353, 'grad_norm': 0.5234375, 'learning_rate': 0.00012455835859095157, 'epoch': 0.36}\n",
      "{'loss': 1.1282, 'grad_norm': 1.3359375, 'learning_rate': 0.00012454699621348772, 'epoch': 0.36}\n",
      "{'loss': 1.2958, 'grad_norm': 0.63671875, 'learning_rate': 0.00012453563383602384, 'epoch': 0.36}\n",
      "{'loss': 1.1899, 'grad_norm': 0.8515625, 'learning_rate': 0.00012452427145856, 'epoch': 0.36}\n",
      "{'loss': 1.2444, 'grad_norm': 0.453125, 'learning_rate': 0.00012451290908109614, 'epoch': 0.36}\n",
      "{'loss': 1.0798, 'grad_norm': 0.56640625, 'learning_rate': 0.00012450154670363227, 'epoch': 0.36}\n",
      "{'loss': 1.0205, 'grad_norm': 0.482421875, 'learning_rate': 0.00012449018432616842, 'epoch': 0.36}\n",
      "{'loss': 1.2434, 'grad_norm': 0.6640625, 'learning_rate': 0.00012447882194870454, 'epoch': 0.36}\n",
      "{'loss': 1.2346, 'grad_norm': 1.109375, 'learning_rate': 0.0001244674595712407, 'epoch': 0.36}\n",
      "{'loss': 1.219, 'grad_norm': 0.416015625, 'learning_rate': 0.00012445609719377682, 'epoch': 0.36}\n",
      "{'loss': 1.3456, 'grad_norm': 0.6875, 'learning_rate': 0.00012444473481631297, 'epoch': 0.36}\n",
      "{'loss': 1.0532, 'grad_norm': 0.98828125, 'learning_rate': 0.00012443337243884912, 'epoch': 0.36}\n",
      "{'loss': 1.3246, 'grad_norm': 0.474609375, 'learning_rate': 0.00012442201006138525, 'epoch': 0.36}\n",
      "{'loss': 1.0958, 'grad_norm': 0.69921875, 'learning_rate': 0.0001244106476839214, 'epoch': 0.36}\n",
      "{'loss': 1.2776, 'grad_norm': 0.40234375, 'learning_rate': 0.00012439928530645752, 'epoch': 0.36}\n",
      "{'loss': 1.2376, 'grad_norm': 0.578125, 'learning_rate': 0.00012438792292899367, 'epoch': 0.36}\n",
      "{'loss': 1.0519, 'grad_norm': 0.5859375, 'learning_rate': 0.0001243765605515298, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2907, 'grad_norm': 0.51953125, 'learning_rate': 0.00012436519817406595, 'epoch': 0.36}\n",
      "{'loss': 1.2332, 'grad_norm': 0.859375, 'learning_rate': 0.0001243538357966021, 'epoch': 0.36}\n",
      "{'loss': 1.291, 'grad_norm': 0.34375, 'learning_rate': 0.00012434247341913823, 'epoch': 0.36}\n",
      "{'loss': 1.2238, 'grad_norm': 0.5546875, 'learning_rate': 0.00012433111104167438, 'epoch': 0.36}\n",
      "{'loss': 1.1675, 'grad_norm': 0.9296875, 'learning_rate': 0.0001243197486642105, 'epoch': 0.36}\n",
      "{'loss': 1.4592, 'grad_norm': 0.453125, 'learning_rate': 0.00012430838628674663, 'epoch': 0.36}\n",
      "{'loss': 1.2097, 'grad_norm': 0.703125, 'learning_rate': 0.0001242970239092828, 'epoch': 0.36}\n",
      "{'loss': 1.237, 'grad_norm': 0.45703125, 'learning_rate': 0.00012428566153181893, 'epoch': 0.36}\n",
      "{'loss': 1.2607, 'grad_norm': 0.640625, 'learning_rate': 0.00012427429915435508, 'epoch': 0.36}\n",
      "{'loss': 1.1975, 'grad_norm': 0.921875, 'learning_rate': 0.0001242629367768912, 'epoch': 0.36}\n",
      "{'loss': 1.4322, 'grad_norm': 0.578125, 'learning_rate': 0.00012425157439942733, 'epoch': 0.36}\n",
      "{'loss': 1.1095, 'grad_norm': 0.6015625, 'learning_rate': 0.00012424021202196348, 'epoch': 0.36}\n",
      "{'loss': 1.2029, 'grad_norm': 0.609375, 'learning_rate': 0.0001242288496444996, 'epoch': 0.36}\n",
      "{'loss': 1.2219, 'grad_norm': 0.5078125, 'learning_rate': 0.00012421748726703578, 'epoch': 0.36}\n",
      "{'loss': 1.1402, 'grad_norm': 1.2890625, 'learning_rate': 0.0001242061248895719, 'epoch': 0.36}\n",
      "{'loss': 1.3018, 'grad_norm': 0.6640625, 'learning_rate': 0.00012419476251210803, 'epoch': 0.36}\n",
      "{'loss': 1.1947, 'grad_norm': 0.69140625, 'learning_rate': 0.00012418340013464418, 'epoch': 0.36}\n",
      "{'loss': 1.2134, 'grad_norm': 0.46484375, 'learning_rate': 0.0001241720377571803, 'epoch': 0.36}\n",
      "{'loss': 1.3503, 'grad_norm': 0.65234375, 'learning_rate': 0.00012416067537971646, 'epoch': 0.36}\n",
      "{'loss': 1.0448, 'grad_norm': 0.80859375, 'learning_rate': 0.00012414931300225258, 'epoch': 0.36}\n",
      "{'loss': 1.3252, 'grad_norm': 0.41015625, 'learning_rate': 0.00012413795062478873, 'epoch': 0.36}\n",
      "{'loss': 1.1811, 'grad_norm': 0.49609375, 'learning_rate': 0.00012412658824732488, 'epoch': 0.36}\n",
      "{'loss': 1.2288, 'grad_norm': 0.435546875, 'learning_rate': 0.000124115225869861, 'epoch': 0.36}\n",
      "{'loss': 1.3144, 'grad_norm': 0.498046875, 'learning_rate': 0.00012410386349239716, 'epoch': 0.36}\n",
      "{'loss': 1.1335, 'grad_norm': 0.5, 'learning_rate': 0.00012409250111493328, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2934, 'grad_norm': 0.44921875, 'learning_rate': 0.00012408113873746944, 'epoch': 0.36}\n",
      "{'loss': 1.2298, 'grad_norm': 0.69921875, 'learning_rate': 0.00012406977636000556, 'epoch': 0.36}\n",
      "{'loss': 1.2268, 'grad_norm': 0.39453125, 'learning_rate': 0.0001240584139825417, 'epoch': 0.36}\n",
      "{'loss': 1.0445, 'grad_norm': 0.8046875, 'learning_rate': 0.00012404705160507786, 'epoch': 0.36}\n",
      "{'loss': 1.0416, 'grad_norm': 1.1015625, 'learning_rate': 0.000124035689227614, 'epoch': 0.36}\n",
      "{'loss': 1.2701, 'grad_norm': 0.5546875, 'learning_rate': 0.00012402432685015014, 'epoch': 0.36}\n",
      "{'loss': 1.1745, 'grad_norm': 0.59765625, 'learning_rate': 0.00012401296447268626, 'epoch': 0.36}\n",
      "{'loss': 1.1281, 'grad_norm': 0.412109375, 'learning_rate': 0.00012400160209522241, 'epoch': 0.36}\n",
      "{'loss': 1.2669, 'grad_norm': 0.51171875, 'learning_rate': 0.00012399023971775857, 'epoch': 0.36}\n",
      "{'loss': 0.9091, 'grad_norm': 1.15625, 'learning_rate': 0.0001239788773402947, 'epoch': 0.36}\n",
      "{'loss': 1.1825, 'grad_norm': 0.70703125, 'learning_rate': 0.00012396751496283084, 'epoch': 0.36}\n",
      "{'loss': 1.2934, 'grad_norm': 0.6171875, 'learning_rate': 0.00012395615258536697, 'epoch': 0.36}\n",
      "{'loss': 1.22, 'grad_norm': 0.412109375, 'learning_rate': 0.00012394479020790312, 'epoch': 0.36}\n",
      "{'loss': 1.3218, 'grad_norm': 0.83203125, 'learning_rate': 0.00012393342783043924, 'epoch': 0.36}\n",
      "{'loss': 1.0249, 'grad_norm': 0.70703125, 'learning_rate': 0.00012392206545297537, 'epoch': 0.36}\n",
      "{'loss': 1.3545, 'grad_norm': 0.451171875, 'learning_rate': 0.00012391070307551154, 'epoch': 0.36}\n",
      "{'loss': 1.1512, 'grad_norm': 0.97265625, 'learning_rate': 0.00012389934069804767, 'epoch': 0.36}\n",
      "{'loss': 1.2336, 'grad_norm': 0.51171875, 'learning_rate': 0.00012388797832058382, 'epoch': 0.36}\n",
      "{'loss': 1.3096, 'grad_norm': 0.498046875, 'learning_rate': 0.00012387661594311994, 'epoch': 0.36}\n",
      "{'loss': 1.0068, 'grad_norm': 0.91015625, 'learning_rate': 0.00012386525356565607, 'epoch': 0.36}\n",
      "{'loss': 1.3706, 'grad_norm': 0.412109375, 'learning_rate': 0.00012385389118819222, 'epoch': 0.36}\n",
      "{'loss': 1.2622, 'grad_norm': 0.61328125, 'learning_rate': 0.00012384252881072834, 'epoch': 0.36}\n",
      "{'loss': 1.1528, 'grad_norm': 0.396484375, 'learning_rate': 0.00012383116643326452, 'epoch': 0.36}\n",
      "{'loss': 1.2233, 'grad_norm': 0.546875, 'learning_rate': 0.00012381980405580065, 'epoch': 0.36}\n",
      "{'loss': 1.2414, 'grad_norm': 1.2265625, 'learning_rate': 0.00012380844167833677, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5103, 'grad_norm': 0.49609375, 'learning_rate': 0.00012379707930087292, 'epoch': 0.36}\n",
      "{'loss': 1.1731, 'grad_norm': 0.84765625, 'learning_rate': 0.00012378571692340905, 'epoch': 0.36}\n",
      "{'loss': 1.3302, 'grad_norm': 0.462890625, 'learning_rate': 0.0001237743545459452, 'epoch': 0.36}\n",
      "{'loss': 1.3633, 'grad_norm': 0.6328125, 'learning_rate': 0.00012376299216848132, 'epoch': 0.36}\n",
      "{'loss': 1.1111, 'grad_norm': 1.2265625, 'learning_rate': 0.00012375162979101747, 'epoch': 0.36}\n",
      "{'loss': 1.3451, 'grad_norm': 0.484375, 'learning_rate': 0.00012374026741355363, 'epoch': 0.36}\n",
      "{'loss': 1.151, 'grad_norm': 0.515625, 'learning_rate': 0.00012372890503608975, 'epoch': 0.36}\n",
      "{'loss': 1.0795, 'grad_norm': 0.455078125, 'learning_rate': 0.0001237175426586259, 'epoch': 0.36}\n",
      "{'loss': 1.1814, 'grad_norm': 0.828125, 'learning_rate': 0.00012370618028116203, 'epoch': 0.36}\n",
      "{'loss': 0.9943, 'grad_norm': 0.84765625, 'learning_rate': 0.00012369481790369818, 'epoch': 0.36}\n",
      "{'loss': 1.4174, 'grad_norm': 0.478515625, 'learning_rate': 0.0001236834555262343, 'epoch': 0.36}\n",
      "{'loss': 1.1195, 'grad_norm': 0.6328125, 'learning_rate': 0.00012367209314877045, 'epoch': 0.36}\n",
      "{'loss': 1.2736, 'grad_norm': 0.435546875, 'learning_rate': 0.0001236607307713066, 'epoch': 0.36}\n",
      "{'loss': 1.1729, 'grad_norm': 1.0625, 'learning_rate': 0.00012364936839384273, 'epoch': 0.36}\n",
      "{'loss': 1.0576, 'grad_norm': 0.451171875, 'learning_rate': 0.00012363800601637888, 'epoch': 0.36}\n",
      "{'loss': 1.429, 'grad_norm': 0.484375, 'learning_rate': 0.000123626643638915, 'epoch': 0.36}\n",
      "{'loss': 1.0773, 'grad_norm': 0.8515625, 'learning_rate': 0.00012361528126145116, 'epoch': 0.36}\n",
      "{'loss': 1.2199, 'grad_norm': 0.5, 'learning_rate': 0.0001236039188839873, 'epoch': 0.36}\n",
      "{'loss': 1.1966, 'grad_norm': 0.74609375, 'learning_rate': 0.00012359255650652343, 'epoch': 0.36}\n",
      "{'loss': 1.092, 'grad_norm': 0.82421875, 'learning_rate': 0.00012358119412905958, 'epoch': 0.36}\n",
      "{'loss': 1.3812, 'grad_norm': 0.482421875, 'learning_rate': 0.0001235698317515957, 'epoch': 0.36}\n",
      "{'loss': 1.2133, 'grad_norm': 0.73828125, 'learning_rate': 0.00012355846937413186, 'epoch': 0.36}\n",
      "{'loss': 1.2984, 'grad_norm': 0.416015625, 'learning_rate': 0.00012354710699666798, 'epoch': 0.36}\n",
      "{'loss': 1.2679, 'grad_norm': 0.66796875, 'learning_rate': 0.0001235357446192041, 'epoch': 0.36}\n",
      "{'loss': 0.9693, 'grad_norm': 0.984375, 'learning_rate': 0.00012352438224174029, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2744, 'grad_norm': 0.80078125, 'learning_rate': 0.0001235130198642764, 'epoch': 0.36}\n",
      "{'loss': 1.2175, 'grad_norm': 0.7421875, 'learning_rate': 0.00012350165748681256, 'epoch': 0.36}\n",
      "{'loss': 1.1472, 'grad_norm': 0.34375, 'learning_rate': 0.00012349029510934869, 'epoch': 0.36}\n",
      "{'loss': 1.4457, 'grad_norm': 0.46484375, 'learning_rate': 0.0001234789327318848, 'epoch': 0.36}\n",
      "{'loss': 1.0228, 'grad_norm': 0.76171875, 'learning_rate': 0.00012346757035442096, 'epoch': 0.36}\n",
      "{'loss': 1.2596, 'grad_norm': 0.52734375, 'learning_rate': 0.00012345620797695709, 'epoch': 0.36}\n",
      "{'loss': 1.1074, 'grad_norm': 0.76953125, 'learning_rate': 0.00012344484559949326, 'epoch': 0.36}\n",
      "{'loss': 1.213, 'grad_norm': 0.4921875, 'learning_rate': 0.0001234334832220294, 'epoch': 0.36}\n",
      "{'loss': 1.2473, 'grad_norm': 0.6640625, 'learning_rate': 0.0001234221208445655, 'epoch': 0.36}\n",
      "{'loss': 0.9978, 'grad_norm': 0.71484375, 'learning_rate': 0.00012341075846710166, 'epoch': 0.36}\n",
      "{'loss': 1.4297, 'grad_norm': 0.5390625, 'learning_rate': 0.0001233993960896378, 'epoch': 0.36}\n",
      "{'loss': 1.1234, 'grad_norm': 0.7265625, 'learning_rate': 0.00012338803371217394, 'epoch': 0.36}\n",
      "{'loss': 1.292, 'grad_norm': 0.5234375, 'learning_rate': 0.00012337667133471006, 'epoch': 0.36}\n",
      "{'loss': 1.2229, 'grad_norm': 0.59765625, 'learning_rate': 0.00012336530895724622, 'epoch': 0.36}\n",
      "{'loss': 1.0862, 'grad_norm': 0.875, 'learning_rate': 0.00012335394657978237, 'epoch': 0.36}\n",
      "{'loss': 1.3764, 'grad_norm': 0.5234375, 'learning_rate': 0.0001233425842023185, 'epoch': 0.36}\n",
      "{'loss': 1.2466, 'grad_norm': 0.671875, 'learning_rate': 0.00012333122182485464, 'epoch': 0.36}\n",
      "{'loss': 1.2828, 'grad_norm': 0.4765625, 'learning_rate': 0.00012331985944739077, 'epoch': 0.36}\n",
      "{'loss': 1.2831, 'grad_norm': 0.8828125, 'learning_rate': 0.00012330849706992692, 'epoch': 0.36}\n",
      "{'loss': 1.0831, 'grad_norm': 0.82421875, 'learning_rate': 0.00012329713469246307, 'epoch': 0.36}\n",
      "{'loss': 1.4102, 'grad_norm': 0.50390625, 'learning_rate': 0.0001232857723149992, 'epoch': 0.36}\n",
      "{'loss': 1.1748, 'grad_norm': 0.64453125, 'learning_rate': 0.00012327440993753535, 'epoch': 0.36}\n",
      "{'loss': 1.1676, 'grad_norm': 0.45703125, 'learning_rate': 0.00012326304756007147, 'epoch': 0.36}\n",
      "{'loss': 1.3026, 'grad_norm': 0.50390625, 'learning_rate': 0.00012325168518260762, 'epoch': 0.36}\n",
      "{'loss': 1.0635, 'grad_norm': 0.287109375, 'learning_rate': 0.00012324032280514375, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4119, 'grad_norm': 0.5078125, 'learning_rate': 0.0001232289604276799, 'epoch': 0.36}\n",
      "{'loss': 1.1857, 'grad_norm': 0.75, 'learning_rate': 0.00012321759805021605, 'epoch': 0.36}\n",
      "{'loss': 1.1743, 'grad_norm': 0.515625, 'learning_rate': 0.00012320623567275217, 'epoch': 0.36}\n",
      "{'loss': 1.2316, 'grad_norm': 0.66796875, 'learning_rate': 0.00012319487329528832, 'epoch': 0.36}\n",
      "{'loss': 1.0439, 'grad_norm': 0.515625, 'learning_rate': 0.00012318351091782445, 'epoch': 0.36}\n",
      "{'loss': 1.3124, 'grad_norm': 0.5078125, 'learning_rate': 0.0001231721485403606, 'epoch': 0.36}\n",
      "{'loss': 1.2252, 'grad_norm': 0.61328125, 'learning_rate': 0.00012316078616289672, 'epoch': 0.36}\n",
      "{'loss': 1.2157, 'grad_norm': 0.408203125, 'learning_rate': 0.00012314942378543285, 'epoch': 0.36}\n",
      "{'loss': 1.0575, 'grad_norm': 0.5859375, 'learning_rate': 0.00012313806140796903, 'epoch': 0.36}\n",
      "{'loss': 1.0265, 'grad_norm': 0.8046875, 'learning_rate': 0.00012312669903050515, 'epoch': 0.36}\n",
      "{'loss': 1.5457, 'grad_norm': 0.51171875, 'learning_rate': 0.0001231153366530413, 'epoch': 0.36}\n",
      "{'loss': 1.3087, 'grad_norm': 0.52734375, 'learning_rate': 0.00012310397427557743, 'epoch': 0.36}\n",
      "{'loss': 1.2344, 'grad_norm': 0.4453125, 'learning_rate': 0.00012309261189811355, 'epoch': 0.36}\n",
      "{'loss': 1.2281, 'grad_norm': 0.703125, 'learning_rate': 0.0001230812495206497, 'epoch': 0.36}\n",
      "{'loss': 1.0168, 'grad_norm': 0.765625, 'learning_rate': 0.00012306988714318583, 'epoch': 0.36}\n",
      "{'loss': 1.296, 'grad_norm': 0.64453125, 'learning_rate': 0.000123058524765722, 'epoch': 0.36}\n",
      "{'loss': 1.1605, 'grad_norm': 0.416015625, 'learning_rate': 0.00012304716238825813, 'epoch': 0.36}\n",
      "{'loss': 1.3343, 'grad_norm': 0.5546875, 'learning_rate': 0.00012303580001079425, 'epoch': 0.36}\n",
      "{'loss': 1.2886, 'grad_norm': 0.80859375, 'learning_rate': 0.0001230244376333304, 'epoch': 0.36}\n",
      "{'loss': 1.0945, 'grad_norm': 1.1796875, 'learning_rate': 0.00012301307525586653, 'epoch': 0.36}\n",
      "{'loss': 1.4116, 'grad_norm': 0.478515625, 'learning_rate': 0.00012300171287840268, 'epoch': 0.36}\n",
      "{'loss': 1.2097, 'grad_norm': 0.703125, 'learning_rate': 0.0001229903505009388, 'epoch': 0.36}\n",
      "{'loss': 1.2717, 'grad_norm': 0.46484375, 'learning_rate': 0.00012297898812347496, 'epoch': 0.36}\n",
      "{'loss': 1.2077, 'grad_norm': 0.58984375, 'learning_rate': 0.0001229676257460111, 'epoch': 0.36}\n",
      "{'loss': 1.0978, 'grad_norm': 1.2421875, 'learning_rate': 0.00012295626336854723, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.328, 'grad_norm': 0.45703125, 'learning_rate': 0.00012294490099108338, 'epoch': 0.36}\n",
      "{'loss': 1.1396, 'grad_norm': 0.41015625, 'learning_rate': 0.0001229335386136195, 'epoch': 0.36}\n",
      "{'loss': 1.3083, 'grad_norm': 0.51171875, 'learning_rate': 0.00012292217623615566, 'epoch': 0.36}\n",
      "{'loss': 1.3007, 'grad_norm': 0.50390625, 'learning_rate': 0.0001229108138586918, 'epoch': 0.36}\n",
      "{'loss': 1.0549, 'grad_norm': 0.76953125, 'learning_rate': 0.00012289945148122794, 'epoch': 0.36}\n",
      "{'loss': 1.526, 'grad_norm': 0.51953125, 'learning_rate': 0.0001228880891037641, 'epoch': 0.36}\n",
      "{'loss': 1.3096, 'grad_norm': 0.5546875, 'learning_rate': 0.0001228767267263002, 'epoch': 0.36}\n",
      "{'loss': 1.2158, 'grad_norm': 0.478515625, 'learning_rate': 0.00012286536434883636, 'epoch': 0.36}\n",
      "{'loss': 1.2691, 'grad_norm': 0.59765625, 'learning_rate': 0.0001228540019713725, 'epoch': 0.36}\n",
      "{'loss': 1.018, 'grad_norm': 0.5, 'learning_rate': 0.00012284263959390864, 'epoch': 0.36}\n",
      "{'loss': 1.4061, 'grad_norm': 0.58984375, 'learning_rate': 0.0001228312772164448, 'epoch': 0.36}\n",
      "{'loss': 1.097, 'grad_norm': 0.4453125, 'learning_rate': 0.00012281991483898091, 'epoch': 0.36}\n",
      "{'loss': 1.158, 'grad_norm': 0.419921875, 'learning_rate': 0.00012280855246151707, 'epoch': 0.36}\n",
      "{'loss': 1.371, 'grad_norm': 0.61328125, 'learning_rate': 0.0001227971900840532, 'epoch': 0.36}\n",
      "{'loss': 1.0901, 'grad_norm': 1.046875, 'learning_rate': 0.00012278582770658934, 'epoch': 0.36}\n",
      "{'loss': 1.2174, 'grad_norm': 0.61328125, 'learning_rate': 0.00012277446532912547, 'epoch': 0.36}\n",
      "{'loss': 1.1885, 'grad_norm': 0.72265625, 'learning_rate': 0.0001227631029516616, 'epoch': 0.36}\n",
      "{'loss': 1.1956, 'grad_norm': 0.427734375, 'learning_rate': 0.00012275174057419777, 'epoch': 0.36}\n",
      "{'loss': 1.2865, 'grad_norm': 0.6484375, 'learning_rate': 0.0001227403781967339, 'epoch': 0.36}\n",
      "{'loss': 1.1105, 'grad_norm': 1.390625, 'learning_rate': 0.00012272901581927004, 'epoch': 0.36}\n",
      "{'loss': 1.2868, 'grad_norm': 0.453125, 'learning_rate': 0.00012271765344180617, 'epoch': 0.36}\n",
      "{'loss': 1.2522, 'grad_norm': 0.7734375, 'learning_rate': 0.0001227062910643423, 'epoch': 0.36}\n",
      "{'loss': 1.1476, 'grad_norm': 0.39453125, 'learning_rate': 0.00012269492868687844, 'epoch': 0.36}\n",
      "{'loss': 1.3156, 'grad_norm': 0.6796875, 'learning_rate': 0.00012268356630941457, 'epoch': 0.36}\n",
      "{'loss': 1.1292, 'grad_norm': 0.87109375, 'learning_rate': 0.00012267220393195075, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3979, 'grad_norm': 0.57421875, 'learning_rate': 0.00012266084155448687, 'epoch': 0.36}\n",
      "{'loss': 1.1407, 'grad_norm': 0.4609375, 'learning_rate': 0.000122649479177023, 'epoch': 0.36}\n",
      "{'loss': 1.4433, 'grad_norm': 0.439453125, 'learning_rate': 0.00012263811679955915, 'epoch': 0.36}\n",
      "{'loss': 1.2947, 'grad_norm': 0.474609375, 'learning_rate': 0.00012262675442209527, 'epoch': 0.36}\n",
      "{'loss': 1.0059, 'grad_norm': 0.859375, 'learning_rate': 0.00012261539204463142, 'epoch': 0.36}\n",
      "{'loss': 1.4469, 'grad_norm': 0.62890625, 'learning_rate': 0.00012260402966716757, 'epoch': 0.36}\n",
      "{'loss': 1.09, 'grad_norm': 0.75, 'learning_rate': 0.0001225926672897037, 'epoch': 0.36}\n",
      "{'loss': 1.0896, 'grad_norm': 0.478515625, 'learning_rate': 0.00012258130491223985, 'epoch': 0.36}\n",
      "{'loss': 1.1463, 'grad_norm': 0.59375, 'learning_rate': 0.00012256994253477597, 'epoch': 0.36}\n",
      "{'loss': 1.1335, 'grad_norm': 0.30859375, 'learning_rate': 0.00012255858015731213, 'epoch': 0.37}\n",
      "{'loss': 1.2572, 'grad_norm': 0.52734375, 'learning_rate': 0.00012254721777984825, 'epoch': 0.37}\n",
      "{'loss': 1.2716, 'grad_norm': 0.91796875, 'learning_rate': 0.0001225358554023844, 'epoch': 0.37}\n",
      "{'loss': 1.22, 'grad_norm': 0.50390625, 'learning_rate': 0.00012252449302492055, 'epoch': 0.37}\n",
      "{'loss': 1.2006, 'grad_norm': 0.890625, 'learning_rate': 0.00012251313064745668, 'epoch': 0.37}\n",
      "{'loss': 1.1632, 'grad_norm': 1.1640625, 'learning_rate': 0.00012250176826999283, 'epoch': 0.37}\n",
      "{'loss': 1.3788, 'grad_norm': 0.54296875, 'learning_rate': 0.00012249040589252895, 'epoch': 0.37}\n",
      "{'loss': 1.2437, 'grad_norm': 0.734375, 'learning_rate': 0.0001224790435150651, 'epoch': 0.37}\n",
      "{'loss': 1.2552, 'grad_norm': 0.396484375, 'learning_rate': 0.00012246768113760123, 'epoch': 0.37}\n",
      "{'loss': 1.1718, 'grad_norm': 0.9921875, 'learning_rate': 0.00012245631876013738, 'epoch': 0.37}\n",
      "{'loss': 1.2148, 'grad_norm': 0.92578125, 'learning_rate': 0.00012244495638267353, 'epoch': 0.37}\n",
      "{'loss': 1.3686, 'grad_norm': 0.66015625, 'learning_rate': 0.00012243359400520966, 'epoch': 0.37}\n",
      "{'loss': 1.0979, 'grad_norm': 0.68359375, 'learning_rate': 0.0001224222316277458, 'epoch': 0.37}\n",
      "{'loss': 1.2558, 'grad_norm': 0.72265625, 'learning_rate': 0.00012241086925028193, 'epoch': 0.37}\n",
      "{'loss': 1.3211, 'grad_norm': 0.765625, 'learning_rate': 0.00012239950687281808, 'epoch': 0.37}\n",
      "{'loss': 1.1643, 'grad_norm': 0.73046875, 'learning_rate': 0.0001223881444953542, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3724, 'grad_norm': 0.482421875, 'learning_rate': 0.00012237678211789033, 'epoch': 0.37}\n",
      "{'loss': 1.207, 'grad_norm': 0.72265625, 'learning_rate': 0.0001223654197404265, 'epoch': 0.37}\n",
      "{'loss': 1.3937, 'grad_norm': 0.439453125, 'learning_rate': 0.00012235405736296263, 'epoch': 0.37}\n",
      "{'loss': 1.2909, 'grad_norm': 0.50390625, 'learning_rate': 0.00012234269498549879, 'epoch': 0.37}\n",
      "{'loss': 1.0326, 'grad_norm': 1.453125, 'learning_rate': 0.0001223313326080349, 'epoch': 0.37}\n",
      "{'loss': 1.3773, 'grad_norm': 0.59765625, 'learning_rate': 0.00012231997023057103, 'epoch': 0.37}\n",
      "{'loss': 1.2721, 'grad_norm': 0.671875, 'learning_rate': 0.00012230860785310719, 'epoch': 0.37}\n",
      "{'loss': 1.0568, 'grad_norm': 0.515625, 'learning_rate': 0.00012229724547564334, 'epoch': 0.37}\n",
      "{'loss': 1.222, 'grad_norm': 0.76171875, 'learning_rate': 0.0001222858830981795, 'epoch': 0.37}\n",
      "{'loss': 1.1128, 'grad_norm': 0.71875, 'learning_rate': 0.0001222745207207156, 'epoch': 0.37}\n",
      "{'loss': 1.3397, 'grad_norm': 0.53125, 'learning_rate': 0.00012226315834325174, 'epoch': 0.37}\n",
      "{'loss': 1.1523, 'grad_norm': 0.625, 'learning_rate': 0.0001222517959657879, 'epoch': 0.37}\n",
      "{'loss': 1.114, 'grad_norm': 0.431640625, 'learning_rate': 0.000122240433588324, 'epoch': 0.37}\n",
      "{'loss': 1.2366, 'grad_norm': 0.578125, 'learning_rate': 0.00012222907121086016, 'epoch': 0.37}\n",
      "{'loss': 1.1417, 'grad_norm': 0.75, 'learning_rate': 0.00012221770883339632, 'epoch': 0.37}\n",
      "{'loss': 1.2765, 'grad_norm': 0.51171875, 'learning_rate': 0.00012220634645593244, 'epoch': 0.37}\n",
      "{'loss': 1.1772, 'grad_norm': 0.8046875, 'learning_rate': 0.0001221949840784686, 'epoch': 0.37}\n",
      "{'loss': 1.218, 'grad_norm': 0.51171875, 'learning_rate': 0.00012218362170100472, 'epoch': 0.37}\n",
      "{'loss': 1.3196, 'grad_norm': 0.58203125, 'learning_rate': 0.00012217225932354087, 'epoch': 0.37}\n",
      "{'loss': 1.0184, 'grad_norm': 0.494140625, 'learning_rate': 0.000122160896946077, 'epoch': 0.37}\n",
      "{'loss': 1.2932, 'grad_norm': 0.79296875, 'learning_rate': 0.00012214953456861314, 'epoch': 0.37}\n",
      "{'loss': 1.1538, 'grad_norm': 0.53515625, 'learning_rate': 0.0001221381721911493, 'epoch': 0.37}\n",
      "{'loss': 1.143, 'grad_norm': 0.6015625, 'learning_rate': 0.00012212680981368542, 'epoch': 0.37}\n",
      "{'loss': 1.2027, 'grad_norm': 0.44921875, 'learning_rate': 0.00012211544743622157, 'epoch': 0.37}\n",
      "{'loss': 1.0598, 'grad_norm': 0.92578125, 'learning_rate': 0.0001221040850587577, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3111, 'grad_norm': 0.6171875, 'learning_rate': 0.00012209272268129385, 'epoch': 0.37}\n",
      "{'loss': 1.2553, 'grad_norm': 0.6640625, 'learning_rate': 0.00012208136030382997, 'epoch': 0.37}\n",
      "{'loss': 1.2974, 'grad_norm': 0.4296875, 'learning_rate': 0.00012206999792636611, 'epoch': 0.37}\n",
      "{'loss': 1.1002, 'grad_norm': 0.55078125, 'learning_rate': 0.00012205863554890227, 'epoch': 0.37}\n",
      "{'loss': 1.0242, 'grad_norm': 1.0390625, 'learning_rate': 0.0001220472731714384, 'epoch': 0.37}\n",
      "{'loss': 1.4013, 'grad_norm': 0.44921875, 'learning_rate': 0.00012203591079397453, 'epoch': 0.37}\n",
      "{'loss': 1.1898, 'grad_norm': 0.8671875, 'learning_rate': 0.00012202454841651067, 'epoch': 0.37}\n",
      "{'loss': 1.1932, 'grad_norm': 0.515625, 'learning_rate': 0.00012201318603904681, 'epoch': 0.37}\n",
      "{'loss': 1.053, 'grad_norm': 0.4375, 'learning_rate': 0.00012200182366158295, 'epoch': 0.37}\n",
      "{'loss': 1.0131, 'grad_norm': 1.15625, 'learning_rate': 0.00012199046128411909, 'epoch': 0.37}\n",
      "{'loss': 1.3134, 'grad_norm': 0.453125, 'learning_rate': 0.00012197909890665524, 'epoch': 0.37}\n",
      "{'loss': 1.0457, 'grad_norm': 0.82421875, 'learning_rate': 0.00012196773652919138, 'epoch': 0.37}\n",
      "{'loss': 1.112, 'grad_norm': 0.498046875, 'learning_rate': 0.00012195637415172751, 'epoch': 0.37}\n",
      "{'loss': 1.314, 'grad_norm': 0.64453125, 'learning_rate': 0.00012194501177426365, 'epoch': 0.37}\n",
      "{'loss': 1.0344, 'grad_norm': 0.80078125, 'learning_rate': 0.00012193364939679979, 'epoch': 0.37}\n",
      "{'loss': 1.3394, 'grad_norm': 0.515625, 'learning_rate': 0.00012192228701933593, 'epoch': 0.37}\n",
      "{'loss': 1.1707, 'grad_norm': 0.76953125, 'learning_rate': 0.00012191092464187208, 'epoch': 0.37}\n",
      "{'loss': 1.1602, 'grad_norm': 0.4921875, 'learning_rate': 0.00012189956226440822, 'epoch': 0.37}\n",
      "{'loss': 1.2337, 'grad_norm': 0.53515625, 'learning_rate': 0.00012188819988694435, 'epoch': 0.37}\n",
      "{'loss': 1.0815, 'grad_norm': 0.8828125, 'learning_rate': 0.00012187683750948049, 'epoch': 0.37}\n",
      "{'loss': 1.3555, 'grad_norm': 0.55078125, 'learning_rate': 0.00012186547513201663, 'epoch': 0.37}\n",
      "{'loss': 1.2362, 'grad_norm': 0.73828125, 'learning_rate': 0.00012185411275455277, 'epoch': 0.37}\n",
      "{'loss': 1.1656, 'grad_norm': 0.478515625, 'learning_rate': 0.0001218427503770889, 'epoch': 0.37}\n",
      "{'loss': 1.3467, 'grad_norm': 0.52734375, 'learning_rate': 0.00012183138799962506, 'epoch': 0.37}\n",
      "{'loss': 1.1474, 'grad_norm': 0.9921875, 'learning_rate': 0.0001218200256221612, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3235, 'grad_norm': 0.578125, 'learning_rate': 0.00012180866324469733, 'epoch': 0.37}\n",
      "{'loss': 1.1521, 'grad_norm': 0.609375, 'learning_rate': 0.00012179730086723347, 'epoch': 0.37}\n",
      "{'loss': 1.1678, 'grad_norm': 0.3984375, 'learning_rate': 0.00012178593848976961, 'epoch': 0.37}\n",
      "{'loss': 1.3113, 'grad_norm': 0.58984375, 'learning_rate': 0.00012177457611230573, 'epoch': 0.37}\n",
      "{'loss': 0.9926, 'grad_norm': 0.875, 'learning_rate': 0.00012176321373484187, 'epoch': 0.37}\n",
      "{'loss': 1.2449, 'grad_norm': 0.50390625, 'learning_rate': 0.00012175185135737804, 'epoch': 0.37}\n",
      "{'loss': 1.2199, 'grad_norm': 0.60546875, 'learning_rate': 0.00012174048897991417, 'epoch': 0.37}\n",
      "{'loss': 1.3686, 'grad_norm': 0.53515625, 'learning_rate': 0.00012172912660245031, 'epoch': 0.37}\n",
      "{'loss': 1.3267, 'grad_norm': 1.0390625, 'learning_rate': 0.00012171776422498644, 'epoch': 0.37}\n",
      "{'loss': 0.9533, 'grad_norm': 1.0546875, 'learning_rate': 0.00012170640184752257, 'epoch': 0.37}\n",
      "{'loss': 1.3096, 'grad_norm': 0.5390625, 'learning_rate': 0.00012169503947005871, 'epoch': 0.37}\n",
      "{'loss': 1.1339, 'grad_norm': 0.58984375, 'learning_rate': 0.00012168367709259485, 'epoch': 0.37}\n",
      "{'loss': 1.1764, 'grad_norm': 0.54296875, 'learning_rate': 0.00012167231471513101, 'epoch': 0.37}\n",
      "{'loss': 1.2572, 'grad_norm': 0.66015625, 'learning_rate': 0.00012166095233766714, 'epoch': 0.37}\n",
      "{'loss': 1.0644, 'grad_norm': 0.82421875, 'learning_rate': 0.00012164958996020328, 'epoch': 0.37}\n",
      "{'loss': 1.3936, 'grad_norm': 0.5546875, 'learning_rate': 0.00012163822758273941, 'epoch': 0.37}\n",
      "{'loss': 1.2143, 'grad_norm': 0.8203125, 'learning_rate': 0.00012162686520527555, 'epoch': 0.37}\n",
      "{'loss': 1.1137, 'grad_norm': 0.65234375, 'learning_rate': 0.00012161550282781169, 'epoch': 0.37}\n",
      "{'loss': 1.2789, 'grad_norm': 0.546875, 'learning_rate': 0.00012160414045034784, 'epoch': 0.37}\n",
      "{'loss': 1.0362, 'grad_norm': 0.8046875, 'learning_rate': 0.00012159277807288398, 'epoch': 0.37}\n",
      "{'loss': 1.3702, 'grad_norm': 0.6015625, 'learning_rate': 0.00012158141569542012, 'epoch': 0.37}\n",
      "{'loss': 1.1271, 'grad_norm': 0.7890625, 'learning_rate': 0.00012157005331795625, 'epoch': 0.37}\n",
      "{'loss': 1.2033, 'grad_norm': 0.4140625, 'learning_rate': 0.00012155869094049239, 'epoch': 0.37}\n",
      "{'loss': 1.2214, 'grad_norm': 0.5078125, 'learning_rate': 0.00012154732856302853, 'epoch': 0.37}\n",
      "{'loss': 1.0621, 'grad_norm': 0.28515625, 'learning_rate': 0.00012153596618556467, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3798, 'grad_norm': 0.62109375, 'learning_rate': 0.00012152460380810082, 'epoch': 0.37}\n",
      "{'loss': 1.2881, 'grad_norm': 0.58984375, 'learning_rate': 0.00012151324143063696, 'epoch': 0.37}\n",
      "{'loss': 1.2227, 'grad_norm': 0.412109375, 'learning_rate': 0.0001215018790531731, 'epoch': 0.37}\n",
      "{'loss': 1.1275, 'grad_norm': 0.60546875, 'learning_rate': 0.00012149051667570923, 'epoch': 0.37}\n",
      "{'loss': 1.007, 'grad_norm': 0.7578125, 'learning_rate': 0.00012147915429824537, 'epoch': 0.37}\n",
      "{'loss': 1.3277, 'grad_norm': 0.54296875, 'learning_rate': 0.00012146779192078151, 'epoch': 0.37}\n",
      "{'loss': 1.1803, 'grad_norm': 0.81640625, 'learning_rate': 0.00012145642954331765, 'epoch': 0.37}\n",
      "{'loss': 1.2511, 'grad_norm': 0.458984375, 'learning_rate': 0.0001214450671658538, 'epoch': 0.37}\n",
      "{'loss': 1.2968, 'grad_norm': 0.76171875, 'learning_rate': 0.00012143370478838994, 'epoch': 0.37}\n",
      "{'loss': 0.9547, 'grad_norm': 0.91015625, 'learning_rate': 0.00012142234241092607, 'epoch': 0.37}\n",
      "{'loss': 1.4027, 'grad_norm': 0.5859375, 'learning_rate': 0.00012141098003346221, 'epoch': 0.37}\n",
      "{'loss': 1.0982, 'grad_norm': 0.85546875, 'learning_rate': 0.00012139961765599835, 'epoch': 0.37}\n",
      "{'loss': 1.2201, 'grad_norm': 0.404296875, 'learning_rate': 0.00012138825527853447, 'epoch': 0.37}\n",
      "{'loss': 1.309, 'grad_norm': 0.8046875, 'learning_rate': 0.00012137689290107061, 'epoch': 0.37}\n",
      "{'loss': 1.0986, 'grad_norm': 0.7890625, 'learning_rate': 0.00012136553052360678, 'epoch': 0.37}\n",
      "{'loss': 1.3159, 'grad_norm': 0.478515625, 'learning_rate': 0.00012135416814614291, 'epoch': 0.37}\n",
      "{'loss': 1.191, 'grad_norm': 0.90625, 'learning_rate': 0.00012134280576867905, 'epoch': 0.37}\n",
      "{'loss': 1.2439, 'grad_norm': 0.609375, 'learning_rate': 0.00012133144339121518, 'epoch': 0.37}\n",
      "{'loss': 1.1542, 'grad_norm': 0.62109375, 'learning_rate': 0.00012132008101375131, 'epoch': 0.37}\n",
      "{'loss': 1.1122, 'grad_norm': 0.58203125, 'learning_rate': 0.00012130871863628745, 'epoch': 0.37}\n",
      "{'loss': 1.4035, 'grad_norm': 0.5234375, 'learning_rate': 0.00012129735625882359, 'epoch': 0.37}\n",
      "{'loss': 1.1664, 'grad_norm': 0.734375, 'learning_rate': 0.00012128599388135976, 'epoch': 0.37}\n",
      "{'loss': 1.2473, 'grad_norm': 0.453125, 'learning_rate': 0.00012127463150389588, 'epoch': 0.37}\n",
      "{'loss': 1.2369, 'grad_norm': 0.7265625, 'learning_rate': 0.00012126326912643202, 'epoch': 0.37}\n",
      "{'loss': 1.1042, 'grad_norm': 0.84375, 'learning_rate': 0.00012125190674896816, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3077, 'grad_norm': 0.65234375, 'learning_rate': 0.00012124054437150429, 'epoch': 0.37}\n",
      "{'loss': 1.2245, 'grad_norm': 0.83984375, 'learning_rate': 0.00012122918199404043, 'epoch': 0.37}\n",
      "{'loss': 1.2341, 'grad_norm': 0.427734375, 'learning_rate': 0.00012121781961657658, 'epoch': 0.37}\n",
      "{'loss': 1.1904, 'grad_norm': 0.63671875, 'learning_rate': 0.00012120645723911272, 'epoch': 0.37}\n",
      "{'loss': 1.167, 'grad_norm': 0.8359375, 'learning_rate': 0.00012119509486164886, 'epoch': 0.37}\n",
      "{'loss': 1.3213, 'grad_norm': 0.4609375, 'learning_rate': 0.000121183732484185, 'epoch': 0.37}\n",
      "{'loss': 1.2628, 'grad_norm': 0.61328125, 'learning_rate': 0.00012117237010672113, 'epoch': 0.37}\n",
      "{'loss': 1.1881, 'grad_norm': 0.53515625, 'learning_rate': 0.00012116100772925727, 'epoch': 0.37}\n",
      "{'loss': 1.13, 'grad_norm': 0.609375, 'learning_rate': 0.00012114964535179341, 'epoch': 0.37}\n",
      "{'loss': 1.1002, 'grad_norm': 0.65625, 'learning_rate': 0.00012113828297432956, 'epoch': 0.37}\n",
      "{'loss': 1.2915, 'grad_norm': 0.486328125, 'learning_rate': 0.0001211269205968657, 'epoch': 0.37}\n",
      "{'loss': 1.2212, 'grad_norm': 0.75, 'learning_rate': 0.00012111555821940184, 'epoch': 0.37}\n",
      "{'loss': 1.2433, 'grad_norm': 0.8515625, 'learning_rate': 0.00012110419584193797, 'epoch': 0.37}\n",
      "{'loss': 1.243, 'grad_norm': 0.6171875, 'learning_rate': 0.00012109283346447411, 'epoch': 0.37}\n",
      "{'loss': 1.1943, 'grad_norm': 1.1015625, 'learning_rate': 0.00012108147108701025, 'epoch': 0.37}\n",
      "{'loss': 1.3466, 'grad_norm': 0.74609375, 'learning_rate': 0.00012107010870954639, 'epoch': 0.37}\n",
      "{'loss': 1.1853, 'grad_norm': 0.62109375, 'learning_rate': 0.00012105874633208254, 'epoch': 0.37}\n",
      "{'loss': 1.1855, 'grad_norm': 0.443359375, 'learning_rate': 0.00012104738395461868, 'epoch': 0.37}\n",
      "{'loss': 1.2459, 'grad_norm': 0.53515625, 'learning_rate': 0.00012103602157715481, 'epoch': 0.37}\n",
      "{'loss': 1.0979, 'grad_norm': 0.8671875, 'learning_rate': 0.00012102465919969095, 'epoch': 0.37}\n",
      "{'loss': 1.3564, 'grad_norm': 0.5, 'learning_rate': 0.00012101329682222709, 'epoch': 0.37}\n",
      "{'loss': 1.1979, 'grad_norm': 0.6875, 'learning_rate': 0.00012100193444476321, 'epoch': 0.37}\n",
      "{'loss': 1.1472, 'grad_norm': 0.5390625, 'learning_rate': 0.00012099057206729935, 'epoch': 0.37}\n",
      "{'loss': 1.1427, 'grad_norm': 0.72265625, 'learning_rate': 0.00012097920968983552, 'epoch': 0.37}\n",
      "{'loss': 0.9877, 'grad_norm': 1.046875, 'learning_rate': 0.00012096784731237166, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.271, 'grad_norm': 0.69921875, 'learning_rate': 0.0001209564849349078, 'epoch': 0.37}\n",
      "{'loss': 1.2091, 'grad_norm': 0.62109375, 'learning_rate': 0.00012094512255744392, 'epoch': 0.37}\n",
      "{'loss': 1.1559, 'grad_norm': 0.41796875, 'learning_rate': 0.00012093376017998006, 'epoch': 0.37}\n",
      "{'loss': 1.2885, 'grad_norm': 0.69921875, 'learning_rate': 0.0001209223978025162, 'epoch': 0.37}\n",
      "{'loss': 1.0665, 'grad_norm': 0.7421875, 'learning_rate': 0.00012091103542505236, 'epoch': 0.37}\n",
      "{'loss': 1.3322, 'grad_norm': 0.53515625, 'learning_rate': 0.0001208996730475885, 'epoch': 0.37}\n",
      "{'loss': 1.2493, 'grad_norm': 0.62890625, 'learning_rate': 0.00012088831067012462, 'epoch': 0.37}\n",
      "{'loss': 1.3258, 'grad_norm': 0.50390625, 'learning_rate': 0.00012087694829266076, 'epoch': 0.37}\n",
      "{'loss': 1.2973, 'grad_norm': 0.78125, 'learning_rate': 0.0001208655859151969, 'epoch': 0.37}\n",
      "{'loss': 1.0858, 'grad_norm': 1.140625, 'learning_rate': 0.00012085422353773303, 'epoch': 0.37}\n",
      "{'loss': 1.2832, 'grad_norm': 0.51953125, 'learning_rate': 0.00012084286116026917, 'epoch': 0.37}\n",
      "{'loss': 1.2008, 'grad_norm': 0.85546875, 'learning_rate': 0.00012083149878280532, 'epoch': 0.37}\n",
      "{'loss': 1.1833, 'grad_norm': 0.46484375, 'learning_rate': 0.00012082013640534146, 'epoch': 0.37}\n",
      "{'loss': 1.1967, 'grad_norm': 0.65234375, 'learning_rate': 0.0001208087740278776, 'epoch': 0.37}\n",
      "{'loss': 1.1932, 'grad_norm': 0.76953125, 'learning_rate': 0.00012079741165041374, 'epoch': 0.37}\n",
      "{'loss': 1.1876, 'grad_norm': 0.8671875, 'learning_rate': 0.00012078604927294987, 'epoch': 0.37}\n",
      "{'loss': 1.1919, 'grad_norm': 0.7265625, 'learning_rate': 0.00012077468689548601, 'epoch': 0.37}\n",
      "{'loss': 1.1869, 'grad_norm': 0.419921875, 'learning_rate': 0.00012076332451802215, 'epoch': 0.37}\n",
      "{'loss': 1.2129, 'grad_norm': 0.68359375, 'learning_rate': 0.0001207519621405583, 'epoch': 0.37}\n",
      "{'loss': 1.0244, 'grad_norm': 0.421875, 'learning_rate': 0.00012074059976309444, 'epoch': 0.37}\n",
      "{'loss': 1.3328, 'grad_norm': 0.5078125, 'learning_rate': 0.00012072923738563058, 'epoch': 0.37}\n",
      "{'loss': 1.135, 'grad_norm': 0.68359375, 'learning_rate': 0.00012071787500816672, 'epoch': 0.37}\n",
      "{'loss': 1.3111, 'grad_norm': 0.46875, 'learning_rate': 0.00012070651263070285, 'epoch': 0.37}\n",
      "{'loss': 1.3167, 'grad_norm': 0.734375, 'learning_rate': 0.00012069515025323899, 'epoch': 0.37}\n",
      "{'loss': 1.0372, 'grad_norm': 1.0625, 'learning_rate': 0.00012068378787577513, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2841, 'grad_norm': 0.5234375, 'learning_rate': 0.00012067242549831128, 'epoch': 0.37}\n",
      "{'loss': 1.0709, 'grad_norm': 0.5234375, 'learning_rate': 0.00012066106312084742, 'epoch': 0.37}\n",
      "{'loss': 1.1009, 'grad_norm': 0.384765625, 'learning_rate': 0.00012064970074338356, 'epoch': 0.37}\n",
      "{'loss': 1.2032, 'grad_norm': 0.56640625, 'learning_rate': 0.0001206383383659197, 'epoch': 0.37}\n",
      "{'loss': 1.092, 'grad_norm': 0.9375, 'learning_rate': 0.00012062697598845583, 'epoch': 0.37}\n",
      "{'loss': 1.4852, 'grad_norm': 0.6640625, 'learning_rate': 0.00012061561361099196, 'epoch': 0.37}\n",
      "{'loss': 1.1266, 'grad_norm': 0.6953125, 'learning_rate': 0.0001206042512335281, 'epoch': 0.37}\n",
      "{'loss': 1.178, 'grad_norm': 0.4296875, 'learning_rate': 0.00012059288885606426, 'epoch': 0.37}\n",
      "{'loss': 1.2485, 'grad_norm': 0.451171875, 'learning_rate': 0.0001205815264786004, 'epoch': 0.37}\n",
      "{'loss': 0.9881, 'grad_norm': 0.8984375, 'learning_rate': 0.00012057016410113653, 'epoch': 0.37}\n",
      "{'loss': 1.3192, 'grad_norm': 0.4921875, 'learning_rate': 0.00012055880172367266, 'epoch': 0.37}\n",
      "{'loss': 1.2055, 'grad_norm': 0.6328125, 'learning_rate': 0.0001205474393462088, 'epoch': 0.37}\n",
      "{'loss': 1.2213, 'grad_norm': 0.458984375, 'learning_rate': 0.00012053607696874493, 'epoch': 0.37}\n",
      "{'loss': 1.2179, 'grad_norm': 0.7265625, 'learning_rate': 0.0001205247145912811, 'epoch': 0.37}\n",
      "{'loss': 1.1286, 'grad_norm': 0.76953125, 'learning_rate': 0.00012051335221381724, 'epoch': 0.37}\n",
      "{'loss': 1.2959, 'grad_norm': 0.50390625, 'learning_rate': 0.00012050198983635336, 'epoch': 0.37}\n",
      "{'loss': 1.184, 'grad_norm': 0.71875, 'learning_rate': 0.0001204906274588895, 'epoch': 0.37}\n",
      "{'loss': 1.208, 'grad_norm': 0.447265625, 'learning_rate': 0.00012047926508142564, 'epoch': 0.37}\n",
      "{'loss': 1.267, 'grad_norm': 0.5703125, 'learning_rate': 0.00012046790270396178, 'epoch': 0.37}\n",
      "{'loss': 1.1211, 'grad_norm': 1.0078125, 'learning_rate': 0.00012045654032649791, 'epoch': 0.37}\n",
      "{'loss': 1.2295, 'grad_norm': 0.5625, 'learning_rate': 0.00012044517794903406, 'epoch': 0.37}\n",
      "{'loss': 1.1233, 'grad_norm': 0.6328125, 'learning_rate': 0.0001204338155715702, 'epoch': 0.37}\n",
      "{'loss': 1.0887, 'grad_norm': 0.671875, 'learning_rate': 0.00012042245319410634, 'epoch': 0.37}\n",
      "{'loss': 1.2071, 'grad_norm': 0.51171875, 'learning_rate': 0.00012041109081664248, 'epoch': 0.37}\n",
      "{'loss': 1.254, 'grad_norm': 0.79296875, 'learning_rate': 0.00012039972843917862, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3184, 'grad_norm': 0.65234375, 'learning_rate': 0.00012038836606171475, 'epoch': 0.37}\n",
      "{'loss': 1.0386, 'grad_norm': 0.671875, 'learning_rate': 0.00012037700368425089, 'epoch': 0.37}\n",
      "{'loss': 1.0203, 'grad_norm': 0.57421875, 'learning_rate': 0.00012036564130678704, 'epoch': 0.37}\n",
      "{'loss': 1.2307, 'grad_norm': 0.58203125, 'learning_rate': 0.00012035427892932318, 'epoch': 0.37}\n",
      "{'loss': 1.1234, 'grad_norm': 0.93359375, 'learning_rate': 0.00012034291655185932, 'epoch': 0.37}\n",
      "{'loss': 1.2005, 'grad_norm': 0.478515625, 'learning_rate': 0.00012033155417439546, 'epoch': 0.37}\n",
      "{'loss': 1.1472, 'grad_norm': 0.74609375, 'learning_rate': 0.0001203201917969316, 'epoch': 0.37}\n",
      "{'loss': 1.155, 'grad_norm': 0.50390625, 'learning_rate': 0.00012030882941946773, 'epoch': 0.37}\n",
      "{'loss': 1.1694, 'grad_norm': 0.57421875, 'learning_rate': 0.00012029746704200387, 'epoch': 0.37}\n",
      "{'loss': 1.1502, 'grad_norm': 0.7109375, 'learning_rate': 0.00012028610466454002, 'epoch': 0.37}\n",
      "{'loss': 1.3301, 'grad_norm': 0.470703125, 'learning_rate': 0.00012027474228707616, 'epoch': 0.38}\n",
      "{'loss': 1.1739, 'grad_norm': 0.80859375, 'learning_rate': 0.0001202633799096123, 'epoch': 0.38}\n",
      "{'loss': 1.1711, 'grad_norm': 0.4921875, 'learning_rate': 0.00012025201753214844, 'epoch': 0.38}\n",
      "{'loss': 1.115, 'grad_norm': 0.68359375, 'learning_rate': 0.00012024065515468457, 'epoch': 0.38}\n",
      "{'loss': 1.1416, 'grad_norm': 0.83984375, 'learning_rate': 0.0001202292927772207, 'epoch': 0.38}\n",
      "{'loss': 1.463, 'grad_norm': 0.6171875, 'learning_rate': 0.00012021793039975686, 'epoch': 0.38}\n",
      "{'loss': 1.2141, 'grad_norm': 0.640625, 'learning_rate': 0.000120206568022293, 'epoch': 0.38}\n",
      "{'loss': 1.2621, 'grad_norm': 0.3515625, 'learning_rate': 0.00012019520564482914, 'epoch': 0.38}\n",
      "{'loss': 1.1024, 'grad_norm': 0.51171875, 'learning_rate': 0.00012018384326736528, 'epoch': 0.38}\n",
      "{'loss': 1.1145, 'grad_norm': 0.75390625, 'learning_rate': 0.0001201724808899014, 'epoch': 0.38}\n",
      "{'loss': 1.3811, 'grad_norm': 0.55078125, 'learning_rate': 0.00012016111851243754, 'epoch': 0.38}\n",
      "{'loss': 1.1668, 'grad_norm': 1.0234375, 'learning_rate': 0.00012014975613497368, 'epoch': 0.38}\n",
      "{'loss': 1.2927, 'grad_norm': 0.431640625, 'learning_rate': 0.00012013839375750984, 'epoch': 0.38}\n",
      "{'loss': 1.2309, 'grad_norm': 0.5234375, 'learning_rate': 0.00012012703138004598, 'epoch': 0.38}\n",
      "{'loss': 1.0565, 'grad_norm': 0.326171875, 'learning_rate': 0.0001201156690025821, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2137, 'grad_norm': 0.52734375, 'learning_rate': 0.00012010430662511824, 'epoch': 0.38}\n",
      "{'loss': 1.286, 'grad_norm': 0.7109375, 'learning_rate': 0.00012009294424765438, 'epoch': 0.38}\n",
      "{'loss': 1.3037, 'grad_norm': 0.48828125, 'learning_rate': 0.00012008158187019052, 'epoch': 0.38}\n",
      "{'loss': 1.2234, 'grad_norm': 0.625, 'learning_rate': 0.00012007021949272665, 'epoch': 0.38}\n",
      "{'loss': 1.0399, 'grad_norm': 0.32421875, 'learning_rate': 0.00012005885711526282, 'epoch': 0.38}\n",
      "{'loss': 1.294, 'grad_norm': 0.46875, 'learning_rate': 0.00012004749473779894, 'epoch': 0.38}\n",
      "{'loss': 1.186, 'grad_norm': 0.85546875, 'learning_rate': 0.00012003613236033508, 'epoch': 0.38}\n",
      "{'loss': 1.2815, 'grad_norm': 0.419921875, 'learning_rate': 0.00012002476998287122, 'epoch': 0.38}\n",
      "{'loss': 1.1742, 'grad_norm': 0.5703125, 'learning_rate': 0.00012001340760540736, 'epoch': 0.38}\n",
      "{'loss': 1.1371, 'grad_norm': 0.90625, 'learning_rate': 0.0001200020452279435, 'epoch': 0.38}\n",
      "{'loss': 1.2339, 'grad_norm': 0.6953125, 'learning_rate': 0.00011999068285047963, 'epoch': 0.38}\n",
      "{'loss': 1.2117, 'grad_norm': 0.9453125, 'learning_rate': 0.00011997932047301578, 'epoch': 0.38}\n",
      "{'loss': 1.2208, 'grad_norm': 0.45703125, 'learning_rate': 0.00011996795809555192, 'epoch': 0.38}\n",
      "{'loss': 1.3329, 'grad_norm': 0.51953125, 'learning_rate': 0.00011995659571808806, 'epoch': 0.38}\n",
      "{'loss': 1.0869, 'grad_norm': 0.57421875, 'learning_rate': 0.0001199452333406242, 'epoch': 0.38}\n",
      "{'loss': 1.2382, 'grad_norm': 0.5625, 'learning_rate': 0.00011993387096316034, 'epoch': 0.38}\n",
      "{'loss': 1.105, 'grad_norm': 1.0078125, 'learning_rate': 0.00011992250858569647, 'epoch': 0.38}\n",
      "{'loss': 1.2068, 'grad_norm': 0.421875, 'learning_rate': 0.00011991114620823261, 'epoch': 0.38}\n",
      "{'loss': 1.2553, 'grad_norm': 0.74609375, 'learning_rate': 0.00011989978383076876, 'epoch': 0.38}\n",
      "{'loss': 1.0639, 'grad_norm': 0.88671875, 'learning_rate': 0.0001198884214533049, 'epoch': 0.38}\n",
      "{'loss': 1.4822, 'grad_norm': 0.4921875, 'learning_rate': 0.00011987705907584104, 'epoch': 0.38}\n",
      "{'loss': 1.1362, 'grad_norm': 0.5234375, 'learning_rate': 0.00011986569669837718, 'epoch': 0.38}\n",
      "{'loss': 1.2104, 'grad_norm': 0.57421875, 'learning_rate': 0.00011985433432091331, 'epoch': 0.38}\n",
      "{'loss': 1.1836, 'grad_norm': 0.57421875, 'learning_rate': 0.00011984297194344944, 'epoch': 0.38}\n",
      "{'loss': 1.0837, 'grad_norm': 0.9765625, 'learning_rate': 0.0001198316095659856, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3237, 'grad_norm': 0.60546875, 'learning_rate': 0.00011982024718852174, 'epoch': 0.38}\n",
      "{'loss': 1.1345, 'grad_norm': 0.78515625, 'learning_rate': 0.00011980888481105788, 'epoch': 0.38}\n",
      "{'loss': 1.2682, 'grad_norm': 0.455078125, 'learning_rate': 0.00011979752243359402, 'epoch': 0.38}\n",
      "{'loss': 1.1959, 'grad_norm': 0.74609375, 'learning_rate': 0.00011978616005613014, 'epoch': 0.38}\n",
      "{'loss': 1.0733, 'grad_norm': 0.79296875, 'learning_rate': 0.00011977479767866628, 'epoch': 0.38}\n",
      "{'loss': 1.3197, 'grad_norm': 0.69140625, 'learning_rate': 0.00011976343530120242, 'epoch': 0.38}\n",
      "{'loss': 1.1132, 'grad_norm': 0.98046875, 'learning_rate': 0.00011975207292373858, 'epoch': 0.38}\n",
      "{'loss': 1.1919, 'grad_norm': 0.45703125, 'learning_rate': 0.00011974071054627472, 'epoch': 0.38}\n",
      "{'loss': 1.1815, 'grad_norm': 0.72265625, 'learning_rate': 0.00011972934816881084, 'epoch': 0.38}\n",
      "{'loss': 1.0967, 'grad_norm': 0.96484375, 'learning_rate': 0.00011971798579134698, 'epoch': 0.38}\n",
      "{'loss': 1.3237, 'grad_norm': 0.419921875, 'learning_rate': 0.00011970662341388312, 'epoch': 0.38}\n",
      "{'loss': 1.1231, 'grad_norm': 0.74609375, 'learning_rate': 0.00011969526103641926, 'epoch': 0.38}\n",
      "{'loss': 1.2573, 'grad_norm': 0.40625, 'learning_rate': 0.0001196838986589554, 'epoch': 0.38}\n",
      "{'loss': 1.3326, 'grad_norm': 0.48828125, 'learning_rate': 0.00011967253628149156, 'epoch': 0.38}\n",
      "{'loss': 1.0643, 'grad_norm': 1.1484375, 'learning_rate': 0.00011966117390402769, 'epoch': 0.38}\n",
      "{'loss': 1.2172, 'grad_norm': 0.478515625, 'learning_rate': 0.00011964981152656382, 'epoch': 0.38}\n",
      "{'loss': 1.2706, 'grad_norm': 0.91015625, 'learning_rate': 0.00011963844914909996, 'epoch': 0.38}\n",
      "{'loss': 1.2055, 'grad_norm': 0.51171875, 'learning_rate': 0.0001196270867716361, 'epoch': 0.38}\n",
      "{'loss': 1.2826, 'grad_norm': 0.427734375, 'learning_rate': 0.00011961572439417224, 'epoch': 0.38}\n",
      "{'loss': 1.0676, 'grad_norm': 0.56640625, 'learning_rate': 0.00011960436201670837, 'epoch': 0.38}\n",
      "{'loss': 1.2969, 'grad_norm': 0.51953125, 'learning_rate': 0.00011959299963924453, 'epoch': 0.38}\n",
      "{'loss': 1.1913, 'grad_norm': 0.63671875, 'learning_rate': 0.00011958163726178066, 'epoch': 0.38}\n",
      "{'loss': 1.1299, 'grad_norm': 0.50390625, 'learning_rate': 0.0001195702748843168, 'epoch': 0.38}\n",
      "{'loss': 1.1683, 'grad_norm': 0.5859375, 'learning_rate': 0.00011955891250685294, 'epoch': 0.38}\n",
      "{'loss': 1.0477, 'grad_norm': 0.828125, 'learning_rate': 0.00011954755012938908, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3794, 'grad_norm': 0.482421875, 'learning_rate': 0.00011953618775192522, 'epoch': 0.38}\n",
      "{'loss': 1.1126, 'grad_norm': 0.7890625, 'learning_rate': 0.00011952482537446137, 'epoch': 0.38}\n",
      "{'loss': 1.1088, 'grad_norm': 0.439453125, 'learning_rate': 0.0001195134629969975, 'epoch': 0.38}\n",
      "{'loss': 1.3368, 'grad_norm': 0.48828125, 'learning_rate': 0.00011950210061953364, 'epoch': 0.38}\n",
      "{'loss': 1.1872, 'grad_norm': 1.2578125, 'learning_rate': 0.00011949073824206978, 'epoch': 0.38}\n",
      "{'loss': 1.314, 'grad_norm': 0.69140625, 'learning_rate': 0.00011947937586460592, 'epoch': 0.38}\n",
      "{'loss': 1.1655, 'grad_norm': 0.56640625, 'learning_rate': 0.00011946801348714206, 'epoch': 0.38}\n",
      "{'loss': 1.3492, 'grad_norm': 0.458984375, 'learning_rate': 0.00011945665110967818, 'epoch': 0.38}\n",
      "{'loss': 1.2408, 'grad_norm': 0.5390625, 'learning_rate': 0.00011944528873221434, 'epoch': 0.38}\n",
      "{'loss': 1.1254, 'grad_norm': 0.62890625, 'learning_rate': 0.00011943392635475048, 'epoch': 0.38}\n",
      "{'loss': 1.3043, 'grad_norm': 0.53125, 'learning_rate': 0.00011942256397728662, 'epoch': 0.38}\n",
      "{'loss': 1.2131, 'grad_norm': 0.828125, 'learning_rate': 0.00011941120159982276, 'epoch': 0.38}\n",
      "{'loss': 1.2722, 'grad_norm': 0.427734375, 'learning_rate': 0.00011939983922235888, 'epoch': 0.38}\n",
      "{'loss': 1.2204, 'grad_norm': 0.5078125, 'learning_rate': 0.00011938847684489502, 'epoch': 0.38}\n",
      "{'loss': 1.1494, 'grad_norm': 1.0625, 'learning_rate': 0.00011937711446743116, 'epoch': 0.38}\n",
      "{'loss': 1.4504, 'grad_norm': 0.3984375, 'learning_rate': 0.00011936575208996732, 'epoch': 0.38}\n",
      "{'loss': 1.1178, 'grad_norm': 1.046875, 'learning_rate': 0.00011935438971250346, 'epoch': 0.38}\n",
      "{'loss': 1.3264, 'grad_norm': 0.4296875, 'learning_rate': 0.00011934302733503959, 'epoch': 0.38}\n",
      "{'loss': 1.0782, 'grad_norm': 0.447265625, 'learning_rate': 0.00011933166495757572, 'epoch': 0.38}\n",
      "{'loss': 0.9943, 'grad_norm': 1.015625, 'learning_rate': 0.00011932030258011186, 'epoch': 0.38}\n",
      "{'loss': 1.2329, 'grad_norm': 0.5078125, 'learning_rate': 0.000119308940202648, 'epoch': 0.38}\n",
      "{'loss': 0.9895, 'grad_norm': 0.65234375, 'learning_rate': 0.00011929757782518414, 'epoch': 0.38}\n",
      "{'loss': 1.2359, 'grad_norm': 0.55078125, 'learning_rate': 0.0001192862154477203, 'epoch': 0.38}\n",
      "{'loss': 1.2343, 'grad_norm': 0.69140625, 'learning_rate': 0.00011927485307025643, 'epoch': 0.38}\n",
      "{'loss': 1.1106, 'grad_norm': 1.0390625, 'learning_rate': 0.00011926349069279256, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3651, 'grad_norm': 0.58203125, 'learning_rate': 0.0001192521283153287, 'epoch': 0.38}\n",
      "{'loss': 1.2511, 'grad_norm': 0.67578125, 'learning_rate': 0.00011924076593786484, 'epoch': 0.38}\n",
      "{'loss': 1.2211, 'grad_norm': 0.46875, 'learning_rate': 0.00011922940356040098, 'epoch': 0.38}\n",
      "{'loss': 1.2419, 'grad_norm': 0.474609375, 'learning_rate': 0.00011921804118293712, 'epoch': 0.38}\n",
      "{'loss': 1.1549, 'grad_norm': 0.578125, 'learning_rate': 0.00011920667880547327, 'epoch': 0.38}\n",
      "{'loss': 1.3001, 'grad_norm': 0.416015625, 'learning_rate': 0.0001191953164280094, 'epoch': 0.38}\n",
      "{'loss': 1.2694, 'grad_norm': 1.0078125, 'learning_rate': 0.00011918395405054554, 'epoch': 0.38}\n",
      "{'loss': 1.0933, 'grad_norm': 0.443359375, 'learning_rate': 0.00011917259167308168, 'epoch': 0.38}\n",
      "{'loss': 1.1512, 'grad_norm': 0.59375, 'learning_rate': 0.00011916122929561782, 'epoch': 0.38}\n",
      "{'loss': 1.1132, 'grad_norm': 0.87109375, 'learning_rate': 0.00011914986691815396, 'epoch': 0.38}\n",
      "{'loss': 1.4066, 'grad_norm': 0.6796875, 'learning_rate': 0.00011913850454069011, 'epoch': 0.38}\n",
      "{'loss': 1.1058, 'grad_norm': 0.625, 'learning_rate': 0.00011912714216322625, 'epoch': 0.38}\n",
      "{'loss': 1.3532, 'grad_norm': 0.396484375, 'learning_rate': 0.00011911577978576238, 'epoch': 0.38}\n",
      "{'loss': 1.3304, 'grad_norm': 0.462890625, 'learning_rate': 0.00011910441740829852, 'epoch': 0.38}\n",
      "{'loss': 1.102, 'grad_norm': 0.46875, 'learning_rate': 0.00011909305503083466, 'epoch': 0.38}\n",
      "{'loss': 1.3117, 'grad_norm': 0.58203125, 'learning_rate': 0.0001190816926533708, 'epoch': 0.38}\n",
      "{'loss': 1.1219, 'grad_norm': 0.62890625, 'learning_rate': 0.00011907033027590692, 'epoch': 0.38}\n",
      "{'loss': 1.1614, 'grad_norm': 0.4609375, 'learning_rate': 0.00011905896789844309, 'epoch': 0.38}\n",
      "{'loss': 1.3063, 'grad_norm': 0.52734375, 'learning_rate': 0.00011904760552097922, 'epoch': 0.38}\n",
      "{'loss': 0.9735, 'grad_norm': 0.318359375, 'learning_rate': 0.00011903624314351536, 'epoch': 0.38}\n",
      "{'loss': 1.3419, 'grad_norm': 0.59765625, 'learning_rate': 0.0001190248807660515, 'epoch': 0.38}\n",
      "{'loss': 1.0446, 'grad_norm': 0.5703125, 'learning_rate': 0.00011901351838858762, 'epoch': 0.38}\n",
      "{'loss': 1.2464, 'grad_norm': 0.46875, 'learning_rate': 0.00011900215601112376, 'epoch': 0.38}\n",
      "{'loss': 1.252, 'grad_norm': 0.72265625, 'learning_rate': 0.0001189907936336599, 'epoch': 0.38}\n",
      "{'loss': 1.0898, 'grad_norm': 0.96875, 'learning_rate': 0.00011897943125619606, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3168, 'grad_norm': 0.5625, 'learning_rate': 0.0001189680688787322, 'epoch': 0.38}\n",
      "{'loss': 1.3133, 'grad_norm': 0.6171875, 'learning_rate': 0.00011895670650126833, 'epoch': 0.38}\n",
      "{'loss': 1.1891, 'grad_norm': 0.470703125, 'learning_rate': 0.00011894534412380446, 'epoch': 0.38}\n",
      "{'loss': 1.2425, 'grad_norm': 0.84765625, 'learning_rate': 0.0001189339817463406, 'epoch': 0.38}\n",
      "{'loss': 1.1754, 'grad_norm': 0.796875, 'learning_rate': 0.00011892261936887674, 'epoch': 0.38}\n",
      "{'loss': 1.3653, 'grad_norm': 0.57421875, 'learning_rate': 0.00011891125699141288, 'epoch': 0.38}\n",
      "{'loss': 1.2207, 'grad_norm': 0.73828125, 'learning_rate': 0.00011889989461394904, 'epoch': 0.38}\n",
      "{'loss': 1.207, 'grad_norm': 0.48828125, 'learning_rate': 0.00011888853223648517, 'epoch': 0.38}\n",
      "{'loss': 1.264, 'grad_norm': 0.78125, 'learning_rate': 0.0001188771698590213, 'epoch': 0.38}\n",
      "{'loss': 1.0368, 'grad_norm': 1.140625, 'learning_rate': 0.00011886580748155744, 'epoch': 0.38}\n",
      "{'loss': 1.3466, 'grad_norm': 0.609375, 'learning_rate': 0.00011885444510409358, 'epoch': 0.38}\n",
      "{'loss': 1.1284, 'grad_norm': 1.0859375, 'learning_rate': 0.00011884308272662972, 'epoch': 0.38}\n",
      "{'loss': 1.2414, 'grad_norm': 0.419921875, 'learning_rate': 0.00011883172034916587, 'epoch': 0.38}\n",
      "{'loss': 1.3335, 'grad_norm': 0.60546875, 'learning_rate': 0.00011882035797170201, 'epoch': 0.38}\n",
      "{'loss': 1.0334, 'grad_norm': 1.046875, 'learning_rate': 0.00011880899559423815, 'epoch': 0.38}\n",
      "{'loss': 1.2571, 'grad_norm': 0.56640625, 'learning_rate': 0.00011879763321677428, 'epoch': 0.38}\n",
      "{'loss': 1.2609, 'grad_norm': 0.6484375, 'learning_rate': 0.00011878627083931042, 'epoch': 0.38}\n",
      "{'loss': 1.3299, 'grad_norm': 0.4140625, 'learning_rate': 0.00011877490846184656, 'epoch': 0.38}\n",
      "{'loss': 1.1937, 'grad_norm': 0.6171875, 'learning_rate': 0.0001187635460843827, 'epoch': 0.38}\n",
      "{'loss': 1.0684, 'grad_norm': 1.0, 'learning_rate': 0.00011875218370691885, 'epoch': 0.38}\n",
      "{'loss': 1.3066, 'grad_norm': 0.498046875, 'learning_rate': 0.00011874082132945499, 'epoch': 0.38}\n",
      "{'loss': 1.175, 'grad_norm': 0.83984375, 'learning_rate': 0.00011872945895199112, 'epoch': 0.38}\n",
      "{'loss': 1.2564, 'grad_norm': 0.4296875, 'learning_rate': 0.00011871809657452726, 'epoch': 0.38}\n",
      "{'loss': 1.2057, 'grad_norm': 0.54296875, 'learning_rate': 0.0001187067341970634, 'epoch': 0.38}\n",
      "{'loss': 1.1589, 'grad_norm': 0.9765625, 'learning_rate': 0.00011869537181959954, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2033, 'grad_norm': 0.486328125, 'learning_rate': 0.00011868400944213566, 'epoch': 0.38}\n",
      "{'loss': 1.0113, 'grad_norm': 0.85546875, 'learning_rate': 0.00011867264706467183, 'epoch': 0.38}\n",
      "{'loss': 1.1805, 'grad_norm': 0.51171875, 'learning_rate': 0.00011866128468720797, 'epoch': 0.38}\n",
      "{'loss': 1.2204, 'grad_norm': 0.5, 'learning_rate': 0.0001186499223097441, 'epoch': 0.38}\n",
      "{'loss': 1.1033, 'grad_norm': 0.87890625, 'learning_rate': 0.00011863855993228024, 'epoch': 0.38}\n",
      "{'loss': 1.3977, 'grad_norm': 0.451171875, 'learning_rate': 0.00011862719755481637, 'epoch': 0.38}\n",
      "{'loss': 1.2032, 'grad_norm': 0.9609375, 'learning_rate': 0.0001186158351773525, 'epoch': 0.38}\n",
      "{'loss': 1.1585, 'grad_norm': 0.46484375, 'learning_rate': 0.00011860447279988864, 'epoch': 0.38}\n",
      "{'loss': 1.2337, 'grad_norm': 0.65625, 'learning_rate': 0.0001185931104224248, 'epoch': 0.38}\n",
      "{'loss': 1.0872, 'grad_norm': 0.73828125, 'learning_rate': 0.00011858174804496094, 'epoch': 0.38}\n",
      "{'loss': 1.3937, 'grad_norm': 0.419921875, 'learning_rate': 0.00011857038566749707, 'epoch': 0.38}\n",
      "{'loss': 1.1644, 'grad_norm': 0.7890625, 'learning_rate': 0.0001185590232900332, 'epoch': 0.38}\n",
      "{'loss': 1.202, 'grad_norm': 0.5234375, 'learning_rate': 0.00011854766091256934, 'epoch': 0.38}\n",
      "{'loss': 1.1385, 'grad_norm': 0.69140625, 'learning_rate': 0.00011853629853510548, 'epoch': 0.38}\n",
      "{'loss': 1.0745, 'grad_norm': 0.890625, 'learning_rate': 0.00011852493615764162, 'epoch': 0.38}\n",
      "{'loss': 1.3292, 'grad_norm': 0.4921875, 'learning_rate': 0.00011851357378017778, 'epoch': 0.38}\n",
      "{'loss': 1.1449, 'grad_norm': 0.99609375, 'learning_rate': 0.00011850221140271391, 'epoch': 0.38}\n",
      "{'loss': 1.1162, 'grad_norm': 0.4375, 'learning_rate': 0.00011849084902525005, 'epoch': 0.38}\n",
      "{'loss': 1.3015, 'grad_norm': 0.6328125, 'learning_rate': 0.00011847948664778618, 'epoch': 0.38}\n",
      "{'loss': 1.177, 'grad_norm': 0.85546875, 'learning_rate': 0.00011846812427032232, 'epoch': 0.38}\n",
      "{'loss': 1.1813, 'grad_norm': 0.4453125, 'learning_rate': 0.00011845676189285846, 'epoch': 0.38}\n",
      "{'loss': 1.0073, 'grad_norm': 0.486328125, 'learning_rate': 0.00011844539951539461, 'epoch': 0.38}\n",
      "{'loss': 1.2726, 'grad_norm': 0.5078125, 'learning_rate': 0.00011843403713793075, 'epoch': 0.38}\n",
      "{'loss': 1.3683, 'grad_norm': 0.77734375, 'learning_rate': 0.00011842267476046689, 'epoch': 0.38}\n",
      "{'loss': 1.0358, 'grad_norm': 0.75390625, 'learning_rate': 0.00011841131238300303, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4161, 'grad_norm': 0.50390625, 'learning_rate': 0.00011839995000553916, 'epoch': 0.38}\n",
      "{'loss': 1.2314, 'grad_norm': 0.63671875, 'learning_rate': 0.0001183885876280753, 'epoch': 0.38}\n",
      "{'loss': 1.2491, 'grad_norm': 0.4609375, 'learning_rate': 0.00011837722525061144, 'epoch': 0.38}\n",
      "{'loss': 1.2043, 'grad_norm': 0.5859375, 'learning_rate': 0.00011836586287314759, 'epoch': 0.38}\n",
      "{'loss': 1.0286, 'grad_norm': 0.9765625, 'learning_rate': 0.00011835450049568373, 'epoch': 0.38}\n",
      "{'loss': 1.4109, 'grad_norm': 0.498046875, 'learning_rate': 0.00011834313811821987, 'epoch': 0.38}\n",
      "{'loss': 1.0835, 'grad_norm': 0.98046875, 'learning_rate': 0.000118331775740756, 'epoch': 0.38}\n",
      "{'loss': 1.2299, 'grad_norm': 0.5078125, 'learning_rate': 0.00011832041336329214, 'epoch': 0.38}\n",
      "{'loss': 1.4122, 'grad_norm': 0.58984375, 'learning_rate': 0.00011830905098582828, 'epoch': 0.38}\n",
      "{'loss': 1.1407, 'grad_norm': 0.421875, 'learning_rate': 0.0001182976886083644, 'epoch': 0.38}\n",
      "{'loss': 1.3135, 'grad_norm': 0.5078125, 'learning_rate': 0.00011828632623090057, 'epoch': 0.38}\n",
      "{'loss': 1.1642, 'grad_norm': 0.6640625, 'learning_rate': 0.0001182749638534367, 'epoch': 0.38}\n",
      "{'loss': 1.1632, 'grad_norm': 0.431640625, 'learning_rate': 0.00011826360147597284, 'epoch': 0.38}\n",
      "{'loss': 1.4007, 'grad_norm': 0.60546875, 'learning_rate': 0.00011825223909850898, 'epoch': 0.38}\n",
      "{'loss': 1.0165, 'grad_norm': 0.68359375, 'learning_rate': 0.0001182408767210451, 'epoch': 0.38}\n",
      "{'loss': 1.2539, 'grad_norm': 0.68359375, 'learning_rate': 0.00011822951434358124, 'epoch': 0.38}\n",
      "{'loss': 1.1097, 'grad_norm': 0.55859375, 'learning_rate': 0.00011821815196611738, 'epoch': 0.38}\n",
      "{'loss': 1.1186, 'grad_norm': 0.443359375, 'learning_rate': 0.00011820678958865355, 'epoch': 0.38}\n",
      "{'loss': 1.229, 'grad_norm': 0.78125, 'learning_rate': 0.00011819542721118969, 'epoch': 0.38}\n",
      "{'loss': 0.9752, 'grad_norm': 0.70703125, 'learning_rate': 0.00011818406483372581, 'epoch': 0.38}\n",
      "{'loss': 1.3293, 'grad_norm': 0.515625, 'learning_rate': 0.00011817270245626195, 'epoch': 0.38}\n",
      "{'loss': 1.1589, 'grad_norm': 1.0625, 'learning_rate': 0.00011816134007879809, 'epoch': 0.38}\n",
      "{'loss': 1.1339, 'grad_norm': 0.4140625, 'learning_rate': 0.00011814997770133422, 'epoch': 0.38}\n",
      "{'loss': 1.3596, 'grad_norm': 0.54296875, 'learning_rate': 0.00011813861532387039, 'epoch': 0.38}\n",
      "{'loss': 1.1173, 'grad_norm': 1.0, 'learning_rate': 0.00011812725294640653, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3086, 'grad_norm': 0.7578125, 'learning_rate': 0.00011811589056894265, 'epoch': 0.38}\n",
      "{'loss': 1.1118, 'grad_norm': 0.73828125, 'learning_rate': 0.00011810452819147879, 'epoch': 0.38}\n",
      "{'loss': 1.2836, 'grad_norm': 0.412109375, 'learning_rate': 0.00011809316581401493, 'epoch': 0.38}\n",
      "{'loss': 1.3111, 'grad_norm': 0.6171875, 'learning_rate': 0.00011808180343655106, 'epoch': 0.38}\n",
      "{'loss': 1.0236, 'grad_norm': 0.6484375, 'learning_rate': 0.0001180704410590872, 'epoch': 0.38}\n",
      "{'loss': 1.3663, 'grad_norm': 0.490234375, 'learning_rate': 0.00011805907868162335, 'epoch': 0.38}\n",
      "{'loss': 1.2706, 'grad_norm': 0.82421875, 'learning_rate': 0.00011804771630415949, 'epoch': 0.38}\n",
      "{'loss': 1.2303, 'grad_norm': 0.55859375, 'learning_rate': 0.00011803635392669563, 'epoch': 0.38}\n",
      "{'loss': 1.197, 'grad_norm': 0.61328125, 'learning_rate': 0.00011802499154923177, 'epoch': 0.38}\n",
      "{'loss': 1.1551, 'grad_norm': 0.51953125, 'learning_rate': 0.0001180136291717679, 'epoch': 0.38}\n",
      "{'loss': 1.4184, 'grad_norm': 0.6015625, 'learning_rate': 0.00011800226679430404, 'epoch': 0.38}\n",
      "{'loss': 1.2233, 'grad_norm': 0.7265625, 'learning_rate': 0.00011799090441684018, 'epoch': 0.38}\n",
      "{'loss': 1.2054, 'grad_norm': 0.3828125, 'learning_rate': 0.00011797954203937633, 'epoch': 0.39}\n",
      "{'loss': 1.2341, 'grad_norm': 0.61328125, 'learning_rate': 0.00011796817966191247, 'epoch': 0.39}\n",
      "{'loss': 1.0922, 'grad_norm': 0.59765625, 'learning_rate': 0.00011795681728444861, 'epoch': 0.39}\n",
      "{'loss': 1.28, 'grad_norm': 0.5625, 'learning_rate': 0.00011794545490698475, 'epoch': 0.39}\n",
      "{'loss': 1.0848, 'grad_norm': 0.546875, 'learning_rate': 0.00011793409252952088, 'epoch': 0.39}\n",
      "{'loss': 1.2673, 'grad_norm': 0.40625, 'learning_rate': 0.00011792273015205702, 'epoch': 0.39}\n",
      "{'loss': 1.3089, 'grad_norm': 0.470703125, 'learning_rate': 0.00011791136777459315, 'epoch': 0.39}\n",
      "{'loss': 1.0361, 'grad_norm': 0.546875, 'learning_rate': 0.00011790000539712931, 'epoch': 0.39}\n",
      "{'loss': 1.2902, 'grad_norm': 0.76171875, 'learning_rate': 0.00011788864301966545, 'epoch': 0.39}\n",
      "{'loss': 1.2094, 'grad_norm': 0.57421875, 'learning_rate': 0.00011787728064220159, 'epoch': 0.39}\n",
      "{'loss': 1.1752, 'grad_norm': 0.484375, 'learning_rate': 0.00011786591826473772, 'epoch': 0.39}\n",
      "{'loss': 1.3312, 'grad_norm': 0.6328125, 'learning_rate': 0.00011785455588727385, 'epoch': 0.39}\n",
      "{'loss': 0.9845, 'grad_norm': 0.38671875, 'learning_rate': 0.00011784319350980999, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3416, 'grad_norm': 0.47265625, 'learning_rate': 0.00011783183113234615, 'epoch': 0.39}\n",
      "{'loss': 1.1393, 'grad_norm': 0.75390625, 'learning_rate': 0.00011782046875488229, 'epoch': 0.39}\n",
      "{'loss': 1.3839, 'grad_norm': 0.451171875, 'learning_rate': 0.00011780910637741843, 'epoch': 0.39}\n",
      "{'loss': 1.1163, 'grad_norm': 0.67578125, 'learning_rate': 0.00011779774399995455, 'epoch': 0.39}\n",
      "{'loss': 1.1439, 'grad_norm': 1.0859375, 'learning_rate': 0.00011778638162249069, 'epoch': 0.39}\n",
      "{'loss': 1.3291, 'grad_norm': 0.5234375, 'learning_rate': 0.00011777501924502683, 'epoch': 0.39}\n",
      "{'loss': 1.2136, 'grad_norm': 0.97265625, 'learning_rate': 0.00011776365686756296, 'epoch': 0.39}\n",
      "{'loss': 1.2403, 'grad_norm': 0.427734375, 'learning_rate': 0.00011775229449009913, 'epoch': 0.39}\n",
      "{'loss': 1.1602, 'grad_norm': 0.9765625, 'learning_rate': 0.00011774093211263527, 'epoch': 0.39}\n",
      "{'loss': 1.0848, 'grad_norm': 1.0234375, 'learning_rate': 0.00011772956973517139, 'epoch': 0.39}\n",
      "{'loss': 1.389, 'grad_norm': 0.44921875, 'learning_rate': 0.00011771820735770753, 'epoch': 0.39}\n",
      "{'loss': 1.0358, 'grad_norm': 0.72265625, 'learning_rate': 0.00011770684498024367, 'epoch': 0.39}\n",
      "{'loss': 1.2054, 'grad_norm': 0.56640625, 'learning_rate': 0.0001176954826027798, 'epoch': 0.39}\n",
      "{'loss': 1.2873, 'grad_norm': 0.466796875, 'learning_rate': 0.00011768412022531594, 'epoch': 0.39}\n",
      "{'loss': 1.1356, 'grad_norm': 1.109375, 'learning_rate': 0.0001176727578478521, 'epoch': 0.39}\n",
      "{'loss': 1.1699, 'grad_norm': 0.44921875, 'learning_rate': 0.00011766139547038823, 'epoch': 0.39}\n",
      "{'loss': 1.1417, 'grad_norm': 0.57421875, 'learning_rate': 0.00011765003309292437, 'epoch': 0.39}\n",
      "{'loss': 1.239, 'grad_norm': 0.49609375, 'learning_rate': 0.00011763867071546051, 'epoch': 0.39}\n",
      "{'loss': 1.202, 'grad_norm': 0.4609375, 'learning_rate': 0.00011762730833799665, 'epoch': 0.39}\n",
      "{'loss': 1.0667, 'grad_norm': 0.6015625, 'learning_rate': 0.00011761594596053278, 'epoch': 0.39}\n",
      "{'loss': 1.1717, 'grad_norm': 0.91015625, 'learning_rate': 0.00011760458358306892, 'epoch': 0.39}\n",
      "{'loss': 1.2075, 'grad_norm': 0.5859375, 'learning_rate': 0.00011759322120560507, 'epoch': 0.39}\n",
      "{'loss': 1.2838, 'grad_norm': 0.62109375, 'learning_rate': 0.00011758185882814121, 'epoch': 0.39}\n",
      "{'loss': 1.2649, 'grad_norm': 0.478515625, 'learning_rate': 0.00011757049645067735, 'epoch': 0.39}\n",
      "{'loss': 1.1432, 'grad_norm': 0.703125, 'learning_rate': 0.00011755913407321349, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.427, 'grad_norm': 0.46875, 'learning_rate': 0.00011754777169574962, 'epoch': 0.39}\n",
      "{'loss': 1.118, 'grad_norm': 0.60546875, 'learning_rate': 0.00011753640931828576, 'epoch': 0.39}\n",
      "{'loss': 1.3162, 'grad_norm': 0.43359375, 'learning_rate': 0.00011752504694082189, 'epoch': 0.39}\n",
      "{'loss': 1.3521, 'grad_norm': 0.5390625, 'learning_rate': 0.00011751368456335805, 'epoch': 0.39}\n",
      "{'loss': 1.1427, 'grad_norm': 0.8203125, 'learning_rate': 0.00011750232218589419, 'epoch': 0.39}\n",
      "{'loss': 1.2609, 'grad_norm': 0.5703125, 'learning_rate': 0.00011749095980843033, 'epoch': 0.39}\n",
      "{'loss': 1.187, 'grad_norm': 0.6640625, 'learning_rate': 0.00011747959743096646, 'epoch': 0.39}\n",
      "{'loss': 1.1635, 'grad_norm': 0.61328125, 'learning_rate': 0.00011746823505350259, 'epoch': 0.39}\n",
      "{'loss': 1.1797, 'grad_norm': 0.5859375, 'learning_rate': 0.00011745687267603873, 'epoch': 0.39}\n",
      "{'loss': 1.1524, 'grad_norm': 1.03125, 'learning_rate': 0.00011744551029857489, 'epoch': 0.39}\n",
      "{'loss': 1.2942, 'grad_norm': 0.5703125, 'learning_rate': 0.00011743414792111103, 'epoch': 0.39}\n",
      "{'loss': 1.251, 'grad_norm': 0.71484375, 'learning_rate': 0.00011742278554364717, 'epoch': 0.39}\n",
      "{'loss': 1.1263, 'grad_norm': 0.38671875, 'learning_rate': 0.00011741142316618329, 'epoch': 0.39}\n",
      "{'loss': 1.1872, 'grad_norm': 0.53515625, 'learning_rate': 0.00011740006078871943, 'epoch': 0.39}\n",
      "{'loss': 1.0251, 'grad_norm': 1.0625, 'learning_rate': 0.00011738869841125557, 'epoch': 0.39}\n",
      "{'loss': 1.2837, 'grad_norm': 0.53515625, 'learning_rate': 0.0001173773360337917, 'epoch': 0.39}\n",
      "{'loss': 1.1681, 'grad_norm': 0.62890625, 'learning_rate': 0.00011736597365632787, 'epoch': 0.39}\n",
      "{'loss': 1.1789, 'grad_norm': 0.421875, 'learning_rate': 0.00011735461127886401, 'epoch': 0.39}\n",
      "{'loss': 1.328, 'grad_norm': 0.65234375, 'learning_rate': 0.00011734324890140013, 'epoch': 0.39}\n",
      "{'loss': 1.2185, 'grad_norm': 0.7734375, 'learning_rate': 0.00011733188652393627, 'epoch': 0.39}\n",
      "{'loss': 1.3009, 'grad_norm': 0.66015625, 'learning_rate': 0.00011732052414647241, 'epoch': 0.39}\n",
      "{'loss': 1.0757, 'grad_norm': 0.69921875, 'learning_rate': 0.00011730916176900855, 'epoch': 0.39}\n",
      "{'loss': 1.1951, 'grad_norm': 0.40625, 'learning_rate': 0.00011729779939154468, 'epoch': 0.39}\n",
      "{'loss': 1.3489, 'grad_norm': 0.5078125, 'learning_rate': 0.00011728643701408084, 'epoch': 0.39}\n",
      "{'loss': 1.0503, 'grad_norm': 0.40625, 'learning_rate': 0.00011727507463661697, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2354, 'grad_norm': 0.4765625, 'learning_rate': 0.00011726371225915311, 'epoch': 0.39}\n",
      "{'loss': 1.1567, 'grad_norm': 0.7265625, 'learning_rate': 0.00011725234988168925, 'epoch': 0.39}\n",
      "{'loss': 1.1908, 'grad_norm': 0.5, 'learning_rate': 0.00011724098750422539, 'epoch': 0.39}\n",
      "{'loss': 1.4158, 'grad_norm': 0.69140625, 'learning_rate': 0.00011722962512676152, 'epoch': 0.39}\n",
      "{'loss': 1.0196, 'grad_norm': 1.2734375, 'learning_rate': 0.00011721826274929766, 'epoch': 0.39}\n",
      "{'loss': 1.3936, 'grad_norm': 0.51953125, 'learning_rate': 0.00011720690037183381, 'epoch': 0.39}\n",
      "{'loss': 1.1453, 'grad_norm': 0.69140625, 'learning_rate': 0.00011719553799436995, 'epoch': 0.39}\n",
      "{'loss': 1.3724, 'grad_norm': 0.453125, 'learning_rate': 0.00011718417561690609, 'epoch': 0.39}\n",
      "{'loss': 1.2753, 'grad_norm': 0.54296875, 'learning_rate': 0.00011717281323944223, 'epoch': 0.39}\n",
      "{'loss': 1.1132, 'grad_norm': 0.9765625, 'learning_rate': 0.00011716145086197837, 'epoch': 0.39}\n",
      "{'loss': 1.3491, 'grad_norm': 0.5078125, 'learning_rate': 0.0001171500884845145, 'epoch': 0.39}\n",
      "{'loss': 1.1078, 'grad_norm': 0.921875, 'learning_rate': 0.00011713872610705065, 'epoch': 0.39}\n",
      "{'loss': 1.0596, 'grad_norm': 0.486328125, 'learning_rate': 0.00011712736372958679, 'epoch': 0.39}\n",
      "{'loss': 1.2214, 'grad_norm': 0.52734375, 'learning_rate': 0.00011711600135212293, 'epoch': 0.39}\n",
      "{'loss': 1.1148, 'grad_norm': 1.0625, 'learning_rate': 0.00011710463897465907, 'epoch': 0.39}\n",
      "{'loss': 1.5176, 'grad_norm': 0.53125, 'learning_rate': 0.0001170932765971952, 'epoch': 0.39}\n",
      "{'loss': 1.2412, 'grad_norm': 0.98046875, 'learning_rate': 0.00011708191421973133, 'epoch': 0.39}\n",
      "{'loss': 1.3193, 'grad_norm': 0.578125, 'learning_rate': 0.00011707055184226747, 'epoch': 0.39}\n",
      "{'loss': 1.1281, 'grad_norm': 0.462890625, 'learning_rate': 0.00011705918946480363, 'epoch': 0.39}\n",
      "{'loss': 1.0532, 'grad_norm': 0.5546875, 'learning_rate': 0.00011704782708733977, 'epoch': 0.39}\n",
      "{'loss': 1.3524, 'grad_norm': 0.46484375, 'learning_rate': 0.00011703646470987591, 'epoch': 0.39}\n",
      "{'loss': 1.0762, 'grad_norm': 0.546875, 'learning_rate': 0.00011702510233241203, 'epoch': 0.39}\n",
      "{'loss': 1.1917, 'grad_norm': 0.439453125, 'learning_rate': 0.00011701373995494817, 'epoch': 0.39}\n",
      "{'loss': 1.2754, 'grad_norm': 0.66796875, 'learning_rate': 0.00011700237757748431, 'epoch': 0.39}\n",
      "{'loss': 0.9546, 'grad_norm': 0.345703125, 'learning_rate': 0.00011699101520002045, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3217, 'grad_norm': 0.455078125, 'learning_rate': 0.00011697965282255661, 'epoch': 0.39}\n",
      "{'loss': 1.1754, 'grad_norm': 0.76171875, 'learning_rate': 0.00011696829044509275, 'epoch': 0.39}\n",
      "{'loss': 1.0691, 'grad_norm': 0.49609375, 'learning_rate': 0.00011695692806762887, 'epoch': 0.39}\n",
      "{'loss': 1.2597, 'grad_norm': 0.64453125, 'learning_rate': 0.00011694556569016501, 'epoch': 0.39}\n",
      "{'loss': 1.1901, 'grad_norm': 0.77734375, 'learning_rate': 0.00011693420331270115, 'epoch': 0.39}\n",
      "{'loss': 1.2809, 'grad_norm': 0.515625, 'learning_rate': 0.00011692284093523729, 'epoch': 0.39}\n",
      "{'loss': 1.2623, 'grad_norm': 0.52734375, 'learning_rate': 0.00011691147855777343, 'epoch': 0.39}\n",
      "{'loss': 1.2297, 'grad_norm': 0.56640625, 'learning_rate': 0.00011690011618030958, 'epoch': 0.39}\n",
      "{'loss': 1.2145, 'grad_norm': 0.46875, 'learning_rate': 0.00011688875380284571, 'epoch': 0.39}\n",
      "{'loss': 0.9836, 'grad_norm': 0.6015625, 'learning_rate': 0.00011687739142538185, 'epoch': 0.39}\n",
      "{'loss': 1.4258, 'grad_norm': 0.54296875, 'learning_rate': 0.00011686602904791799, 'epoch': 0.39}\n",
      "{'loss': 1.222, 'grad_norm': 0.89453125, 'learning_rate': 0.00011685466667045413, 'epoch': 0.39}\n",
      "{'loss': 1.2011, 'grad_norm': 0.390625, 'learning_rate': 0.00011684330429299027, 'epoch': 0.39}\n",
      "{'loss': 1.2254, 'grad_norm': 0.55078125, 'learning_rate': 0.0001168319419155264, 'epoch': 0.39}\n",
      "{'loss': 1.0584, 'grad_norm': 0.76953125, 'learning_rate': 0.00011682057953806256, 'epoch': 0.39}\n",
      "{'loss': 1.4176, 'grad_norm': 0.55078125, 'learning_rate': 0.00011680921716059869, 'epoch': 0.39}\n",
      "{'loss': 1.3049, 'grad_norm': 0.71875, 'learning_rate': 0.00011679785478313483, 'epoch': 0.39}\n",
      "{'loss': 1.177, 'grad_norm': 0.5625, 'learning_rate': 0.00011678649240567097, 'epoch': 0.39}\n",
      "{'loss': 1.1993, 'grad_norm': 0.64453125, 'learning_rate': 0.0001167751300282071, 'epoch': 0.39}\n",
      "{'loss': 0.9928, 'grad_norm': 0.7109375, 'learning_rate': 0.00011676376765074324, 'epoch': 0.39}\n",
      "{'loss': 1.2974, 'grad_norm': 0.5078125, 'learning_rate': 0.0001167524052732794, 'epoch': 0.39}\n",
      "{'loss': 1.1761, 'grad_norm': 0.99609375, 'learning_rate': 0.00011674104289581553, 'epoch': 0.39}\n",
      "{'loss': 1.1457, 'grad_norm': 0.44140625, 'learning_rate': 0.00011672968051835167, 'epoch': 0.39}\n",
      "{'loss': 1.1596, 'grad_norm': 0.57421875, 'learning_rate': 0.00011671831814088781, 'epoch': 0.39}\n",
      "{'loss': 1.0586, 'grad_norm': 1.0078125, 'learning_rate': 0.00011670695576342395, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3279, 'grad_norm': 0.6640625, 'learning_rate': 0.00011669559338596007, 'epoch': 0.39}\n",
      "{'loss': 1.2221, 'grad_norm': 0.84765625, 'learning_rate': 0.00011668423100849621, 'epoch': 0.39}\n",
      "{'loss': 1.2727, 'grad_norm': 0.6796875, 'learning_rate': 0.00011667286863103237, 'epoch': 0.39}\n",
      "{'loss': 1.2926, 'grad_norm': 0.6484375, 'learning_rate': 0.00011666150625356851, 'epoch': 0.39}\n",
      "{'loss': 1.0145, 'grad_norm': 1.3046875, 'learning_rate': 0.00011665014387610465, 'epoch': 0.39}\n",
      "{'loss': 1.2708, 'grad_norm': 0.6953125, 'learning_rate': 0.00011663878149864077, 'epoch': 0.39}\n",
      "{'loss': 1.1136, 'grad_norm': 0.63671875, 'learning_rate': 0.00011662741912117691, 'epoch': 0.39}\n",
      "{'loss': 1.1851, 'grad_norm': 0.76171875, 'learning_rate': 0.00011661605674371305, 'epoch': 0.39}\n",
      "{'loss': 1.295, 'grad_norm': 0.62890625, 'learning_rate': 0.00011660469436624919, 'epoch': 0.39}\n",
      "{'loss': 1.0692, 'grad_norm': 0.91796875, 'learning_rate': 0.00011659333198878535, 'epoch': 0.39}\n",
      "{'loss': 1.2752, 'grad_norm': 0.46875, 'learning_rate': 0.00011658196961132149, 'epoch': 0.39}\n",
      "{'loss': 1.2643, 'grad_norm': 0.63671875, 'learning_rate': 0.00011657060723385762, 'epoch': 0.39}\n",
      "{'loss': 1.2126, 'grad_norm': 0.57421875, 'learning_rate': 0.00011655924485639375, 'epoch': 0.39}\n",
      "{'loss': 1.4367, 'grad_norm': 0.71484375, 'learning_rate': 0.00011654788247892989, 'epoch': 0.39}\n",
      "{'loss': 1.0263, 'grad_norm': 0.76953125, 'learning_rate': 0.00011653652010146603, 'epoch': 0.39}\n",
      "{'loss': 1.2742, 'grad_norm': 0.55859375, 'learning_rate': 0.00011652515772400217, 'epoch': 0.39}\n",
      "{'loss': 1.1797, 'grad_norm': 0.75, 'learning_rate': 0.00011651379534653832, 'epoch': 0.39}\n",
      "{'loss': 1.1564, 'grad_norm': 0.61328125, 'learning_rate': 0.00011650243296907446, 'epoch': 0.39}\n",
      "{'loss': 1.222, 'grad_norm': 0.640625, 'learning_rate': 0.0001164910705916106, 'epoch': 0.39}\n",
      "{'loss': 1.0123, 'grad_norm': 0.7578125, 'learning_rate': 0.00011647970821414673, 'epoch': 0.39}\n",
      "{'loss': 1.2789, 'grad_norm': 0.435546875, 'learning_rate': 0.00011646834583668287, 'epoch': 0.39}\n",
      "{'loss': 1.2504, 'grad_norm': 0.625, 'learning_rate': 0.00011645698345921901, 'epoch': 0.39}\n",
      "{'loss': 1.2141, 'grad_norm': 0.404296875, 'learning_rate': 0.00011644562108175516, 'epoch': 0.39}\n",
      "{'loss': 1.2348, 'grad_norm': 0.6796875, 'learning_rate': 0.0001164342587042913, 'epoch': 0.39}\n",
      "{'loss': 1.0742, 'grad_norm': 1.1015625, 'learning_rate': 0.00011642289632682743, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.274, 'grad_norm': 0.53515625, 'learning_rate': 0.00011641153394936357, 'epoch': 0.39}\n",
      "{'loss': 1.0545, 'grad_norm': 0.73828125, 'learning_rate': 0.00011640017157189971, 'epoch': 0.39}\n",
      "{'loss': 1.2248, 'grad_norm': 0.44140625, 'learning_rate': 0.00011638880919443585, 'epoch': 0.39}\n",
      "{'loss': 1.2662, 'grad_norm': 0.5703125, 'learning_rate': 0.00011637744681697199, 'epoch': 0.39}\n",
      "{'loss': 1.0389, 'grad_norm': 0.7109375, 'learning_rate': 0.00011636608443950814, 'epoch': 0.39}\n",
      "{'loss': 1.452, 'grad_norm': 0.47265625, 'learning_rate': 0.00011635472206204428, 'epoch': 0.39}\n",
      "{'loss': 1.114, 'grad_norm': 0.59375, 'learning_rate': 0.00011634335968458041, 'epoch': 0.39}\n",
      "{'loss': 1.078, 'grad_norm': 0.439453125, 'learning_rate': 0.00011633199730711655, 'epoch': 0.39}\n",
      "{'loss': 1.2553, 'grad_norm': 0.50390625, 'learning_rate': 0.00011632063492965269, 'epoch': 0.39}\n",
      "{'loss': 1.0802, 'grad_norm': 0.75, 'learning_rate': 0.00011630927255218881, 'epoch': 0.39}\n",
      "{'loss': 1.3646, 'grad_norm': 0.451171875, 'learning_rate': 0.00011629791017472495, 'epoch': 0.39}\n",
      "{'loss': 1.2914, 'grad_norm': 0.7890625, 'learning_rate': 0.00011628654779726112, 'epoch': 0.39}\n",
      "{'loss': 1.2281, 'grad_norm': 0.447265625, 'learning_rate': 0.00011627518541979725, 'epoch': 0.39}\n",
      "{'loss': 1.1453, 'grad_norm': 0.84765625, 'learning_rate': 0.00011626382304233339, 'epoch': 0.39}\n",
      "{'loss': 1.0912, 'grad_norm': 0.78515625, 'learning_rate': 0.00011625246066486952, 'epoch': 0.39}\n",
      "{'loss': 1.3513, 'grad_norm': 0.47265625, 'learning_rate': 0.00011624109828740565, 'epoch': 0.39}\n",
      "{'loss': 1.1979, 'grad_norm': 0.74609375, 'learning_rate': 0.00011622973590994179, 'epoch': 0.39}\n",
      "{'loss': 1.2329, 'grad_norm': 0.494140625, 'learning_rate': 0.00011621837353247793, 'epoch': 0.39}\n",
      "{'loss': 1.2633, 'grad_norm': 0.59765625, 'learning_rate': 0.0001162070111550141, 'epoch': 0.39}\n",
      "{'loss': 1.0214, 'grad_norm': 1.1640625, 'learning_rate': 0.00011619564877755023, 'epoch': 0.39}\n",
      "{'loss': 1.2966, 'grad_norm': 0.80859375, 'learning_rate': 0.00011618428640008636, 'epoch': 0.39}\n",
      "{'loss': 1.1777, 'grad_norm': 0.65234375, 'learning_rate': 0.0001161729240226225, 'epoch': 0.39}\n",
      "{'loss': 1.322, 'grad_norm': 0.498046875, 'learning_rate': 0.00011616156164515863, 'epoch': 0.39}\n",
      "{'loss': 1.1545, 'grad_norm': 0.671875, 'learning_rate': 0.00011615019926769477, 'epoch': 0.39}\n",
      "{'loss': 0.9978, 'grad_norm': 0.373046875, 'learning_rate': 0.00011613883689023091, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3208, 'grad_norm': 0.484375, 'learning_rate': 0.00011612747451276706, 'epoch': 0.39}\n",
      "{'loss': 1.1373, 'grad_norm': 0.8046875, 'learning_rate': 0.0001161161121353032, 'epoch': 0.39}\n",
      "{'loss': 1.1636, 'grad_norm': 0.47265625, 'learning_rate': 0.00011610474975783933, 'epoch': 0.39}\n",
      "{'loss': 1.2103, 'grad_norm': 0.50390625, 'learning_rate': 0.00011609338738037547, 'epoch': 0.39}\n",
      "{'loss': 0.9213, 'grad_norm': 0.328125, 'learning_rate': 0.00011608202500291161, 'epoch': 0.39}\n",
      "{'loss': 1.2876, 'grad_norm': 0.498046875, 'learning_rate': 0.00011607066262544775, 'epoch': 0.39}\n",
      "{'loss': 1.2122, 'grad_norm': 0.71875, 'learning_rate': 0.0001160593002479839, 'epoch': 0.39}\n",
      "{'loss': 1.1152, 'grad_norm': 0.431640625, 'learning_rate': 0.00011604793787052004, 'epoch': 0.39}\n",
      "{'loss': 1.2258, 'grad_norm': 0.72265625, 'learning_rate': 0.00011603657549305618, 'epoch': 0.39}\n",
      "{'loss': 1.0168, 'grad_norm': 0.498046875, 'learning_rate': 0.00011602521311559231, 'epoch': 0.39}\n",
      "{'loss': 1.3065, 'grad_norm': 0.48046875, 'learning_rate': 0.00011601385073812845, 'epoch': 0.39}\n",
      "{'loss': 1.1978, 'grad_norm': 0.734375, 'learning_rate': 0.00011600248836066459, 'epoch': 0.39}\n",
      "{'loss': 1.2728, 'grad_norm': 0.46484375, 'learning_rate': 0.00011599112598320073, 'epoch': 0.39}\n",
      "{'loss': 1.2625, 'grad_norm': 0.59375, 'learning_rate': 0.00011597976360573688, 'epoch': 0.39}\n",
      "{'loss': 1.1367, 'grad_norm': 0.56640625, 'learning_rate': 0.00011596840122827302, 'epoch': 0.39}\n",
      "{'loss': 1.3321, 'grad_norm': 0.66015625, 'learning_rate': 0.00011595703885080915, 'epoch': 0.39}\n",
      "{'loss': 1.182, 'grad_norm': 0.6171875, 'learning_rate': 0.00011594567647334529, 'epoch': 0.39}\n",
      "{'loss': 1.2563, 'grad_norm': 0.462890625, 'learning_rate': 0.00011593431409588143, 'epoch': 0.39}\n",
      "{'loss': 1.3645, 'grad_norm': 0.58984375, 'learning_rate': 0.00011592295171841755, 'epoch': 0.39}\n",
      "{'loss': 1.0813, 'grad_norm': 0.5859375, 'learning_rate': 0.00011591158934095369, 'epoch': 0.39}\n",
      "{'loss': 1.2374, 'grad_norm': 0.4609375, 'learning_rate': 0.00011590022696348986, 'epoch': 0.39}\n",
      "{'loss': 1.2045, 'grad_norm': 0.6953125, 'learning_rate': 0.000115888864586026, 'epoch': 0.39}\n",
      "{'loss': 1.3744, 'grad_norm': 0.42578125, 'learning_rate': 0.00011587750220856213, 'epoch': 0.39}\n",
      "{'loss': 1.3215, 'grad_norm': 0.58984375, 'learning_rate': 0.00011586613983109826, 'epoch': 0.39}\n",
      "{'loss': 1.2133, 'grad_norm': 1.0859375, 'learning_rate': 0.0001158547774536344, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3934, 'grad_norm': 0.66796875, 'learning_rate': 0.00011584341507617053, 'epoch': 0.39}\n",
      "{'loss': 1.3002, 'grad_norm': 0.625, 'learning_rate': 0.00011583205269870667, 'epoch': 0.39}\n",
      "{'loss': 1.1749, 'grad_norm': 0.486328125, 'learning_rate': 0.00011582069032124284, 'epoch': 0.39}\n",
      "{'loss': 1.2222, 'grad_norm': 0.546875, 'learning_rate': 0.00011580932794377897, 'epoch': 0.39}\n",
      "{'loss': 1.0532, 'grad_norm': 0.83203125, 'learning_rate': 0.0001157979655663151, 'epoch': 0.39}\n",
      "{'loss': 1.4821, 'grad_norm': 0.44921875, 'learning_rate': 0.00011578660318885124, 'epoch': 0.39}\n",
      "{'loss': 1.0909, 'grad_norm': 0.6875, 'learning_rate': 0.00011577524081138737, 'epoch': 0.39}\n",
      "{'loss': 1.2383, 'grad_norm': 0.46875, 'learning_rate': 0.00011576387843392351, 'epoch': 0.39}\n",
      "{'loss': 1.1328, 'grad_norm': 0.5703125, 'learning_rate': 0.00011575251605645968, 'epoch': 0.39}\n",
      "{'loss': 1.0888, 'grad_norm': 0.796875, 'learning_rate': 0.0001157411536789958, 'epoch': 0.39}\n",
      "{'loss': 1.2252, 'grad_norm': 0.71875, 'learning_rate': 0.00011572979130153194, 'epoch': 0.39}\n",
      "{'loss': 1.1745, 'grad_norm': 0.63671875, 'learning_rate': 0.00011571842892406808, 'epoch': 0.39}\n",
      "{'loss': 1.2996, 'grad_norm': 0.7421875, 'learning_rate': 0.00011570706654660421, 'epoch': 0.39}\n",
      "{'loss': 1.1722, 'grad_norm': 0.57421875, 'learning_rate': 0.00011569570416914035, 'epoch': 0.39}\n",
      "{'loss': 1.0488, 'grad_norm': 0.890625, 'learning_rate': 0.00011568434179167649, 'epoch': 0.4}\n",
      "{'loss': 1.2966, 'grad_norm': 0.5234375, 'learning_rate': 0.00011567297941421264, 'epoch': 0.4}\n",
      "{'loss': 1.2496, 'grad_norm': 1.1015625, 'learning_rate': 0.00011566161703674878, 'epoch': 0.4}\n",
      "{'loss': 1.3768, 'grad_norm': 0.46875, 'learning_rate': 0.00011565025465928492, 'epoch': 0.4}\n",
      "{'loss': 1.2906, 'grad_norm': 0.8359375, 'learning_rate': 0.00011563889228182105, 'epoch': 0.4}\n",
      "{'loss': 1.0146, 'grad_norm': 0.6484375, 'learning_rate': 0.00011562752990435719, 'epoch': 0.4}\n",
      "{'loss': 1.3954, 'grad_norm': 0.58984375, 'learning_rate': 0.00011561616752689333, 'epoch': 0.4}\n",
      "{'loss': 1.1268, 'grad_norm': 0.8125, 'learning_rate': 0.00011560480514942947, 'epoch': 0.4}\n",
      "{'loss': 1.0733, 'grad_norm': 0.60546875, 'learning_rate': 0.00011559344277196562, 'epoch': 0.4}\n",
      "{'loss': 1.1658, 'grad_norm': 0.640625, 'learning_rate': 0.00011558208039450176, 'epoch': 0.4}\n",
      "{'loss': 1.0179, 'grad_norm': 0.7109375, 'learning_rate': 0.0001155707180170379, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1942, 'grad_norm': 0.609375, 'learning_rate': 0.00011555935563957403, 'epoch': 0.4}\n",
      "{'loss': 1.125, 'grad_norm': 0.65234375, 'learning_rate': 0.00011554799326211017, 'epoch': 0.4}\n",
      "{'loss': 1.194, 'grad_norm': 0.5078125, 'learning_rate': 0.0001155366308846463, 'epoch': 0.4}\n",
      "{'loss': 1.2034, 'grad_norm': 0.6953125, 'learning_rate': 0.00011552526850718243, 'epoch': 0.4}\n",
      "{'loss': 1.1012, 'grad_norm': 0.474609375, 'learning_rate': 0.0001155139061297186, 'epoch': 0.4}\n",
      "{'loss': 1.4354, 'grad_norm': 0.52734375, 'learning_rate': 0.00011550254375225474, 'epoch': 0.4}\n",
      "{'loss': 1.2527, 'grad_norm': 1.09375, 'learning_rate': 0.00011549118137479087, 'epoch': 0.4}\n",
      "{'loss': 1.0118, 'grad_norm': 0.478515625, 'learning_rate': 0.00011547981899732701, 'epoch': 0.4}\n",
      "{'loss': 1.177, 'grad_norm': 0.63671875, 'learning_rate': 0.00011546845661986314, 'epoch': 0.4}\n",
      "{'loss': 1.1151, 'grad_norm': 0.83203125, 'learning_rate': 0.00011545709424239927, 'epoch': 0.4}\n",
      "{'loss': 1.3934, 'grad_norm': 0.5078125, 'learning_rate': 0.00011544573186493541, 'epoch': 0.4}\n",
      "{'loss': 1.2563, 'grad_norm': 0.8984375, 'learning_rate': 0.00011543436948747158, 'epoch': 0.4}\n",
      "{'loss': 1.1961, 'grad_norm': 0.462890625, 'learning_rate': 0.00011542300711000771, 'epoch': 0.4}\n",
      "{'loss': 1.2471, 'grad_norm': 0.625, 'learning_rate': 0.00011541164473254384, 'epoch': 0.4}\n",
      "{'loss': 1.1943, 'grad_norm': 1.90625, 'learning_rate': 0.00011540028235507998, 'epoch': 0.4}\n",
      "{'loss': 1.1831, 'grad_norm': 0.703125, 'learning_rate': 0.00011538891997761611, 'epoch': 0.4}\n",
      "{'loss': 1.2694, 'grad_norm': 0.64453125, 'learning_rate': 0.00011537755760015225, 'epoch': 0.4}\n",
      "{'loss': 1.2659, 'grad_norm': 0.48828125, 'learning_rate': 0.00011536619522268842, 'epoch': 0.4}\n",
      "{'loss': 1.3195, 'grad_norm': 0.60546875, 'learning_rate': 0.00011535483284522454, 'epoch': 0.4}\n",
      "{'loss': 1.1176, 'grad_norm': 0.734375, 'learning_rate': 0.00011534347046776068, 'epoch': 0.4}\n",
      "{'loss': 1.2505, 'grad_norm': 0.47265625, 'learning_rate': 0.00011533210809029682, 'epoch': 0.4}\n",
      "{'loss': 1.2283, 'grad_norm': 0.8203125, 'learning_rate': 0.00011532074571283296, 'epoch': 0.4}\n",
      "{'loss': 1.2162, 'grad_norm': 0.400390625, 'learning_rate': 0.00011530938333536909, 'epoch': 0.4}\n",
      "{'loss': 1.3461, 'grad_norm': 0.921875, 'learning_rate': 0.00011529802095790523, 'epoch': 0.4}\n",
      "{'loss': 0.9857, 'grad_norm': 1.0390625, 'learning_rate': 0.00011528665858044138, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2486, 'grad_norm': 0.5703125, 'learning_rate': 0.00011527529620297752, 'epoch': 0.4}\n",
      "{'loss': 1.2424, 'grad_norm': 0.7890625, 'learning_rate': 0.00011526393382551366, 'epoch': 0.4}\n",
      "{'loss': 1.2463, 'grad_norm': 0.421875, 'learning_rate': 0.0001152525714480498, 'epoch': 0.4}\n",
      "{'loss': 1.1865, 'grad_norm': 0.67578125, 'learning_rate': 0.00011524120907058593, 'epoch': 0.4}\n",
      "{'loss': 1.0899, 'grad_norm': 1.625, 'learning_rate': 0.00011522984669312207, 'epoch': 0.4}\n",
      "{'loss': 1.3165, 'grad_norm': 0.51171875, 'learning_rate': 0.00011521848431565821, 'epoch': 0.4}\n",
      "{'loss': 1.1972, 'grad_norm': 0.79296875, 'learning_rate': 0.00011520712193819436, 'epoch': 0.4}\n",
      "{'loss': 1.0892, 'grad_norm': 0.40625, 'learning_rate': 0.0001151957595607305, 'epoch': 0.4}\n",
      "{'loss': 1.3146, 'grad_norm': 0.68359375, 'learning_rate': 0.00011518439718326664, 'epoch': 0.4}\n",
      "{'loss': 1.1066, 'grad_norm': 0.78515625, 'learning_rate': 0.00011517303480580277, 'epoch': 0.4}\n",
      "{'loss': 1.372, 'grad_norm': 0.470703125, 'learning_rate': 0.00011516167242833891, 'epoch': 0.4}\n",
      "{'loss': 1.1835, 'grad_norm': 0.703125, 'learning_rate': 0.00011515031005087504, 'epoch': 0.4}\n",
      "{'loss': 1.2688, 'grad_norm': 0.45703125, 'learning_rate': 0.00011513894767341117, 'epoch': 0.4}\n",
      "{'loss': 1.224, 'grad_norm': 0.54296875, 'learning_rate': 0.00011512758529594734, 'epoch': 0.4}\n",
      "{'loss': 1.053, 'grad_norm': 0.91796875, 'learning_rate': 0.00011511622291848348, 'epoch': 0.4}\n",
      "{'loss': 1.2831, 'grad_norm': 0.45703125, 'learning_rate': 0.00011510486054101962, 'epoch': 0.4}\n",
      "{'loss': 1.2337, 'grad_norm': 0.5703125, 'learning_rate': 0.00011509349816355575, 'epoch': 0.4}\n",
      "{'loss': 1.2483, 'grad_norm': 0.44140625, 'learning_rate': 0.00011508213578609188, 'epoch': 0.4}\n",
      "{'loss': 1.1451, 'grad_norm': 0.63671875, 'learning_rate': 0.00011507077340862802, 'epoch': 0.4}\n",
      "{'loss': 1.1506, 'grad_norm': 0.458984375, 'learning_rate': 0.00011505941103116418, 'epoch': 0.4}\n",
      "{'loss': 1.2261, 'grad_norm': 0.5625, 'learning_rate': 0.00011504804865370032, 'epoch': 0.4}\n",
      "{'loss': 1.0414, 'grad_norm': 0.52734375, 'learning_rate': 0.00011503668627623646, 'epoch': 0.4}\n",
      "{'loss': 1.1295, 'grad_norm': 0.6015625, 'learning_rate': 0.00011502532389877258, 'epoch': 0.4}\n",
      "{'loss': 1.2525, 'grad_norm': 0.6015625, 'learning_rate': 0.00011501396152130872, 'epoch': 0.4}\n",
      "{'loss': 1.2247, 'grad_norm': 0.890625, 'learning_rate': 0.00011500259914384486, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2659, 'grad_norm': 0.44921875, 'learning_rate': 0.000114991236766381, 'epoch': 0.4}\n",
      "{'loss': 1.0932, 'grad_norm': 0.6640625, 'learning_rate': 0.00011497987438891716, 'epoch': 0.4}\n",
      "{'loss': 1.2582, 'grad_norm': 0.447265625, 'learning_rate': 0.00011496851201145328, 'epoch': 0.4}\n",
      "{'loss': 1.2546, 'grad_norm': 0.68359375, 'learning_rate': 0.00011495714963398942, 'epoch': 0.4}\n",
      "{'loss': 1.0795, 'grad_norm': 0.81640625, 'learning_rate': 0.00011494578725652556, 'epoch': 0.4}\n",
      "{'loss': 1.3519, 'grad_norm': 0.462890625, 'learning_rate': 0.0001149344248790617, 'epoch': 0.4}\n",
      "{'loss': 1.0679, 'grad_norm': 0.6875, 'learning_rate': 0.00011492306250159783, 'epoch': 0.4}\n",
      "{'loss': 1.0785, 'grad_norm': 0.404296875, 'learning_rate': 0.00011491170012413397, 'epoch': 0.4}\n",
      "{'loss': 1.2129, 'grad_norm': 0.52734375, 'learning_rate': 0.00011490033774667012, 'epoch': 0.4}\n",
      "{'loss': 1.1147, 'grad_norm': 0.52734375, 'learning_rate': 0.00011488897536920626, 'epoch': 0.4}\n",
      "{'loss': 1.4026, 'grad_norm': 0.58984375, 'learning_rate': 0.0001148776129917424, 'epoch': 0.4}\n",
      "{'loss': 1.2767, 'grad_norm': 0.8125, 'learning_rate': 0.00011486625061427854, 'epoch': 0.4}\n",
      "{'loss': 1.1238, 'grad_norm': 0.5234375, 'learning_rate': 0.00011485488823681468, 'epoch': 0.4}\n",
      "{'loss': 1.2423, 'grad_norm': 0.8515625, 'learning_rate': 0.00011484352585935081, 'epoch': 0.4}\n",
      "{'loss': 0.9847, 'grad_norm': 0.7890625, 'learning_rate': 0.00011483216348188695, 'epoch': 0.4}\n",
      "{'loss': 1.4122, 'grad_norm': 0.6328125, 'learning_rate': 0.0001148208011044231, 'epoch': 0.4}\n",
      "{'loss': 1.2762, 'grad_norm': 0.76953125, 'learning_rate': 0.00011480943872695924, 'epoch': 0.4}\n",
      "{'loss': 1.1489, 'grad_norm': 0.451171875, 'learning_rate': 0.00011479807634949538, 'epoch': 0.4}\n",
      "{'loss': 1.1989, 'grad_norm': 0.470703125, 'learning_rate': 0.00011478671397203152, 'epoch': 0.4}\n",
      "{'loss': 1.16, 'grad_norm': 0.6875, 'learning_rate': 0.00011477535159456765, 'epoch': 0.4}\n",
      "{'loss': 1.1988, 'grad_norm': 0.60546875, 'learning_rate': 0.00011476398921710378, 'epoch': 0.4}\n",
      "{'loss': 1.091, 'grad_norm': 0.71875, 'learning_rate': 0.00011475262683963992, 'epoch': 0.4}\n",
      "{'loss': 1.1925, 'grad_norm': 0.40625, 'learning_rate': 0.00011474126446217608, 'epoch': 0.4}\n",
      "{'loss': 1.3234, 'grad_norm': 0.55859375, 'learning_rate': 0.00011472990208471222, 'epoch': 0.4}\n",
      "{'loss': 1.1286, 'grad_norm': 1.6171875, 'learning_rate': 0.00011471853970724836, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2206, 'grad_norm': 0.45703125, 'learning_rate': 0.0001147071773297845, 'epoch': 0.4}\n",
      "{'loss': 1.1719, 'grad_norm': 0.80078125, 'learning_rate': 0.00011469581495232062, 'epoch': 0.4}\n",
      "{'loss': 1.1025, 'grad_norm': 0.58203125, 'learning_rate': 0.00011468445257485676, 'epoch': 0.4}\n",
      "{'loss': 1.2492, 'grad_norm': 0.5859375, 'learning_rate': 0.00011467309019739292, 'epoch': 0.4}\n",
      "{'loss': 1.0311, 'grad_norm': 0.8125, 'learning_rate': 0.00011466172781992906, 'epoch': 0.4}\n",
      "{'loss': 1.3014, 'grad_norm': 0.5625, 'learning_rate': 0.0001146503654424652, 'epoch': 0.4}\n",
      "{'loss': 1.197, 'grad_norm': 0.62109375, 'learning_rate': 0.00011463900306500132, 'epoch': 0.4}\n",
      "{'loss': 1.2534, 'grad_norm': 0.60546875, 'learning_rate': 0.00011462764068753746, 'epoch': 0.4}\n",
      "{'loss': 1.2019, 'grad_norm': 0.55078125, 'learning_rate': 0.0001146162783100736, 'epoch': 0.4}\n",
      "{'loss': 1.157, 'grad_norm': 0.8828125, 'learning_rate': 0.00011460491593260974, 'epoch': 0.4}\n",
      "{'loss': 1.3668, 'grad_norm': 0.46875, 'learning_rate': 0.0001145935535551459, 'epoch': 0.4}\n",
      "{'loss': 1.1858, 'grad_norm': 0.62890625, 'learning_rate': 0.00011458219117768202, 'epoch': 0.4}\n",
      "{'loss': 1.0984, 'grad_norm': 0.439453125, 'learning_rate': 0.00011457082880021816, 'epoch': 0.4}\n",
      "{'loss': 1.1929, 'grad_norm': 0.52734375, 'learning_rate': 0.0001145594664227543, 'epoch': 0.4}\n",
      "{'loss': 1.0206, 'grad_norm': 0.6953125, 'learning_rate': 0.00011454810404529044, 'epoch': 0.4}\n",
      "{'loss': 1.3217, 'grad_norm': 0.470703125, 'learning_rate': 0.00011453674166782658, 'epoch': 0.4}\n",
      "{'loss': 1.1828, 'grad_norm': 0.72265625, 'learning_rate': 0.00011452537929036271, 'epoch': 0.4}\n",
      "{'loss': 1.2584, 'grad_norm': 0.6171875, 'learning_rate': 0.00011451401691289886, 'epoch': 0.4}\n",
      "{'loss': 1.3022, 'grad_norm': 0.578125, 'learning_rate': 0.000114502654535435, 'epoch': 0.4}\n",
      "{'loss': 1.0885, 'grad_norm': 0.51953125, 'learning_rate': 0.00011449129215797114, 'epoch': 0.4}\n",
      "{'loss': 1.3366, 'grad_norm': 0.5, 'learning_rate': 0.00011447992978050728, 'epoch': 0.4}\n",
      "{'loss': 1.1114, 'grad_norm': 0.66796875, 'learning_rate': 0.00011446856740304342, 'epoch': 0.4}\n",
      "{'loss': 1.1252, 'grad_norm': 0.384765625, 'learning_rate': 0.00011445720502557955, 'epoch': 0.4}\n",
      "{'loss': 1.1417, 'grad_norm': 0.578125, 'learning_rate': 0.00011444584264811569, 'epoch': 0.4}\n",
      "{'loss': 1.0716, 'grad_norm': 0.7109375, 'learning_rate': 0.00011443448027065184, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4, 'grad_norm': 0.50390625, 'learning_rate': 0.00011442311789318798, 'epoch': 0.4}\n",
      "{'loss': 1.1413, 'grad_norm': 1.53125, 'learning_rate': 0.00011441175551572412, 'epoch': 0.4}\n",
      "{'loss': 1.0914, 'grad_norm': 0.416015625, 'learning_rate': 0.00011440039313826026, 'epoch': 0.4}\n",
      "{'loss': 1.1523, 'grad_norm': 0.640625, 'learning_rate': 0.0001143890307607964, 'epoch': 0.4}\n",
      "{'loss': 0.9501, 'grad_norm': 0.953125, 'learning_rate': 0.00011437766838333252, 'epoch': 0.4}\n",
      "{'loss': 1.2215, 'grad_norm': 0.57421875, 'learning_rate': 0.00011436630600586868, 'epoch': 0.4}\n",
      "{'loss': 1.2111, 'grad_norm': 0.53515625, 'learning_rate': 0.00011435494362840482, 'epoch': 0.4}\n",
      "{'loss': 1.2059, 'grad_norm': 0.34765625, 'learning_rate': 0.00011434358125094096, 'epoch': 0.4}\n",
      "{'loss': 1.2283, 'grad_norm': 0.52734375, 'learning_rate': 0.0001143322188734771, 'epoch': 0.4}\n",
      "{'loss': 1.1153, 'grad_norm': 0.8046875, 'learning_rate': 0.00011432085649601324, 'epoch': 0.4}\n",
      "{'loss': 1.4299, 'grad_norm': 0.48828125, 'learning_rate': 0.00011430949411854936, 'epoch': 0.4}\n",
      "{'loss': 1.1128, 'grad_norm': 0.66796875, 'learning_rate': 0.0001142981317410855, 'epoch': 0.4}\n",
      "{'loss': 1.2422, 'grad_norm': 0.447265625, 'learning_rate': 0.00011428676936362166, 'epoch': 0.4}\n",
      "{'loss': 1.1635, 'grad_norm': 0.6328125, 'learning_rate': 0.0001142754069861578, 'epoch': 0.4}\n",
      "{'loss': 1.1997, 'grad_norm': 0.93359375, 'learning_rate': 0.00011426404460869394, 'epoch': 0.4}\n",
      "{'loss': 1.2938, 'grad_norm': 0.484375, 'learning_rate': 0.00011425268223123006, 'epoch': 0.4}\n",
      "{'loss': 1.2577, 'grad_norm': 0.6796875, 'learning_rate': 0.0001142413198537662, 'epoch': 0.4}\n",
      "{'loss': 1.1212, 'grad_norm': 0.515625, 'learning_rate': 0.00011422995747630234, 'epoch': 0.4}\n",
      "{'loss': 1.2442, 'grad_norm': 0.6328125, 'learning_rate': 0.00011421859509883848, 'epoch': 0.4}\n",
      "{'loss': 1.0804, 'grad_norm': 0.6875, 'learning_rate': 0.00011420723272137464, 'epoch': 0.4}\n",
      "{'loss': 1.2377, 'grad_norm': 0.474609375, 'learning_rate': 0.00011419587034391077, 'epoch': 0.4}\n",
      "{'loss': 1.2668, 'grad_norm': 1.0234375, 'learning_rate': 0.0001141845079664469, 'epoch': 0.4}\n",
      "{'loss': 1.1099, 'grad_norm': 0.42578125, 'learning_rate': 0.00011417314558898304, 'epoch': 0.4}\n",
      "{'loss': 1.24, 'grad_norm': 0.466796875, 'learning_rate': 0.00011416178321151918, 'epoch': 0.4}\n",
      "{'loss': 1.0782, 'grad_norm': 1.359375, 'learning_rate': 0.00011415042083405532, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3501, 'grad_norm': 0.64453125, 'learning_rate': 0.00011413905845659145, 'epoch': 0.4}\n",
      "{'loss': 1.2902, 'grad_norm': 0.6875, 'learning_rate': 0.0001141276960791276, 'epoch': 0.4}\n",
      "{'loss': 1.099, 'grad_norm': 0.5, 'learning_rate': 0.00011411633370166374, 'epoch': 0.4}\n",
      "{'loss': 1.284, 'grad_norm': 0.423828125, 'learning_rate': 0.00011410497132419988, 'epoch': 0.4}\n",
      "{'loss': 1.151, 'grad_norm': 0.875, 'learning_rate': 0.00011409360894673602, 'epoch': 0.4}\n",
      "{'loss': 1.4204, 'grad_norm': 0.5, 'learning_rate': 0.00011408224656927216, 'epoch': 0.4}\n",
      "{'loss': 1.1238, 'grad_norm': 0.498046875, 'learning_rate': 0.0001140708841918083, 'epoch': 0.4}\n",
      "{'loss': 1.2148, 'grad_norm': 0.4921875, 'learning_rate': 0.00011405952181434445, 'epoch': 0.4}\n",
      "{'loss': 1.2473, 'grad_norm': 0.56640625, 'learning_rate': 0.00011404815943688058, 'epoch': 0.4}\n",
      "{'loss': 0.9658, 'grad_norm': 1.296875, 'learning_rate': 0.00011403679705941672, 'epoch': 0.4}\n",
      "{'loss': 1.3005, 'grad_norm': 0.5078125, 'learning_rate': 0.00011402543468195286, 'epoch': 0.4}\n",
      "{'loss': 1.1545, 'grad_norm': 0.609375, 'learning_rate': 0.000114014072304489, 'epoch': 0.4}\n",
      "{'loss': 1.2596, 'grad_norm': 0.45703125, 'learning_rate': 0.00011400270992702514, 'epoch': 0.4}\n",
      "{'loss': 1.2246, 'grad_norm': 0.5546875, 'learning_rate': 0.00011399134754956126, 'epoch': 0.4}\n",
      "{'loss': 1.087, 'grad_norm': 1.15625, 'learning_rate': 0.00011397998517209743, 'epoch': 0.4}\n",
      "{'loss': 1.229, 'grad_norm': 0.5546875, 'learning_rate': 0.00011396862279463356, 'epoch': 0.4}\n",
      "{'loss': 1.224, 'grad_norm': 1.1640625, 'learning_rate': 0.0001139572604171697, 'epoch': 0.4}\n",
      "{'loss': 1.2548, 'grad_norm': 0.462890625, 'learning_rate': 0.00011394589803970584, 'epoch': 0.4}\n",
      "{'loss': 1.1826, 'grad_norm': 0.58203125, 'learning_rate': 0.00011393453566224198, 'epoch': 0.4}\n",
      "{'loss': 1.1518, 'grad_norm': 0.96875, 'learning_rate': 0.0001139231732847781, 'epoch': 0.4}\n",
      "{'loss': 1.2639, 'grad_norm': 0.55078125, 'learning_rate': 0.00011391181090731424, 'epoch': 0.4}\n",
      "{'loss': 1.1415, 'grad_norm': 0.5859375, 'learning_rate': 0.0001139004485298504, 'epoch': 0.4}\n",
      "{'loss': 1.1382, 'grad_norm': 0.55078125, 'learning_rate': 0.00011388908615238654, 'epoch': 0.4}\n",
      "{'loss': 1.103, 'grad_norm': 0.50390625, 'learning_rate': 0.00011387772377492268, 'epoch': 0.4}\n",
      "{'loss': 1.0313, 'grad_norm': 1.2890625, 'learning_rate': 0.0001138663613974588, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3646, 'grad_norm': 0.451171875, 'learning_rate': 0.00011385499901999494, 'epoch': 0.4}\n",
      "{'loss': 1.2003, 'grad_norm': 0.6953125, 'learning_rate': 0.00011384363664253108, 'epoch': 0.4}\n",
      "{'loss': 1.1917, 'grad_norm': 0.41796875, 'learning_rate': 0.00011383227426506722, 'epoch': 0.4}\n",
      "{'loss': 1.1854, 'grad_norm': 0.55859375, 'learning_rate': 0.00011382091188760338, 'epoch': 0.4}\n",
      "{'loss': 1.1227, 'grad_norm': 0.6796875, 'learning_rate': 0.00011380954951013951, 'epoch': 0.4}\n",
      "{'loss': 1.4079, 'grad_norm': 0.578125, 'learning_rate': 0.00011379818713267564, 'epoch': 0.4}\n",
      "{'loss': 1.2461, 'grad_norm': 0.53125, 'learning_rate': 0.00011378682475521178, 'epoch': 0.4}\n",
      "{'loss': 1.2862, 'grad_norm': 0.443359375, 'learning_rate': 0.00011377546237774792, 'epoch': 0.4}\n",
      "{'loss': 1.2049, 'grad_norm': 0.5078125, 'learning_rate': 0.00011376410000028406, 'epoch': 0.4}\n",
      "{'loss': 1.0529, 'grad_norm': 0.62109375, 'learning_rate': 0.0001137527376228202, 'epoch': 0.4}\n",
      "{'loss': 1.3543, 'grad_norm': 0.451171875, 'learning_rate': 0.00011374137524535635, 'epoch': 0.4}\n",
      "{'loss': 1.2511, 'grad_norm': 0.87890625, 'learning_rate': 0.00011373001286789249, 'epoch': 0.4}\n",
      "{'loss': 1.2143, 'grad_norm': 0.408203125, 'learning_rate': 0.00011371865049042862, 'epoch': 0.4}\n",
      "{'loss': 1.2456, 'grad_norm': 0.57421875, 'learning_rate': 0.00011370728811296476, 'epoch': 0.4}\n",
      "{'loss': 1.0585, 'grad_norm': 0.671875, 'learning_rate': 0.0001136959257355009, 'epoch': 0.4}\n",
      "{'loss': 1.359, 'grad_norm': 0.486328125, 'learning_rate': 0.00011368456335803704, 'epoch': 0.4}\n",
      "{'loss': 1.0742, 'grad_norm': 0.54296875, 'learning_rate': 0.00011367320098057319, 'epoch': 0.4}\n",
      "{'loss': 1.1714, 'grad_norm': 0.373046875, 'learning_rate': 0.00011366183860310933, 'epoch': 0.4}\n",
      "{'loss': 1.3528, 'grad_norm': 0.953125, 'learning_rate': 0.00011365047622564546, 'epoch': 0.4}\n",
      "{'loss': 1.2776, 'grad_norm': 0.72265625, 'learning_rate': 0.0001136391138481816, 'epoch': 0.4}\n",
      "{'loss': 1.3912, 'grad_norm': 0.5625, 'learning_rate': 0.00011362775147071774, 'epoch': 0.4}\n",
      "{'loss': 1.2659, 'grad_norm': 0.64453125, 'learning_rate': 0.00011361638909325388, 'epoch': 0.4}\n",
      "{'loss': 1.2581, 'grad_norm': 0.48046875, 'learning_rate': 0.00011360502671579, 'epoch': 0.4}\n",
      "{'loss': 1.2759, 'grad_norm': 0.54296875, 'learning_rate': 0.00011359366433832617, 'epoch': 0.4}\n",
      "{'loss': 1.0419, 'grad_norm': 0.85546875, 'learning_rate': 0.0001135823019608623, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3347, 'grad_norm': 0.49609375, 'learning_rate': 0.00011357093958339844, 'epoch': 0.4}\n",
      "{'loss': 1.1429, 'grad_norm': 0.81640625, 'learning_rate': 0.00011355957720593458, 'epoch': 0.4}\n",
      "{'loss': 1.1184, 'grad_norm': 0.53125, 'learning_rate': 0.00011354821482847072, 'epoch': 0.4}\n",
      "{'loss': 1.1612, 'grad_norm': 0.80078125, 'learning_rate': 0.00011353685245100684, 'epoch': 0.4}\n",
      "{'loss': 1.0188, 'grad_norm': 0.4765625, 'learning_rate': 0.00011352549007354298, 'epoch': 0.4}\n",
      "{'loss': 1.1998, 'grad_norm': 0.515625, 'learning_rate': 0.00011351412769607915, 'epoch': 0.4}\n",
      "{'loss': 1.0211, 'grad_norm': 0.63671875, 'learning_rate': 0.00011350276531861528, 'epoch': 0.4}\n",
      "{'loss': 1.1501, 'grad_norm': 0.546875, 'learning_rate': 0.00011349140294115142, 'epoch': 0.4}\n",
      "{'loss': 1.1028, 'grad_norm': 0.79296875, 'learning_rate': 0.00011348004056368755, 'epoch': 0.4}\n",
      "{'loss': 0.9874, 'grad_norm': 0.7734375, 'learning_rate': 0.00011346867818622368, 'epoch': 0.4}\n",
      "{'loss': 1.3145, 'grad_norm': 0.68359375, 'learning_rate': 0.00011345731580875982, 'epoch': 0.4}\n",
      "{'loss': 1.1531, 'grad_norm': 0.68359375, 'learning_rate': 0.00011344595343129596, 'epoch': 0.4}\n",
      "{'loss': 1.1423, 'grad_norm': 0.53515625, 'learning_rate': 0.00011343459105383212, 'epoch': 0.4}\n",
      "{'loss': 1.1202, 'grad_norm': 0.5625, 'learning_rate': 0.00011342322867636825, 'epoch': 0.4}\n",
      "{'loss': 1.0861, 'grad_norm': 0.9921875, 'learning_rate': 0.00011341186629890439, 'epoch': 0.4}\n",
      "{'loss': 1.3649, 'grad_norm': 0.4609375, 'learning_rate': 0.00011340050392144052, 'epoch': 0.41}\n",
      "{'loss': 1.2143, 'grad_norm': 0.8828125, 'learning_rate': 0.00011338914154397666, 'epoch': 0.41}\n",
      "{'loss': 1.1358, 'grad_norm': 0.58203125, 'learning_rate': 0.0001133777791665128, 'epoch': 0.41}\n",
      "{'loss': 1.1981, 'grad_norm': 0.8359375, 'learning_rate': 0.00011336641678904895, 'epoch': 0.41}\n",
      "{'loss': 1.1884, 'grad_norm': 0.9375, 'learning_rate': 0.00011335505441158509, 'epoch': 0.41}\n",
      "{'loss': 1.3804, 'grad_norm': 0.52734375, 'learning_rate': 0.00011334369203412123, 'epoch': 0.41}\n",
      "{'loss': 1.1397, 'grad_norm': 0.78515625, 'learning_rate': 0.00011333232965665736, 'epoch': 0.41}\n",
      "{'loss': 1.1862, 'grad_norm': 0.45703125, 'learning_rate': 0.0001133209672791935, 'epoch': 0.41}\n",
      "{'loss': 1.3011, 'grad_norm': 0.64453125, 'learning_rate': 0.00011330960490172964, 'epoch': 0.41}\n",
      "{'loss': 1.0718, 'grad_norm': 0.73828125, 'learning_rate': 0.00011329824252426578, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3376, 'grad_norm': 0.86328125, 'learning_rate': 0.00011328688014680193, 'epoch': 0.41}\n",
      "{'loss': 1.1442, 'grad_norm': 0.62890625, 'learning_rate': 0.00011327551776933807, 'epoch': 0.41}\n",
      "{'loss': 1.1722, 'grad_norm': 0.478515625, 'learning_rate': 0.0001132641553918742, 'epoch': 0.41}\n",
      "{'loss': 1.2481, 'grad_norm': 0.68359375, 'learning_rate': 0.00011325279301441034, 'epoch': 0.41}\n",
      "{'loss': 1.1003, 'grad_norm': 1.0234375, 'learning_rate': 0.00011324143063694648, 'epoch': 0.41}\n",
      "{'loss': 1.463, 'grad_norm': 0.48046875, 'learning_rate': 0.00011323006825948262, 'epoch': 0.41}\n",
      "{'loss': 1.15, 'grad_norm': 0.61328125, 'learning_rate': 0.00011321870588201874, 'epoch': 0.41}\n",
      "{'loss': 1.0984, 'grad_norm': 0.515625, 'learning_rate': 0.00011320734350455491, 'epoch': 0.41}\n",
      "{'loss': 1.1582, 'grad_norm': 0.546875, 'learning_rate': 0.00011319598112709105, 'epoch': 0.41}\n",
      "{'loss': 1.1362, 'grad_norm': 0.59765625, 'learning_rate': 0.00011318461874962718, 'epoch': 0.41}\n",
      "{'loss': 1.4247, 'grad_norm': 0.52734375, 'learning_rate': 0.00011317325637216332, 'epoch': 0.41}\n",
      "{'loss': 1.2803, 'grad_norm': 1.28125, 'learning_rate': 0.00011316189399469946, 'epoch': 0.41}\n",
      "{'loss': 1.1706, 'grad_norm': 0.44921875, 'learning_rate': 0.00011315053161723558, 'epoch': 0.41}\n",
      "{'loss': 1.2947, 'grad_norm': 0.55859375, 'learning_rate': 0.00011313916923977172, 'epoch': 0.41}\n",
      "{'loss': 1.0678, 'grad_norm': 0.64453125, 'learning_rate': 0.00011312780686230789, 'epoch': 0.41}\n",
      "{'loss': 1.2287, 'grad_norm': 0.462890625, 'learning_rate': 0.00011311644448484402, 'epoch': 0.41}\n",
      "{'loss': 1.0737, 'grad_norm': 0.72265625, 'learning_rate': 0.00011310508210738016, 'epoch': 0.41}\n",
      "{'loss': 1.1491, 'grad_norm': 0.470703125, 'learning_rate': 0.00011309371972991629, 'epoch': 0.41}\n",
      "{'loss': 1.1828, 'grad_norm': 0.51171875, 'learning_rate': 0.00011308235735245242, 'epoch': 0.41}\n",
      "{'loss': 1.194, 'grad_norm': 1.09375, 'learning_rate': 0.00011307099497498856, 'epoch': 0.41}\n",
      "{'loss': 1.2419, 'grad_norm': 0.5390625, 'learning_rate': 0.0001130596325975247, 'epoch': 0.41}\n",
      "{'loss': 1.0793, 'grad_norm': 0.66796875, 'learning_rate': 0.00011304827022006087, 'epoch': 0.41}\n",
      "{'loss': 1.2274, 'grad_norm': 0.44921875, 'learning_rate': 0.00011303690784259699, 'epoch': 0.41}\n",
      "{'loss': 1.2979, 'grad_norm': 0.8046875, 'learning_rate': 0.00011302554546513313, 'epoch': 0.41}\n",
      "{'loss': 1.0973, 'grad_norm': 0.9296875, 'learning_rate': 0.00011301418308766927, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2477, 'grad_norm': 0.45703125, 'learning_rate': 0.0001130028207102054, 'epoch': 0.41}\n",
      "{'loss': 1.2399, 'grad_norm': 0.79296875, 'learning_rate': 0.00011299145833274154, 'epoch': 0.41}\n",
      "{'loss': 1.2196, 'grad_norm': 0.447265625, 'learning_rate': 0.00011298009595527769, 'epoch': 0.41}\n",
      "{'loss': 1.3353, 'grad_norm': 0.62109375, 'learning_rate': 0.00011296873357781383, 'epoch': 0.41}\n",
      "{'loss': 1.0812, 'grad_norm': 0.7421875, 'learning_rate': 0.00011295737120034997, 'epoch': 0.41}\n",
      "{'loss': 1.5166, 'grad_norm': 0.50390625, 'learning_rate': 0.0001129460088228861, 'epoch': 0.41}\n",
      "{'loss': 1.1854, 'grad_norm': 0.73828125, 'learning_rate': 0.00011293464644542224, 'epoch': 0.41}\n",
      "{'loss': 1.1928, 'grad_norm': 0.41796875, 'learning_rate': 0.00011292328406795838, 'epoch': 0.41}\n",
      "{'loss': 1.2195, 'grad_norm': 0.56640625, 'learning_rate': 0.00011291192169049452, 'epoch': 0.41}\n",
      "{'loss': 1.0889, 'grad_norm': 0.60546875, 'learning_rate': 0.00011290055931303067, 'epoch': 0.41}\n",
      "{'loss': 1.4559, 'grad_norm': 0.48828125, 'learning_rate': 0.00011288919693556681, 'epoch': 0.41}\n",
      "{'loss': 1.1477, 'grad_norm': 0.62109375, 'learning_rate': 0.00011287783455810295, 'epoch': 0.41}\n",
      "{'loss': 1.1173, 'grad_norm': 0.57421875, 'learning_rate': 0.00011286647218063908, 'epoch': 0.41}\n",
      "{'loss': 1.2182, 'grad_norm': 0.59375, 'learning_rate': 0.00011285510980317522, 'epoch': 0.41}\n",
      "{'loss': 1.0611, 'grad_norm': 1.0390625, 'learning_rate': 0.00011284374742571136, 'epoch': 0.41}\n",
      "{'loss': 1.3153, 'grad_norm': 0.56640625, 'learning_rate': 0.00011283238504824748, 'epoch': 0.41}\n",
      "{'loss': 1.1522, 'grad_norm': 0.8046875, 'learning_rate': 0.00011282102267078365, 'epoch': 0.41}\n",
      "{'loss': 1.1584, 'grad_norm': 0.419921875, 'learning_rate': 0.00011280966029331979, 'epoch': 0.41}\n",
      "{'loss': 1.2187, 'grad_norm': 0.5546875, 'learning_rate': 0.00011279829791585592, 'epoch': 0.41}\n",
      "{'loss': 1.1757, 'grad_norm': 1.28125, 'learning_rate': 0.00011278693553839206, 'epoch': 0.41}\n",
      "{'loss': 1.3338, 'grad_norm': 0.62890625, 'learning_rate': 0.0001127755731609282, 'epoch': 0.41}\n",
      "{'loss': 1.2177, 'grad_norm': 0.70703125, 'learning_rate': 0.00011276421078346432, 'epoch': 0.41}\n",
      "{'loss': 1.1796, 'grad_norm': 0.6484375, 'learning_rate': 0.00011275284840600046, 'epoch': 0.41}\n",
      "{'loss': 1.0976, 'grad_norm': 0.49609375, 'learning_rate': 0.00011274148602853663, 'epoch': 0.41}\n",
      "{'loss': 1.1706, 'grad_norm': 0.578125, 'learning_rate': 0.00011273012365107277, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4483, 'grad_norm': 0.466796875, 'learning_rate': 0.0001127187612736089, 'epoch': 0.41}\n",
      "{'loss': 1.1486, 'grad_norm': 0.498046875, 'learning_rate': 0.00011270739889614503, 'epoch': 0.41}\n",
      "{'loss': 1.2445, 'grad_norm': 0.49609375, 'learning_rate': 0.00011269603651868117, 'epoch': 0.41}\n",
      "{'loss': 1.2445, 'grad_norm': 0.71484375, 'learning_rate': 0.0001126846741412173, 'epoch': 0.41}\n",
      "{'loss': 1.1254, 'grad_norm': 0.890625, 'learning_rate': 0.00011267331176375347, 'epoch': 0.41}\n",
      "{'loss': 1.29, 'grad_norm': 0.6640625, 'learning_rate': 0.0001126619493862896, 'epoch': 0.41}\n",
      "{'loss': 1.1184, 'grad_norm': 1.328125, 'learning_rate': 0.00011265058700882573, 'epoch': 0.41}\n",
      "{'loss': 1.284, 'grad_norm': 0.431640625, 'learning_rate': 0.00011263922463136187, 'epoch': 0.41}\n",
      "{'loss': 1.2041, 'grad_norm': 0.4765625, 'learning_rate': 0.000112627862253898, 'epoch': 0.41}\n",
      "{'loss': 1.1505, 'grad_norm': 0.6875, 'learning_rate': 0.00011261649987643414, 'epoch': 0.41}\n",
      "{'loss': 1.2106, 'grad_norm': 0.52734375, 'learning_rate': 0.00011260513749897028, 'epoch': 0.41}\n",
      "{'loss': 1.1333, 'grad_norm': 0.72265625, 'learning_rate': 0.00011259377512150643, 'epoch': 0.41}\n",
      "{'loss': 1.2514, 'grad_norm': 0.7109375, 'learning_rate': 0.00011258241274404257, 'epoch': 0.41}\n",
      "{'loss': 1.1902, 'grad_norm': 0.58203125, 'learning_rate': 0.00011257105036657871, 'epoch': 0.41}\n",
      "{'loss': 1.1304, 'grad_norm': 0.56640625, 'learning_rate': 0.00011255968798911485, 'epoch': 0.41}\n",
      "{'loss': 1.4017, 'grad_norm': 0.43359375, 'learning_rate': 0.00011254832561165098, 'epoch': 0.41}\n",
      "{'loss': 1.1097, 'grad_norm': 0.76953125, 'learning_rate': 0.00011253696323418712, 'epoch': 0.41}\n",
      "{'loss': 1.0539, 'grad_norm': 0.482421875, 'learning_rate': 0.00011252560085672326, 'epoch': 0.41}\n",
      "{'loss': 1.3724, 'grad_norm': 0.58984375, 'learning_rate': 0.00011251423847925941, 'epoch': 0.41}\n",
      "{'loss': 1.0727, 'grad_norm': 0.875, 'learning_rate': 0.00011250287610179555, 'epoch': 0.41}\n",
      "{'loss': 1.4109, 'grad_norm': 0.5546875, 'learning_rate': 0.00011249151372433169, 'epoch': 0.41}\n",
      "{'loss': 1.1355, 'grad_norm': 0.8671875, 'learning_rate': 0.00011248015134686783, 'epoch': 0.41}\n",
      "{'loss': 1.2136, 'grad_norm': 0.59375, 'learning_rate': 0.00011246878896940396, 'epoch': 0.41}\n",
      "{'loss': 1.2732, 'grad_norm': 0.64453125, 'learning_rate': 0.0001124574265919401, 'epoch': 0.41}\n",
      "{'loss': 1.0489, 'grad_norm': 0.6015625, 'learning_rate': 0.00011244606421447623, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2802, 'grad_norm': 0.498046875, 'learning_rate': 0.00011243470183701239, 'epoch': 0.41}\n",
      "{'loss': 1.2581, 'grad_norm': 0.50390625, 'learning_rate': 0.00011242333945954853, 'epoch': 0.41}\n",
      "{'loss': 1.2608, 'grad_norm': 0.45703125, 'learning_rate': 0.00011241197708208467, 'epoch': 0.41}\n",
      "{'loss': 1.1585, 'grad_norm': 0.8515625, 'learning_rate': 0.0001124006147046208, 'epoch': 0.41}\n",
      "{'loss': 0.9737, 'grad_norm': 0.546875, 'learning_rate': 0.00011238925232715694, 'epoch': 0.41}\n",
      "{'loss': 1.3632, 'grad_norm': 0.390625, 'learning_rate': 0.00011237788994969307, 'epoch': 0.41}\n",
      "{'loss': 1.1662, 'grad_norm': 0.5390625, 'learning_rate': 0.0001123665275722292, 'epoch': 0.41}\n",
      "{'loss': 1.2858, 'grad_norm': 0.4296875, 'learning_rate': 0.00011235516519476537, 'epoch': 0.41}\n",
      "{'loss': 1.3077, 'grad_norm': 0.5234375, 'learning_rate': 0.00011234380281730151, 'epoch': 0.41}\n",
      "{'loss': 1.1279, 'grad_norm': 0.59375, 'learning_rate': 0.00011233244043983764, 'epoch': 0.41}\n",
      "{'loss': 1.3394, 'grad_norm': 0.5, 'learning_rate': 0.00011232107806237377, 'epoch': 0.41}\n",
      "{'loss': 1.1994, 'grad_norm': 0.71875, 'learning_rate': 0.00011230971568490991, 'epoch': 0.41}\n",
      "{'loss': 1.1667, 'grad_norm': 0.419921875, 'learning_rate': 0.00011229835330744604, 'epoch': 0.41}\n",
      "{'loss': 1.3252, 'grad_norm': 0.5859375, 'learning_rate': 0.00011228699092998221, 'epoch': 0.41}\n",
      "{'loss': 1.0858, 'grad_norm': 1.3359375, 'learning_rate': 0.00011227562855251835, 'epoch': 0.41}\n",
      "{'loss': 1.4488, 'grad_norm': 0.515625, 'learning_rate': 0.00011226426617505447, 'epoch': 0.41}\n",
      "{'loss': 1.0931, 'grad_norm': 0.74609375, 'learning_rate': 0.00011225290379759061, 'epoch': 0.41}\n",
      "{'loss': 1.2409, 'grad_norm': 0.47265625, 'learning_rate': 0.00011224154142012675, 'epoch': 0.41}\n",
      "{'loss': 1.2291, 'grad_norm': 0.69921875, 'learning_rate': 0.00011223017904266289, 'epoch': 0.41}\n",
      "{'loss': 1.1561, 'grad_norm': 0.70703125, 'learning_rate': 0.00011221881666519902, 'epoch': 0.41}\n",
      "{'loss': 1.4054, 'grad_norm': 0.49609375, 'learning_rate': 0.00011220745428773517, 'epoch': 0.41}\n",
      "{'loss': 1.2617, 'grad_norm': 0.578125, 'learning_rate': 0.00011219609191027131, 'epoch': 0.41}\n",
      "{'loss': 1.2194, 'grad_norm': 0.435546875, 'learning_rate': 0.00011218472953280745, 'epoch': 0.41}\n",
      "{'loss': 1.3481, 'grad_norm': 0.7578125, 'learning_rate': 0.00011217336715534359, 'epoch': 0.41}\n",
      "{'loss': 0.848, 'grad_norm': 0.7890625, 'learning_rate': 0.00011216200477787973, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2994, 'grad_norm': 0.625, 'learning_rate': 0.00011215064240041586, 'epoch': 0.41}\n",
      "{'loss': 1.23, 'grad_norm': 0.65234375, 'learning_rate': 0.000112139280022952, 'epoch': 0.41}\n",
      "{'loss': 1.2128, 'grad_norm': 0.40234375, 'learning_rate': 0.00011212791764548815, 'epoch': 0.41}\n",
      "{'loss': 1.2476, 'grad_norm': 0.6484375, 'learning_rate': 0.00011211655526802429, 'epoch': 0.41}\n",
      "{'loss': 0.9271, 'grad_norm': 0.88671875, 'learning_rate': 0.00011210519289056043, 'epoch': 0.41}\n",
      "{'loss': 1.2534, 'grad_norm': 0.494140625, 'learning_rate': 0.00011209383051309657, 'epoch': 0.41}\n",
      "{'loss': 1.1472, 'grad_norm': 0.65625, 'learning_rate': 0.0001120824681356327, 'epoch': 0.41}\n",
      "{'loss': 1.2409, 'grad_norm': 0.79296875, 'learning_rate': 0.00011207110575816884, 'epoch': 0.41}\n",
      "{'loss': 1.3267, 'grad_norm': 0.458984375, 'learning_rate': 0.00011205974338070497, 'epoch': 0.41}\n",
      "{'loss': 0.9913, 'grad_norm': 0.435546875, 'learning_rate': 0.00011204838100324113, 'epoch': 0.41}\n",
      "{'loss': 1.2004, 'grad_norm': 0.59765625, 'learning_rate': 0.00011203701862577727, 'epoch': 0.41}\n",
      "{'loss': 1.193, 'grad_norm': 0.6015625, 'learning_rate': 0.00011202565624831341, 'epoch': 0.41}\n",
      "{'loss': 1.2623, 'grad_norm': 0.65234375, 'learning_rate': 0.00011201429387084955, 'epoch': 0.41}\n",
      "{'loss': 1.1896, 'grad_norm': 0.54296875, 'learning_rate': 0.00011200293149338568, 'epoch': 0.41}\n",
      "{'loss': 0.9439, 'grad_norm': 0.400390625, 'learning_rate': 0.00011199156911592181, 'epoch': 0.41}\n",
      "{'loss': 1.1526, 'grad_norm': 0.50390625, 'learning_rate': 0.00011198020673845797, 'epoch': 0.41}\n",
      "{'loss': 1.1379, 'grad_norm': 0.6640625, 'learning_rate': 0.00011196884436099411, 'epoch': 0.41}\n",
      "{'loss': 1.3475, 'grad_norm': 0.4296875, 'learning_rate': 0.00011195748198353025, 'epoch': 0.41}\n",
      "{'loss': 1.1065, 'grad_norm': 0.54296875, 'learning_rate': 0.00011194611960606639, 'epoch': 0.41}\n",
      "{'loss': 1.0318, 'grad_norm': 0.69140625, 'learning_rate': 0.00011193475722860251, 'epoch': 0.41}\n",
      "{'loss': 1.2972, 'grad_norm': 0.6171875, 'learning_rate': 0.00011192339485113865, 'epoch': 0.41}\n",
      "{'loss': 1.2182, 'grad_norm': 0.62109375, 'learning_rate': 0.00011191203247367479, 'epoch': 0.41}\n",
      "{'loss': 1.1993, 'grad_norm': 0.51953125, 'learning_rate': 0.00011190067009621095, 'epoch': 0.41}\n",
      "{'loss': 1.2249, 'grad_norm': 0.64453125, 'learning_rate': 0.00011188930771874709, 'epoch': 0.41}\n",
      "{'loss': 0.945, 'grad_norm': 0.8515625, 'learning_rate': 0.00011187794534128321, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3448, 'grad_norm': 0.57421875, 'learning_rate': 0.00011186658296381935, 'epoch': 0.41}\n",
      "{'loss': 1.2281, 'grad_norm': 0.8125, 'learning_rate': 0.00011185522058635549, 'epoch': 0.41}\n",
      "{'loss': 1.166, 'grad_norm': 0.6484375, 'learning_rate': 0.00011184385820889163, 'epoch': 0.41}\n",
      "{'loss': 1.3382, 'grad_norm': 0.55859375, 'learning_rate': 0.00011183249583142776, 'epoch': 0.41}\n",
      "{'loss': 1.0742, 'grad_norm': 0.7109375, 'learning_rate': 0.00011182113345396392, 'epoch': 0.41}\n",
      "{'loss': 1.3607, 'grad_norm': 0.84375, 'learning_rate': 0.00011180977107650005, 'epoch': 0.41}\n",
      "{'loss': 1.1644, 'grad_norm': 0.72265625, 'learning_rate': 0.00011179840869903619, 'epoch': 0.41}\n",
      "{'loss': 1.1662, 'grad_norm': 0.45703125, 'learning_rate': 0.00011178704632157233, 'epoch': 0.41}\n",
      "{'loss': 1.4424, 'grad_norm': 0.578125, 'learning_rate': 0.00011177568394410847, 'epoch': 0.41}\n",
      "{'loss': 1.1021, 'grad_norm': 0.78515625, 'learning_rate': 0.0001117643215666446, 'epoch': 0.41}\n",
      "{'loss': 1.4194, 'grad_norm': 0.54296875, 'learning_rate': 0.00011175295918918074, 'epoch': 0.41}\n",
      "{'loss': 1.1283, 'grad_norm': 0.76953125, 'learning_rate': 0.0001117415968117169, 'epoch': 0.41}\n",
      "{'loss': 1.1048, 'grad_norm': 0.53125, 'learning_rate': 0.00011173023443425303, 'epoch': 0.41}\n",
      "{'loss': 1.2983, 'grad_norm': 0.6171875, 'learning_rate': 0.00011171887205678917, 'epoch': 0.41}\n",
      "{'loss': 1.1354, 'grad_norm': 0.77734375, 'learning_rate': 0.00011170750967932531, 'epoch': 0.41}\n",
      "{'loss': 1.3217, 'grad_norm': 0.49609375, 'learning_rate': 0.00011169614730186145, 'epoch': 0.41}\n",
      "{'loss': 1.1698, 'grad_norm': 0.984375, 'learning_rate': 0.00011168478492439758, 'epoch': 0.41}\n",
      "{'loss': 1.2302, 'grad_norm': 0.57421875, 'learning_rate': 0.00011167342254693371, 'epoch': 0.41}\n",
      "{'loss': 1.179, 'grad_norm': 0.70703125, 'learning_rate': 0.00011166206016946987, 'epoch': 0.41}\n",
      "{'loss': 1.1587, 'grad_norm': 1.296875, 'learning_rate': 0.00011165069779200601, 'epoch': 0.41}\n",
      "{'loss': 1.2664, 'grad_norm': 0.56640625, 'learning_rate': 0.00011163933541454215, 'epoch': 0.41}\n",
      "{'loss': 1.1511, 'grad_norm': 0.6328125, 'learning_rate': 0.00011162797303707829, 'epoch': 0.41}\n",
      "{'loss': 1.1492, 'grad_norm': 0.4453125, 'learning_rate': 0.00011161661065961442, 'epoch': 0.41}\n",
      "{'loss': 1.2429, 'grad_norm': 0.609375, 'learning_rate': 0.00011160524828215055, 'epoch': 0.41}\n",
      "{'loss': 1.0853, 'grad_norm': 0.69921875, 'learning_rate': 0.00011159388590468671, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4727, 'grad_norm': 0.51953125, 'learning_rate': 0.00011158252352722285, 'epoch': 0.41}\n",
      "{'loss': 1.1192, 'grad_norm': 0.68359375, 'learning_rate': 0.00011157116114975899, 'epoch': 0.41}\n",
      "{'loss': 1.1498, 'grad_norm': 0.431640625, 'learning_rate': 0.00011155979877229513, 'epoch': 0.41}\n",
      "{'loss': 1.2699, 'grad_norm': 0.55859375, 'learning_rate': 0.00011154843639483125, 'epoch': 0.41}\n",
      "{'loss': 1.123, 'grad_norm': 1.1953125, 'learning_rate': 0.00011153707401736739, 'epoch': 0.41}\n",
      "{'loss': 1.4332, 'grad_norm': 0.466796875, 'learning_rate': 0.00011152571163990353, 'epoch': 0.41}\n",
      "{'loss': 1.1465, 'grad_norm': 0.6875, 'learning_rate': 0.00011151434926243969, 'epoch': 0.41}\n",
      "{'loss': 1.2291, 'grad_norm': 0.474609375, 'learning_rate': 0.00011150298688497583, 'epoch': 0.41}\n",
      "{'loss': 1.1866, 'grad_norm': 0.515625, 'learning_rate': 0.00011149162450751195, 'epoch': 0.41}\n",
      "{'loss': 1.1032, 'grad_norm': 0.357421875, 'learning_rate': 0.00011148026213004809, 'epoch': 0.41}\n",
      "{'loss': 1.2382, 'grad_norm': 0.609375, 'learning_rate': 0.00011146889975258423, 'epoch': 0.41}\n",
      "{'loss': 1.0797, 'grad_norm': 0.72265625, 'learning_rate': 0.00011145753737512037, 'epoch': 0.41}\n",
      "{'loss': 1.1763, 'grad_norm': 0.62109375, 'learning_rate': 0.0001114461749976565, 'epoch': 0.41}\n",
      "{'loss': 1.2635, 'grad_norm': 0.640625, 'learning_rate': 0.00011143481262019266, 'epoch': 0.41}\n",
      "{'loss': 1.1053, 'grad_norm': 1.265625, 'learning_rate': 0.0001114234502427288, 'epoch': 0.41}\n",
      "{'loss': 1.4272, 'grad_norm': 0.55078125, 'learning_rate': 0.00011141208786526493, 'epoch': 0.41}\n",
      "{'loss': 1.1182, 'grad_norm': 0.71875, 'learning_rate': 0.00011140072548780107, 'epoch': 0.41}\n",
      "{'loss': 1.1385, 'grad_norm': 0.3828125, 'learning_rate': 0.00011138936311033721, 'epoch': 0.41}\n",
      "{'loss': 1.2184, 'grad_norm': 0.546875, 'learning_rate': 0.00011137800073287335, 'epoch': 0.41}\n",
      "{'loss': 1.1171, 'grad_norm': 0.80859375, 'learning_rate': 0.00011136663835540948, 'epoch': 0.41}\n",
      "{'loss': 1.3024, 'grad_norm': 0.470703125, 'learning_rate': 0.00011135527597794564, 'epoch': 0.41}\n",
      "{'loss': 1.1646, 'grad_norm': 0.7734375, 'learning_rate': 0.00011134391360048177, 'epoch': 0.41}\n",
      "{'loss': 1.2474, 'grad_norm': 0.3828125, 'learning_rate': 0.00011133255122301791, 'epoch': 0.41}\n",
      "{'loss': 1.1705, 'grad_norm': 0.63671875, 'learning_rate': 0.00011132118884555405, 'epoch': 0.41}\n",
      "{'loss': 1.0545, 'grad_norm': 0.82421875, 'learning_rate': 0.00011130982646809019, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4905, 'grad_norm': 0.5234375, 'learning_rate': 0.00011129846409062632, 'epoch': 0.41}\n",
      "{'loss': 1.116, 'grad_norm': 0.62890625, 'learning_rate': 0.00011128710171316248, 'epoch': 0.41}\n",
      "{'loss': 1.1192, 'grad_norm': 0.5546875, 'learning_rate': 0.00011127573933569861, 'epoch': 0.41}\n",
      "{'loss': 1.2885, 'grad_norm': 0.5234375, 'learning_rate': 0.00011126437695823475, 'epoch': 0.41}\n",
      "{'loss': 1.0763, 'grad_norm': 0.72265625, 'learning_rate': 0.00011125301458077089, 'epoch': 0.41}\n",
      "{'loss': 1.2519, 'grad_norm': 0.494140625, 'learning_rate': 0.00011124165220330703, 'epoch': 0.41}\n",
      "{'loss': 1.144, 'grad_norm': 0.78515625, 'learning_rate': 0.00011123028982584317, 'epoch': 0.41}\n",
      "{'loss': 1.2064, 'grad_norm': 0.5625, 'learning_rate': 0.00011121892744837929, 'epoch': 0.41}\n",
      "{'loss': 1.2573, 'grad_norm': 0.65234375, 'learning_rate': 0.00011120756507091545, 'epoch': 0.41}\n",
      "{'loss': 1.0723, 'grad_norm': 0.78515625, 'learning_rate': 0.00011119620269345159, 'epoch': 0.41}\n",
      "{'loss': 1.3272, 'grad_norm': 0.5234375, 'learning_rate': 0.00011118484031598773, 'epoch': 0.41}\n",
      "{'loss': 1.2519, 'grad_norm': 0.75390625, 'learning_rate': 0.00011117347793852387, 'epoch': 0.41}\n",
      "{'loss': 1.2446, 'grad_norm': 0.51171875, 'learning_rate': 0.00011116211556105999, 'epoch': 0.41}\n",
      "{'loss': 1.2701, 'grad_norm': 0.70703125, 'learning_rate': 0.00011115075318359613, 'epoch': 0.41}\n",
      "{'loss': 1.0818, 'grad_norm': 0.72265625, 'learning_rate': 0.00011113939080613227, 'epoch': 0.41}\n",
      "{'loss': 1.2326, 'grad_norm': 0.56640625, 'learning_rate': 0.00011112802842866843, 'epoch': 0.41}\n",
      "{'loss': 1.1526, 'grad_norm': 0.671875, 'learning_rate': 0.00011111666605120457, 'epoch': 0.41}\n",
      "{'loss': 1.2237, 'grad_norm': 0.416015625, 'learning_rate': 0.0001111053036737407, 'epoch': 0.42}\n",
      "{'loss': 1.3066, 'grad_norm': 0.75, 'learning_rate': 0.00011109394129627683, 'epoch': 0.42}\n",
      "{'loss': 1.0108, 'grad_norm': 0.73046875, 'learning_rate': 0.00011108257891881297, 'epoch': 0.42}\n",
      "{'loss': 1.3104, 'grad_norm': 0.3984375, 'learning_rate': 0.00011107121654134911, 'epoch': 0.42}\n",
      "{'loss': 1.1471, 'grad_norm': 0.71484375, 'learning_rate': 0.00011105985416388525, 'epoch': 0.42}\n",
      "{'loss': 1.1465, 'grad_norm': 0.490234375, 'learning_rate': 0.0001110484917864214, 'epoch': 0.42}\n",
      "{'loss': 1.2069, 'grad_norm': 0.80859375, 'learning_rate': 0.00011103712940895754, 'epoch': 0.42}\n",
      "{'loss': 1.076, 'grad_norm': 0.671875, 'learning_rate': 0.00011102576703149367, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3399, 'grad_norm': 0.498046875, 'learning_rate': 0.00011101440465402981, 'epoch': 0.42}\n",
      "{'loss': 1.1458, 'grad_norm': 0.71484375, 'learning_rate': 0.00011100304227656595, 'epoch': 0.42}\n",
      "{'loss': 1.1375, 'grad_norm': 0.458984375, 'learning_rate': 0.00011099167989910209, 'epoch': 0.42}\n",
      "{'loss': 1.3448, 'grad_norm': 0.66796875, 'learning_rate': 0.00011098031752163823, 'epoch': 0.42}\n",
      "{'loss': 1.0795, 'grad_norm': 0.76953125, 'learning_rate': 0.00011096895514417438, 'epoch': 0.42}\n",
      "{'loss': 1.2416, 'grad_norm': 0.5703125, 'learning_rate': 0.00011095759276671051, 'epoch': 0.42}\n",
      "{'loss': 1.087, 'grad_norm': 0.921875, 'learning_rate': 0.00011094623038924665, 'epoch': 0.42}\n",
      "{'loss': 1.2373, 'grad_norm': 0.455078125, 'learning_rate': 0.00011093486801178279, 'epoch': 0.42}\n",
      "{'loss': 1.2276, 'grad_norm': 0.63671875, 'learning_rate': 0.00011092350563431893, 'epoch': 0.42}\n",
      "{'loss': 1.1141, 'grad_norm': 1.171875, 'learning_rate': 0.00011091214325685507, 'epoch': 0.42}\n",
      "{'loss': 1.2862, 'grad_norm': 0.5078125, 'learning_rate': 0.00011090078087939122, 'epoch': 0.42}\n",
      "{'loss': 1.2443, 'grad_norm': 0.9453125, 'learning_rate': 0.00011088941850192736, 'epoch': 0.42}\n",
      "{'loss': 1.2047, 'grad_norm': 0.45703125, 'learning_rate': 0.0001108780561244635, 'epoch': 0.42}\n",
      "{'loss': 1.184, 'grad_norm': 0.6015625, 'learning_rate': 0.00011086669374699963, 'epoch': 0.42}\n",
      "{'loss': 1.0385, 'grad_norm': 0.69140625, 'learning_rate': 0.00011085533136953577, 'epoch': 0.42}\n",
      "{'loss': 1.4349, 'grad_norm': 0.5078125, 'learning_rate': 0.00011084396899207191, 'epoch': 0.42}\n",
      "{'loss': 1.2631, 'grad_norm': 0.921875, 'learning_rate': 0.00011083260661460803, 'epoch': 0.42}\n",
      "{'loss': 1.0976, 'grad_norm': 0.50390625, 'learning_rate': 0.0001108212442371442, 'epoch': 0.42}\n",
      "{'loss': 1.2607, 'grad_norm': 0.703125, 'learning_rate': 0.00011080988185968033, 'epoch': 0.42}\n",
      "{'loss': 1.1755, 'grad_norm': 0.58203125, 'learning_rate': 0.00011079851948221647, 'epoch': 0.42}\n",
      "{'loss': 1.5104, 'grad_norm': 0.51171875, 'learning_rate': 0.00011078715710475261, 'epoch': 0.42}\n",
      "{'loss': 1.1395, 'grad_norm': 0.92578125, 'learning_rate': 0.00011077579472728873, 'epoch': 0.42}\n",
      "{'loss': 1.167, 'grad_norm': 0.625, 'learning_rate': 0.00011076443234982487, 'epoch': 0.42}\n",
      "{'loss': 1.297, 'grad_norm': 0.546875, 'learning_rate': 0.00011075306997236101, 'epoch': 0.42}\n",
      "{'loss': 0.9707, 'grad_norm': 0.7265625, 'learning_rate': 0.00011074170759489717, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3701, 'grad_norm': 0.5859375, 'learning_rate': 0.00011073034521743331, 'epoch': 0.42}\n",
      "{'loss': 1.1848, 'grad_norm': 0.65625, 'learning_rate': 0.00011071898283996944, 'epoch': 0.42}\n",
      "{'loss': 1.2742, 'grad_norm': 0.57421875, 'learning_rate': 0.00011070762046250557, 'epoch': 0.42}\n",
      "{'loss': 1.1349, 'grad_norm': 0.58984375, 'learning_rate': 0.00011069625808504171, 'epoch': 0.42}\n",
      "{'loss': 1.0888, 'grad_norm': 1.0859375, 'learning_rate': 0.00011068489570757785, 'epoch': 0.42}\n",
      "{'loss': 1.4619, 'grad_norm': 0.58984375, 'learning_rate': 0.00011067353333011399, 'epoch': 0.42}\n",
      "{'loss': 1.1895, 'grad_norm': 0.50390625, 'learning_rate': 0.00011066217095265014, 'epoch': 0.42}\n",
      "{'loss': 1.3541, 'grad_norm': 0.48828125, 'learning_rate': 0.00011065080857518628, 'epoch': 0.42}\n",
      "{'loss': 1.1523, 'grad_norm': 0.6484375, 'learning_rate': 0.00011063944619772242, 'epoch': 0.42}\n",
      "{'loss': 1.1893, 'grad_norm': 0.8046875, 'learning_rate': 0.00011062808382025855, 'epoch': 0.42}\n",
      "{'loss': 1.2659, 'grad_norm': 0.6328125, 'learning_rate': 0.00011061672144279469, 'epoch': 0.42}\n",
      "{'loss': 1.1817, 'grad_norm': 1.09375, 'learning_rate': 0.00011060535906533083, 'epoch': 0.42}\n",
      "{'loss': 1.2801, 'grad_norm': 0.6015625, 'learning_rate': 0.00011059399668786698, 'epoch': 0.42}\n",
      "{'loss': 1.2075, 'grad_norm': 0.765625, 'learning_rate': 0.00011058263431040312, 'epoch': 0.42}\n",
      "{'loss': 1.0178, 'grad_norm': 0.8203125, 'learning_rate': 0.00011057127193293926, 'epoch': 0.42}\n",
      "{'loss': 1.2986, 'grad_norm': 0.46875, 'learning_rate': 0.0001105599095554754, 'epoch': 0.42}\n",
      "{'loss': 1.1675, 'grad_norm': 1.34375, 'learning_rate': 0.00011054854717801153, 'epoch': 0.42}\n",
      "{'loss': 1.3428, 'grad_norm': 0.58203125, 'learning_rate': 0.00011053718480054767, 'epoch': 0.42}\n",
      "{'loss': 1.233, 'grad_norm': 0.57421875, 'learning_rate': 0.00011052582242308381, 'epoch': 0.42}\n",
      "{'loss': 1.2161, 'grad_norm': 1.0703125, 'learning_rate': 0.00011051446004561996, 'epoch': 0.42}\n",
      "{'loss': 1.3974, 'grad_norm': 0.46875, 'learning_rate': 0.0001105030976681561, 'epoch': 0.42}\n",
      "{'loss': 1.0694, 'grad_norm': 0.6171875, 'learning_rate': 0.00011049173529069223, 'epoch': 0.42}\n",
      "{'loss': 1.2488, 'grad_norm': 0.53515625, 'learning_rate': 0.00011048037291322837, 'epoch': 0.42}\n",
      "{'loss': 1.2469, 'grad_norm': 0.67578125, 'learning_rate': 0.00011046901053576451, 'epoch': 0.42}\n",
      "{'loss': 0.969, 'grad_norm': 0.67578125, 'learning_rate': 0.00011045764815830065, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3721, 'grad_norm': 0.58984375, 'learning_rate': 0.00011044628578083677, 'epoch': 0.42}\n",
      "{'loss': 1.159, 'grad_norm': 0.81640625, 'learning_rate': 0.00011043492340337294, 'epoch': 0.42}\n",
      "{'loss': 1.1907, 'grad_norm': 0.45703125, 'learning_rate': 0.00011042356102590908, 'epoch': 0.42}\n",
      "{'loss': 1.311, 'grad_norm': 0.75, 'learning_rate': 0.00011041219864844521, 'epoch': 0.42}\n",
      "{'loss': 1.0773, 'grad_norm': 0.6484375, 'learning_rate': 0.00011040083627098135, 'epoch': 0.42}\n",
      "{'loss': 1.285, 'grad_norm': 0.52734375, 'learning_rate': 0.00011038947389351748, 'epoch': 0.42}\n",
      "{'loss': 1.3067, 'grad_norm': 0.9453125, 'learning_rate': 0.00011037811151605361, 'epoch': 0.42}\n",
      "{'loss': 1.1675, 'grad_norm': 0.46875, 'learning_rate': 0.00011036674913858975, 'epoch': 0.42}\n",
      "{'loss': 1.2584, 'grad_norm': 0.59765625, 'learning_rate': 0.00011035538676112592, 'epoch': 0.42}\n",
      "{'loss': 1.1943, 'grad_norm': 0.330078125, 'learning_rate': 0.00011034402438366205, 'epoch': 0.42}\n",
      "{'loss': 1.2377, 'grad_norm': 0.50390625, 'learning_rate': 0.00011033266200619818, 'epoch': 0.42}\n",
      "{'loss': 1.1439, 'grad_norm': 0.66796875, 'learning_rate': 0.00011032129962873432, 'epoch': 0.42}\n",
      "{'loss': 1.1157, 'grad_norm': 0.45703125, 'learning_rate': 0.00011030993725127045, 'epoch': 0.42}\n",
      "{'loss': 1.1953, 'grad_norm': 0.703125, 'learning_rate': 0.00011029857487380659, 'epoch': 0.42}\n",
      "{'loss': 1.1405, 'grad_norm': 0.6796875, 'learning_rate': 0.00011028721249634276, 'epoch': 0.42}\n",
      "{'loss': 1.2554, 'grad_norm': 0.546875, 'learning_rate': 0.00011027585011887888, 'epoch': 0.42}\n",
      "{'loss': 1.0281, 'grad_norm': 0.7265625, 'learning_rate': 0.00011026448774141502, 'epoch': 0.42}\n",
      "{'loss': 1.1128, 'grad_norm': 0.48046875, 'learning_rate': 0.00011025312536395116, 'epoch': 0.42}\n",
      "{'loss': 1.2324, 'grad_norm': 0.63671875, 'learning_rate': 0.0001102417629864873, 'epoch': 0.42}\n",
      "{'loss': 1.0119, 'grad_norm': 0.50390625, 'learning_rate': 0.00011023040060902343, 'epoch': 0.42}\n",
      "{'loss': 1.3308, 'grad_norm': 0.58203125, 'learning_rate': 0.00011021903823155957, 'epoch': 0.42}\n",
      "{'loss': 1.1054, 'grad_norm': 0.64453125, 'learning_rate': 0.00011020767585409572, 'epoch': 0.42}\n",
      "{'loss': 1.3338, 'grad_norm': 0.458984375, 'learning_rate': 0.00011019631347663186, 'epoch': 0.42}\n",
      "{'loss': 1.2311, 'grad_norm': 0.52734375, 'learning_rate': 0.000110184951099168, 'epoch': 0.42}\n",
      "{'loss': 1.0316, 'grad_norm': 0.80078125, 'learning_rate': 0.00011017358872170414, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.392, 'grad_norm': 0.484375, 'learning_rate': 0.00011016222634424027, 'epoch': 0.42}\n",
      "{'loss': 1.2456, 'grad_norm': 0.66796875, 'learning_rate': 0.00011015086396677641, 'epoch': 0.42}\n",
      "{'loss': 1.2556, 'grad_norm': 0.431640625, 'learning_rate': 0.00011013950158931255, 'epoch': 0.42}\n",
      "{'loss': 1.0886, 'grad_norm': 0.68359375, 'learning_rate': 0.0001101281392118487, 'epoch': 0.42}\n",
      "{'loss': 0.9838, 'grad_norm': 0.6328125, 'learning_rate': 0.00011011677683438484, 'epoch': 0.42}\n",
      "{'loss': 1.3427, 'grad_norm': 0.578125, 'learning_rate': 0.00011010541445692098, 'epoch': 0.42}\n",
      "{'loss': 1.1683, 'grad_norm': 0.7578125, 'learning_rate': 0.00011009405207945711, 'epoch': 0.42}\n",
      "{'loss': 1.0803, 'grad_norm': 0.51953125, 'learning_rate': 0.00011008268970199325, 'epoch': 0.42}\n",
      "{'loss': 1.371, 'grad_norm': 0.58984375, 'learning_rate': 0.00011007132732452939, 'epoch': 0.42}\n",
      "{'loss': 1.0406, 'grad_norm': 0.91796875, 'learning_rate': 0.00011005996494706551, 'epoch': 0.42}\n",
      "{'loss': 1.3563, 'grad_norm': 0.4765625, 'learning_rate': 0.00011004860256960168, 'epoch': 0.42}\n",
      "{'loss': 1.1626, 'grad_norm': 0.6796875, 'learning_rate': 0.00011003724019213782, 'epoch': 0.42}\n",
      "{'loss': 1.2422, 'grad_norm': 0.470703125, 'learning_rate': 0.00011002587781467395, 'epoch': 0.42}\n",
      "{'loss': 1.3258, 'grad_norm': 0.59375, 'learning_rate': 0.00011001451543721009, 'epoch': 0.42}\n",
      "{'loss': 1.2271, 'grad_norm': 1.796875, 'learning_rate': 0.00011000315305974622, 'epoch': 0.42}\n",
      "{'loss': 1.1728, 'grad_norm': 0.45703125, 'learning_rate': 0.00010999179068228235, 'epoch': 0.42}\n",
      "{'loss': 1.1927, 'grad_norm': 0.640625, 'learning_rate': 0.00010998042830481849, 'epoch': 0.42}\n",
      "{'loss': 1.2817, 'grad_norm': 0.427734375, 'learning_rate': 0.00010996906592735466, 'epoch': 0.42}\n",
      "{'loss': 1.1424, 'grad_norm': 0.6953125, 'learning_rate': 0.0001099577035498908, 'epoch': 0.42}\n",
      "{'loss': 1.0292, 'grad_norm': 1.109375, 'learning_rate': 0.00010994634117242692, 'epoch': 0.42}\n",
      "{'loss': 1.2048, 'grad_norm': 0.50390625, 'learning_rate': 0.00010993497879496306, 'epoch': 0.42}\n",
      "{'loss': 1.1818, 'grad_norm': 0.79296875, 'learning_rate': 0.0001099236164174992, 'epoch': 0.42}\n",
      "{'loss': 1.1442, 'grad_norm': 0.44140625, 'learning_rate': 0.00010991225404003533, 'epoch': 0.42}\n",
      "{'loss': 1.1784, 'grad_norm': 0.8359375, 'learning_rate': 0.0001099008916625715, 'epoch': 0.42}\n",
      "{'loss': 1.0804, 'grad_norm': 0.9375, 'learning_rate': 0.00010988952928510762, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3408, 'grad_norm': 0.5625, 'learning_rate': 0.00010987816690764376, 'epoch': 0.42}\n",
      "{'loss': 1.1827, 'grad_norm': 0.9296875, 'learning_rate': 0.0001098668045301799, 'epoch': 0.42}\n",
      "{'loss': 1.1404, 'grad_norm': 0.375, 'learning_rate': 0.00010985544215271604, 'epoch': 0.42}\n",
      "{'loss': 1.2626, 'grad_norm': 0.69921875, 'learning_rate': 0.00010984407977525217, 'epoch': 0.42}\n",
      "{'loss': 1.0949, 'grad_norm': 0.65234375, 'learning_rate': 0.00010983271739778831, 'epoch': 0.42}\n",
      "{'loss': 1.2635, 'grad_norm': 0.5625, 'learning_rate': 0.00010982135502032446, 'epoch': 0.42}\n",
      "{'loss': 1.3309, 'grad_norm': 0.8203125, 'learning_rate': 0.0001098099926428606, 'epoch': 0.42}\n",
      "{'loss': 1.2871, 'grad_norm': 0.80859375, 'learning_rate': 0.00010979863026539674, 'epoch': 0.42}\n",
      "{'loss': 1.2468, 'grad_norm': 0.87109375, 'learning_rate': 0.00010978726788793288, 'epoch': 0.42}\n",
      "{'loss': 1.0805, 'grad_norm': 1.15625, 'learning_rate': 0.00010977590551046901, 'epoch': 0.42}\n",
      "{'loss': 1.3571, 'grad_norm': 0.68359375, 'learning_rate': 0.00010976454313300515, 'epoch': 0.42}\n",
      "{'loss': 1.193, 'grad_norm': 0.87109375, 'learning_rate': 0.00010975318075554129, 'epoch': 0.42}\n",
      "{'loss': 1.2569, 'grad_norm': 0.431640625, 'learning_rate': 0.00010974181837807744, 'epoch': 0.42}\n",
      "{'loss': 1.1725, 'grad_norm': 0.61328125, 'learning_rate': 0.00010973045600061358, 'epoch': 0.42}\n",
      "{'loss': 1.1216, 'grad_norm': 0.8515625, 'learning_rate': 0.00010971909362314972, 'epoch': 0.42}\n",
      "{'loss': 1.3513, 'grad_norm': 0.478515625, 'learning_rate': 0.00010970773124568585, 'epoch': 0.42}\n",
      "{'loss': 1.265, 'grad_norm': 0.77734375, 'learning_rate': 0.00010969636886822199, 'epoch': 0.42}\n",
      "{'loss': 1.3029, 'grad_norm': 0.46484375, 'learning_rate': 0.00010968500649075813, 'epoch': 0.42}\n",
      "{'loss': 1.3144, 'grad_norm': 0.8125, 'learning_rate': 0.00010967364411329425, 'epoch': 0.42}\n",
      "{'loss': 1.0869, 'grad_norm': 0.93359375, 'learning_rate': 0.00010966228173583042, 'epoch': 0.42}\n",
      "{'loss': 1.3766, 'grad_norm': 0.58984375, 'learning_rate': 0.00010965091935836656, 'epoch': 0.42}\n",
      "{'loss': 1.1863, 'grad_norm': 0.6328125, 'learning_rate': 0.0001096395569809027, 'epoch': 0.42}\n",
      "{'loss': 1.1845, 'grad_norm': 0.50390625, 'learning_rate': 0.00010962819460343883, 'epoch': 0.42}\n",
      "{'loss': 1.2887, 'grad_norm': 0.6328125, 'learning_rate': 0.00010961683222597496, 'epoch': 0.42}\n",
      "{'loss': 1.1372, 'grad_norm': 0.7890625, 'learning_rate': 0.0001096054698485111, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2897, 'grad_norm': 0.54296875, 'learning_rate': 0.00010959410747104726, 'epoch': 0.42}\n",
      "{'loss': 1.2779, 'grad_norm': 0.515625, 'learning_rate': 0.0001095827450935834, 'epoch': 0.42}\n",
      "{'loss': 1.2284, 'grad_norm': 0.451171875, 'learning_rate': 0.00010957138271611954, 'epoch': 0.42}\n",
      "{'loss': 1.2386, 'grad_norm': 0.703125, 'learning_rate': 0.00010956002033865566, 'epoch': 0.42}\n",
      "{'loss': 1.0164, 'grad_norm': 0.71875, 'learning_rate': 0.0001095486579611918, 'epoch': 0.42}\n",
      "{'loss': 1.3932, 'grad_norm': 0.482421875, 'learning_rate': 0.00010953729558372794, 'epoch': 0.42}\n",
      "{'loss': 1.113, 'grad_norm': 0.67578125, 'learning_rate': 0.00010952593320626407, 'epoch': 0.42}\n",
      "{'loss': 1.2266, 'grad_norm': 0.50390625, 'learning_rate': 0.00010951457082880024, 'epoch': 0.42}\n",
      "{'loss': 1.2936, 'grad_norm': 0.63671875, 'learning_rate': 0.00010950320845133636, 'epoch': 0.42}\n",
      "{'loss': 1.2555, 'grad_norm': 1.140625, 'learning_rate': 0.0001094918460738725, 'epoch': 0.42}\n",
      "{'loss': 1.4044, 'grad_norm': 0.6015625, 'learning_rate': 0.00010948048369640864, 'epoch': 0.42}\n",
      "{'loss': 1.2532, 'grad_norm': 0.64453125, 'learning_rate': 0.00010946912131894478, 'epoch': 0.42}\n",
      "{'loss': 1.3044, 'grad_norm': 0.443359375, 'learning_rate': 0.00010945775894148091, 'epoch': 0.42}\n",
      "{'loss': 1.2473, 'grad_norm': 0.55078125, 'learning_rate': 0.00010944639656401705, 'epoch': 0.42}\n",
      "{'loss': 1.1389, 'grad_norm': 0.828125, 'learning_rate': 0.0001094350341865532, 'epoch': 0.42}\n",
      "{'loss': 1.1829, 'grad_norm': 0.5078125, 'learning_rate': 0.00010942367180908934, 'epoch': 0.42}\n",
      "{'loss': 1.1195, 'grad_norm': 0.66015625, 'learning_rate': 0.00010941230943162548, 'epoch': 0.42}\n",
      "{'loss': 1.1021, 'grad_norm': 0.515625, 'learning_rate': 0.00010940094705416162, 'epoch': 0.42}\n",
      "{'loss': 1.195, 'grad_norm': 0.50390625, 'learning_rate': 0.00010938958467669776, 'epoch': 0.42}\n",
      "{'loss': 1.048, 'grad_norm': 1.453125, 'learning_rate': 0.0001093782222992339, 'epoch': 0.42}\n",
      "{'loss': 1.2517, 'grad_norm': 0.6875, 'learning_rate': 0.00010936685992177003, 'epoch': 0.42}\n",
      "{'loss': 1.0717, 'grad_norm': 0.60546875, 'learning_rate': 0.00010935549754430618, 'epoch': 0.42}\n",
      "{'loss': 1.2455, 'grad_norm': 0.404296875, 'learning_rate': 0.00010934413516684232, 'epoch': 0.42}\n",
      "{'loss': 1.2633, 'grad_norm': 0.640625, 'learning_rate': 0.00010933277278937846, 'epoch': 0.42}\n",
      "{'loss': 1.0545, 'grad_norm': 0.4140625, 'learning_rate': 0.0001093214104119146, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3899, 'grad_norm': 0.5, 'learning_rate': 0.00010931004803445073, 'epoch': 0.42}\n",
      "{'loss': 1.1863, 'grad_norm': 0.90234375, 'learning_rate': 0.00010929868565698687, 'epoch': 0.42}\n",
      "{'loss': 1.2429, 'grad_norm': 0.59375, 'learning_rate': 0.000109287323279523, 'epoch': 0.42}\n",
      "{'loss': 1.1443, 'grad_norm': 0.54296875, 'learning_rate': 0.00010927596090205916, 'epoch': 0.42}\n",
      "{'loss': 1.0387, 'grad_norm': 1.4140625, 'learning_rate': 0.0001092645985245953, 'epoch': 0.42}\n",
      "{'loss': 1.2447, 'grad_norm': 0.52734375, 'learning_rate': 0.00010925323614713144, 'epoch': 0.42}\n",
      "{'loss': 1.0804, 'grad_norm': 0.70703125, 'learning_rate': 0.00010924187376966757, 'epoch': 0.42}\n",
      "{'loss': 1.2333, 'grad_norm': 0.419921875, 'learning_rate': 0.0001092305113922037, 'epoch': 0.42}\n",
      "{'loss': 1.2181, 'grad_norm': 0.61328125, 'learning_rate': 0.00010921914901473984, 'epoch': 0.42}\n",
      "{'loss': 1.1231, 'grad_norm': 0.734375, 'learning_rate': 0.000109207786637276, 'epoch': 0.42}\n",
      "{'loss': 1.3321, 'grad_norm': 0.5234375, 'learning_rate': 0.00010919642425981214, 'epoch': 0.42}\n",
      "{'loss': 1.1956, 'grad_norm': 1.265625, 'learning_rate': 0.00010918506188234828, 'epoch': 0.42}\n",
      "{'loss': 1.2254, 'grad_norm': 0.451171875, 'learning_rate': 0.0001091736995048844, 'epoch': 0.42}\n",
      "{'loss': 1.2539, 'grad_norm': 0.55078125, 'learning_rate': 0.00010916233712742054, 'epoch': 0.42}\n",
      "{'loss': 1.0346, 'grad_norm': 0.98046875, 'learning_rate': 0.00010915097474995668, 'epoch': 0.42}\n",
      "{'loss': 1.3355, 'grad_norm': 0.470703125, 'learning_rate': 0.00010913961237249282, 'epoch': 0.42}\n",
      "{'loss': 1.2881, 'grad_norm': 0.98828125, 'learning_rate': 0.00010912824999502898, 'epoch': 0.42}\n",
      "{'loss': 1.098, 'grad_norm': 0.6796875, 'learning_rate': 0.0001091168876175651, 'epoch': 0.42}\n",
      "{'loss': 1.1833, 'grad_norm': 0.75390625, 'learning_rate': 0.00010910552524010124, 'epoch': 0.42}\n",
      "{'loss': 0.9711, 'grad_norm': 0.28515625, 'learning_rate': 0.00010909416286263738, 'epoch': 0.42}\n",
      "{'loss': 1.3645, 'grad_norm': 0.51171875, 'learning_rate': 0.00010908280048517352, 'epoch': 0.42}\n",
      "{'loss': 1.1777, 'grad_norm': 0.74609375, 'learning_rate': 0.00010907143810770966, 'epoch': 0.42}\n",
      "{'loss': 1.1343, 'grad_norm': 0.67578125, 'learning_rate': 0.0001090600757302458, 'epoch': 0.42}\n",
      "{'loss': 1.2705, 'grad_norm': 0.58203125, 'learning_rate': 0.00010904871335278195, 'epoch': 0.42}\n",
      "{'loss': 1.0219, 'grad_norm': 1.34375, 'learning_rate': 0.00010903735097531808, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3218, 'grad_norm': 0.51171875, 'learning_rate': 0.00010902598859785422, 'epoch': 0.42}\n",
      "{'loss': 1.138, 'grad_norm': 0.5625, 'learning_rate': 0.00010901462622039036, 'epoch': 0.42}\n",
      "{'loss': 1.1515, 'grad_norm': 0.4921875, 'learning_rate': 0.0001090032638429265, 'epoch': 0.42}\n",
      "{'loss': 1.3038, 'grad_norm': 0.61328125, 'learning_rate': 0.00010899190146546263, 'epoch': 0.42}\n",
      "{'loss': 1.0682, 'grad_norm': 1.015625, 'learning_rate': 0.00010898053908799877, 'epoch': 0.42}\n",
      "{'loss': 1.3049, 'grad_norm': 0.486328125, 'learning_rate': 0.00010896917671053492, 'epoch': 0.42}\n",
      "{'loss': 1.1771, 'grad_norm': 0.72265625, 'learning_rate': 0.00010895781433307106, 'epoch': 0.42}\n",
      "{'loss': 1.2358, 'grad_norm': 0.408203125, 'learning_rate': 0.0001089464519556072, 'epoch': 0.42}\n",
      "{'loss': 1.0863, 'grad_norm': 0.76953125, 'learning_rate': 0.00010893508957814334, 'epoch': 0.42}\n",
      "{'loss': 1.059, 'grad_norm': 0.94921875, 'learning_rate': 0.00010892372720067948, 'epoch': 0.42}\n",
      "{'loss': 1.3641, 'grad_norm': 0.6953125, 'learning_rate': 0.00010891236482321561, 'epoch': 0.42}\n",
      "{'loss': 1.1087, 'grad_norm': 0.625, 'learning_rate': 0.00010890100244575176, 'epoch': 0.42}\n",
      "{'loss': 1.1223, 'grad_norm': 0.69921875, 'learning_rate': 0.0001088896400682879, 'epoch': 0.42}\n",
      "{'loss': 1.2336, 'grad_norm': 0.6875, 'learning_rate': 0.00010887827769082404, 'epoch': 0.42}\n",
      "{'loss': 1.0344, 'grad_norm': 0.76953125, 'learning_rate': 0.00010886691531336018, 'epoch': 0.42}\n",
      "{'loss': 1.3852, 'grad_norm': 0.54296875, 'learning_rate': 0.00010885555293589632, 'epoch': 0.42}\n",
      "{'loss': 1.1999, 'grad_norm': 0.85546875, 'learning_rate': 0.00010884419055843244, 'epoch': 0.42}\n",
      "{'loss': 1.2379, 'grad_norm': 0.4765625, 'learning_rate': 0.00010883282818096858, 'epoch': 0.42}\n",
      "{'loss': 1.3616, 'grad_norm': 0.67578125, 'learning_rate': 0.00010882146580350474, 'epoch': 0.42}\n",
      "{'loss': 1.096, 'grad_norm': 1.46875, 'learning_rate': 0.00010881010342604088, 'epoch': 0.43}\n",
      "{'loss': 1.3649, 'grad_norm': 0.44921875, 'learning_rate': 0.00010879874104857702, 'epoch': 0.43}\n",
      "{'loss': 1.2545, 'grad_norm': 0.59375, 'learning_rate': 0.00010878737867111314, 'epoch': 0.43}\n",
      "{'loss': 1.1639, 'grad_norm': 0.55859375, 'learning_rate': 0.00010877601629364928, 'epoch': 0.43}\n",
      "{'loss': 1.1492, 'grad_norm': 1.15625, 'learning_rate': 0.00010876465391618542, 'epoch': 0.43}\n",
      "{'loss': 1.109, 'grad_norm': 1.0078125, 'learning_rate': 0.00010875329153872156, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3532, 'grad_norm': 0.48828125, 'learning_rate': 0.00010874192916125772, 'epoch': 0.43}\n",
      "{'loss': 1.0975, 'grad_norm': 0.80859375, 'learning_rate': 0.00010873056678379385, 'epoch': 0.43}\n",
      "{'loss': 1.2384, 'grad_norm': 0.490234375, 'learning_rate': 0.00010871920440632998, 'epoch': 0.43}\n",
      "{'loss': 1.2826, 'grad_norm': 0.6484375, 'learning_rate': 0.00010870784202886612, 'epoch': 0.43}\n",
      "{'loss': 1.0524, 'grad_norm': 0.68359375, 'learning_rate': 0.00010869647965140226, 'epoch': 0.43}\n",
      "{'loss': 1.2286, 'grad_norm': 0.65234375, 'learning_rate': 0.0001086851172739384, 'epoch': 0.43}\n",
      "{'loss': 1.2015, 'grad_norm': 0.7734375, 'learning_rate': 0.00010867375489647454, 'epoch': 0.43}\n",
      "{'loss': 1.1625, 'grad_norm': 0.515625, 'learning_rate': 0.00010866239251901069, 'epoch': 0.43}\n",
      "{'loss': 1.2121, 'grad_norm': 0.56640625, 'learning_rate': 0.00010865103014154682, 'epoch': 0.43}\n",
      "{'loss': 1.1627, 'grad_norm': 0.96875, 'learning_rate': 0.00010863966776408296, 'epoch': 0.43}\n",
      "{'loss': 1.3089, 'grad_norm': 0.5234375, 'learning_rate': 0.0001086283053866191, 'epoch': 0.43}\n",
      "{'loss': 1.1551, 'grad_norm': 0.703125, 'learning_rate': 0.00010861694300915524, 'epoch': 0.43}\n",
      "{'loss': 1.2254, 'grad_norm': 0.43359375, 'learning_rate': 0.00010860558063169138, 'epoch': 0.43}\n",
      "{'loss': 1.2669, 'grad_norm': 0.63671875, 'learning_rate': 0.00010859421825422751, 'epoch': 0.43}\n",
      "{'loss': 1.1344, 'grad_norm': 0.71875, 'learning_rate': 0.00010858285587676367, 'epoch': 0.43}\n",
      "{'loss': 1.3588, 'grad_norm': 0.65234375, 'learning_rate': 0.0001085714934992998, 'epoch': 0.43}\n",
      "{'loss': 1.3001, 'grad_norm': 0.65625, 'learning_rate': 0.00010856013112183594, 'epoch': 0.43}\n",
      "{'loss': 1.1514, 'grad_norm': 0.48046875, 'learning_rate': 0.00010854876874437208, 'epoch': 0.43}\n",
      "{'loss': 1.2106, 'grad_norm': 0.60546875, 'learning_rate': 0.00010853740636690822, 'epoch': 0.43}\n",
      "{'loss': 1.1481, 'grad_norm': 0.69921875, 'learning_rate': 0.00010852604398944435, 'epoch': 0.43}\n",
      "{'loss': 1.4308, 'grad_norm': 0.6015625, 'learning_rate': 0.0001085146816119805, 'epoch': 0.43}\n",
      "{'loss': 1.3109, 'grad_norm': 0.6484375, 'learning_rate': 0.00010850331923451664, 'epoch': 0.43}\n",
      "{'loss': 1.4213, 'grad_norm': 0.455078125, 'learning_rate': 0.00010849195685705278, 'epoch': 0.43}\n",
      "{'loss': 1.267, 'grad_norm': 0.609375, 'learning_rate': 0.00010848059447958892, 'epoch': 0.43}\n",
      "{'loss': 0.9861, 'grad_norm': 0.330078125, 'learning_rate': 0.00010846923210212506, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3356, 'grad_norm': 0.462890625, 'learning_rate': 0.00010845786972466118, 'epoch': 0.43}\n",
      "{'loss': 1.1675, 'grad_norm': 0.6484375, 'learning_rate': 0.00010844650734719732, 'epoch': 0.43}\n",
      "{'loss': 1.3071, 'grad_norm': 0.42578125, 'learning_rate': 0.00010843514496973348, 'epoch': 0.43}\n",
      "{'loss': 1.2023, 'grad_norm': 0.6171875, 'learning_rate': 0.00010842378259226962, 'epoch': 0.43}\n",
      "{'loss': 1.1421, 'grad_norm': 0.7265625, 'learning_rate': 0.00010841242021480576, 'epoch': 0.43}\n",
      "{'loss': 1.3189, 'grad_norm': 0.5703125, 'learning_rate': 0.00010840105783734188, 'epoch': 0.43}\n",
      "{'loss': 1.1184, 'grad_norm': 0.5625, 'learning_rate': 0.00010838969545987802, 'epoch': 0.43}\n",
      "{'loss': 1.2911, 'grad_norm': 0.50390625, 'learning_rate': 0.00010837833308241416, 'epoch': 0.43}\n",
      "{'loss': 1.1887, 'grad_norm': 0.72265625, 'learning_rate': 0.0001083669707049503, 'epoch': 0.43}\n",
      "{'loss': 1.1422, 'grad_norm': 0.64453125, 'learning_rate': 0.00010835560832748646, 'epoch': 0.43}\n",
      "{'loss': 1.2967, 'grad_norm': 0.7421875, 'learning_rate': 0.00010834424595002259, 'epoch': 0.43}\n",
      "{'loss': 1.1682, 'grad_norm': 0.59375, 'learning_rate': 0.00010833288357255873, 'epoch': 0.43}\n",
      "{'loss': 1.2219, 'grad_norm': 0.392578125, 'learning_rate': 0.00010832152119509486, 'epoch': 0.43}\n",
      "{'loss': 1.2262, 'grad_norm': 0.5625, 'learning_rate': 0.000108310158817631, 'epoch': 0.43}\n",
      "{'loss': 1.1478, 'grad_norm': 1.078125, 'learning_rate': 0.00010829879644016714, 'epoch': 0.43}\n",
      "{'loss': 1.2757, 'grad_norm': 0.474609375, 'learning_rate': 0.00010828743406270328, 'epoch': 0.43}\n",
      "{'loss': 1.1398, 'grad_norm': 0.69140625, 'learning_rate': 0.00010827607168523943, 'epoch': 0.43}\n",
      "{'loss': 1.151, 'grad_norm': 0.46484375, 'learning_rate': 0.00010826470930777557, 'epoch': 0.43}\n",
      "{'loss': 1.1943, 'grad_norm': 0.80859375, 'learning_rate': 0.0001082533469303117, 'epoch': 0.43}\n",
      "{'loss': 1.1025, 'grad_norm': 0.66015625, 'learning_rate': 0.00010824198455284784, 'epoch': 0.43}\n",
      "{'loss': 1.37, 'grad_norm': 0.484375, 'learning_rate': 0.00010823062217538398, 'epoch': 0.43}\n",
      "{'loss': 0.9698, 'grad_norm': 0.56640625, 'learning_rate': 0.00010821925979792012, 'epoch': 0.43}\n",
      "{'loss': 1.1833, 'grad_norm': 0.578125, 'learning_rate': 0.00010820789742045627, 'epoch': 0.43}\n",
      "{'loss': 1.2138, 'grad_norm': 0.5078125, 'learning_rate': 0.0001081965350429924, 'epoch': 0.43}\n",
      "{'loss': 1.0751, 'grad_norm': 0.6484375, 'learning_rate': 0.00010818517266552854, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3061, 'grad_norm': 0.72265625, 'learning_rate': 0.00010817381028806468, 'epoch': 0.43}\n",
      "{'loss': 1.1625, 'grad_norm': 0.703125, 'learning_rate': 0.00010816244791060082, 'epoch': 0.43}\n",
      "{'loss': 1.1336, 'grad_norm': 0.51171875, 'learning_rate': 0.00010815108553313696, 'epoch': 0.43}\n",
      "{'loss': 1.3566, 'grad_norm': 0.58984375, 'learning_rate': 0.0001081397231556731, 'epoch': 0.43}\n",
      "{'loss': 1.064, 'grad_norm': 0.671875, 'learning_rate': 0.00010812836077820925, 'epoch': 0.43}\n",
      "{'loss': 1.3878, 'grad_norm': 0.75, 'learning_rate': 0.00010811699840074538, 'epoch': 0.43}\n",
      "{'loss': 1.127, 'grad_norm': 0.93359375, 'learning_rate': 0.00010810563602328152, 'epoch': 0.43}\n",
      "{'loss': 1.1546, 'grad_norm': 0.4453125, 'learning_rate': 0.00010809427364581766, 'epoch': 0.43}\n",
      "{'loss': 1.0866, 'grad_norm': 0.71875, 'learning_rate': 0.0001080829112683538, 'epoch': 0.43}\n",
      "{'loss': 0.9517, 'grad_norm': 1.328125, 'learning_rate': 0.00010807154889088992, 'epoch': 0.43}\n",
      "{'loss': 1.2471, 'grad_norm': 0.68359375, 'learning_rate': 0.00010806018651342606, 'epoch': 0.43}\n",
      "{'loss': 1.0583, 'grad_norm': 0.6171875, 'learning_rate': 0.00010804882413596223, 'epoch': 0.43}\n",
      "{'loss': 1.0899, 'grad_norm': 0.392578125, 'learning_rate': 0.00010803746175849836, 'epoch': 0.43}\n",
      "{'loss': 1.2399, 'grad_norm': 0.50390625, 'learning_rate': 0.0001080260993810345, 'epoch': 0.43}\n",
      "{'loss': 1.1324, 'grad_norm': 0.80859375, 'learning_rate': 0.00010801473700357063, 'epoch': 0.43}\n",
      "{'loss': 1.2878, 'grad_norm': 0.5625, 'learning_rate': 0.00010800337462610676, 'epoch': 0.43}\n",
      "{'loss': 1.2884, 'grad_norm': 0.83203125, 'learning_rate': 0.0001079920122486429, 'epoch': 0.43}\n",
      "{'loss': 1.1531, 'grad_norm': 0.38671875, 'learning_rate': 0.00010798064987117904, 'epoch': 0.43}\n",
      "{'loss': 1.2823, 'grad_norm': 0.69140625, 'learning_rate': 0.0001079692874937152, 'epoch': 0.43}\n",
      "{'loss': 1.1252, 'grad_norm': 0.84375, 'learning_rate': 0.00010795792511625133, 'epoch': 0.43}\n",
      "{'loss': 1.4138, 'grad_norm': 0.54296875, 'learning_rate': 0.00010794656273878747, 'epoch': 0.43}\n",
      "{'loss': 1.1272, 'grad_norm': 0.56640625, 'learning_rate': 0.0001079352003613236, 'epoch': 0.43}\n",
      "{'loss': 1.1889, 'grad_norm': 0.53125, 'learning_rate': 0.00010792383798385974, 'epoch': 0.43}\n",
      "{'loss': 1.2053, 'grad_norm': 0.466796875, 'learning_rate': 0.00010791247560639588, 'epoch': 0.43}\n",
      "{'loss': 1.0261, 'grad_norm': 0.390625, 'learning_rate': 0.00010790111322893202, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3753, 'grad_norm': 0.59375, 'learning_rate': 0.00010788975085146817, 'epoch': 0.43}\n",
      "{'loss': 1.1574, 'grad_norm': 0.7109375, 'learning_rate': 0.00010787838847400431, 'epoch': 0.43}\n",
      "{'loss': 1.21, 'grad_norm': 0.390625, 'learning_rate': 0.00010786702609654044, 'epoch': 0.43}\n",
      "{'loss': 1.2773, 'grad_norm': 0.5703125, 'learning_rate': 0.00010785566371907658, 'epoch': 0.43}\n",
      "{'loss': 0.9716, 'grad_norm': 0.84375, 'learning_rate': 0.00010784430134161272, 'epoch': 0.43}\n",
      "{'loss': 1.2951, 'grad_norm': 0.52734375, 'learning_rate': 0.00010783293896414886, 'epoch': 0.43}\n",
      "{'loss': 1.3193, 'grad_norm': 0.63671875, 'learning_rate': 0.00010782157658668501, 'epoch': 0.43}\n",
      "{'loss': 1.0862, 'grad_norm': 0.412109375, 'learning_rate': 0.00010781021420922115, 'epoch': 0.43}\n",
      "{'loss': 1.2661, 'grad_norm': 0.5859375, 'learning_rate': 0.00010779885183175729, 'epoch': 0.43}\n",
      "{'loss': 1.0951, 'grad_norm': 1.6796875, 'learning_rate': 0.00010778748945429342, 'epoch': 0.43}\n",
      "{'loss': 1.4223, 'grad_norm': 0.462890625, 'learning_rate': 0.00010777612707682956, 'epoch': 0.43}\n",
      "{'loss': 1.2459, 'grad_norm': 0.703125, 'learning_rate': 0.0001077647646993657, 'epoch': 0.43}\n",
      "{'loss': 1.2467, 'grad_norm': 0.375, 'learning_rate': 0.00010775340232190184, 'epoch': 0.43}\n",
      "{'loss': 1.1756, 'grad_norm': 0.55859375, 'learning_rate': 0.00010774203994443799, 'epoch': 0.43}\n",
      "{'loss': 1.0312, 'grad_norm': 0.54296875, 'learning_rate': 0.00010773067756697413, 'epoch': 0.43}\n",
      "{'loss': 1.3574, 'grad_norm': 0.478515625, 'learning_rate': 0.00010771931518951026, 'epoch': 0.43}\n",
      "{'loss': 1.2209, 'grad_norm': 0.8359375, 'learning_rate': 0.0001077079528120464, 'epoch': 0.43}\n",
      "{'loss': 1.3189, 'grad_norm': 0.443359375, 'learning_rate': 0.00010769659043458254, 'epoch': 0.43}\n",
      "{'loss': 1.3007, 'grad_norm': 0.59375, 'learning_rate': 0.00010768522805711866, 'epoch': 0.43}\n",
      "{'loss': 1.0205, 'grad_norm': 0.87890625, 'learning_rate': 0.0001076738656796548, 'epoch': 0.43}\n",
      "{'loss': 1.1852, 'grad_norm': 0.609375, 'learning_rate': 0.00010766250330219097, 'epoch': 0.43}\n",
      "{'loss': 1.1304, 'grad_norm': 0.59375, 'learning_rate': 0.0001076511409247271, 'epoch': 0.43}\n",
      "{'loss': 1.2021, 'grad_norm': 0.53125, 'learning_rate': 0.00010763977854726324, 'epoch': 0.43}\n",
      "{'loss': 1.1422, 'grad_norm': 0.59375, 'learning_rate': 0.00010762841616979937, 'epoch': 0.43}\n",
      "{'loss': 1.0169, 'grad_norm': 1.0234375, 'learning_rate': 0.0001076170537923355, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2885, 'grad_norm': 0.7578125, 'learning_rate': 0.00010760569141487164, 'epoch': 0.43}\n",
      "{'loss': 1.2258, 'grad_norm': 1.0, 'learning_rate': 0.00010759432903740778, 'epoch': 0.43}\n",
      "{'loss': 1.3119, 'grad_norm': 0.380859375, 'learning_rate': 0.00010758296665994395, 'epoch': 0.43}\n",
      "{'loss': 1.2391, 'grad_norm': 0.53125, 'learning_rate': 0.00010757160428248007, 'epoch': 0.43}\n",
      "{'loss': 1.0524, 'grad_norm': 0.6640625, 'learning_rate': 0.00010756024190501621, 'epoch': 0.43}\n",
      "{'loss': 1.4569, 'grad_norm': 0.5, 'learning_rate': 0.00010754887952755235, 'epoch': 0.43}\n",
      "{'loss': 1.221, 'grad_norm': 0.82421875, 'learning_rate': 0.00010753751715008848, 'epoch': 0.43}\n",
      "{'loss': 1.2768, 'grad_norm': 0.48828125, 'learning_rate': 0.00010752615477262462, 'epoch': 0.43}\n",
      "{'loss': 1.2888, 'grad_norm': 0.55078125, 'learning_rate': 0.00010751479239516079, 'epoch': 0.43}\n",
      "{'loss': 1.1087, 'grad_norm': 0.6015625, 'learning_rate': 0.00010750343001769691, 'epoch': 0.43}\n",
      "{'loss': 1.2388, 'grad_norm': 0.4296875, 'learning_rate': 0.00010749206764023305, 'epoch': 0.43}\n",
      "{'loss': 1.1861, 'grad_norm': 0.734375, 'learning_rate': 0.00010748070526276919, 'epoch': 0.43}\n",
      "{'loss': 1.1656, 'grad_norm': 0.427734375, 'learning_rate': 0.00010746934288530532, 'epoch': 0.43}\n",
      "{'loss': 1.3125, 'grad_norm': 0.671875, 'learning_rate': 0.00010745798050784146, 'epoch': 0.43}\n",
      "{'loss': 1.0676, 'grad_norm': 0.7890625, 'learning_rate': 0.0001074466181303776, 'epoch': 0.43}\n",
      "{'loss': 1.3342, 'grad_norm': 0.4609375, 'learning_rate': 0.00010743525575291375, 'epoch': 0.43}\n",
      "{'loss': 1.2031, 'grad_norm': 0.484375, 'learning_rate': 0.00010742389337544989, 'epoch': 0.43}\n",
      "{'loss': 1.3038, 'grad_norm': 0.46875, 'learning_rate': 0.00010741253099798603, 'epoch': 0.43}\n",
      "{'loss': 1.2228, 'grad_norm': 0.64453125, 'learning_rate': 0.00010740116862052216, 'epoch': 0.43}\n",
      "{'loss': 1.1421, 'grad_norm': 0.765625, 'learning_rate': 0.0001073898062430583, 'epoch': 0.43}\n",
      "{'loss': 1.3185, 'grad_norm': 0.7421875, 'learning_rate': 0.00010737844386559444, 'epoch': 0.43}\n",
      "{'loss': 1.145, 'grad_norm': 0.6015625, 'learning_rate': 0.00010736708148813058, 'epoch': 0.43}\n",
      "{'loss': 1.1857, 'grad_norm': 0.474609375, 'learning_rate': 0.00010735571911066673, 'epoch': 0.43}\n",
      "{'loss': 1.2057, 'grad_norm': 0.53125, 'learning_rate': 0.00010734435673320287, 'epoch': 0.43}\n",
      "{'loss': 1.1078, 'grad_norm': 0.5234375, 'learning_rate': 0.000107332994355739, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3005, 'grad_norm': 0.94140625, 'learning_rate': 0.00010732163197827514, 'epoch': 0.43}\n",
      "{'loss': 1.1535, 'grad_norm': 0.7265625, 'learning_rate': 0.00010731026960081128, 'epoch': 0.43}\n",
      "{'loss': 1.2257, 'grad_norm': 0.65625, 'learning_rate': 0.0001072989072233474, 'epoch': 0.43}\n",
      "{'loss': 1.2549, 'grad_norm': 0.52734375, 'learning_rate': 0.00010728754484588354, 'epoch': 0.43}\n",
      "{'loss': 1.2113, 'grad_norm': 1.0390625, 'learning_rate': 0.00010727618246841971, 'epoch': 0.43}\n",
      "{'loss': 1.245, 'grad_norm': 0.671875, 'learning_rate': 0.00010726482009095585, 'epoch': 0.43}\n",
      "{'loss': 1.176, 'grad_norm': 0.828125, 'learning_rate': 0.00010725345771349198, 'epoch': 0.43}\n",
      "{'loss': 1.1971, 'grad_norm': 0.5859375, 'learning_rate': 0.00010724209533602811, 'epoch': 0.43}\n",
      "{'loss': 1.2641, 'grad_norm': 0.6796875, 'learning_rate': 0.00010723073295856425, 'epoch': 0.43}\n",
      "{'loss': 0.9793, 'grad_norm': 1.078125, 'learning_rate': 0.00010721937058110038, 'epoch': 0.43}\n",
      "{'loss': 1.2981, 'grad_norm': 0.6015625, 'learning_rate': 0.00010720800820363652, 'epoch': 0.43}\n",
      "{'loss': 1.1708, 'grad_norm': 0.94140625, 'learning_rate': 0.00010719664582617269, 'epoch': 0.43}\n",
      "{'loss': 1.153, 'grad_norm': 0.408203125, 'learning_rate': 0.00010718528344870881, 'epoch': 0.43}\n",
      "{'loss': 1.3325, 'grad_norm': 0.55859375, 'learning_rate': 0.00010717392107124495, 'epoch': 0.43}\n",
      "{'loss': 1.1391, 'grad_norm': 0.73046875, 'learning_rate': 0.00010716255869378109, 'epoch': 0.43}\n",
      "{'loss': 1.388, 'grad_norm': 0.416015625, 'learning_rate': 0.00010715119631631722, 'epoch': 0.43}\n",
      "{'loss': 1.125, 'grad_norm': 0.75, 'learning_rate': 0.00010713983393885336, 'epoch': 0.43}\n",
      "{'loss': 1.2624, 'grad_norm': 0.37890625, 'learning_rate': 0.00010712847156138953, 'epoch': 0.43}\n",
      "{'loss': 1.3192, 'grad_norm': 0.58203125, 'learning_rate': 0.00010711710918392565, 'epoch': 0.43}\n",
      "{'loss': 1.1172, 'grad_norm': 0.70703125, 'learning_rate': 0.00010710574680646179, 'epoch': 0.43}\n",
      "{'loss': 1.3138, 'grad_norm': 0.48046875, 'learning_rate': 0.00010709438442899793, 'epoch': 0.43}\n",
      "{'loss': 1.2006, 'grad_norm': 0.71875, 'learning_rate': 0.00010708302205153407, 'epoch': 0.43}\n",
      "{'loss': 1.1251, 'grad_norm': 0.41015625, 'learning_rate': 0.0001070716596740702, 'epoch': 0.43}\n",
      "{'loss': 1.3273, 'grad_norm': 0.62890625, 'learning_rate': 0.00010706029729660634, 'epoch': 0.43}\n",
      "{'loss': 1.106, 'grad_norm': 0.828125, 'learning_rate': 0.00010704893491914249, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3083, 'grad_norm': 0.498046875, 'learning_rate': 0.00010703757254167863, 'epoch': 0.43}\n",
      "{'loss': 1.2584, 'grad_norm': 0.6640625, 'learning_rate': 0.00010702621016421477, 'epoch': 0.43}\n",
      "{'loss': 1.1409, 'grad_norm': 0.384765625, 'learning_rate': 0.0001070148477867509, 'epoch': 0.43}\n",
      "{'loss': 1.253, 'grad_norm': 0.62109375, 'learning_rate': 0.00010700348540928704, 'epoch': 0.43}\n",
      "{'loss': 1.1572, 'grad_norm': 1.2578125, 'learning_rate': 0.00010699212303182318, 'epoch': 0.43}\n",
      "{'loss': 1.405, 'grad_norm': 0.70703125, 'learning_rate': 0.00010698076065435932, 'epoch': 0.43}\n",
      "{'loss': 1.1454, 'grad_norm': 0.5859375, 'learning_rate': 0.00010696939827689547, 'epoch': 0.43}\n",
      "{'loss': 1.2256, 'grad_norm': 0.484375, 'learning_rate': 0.00010695803589943161, 'epoch': 0.43}\n",
      "{'loss': 1.2638, 'grad_norm': 0.6015625, 'learning_rate': 0.00010694667352196775, 'epoch': 0.43}\n",
      "{'loss': 1.0543, 'grad_norm': 0.74609375, 'learning_rate': 0.00010693531114450388, 'epoch': 0.43}\n",
      "{'loss': 1.3916, 'grad_norm': 0.451171875, 'learning_rate': 0.00010692394876704002, 'epoch': 0.43}\n",
      "{'loss': 1.2633, 'grad_norm': 0.50390625, 'learning_rate': 0.00010691258638957615, 'epoch': 0.43}\n",
      "{'loss': 1.2036, 'grad_norm': 0.53125, 'learning_rate': 0.00010690122401211228, 'epoch': 0.43}\n",
      "{'loss': 1.3138, 'grad_norm': 0.6015625, 'learning_rate': 0.00010688986163464845, 'epoch': 0.43}\n",
      "{'loss': 1.0401, 'grad_norm': 0.51171875, 'learning_rate': 0.00010687849925718459, 'epoch': 0.43}\n",
      "{'loss': 1.3876, 'grad_norm': 0.4765625, 'learning_rate': 0.00010686713687972073, 'epoch': 0.43}\n",
      "{'loss': 1.2506, 'grad_norm': 0.765625, 'learning_rate': 0.00010685577450225685, 'epoch': 0.43}\n",
      "{'loss': 1.2573, 'grad_norm': 0.3828125, 'learning_rate': 0.00010684441212479299, 'epoch': 0.43}\n",
      "{'loss': 1.2282, 'grad_norm': 0.59375, 'learning_rate': 0.00010683304974732913, 'epoch': 0.43}\n",
      "{'loss': 1.1402, 'grad_norm': 0.68359375, 'learning_rate': 0.00010682168736986529, 'epoch': 0.43}\n",
      "{'loss': 1.1777, 'grad_norm': 0.482421875, 'learning_rate': 0.00010681032499240143, 'epoch': 0.43}\n",
      "{'loss': 1.1488, 'grad_norm': 0.59375, 'learning_rate': 0.00010679896261493755, 'epoch': 0.43}\n",
      "{'loss': 1.2947, 'grad_norm': 0.396484375, 'learning_rate': 0.00010678760023747369, 'epoch': 0.43}\n",
      "{'loss': 1.3324, 'grad_norm': 0.46875, 'learning_rate': 0.00010677623786000983, 'epoch': 0.43}\n",
      "{'loss': 1.02, 'grad_norm': 0.765625, 'learning_rate': 0.00010676487548254597, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2457, 'grad_norm': 0.478515625, 'learning_rate': 0.0001067535131050821, 'epoch': 0.43}\n",
      "{'loss': 1.2271, 'grad_norm': 0.77734375, 'learning_rate': 0.00010674215072761827, 'epoch': 0.43}\n",
      "{'loss': 1.1154, 'grad_norm': 0.5546875, 'learning_rate': 0.00010673078835015439, 'epoch': 0.43}\n",
      "{'loss': 1.2062, 'grad_norm': 0.451171875, 'learning_rate': 0.00010671942597269053, 'epoch': 0.43}\n",
      "{'loss': 1.0216, 'grad_norm': 0.59765625, 'learning_rate': 0.00010670806359522667, 'epoch': 0.43}\n",
      "{'loss': 1.2908, 'grad_norm': 0.6796875, 'learning_rate': 0.0001066967012177628, 'epoch': 0.43}\n",
      "{'loss': 1.1981, 'grad_norm': 0.64453125, 'learning_rate': 0.00010668533884029894, 'epoch': 0.43}\n",
      "{'loss': 1.133, 'grad_norm': 0.5546875, 'learning_rate': 0.00010667397646283508, 'epoch': 0.43}\n",
      "{'loss': 1.2575, 'grad_norm': 0.64453125, 'learning_rate': 0.00010666261408537123, 'epoch': 0.43}\n",
      "{'loss': 0.945, 'grad_norm': 1.15625, 'learning_rate': 0.00010665125170790737, 'epoch': 0.43}\n",
      "{'loss': 1.3498, 'grad_norm': 0.57421875, 'learning_rate': 0.00010663988933044351, 'epoch': 0.43}\n",
      "{'loss': 1.1425, 'grad_norm': 0.6953125, 'learning_rate': 0.00010662852695297965, 'epoch': 0.43}\n",
      "{'loss': 1.1158, 'grad_norm': 0.5625, 'learning_rate': 0.00010661716457551579, 'epoch': 0.43}\n",
      "{'loss': 1.3071, 'grad_norm': 0.5546875, 'learning_rate': 0.00010660580219805192, 'epoch': 0.43}\n",
      "{'loss': 1.2424, 'grad_norm': 0.43359375, 'learning_rate': 0.00010659443982058806, 'epoch': 0.43}\n",
      "{'loss': 1.3167, 'grad_norm': 0.57421875, 'learning_rate': 0.00010658307744312421, 'epoch': 0.43}\n",
      "{'loss': 1.2306, 'grad_norm': 0.9765625, 'learning_rate': 0.00010657171506566035, 'epoch': 0.43}\n",
      "{'loss': 1.0898, 'grad_norm': 0.427734375, 'learning_rate': 0.00010656035268819649, 'epoch': 0.43}\n",
      "{'loss': 1.159, 'grad_norm': 0.67578125, 'learning_rate': 0.00010654899031073263, 'epoch': 0.43}\n",
      "{'loss': 1.1624, 'grad_norm': 1.546875, 'learning_rate': 0.00010653762793326876, 'epoch': 0.43}\n",
      "{'loss': 1.2743, 'grad_norm': 0.51171875, 'learning_rate': 0.00010652626555580489, 'epoch': 0.44}\n",
      "{'loss': 1.0294, 'grad_norm': 0.6796875, 'learning_rate': 0.00010651490317834105, 'epoch': 0.44}\n",
      "{'loss': 1.1902, 'grad_norm': 0.474609375, 'learning_rate': 0.00010650354080087719, 'epoch': 0.44}\n",
      "{'loss': 1.2107, 'grad_norm': 0.5546875, 'learning_rate': 0.00010649217842341333, 'epoch': 0.44}\n",
      "{'loss': 1.2354, 'grad_norm': 0.90625, 'learning_rate': 0.00010648081604594947, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.304, 'grad_norm': 0.55078125, 'learning_rate': 0.00010646945366848559, 'epoch': 0.44}\n",
      "{'loss': 1.2463, 'grad_norm': 0.71875, 'learning_rate': 0.00010645809129102173, 'epoch': 0.44}\n",
      "{'loss': 1.2092, 'grad_norm': 0.625, 'learning_rate': 0.00010644672891355787, 'epoch': 0.44}\n",
      "{'loss': 1.1954, 'grad_norm': 0.57421875, 'learning_rate': 0.00010643536653609403, 'epoch': 0.44}\n",
      "{'loss': 1.0298, 'grad_norm': 0.96875, 'learning_rate': 0.00010642400415863017, 'epoch': 0.44}\n",
      "{'loss': 1.4395, 'grad_norm': 0.515625, 'learning_rate': 0.0001064126417811663, 'epoch': 0.44}\n",
      "{'loss': 1.2909, 'grad_norm': 1.0546875, 'learning_rate': 0.00010640127940370243, 'epoch': 0.44}\n",
      "{'loss': 1.1649, 'grad_norm': 0.443359375, 'learning_rate': 0.00010638991702623857, 'epoch': 0.44}\n",
      "{'loss': 1.2017, 'grad_norm': 0.73046875, 'learning_rate': 0.00010637855464877471, 'epoch': 0.44}\n",
      "{'loss': 1.0493, 'grad_norm': 0.546875, 'learning_rate': 0.00010636719227131084, 'epoch': 0.44}\n",
      "{'loss': 1.3334, 'grad_norm': 0.6015625, 'learning_rate': 0.00010635582989384701, 'epoch': 0.44}\n",
      "{'loss': 1.1359, 'grad_norm': 0.73828125, 'learning_rate': 0.00010634446751638313, 'epoch': 0.44}\n",
      "{'loss': 1.2297, 'grad_norm': 0.640625, 'learning_rate': 0.00010633310513891927, 'epoch': 0.44}\n",
      "{'loss': 1.144, 'grad_norm': 0.55859375, 'learning_rate': 0.00010632174276145541, 'epoch': 0.44}\n",
      "{'loss': 0.966, 'grad_norm': 0.8828125, 'learning_rate': 0.00010631038038399155, 'epoch': 0.44}\n",
      "{'loss': 1.3758, 'grad_norm': 0.75390625, 'learning_rate': 0.00010629901800652769, 'epoch': 0.44}\n",
      "{'loss': 1.1055, 'grad_norm': 0.455078125, 'learning_rate': 0.00010628765562906382, 'epoch': 0.44}\n",
      "{'loss': 1.3106, 'grad_norm': 0.33984375, 'learning_rate': 0.00010627629325159997, 'epoch': 0.44}\n",
      "{'loss': 1.2231, 'grad_norm': 0.74609375, 'learning_rate': 0.00010626493087413611, 'epoch': 0.44}\n",
      "{'loss': 1.1122, 'grad_norm': 0.92578125, 'learning_rate': 0.00010625356849667225, 'epoch': 0.44}\n",
      "{'loss': 1.3361, 'grad_norm': 0.51953125, 'learning_rate': 0.00010624220611920839, 'epoch': 0.44}\n",
      "{'loss': 1.2668, 'grad_norm': 0.5234375, 'learning_rate': 0.00010623084374174453, 'epoch': 0.44}\n",
      "{'loss': 1.3973, 'grad_norm': 0.66796875, 'learning_rate': 0.00010621948136428066, 'epoch': 0.44}\n",
      "{'loss': 1.252, 'grad_norm': 0.6015625, 'learning_rate': 0.0001062081189868168, 'epoch': 0.44}\n",
      "{'loss': 1.0342, 'grad_norm': 1.1171875, 'learning_rate': 0.00010619675660935295, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2686, 'grad_norm': 0.458984375, 'learning_rate': 0.00010618539423188909, 'epoch': 0.44}\n",
      "{'loss': 1.1703, 'grad_norm': 0.6953125, 'learning_rate': 0.00010617403185442523, 'epoch': 0.44}\n",
      "{'loss': 1.1794, 'grad_norm': 0.4765625, 'learning_rate': 0.00010616266947696137, 'epoch': 0.44}\n",
      "{'loss': 1.2087, 'grad_norm': 0.6640625, 'learning_rate': 0.0001061513070994975, 'epoch': 0.44}\n",
      "{'loss': 1.0166, 'grad_norm': 0.87890625, 'learning_rate': 0.00010613994472203363, 'epoch': 0.44}\n",
      "{'loss': 1.2162, 'grad_norm': 0.6015625, 'learning_rate': 0.0001061285823445698, 'epoch': 0.44}\n",
      "{'loss': 1.2164, 'grad_norm': 0.609375, 'learning_rate': 0.00010611721996710593, 'epoch': 0.44}\n",
      "{'loss': 1.2427, 'grad_norm': 0.482421875, 'learning_rate': 0.00010610585758964207, 'epoch': 0.44}\n",
      "{'loss': 1.286, 'grad_norm': 0.58203125, 'learning_rate': 0.00010609449521217821, 'epoch': 0.44}\n",
      "{'loss': 1.0952, 'grad_norm': 1.1484375, 'learning_rate': 0.00010608313283471433, 'epoch': 0.44}\n",
      "{'loss': 1.244, 'grad_norm': 0.55078125, 'learning_rate': 0.00010607177045725047, 'epoch': 0.44}\n",
      "{'loss': 1.0379, 'grad_norm': 0.6015625, 'learning_rate': 0.00010606040807978661, 'epoch': 0.44}\n",
      "{'loss': 1.0884, 'grad_norm': 0.470703125, 'learning_rate': 0.00010604904570232277, 'epoch': 0.44}\n",
      "{'loss': 1.2896, 'grad_norm': 0.578125, 'learning_rate': 0.00010603768332485891, 'epoch': 0.44}\n",
      "{'loss': 1.2047, 'grad_norm': 0.76953125, 'learning_rate': 0.00010602632094739503, 'epoch': 0.44}\n",
      "{'loss': 1.345, 'grad_norm': 0.8203125, 'learning_rate': 0.00010601495856993117, 'epoch': 0.44}\n",
      "{'loss': 1.1482, 'grad_norm': 0.6953125, 'learning_rate': 0.00010600359619246731, 'epoch': 0.44}\n",
      "{'loss': 1.2251, 'grad_norm': 0.51953125, 'learning_rate': 0.00010599223381500345, 'epoch': 0.44}\n",
      "{'loss': 1.3039, 'grad_norm': 0.65625, 'learning_rate': 0.00010598087143753959, 'epoch': 0.44}\n",
      "{'loss': 0.9762, 'grad_norm': 1.4609375, 'learning_rate': 0.00010596950906007575, 'epoch': 0.44}\n",
      "{'loss': 1.267, 'grad_norm': 0.4375, 'learning_rate': 0.00010595814668261188, 'epoch': 0.44}\n",
      "{'loss': 1.2043, 'grad_norm': 0.45703125, 'learning_rate': 0.00010594678430514801, 'epoch': 0.44}\n",
      "{'loss': 1.1009, 'grad_norm': 0.412109375, 'learning_rate': 0.00010593542192768415, 'epoch': 0.44}\n",
      "{'loss': 1.181, 'grad_norm': 0.5625, 'learning_rate': 0.00010592405955022029, 'epoch': 0.44}\n",
      "{'loss': 1.0191, 'grad_norm': 0.5703125, 'learning_rate': 0.00010591269717275643, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3727, 'grad_norm': 0.54296875, 'learning_rate': 0.00010590133479529256, 'epoch': 0.44}\n",
      "{'loss': 1.0816, 'grad_norm': 0.98828125, 'learning_rate': 0.00010588997241782872, 'epoch': 0.44}\n",
      "{'loss': 1.1929, 'grad_norm': 0.48046875, 'learning_rate': 0.00010587861004036485, 'epoch': 0.44}\n",
      "{'loss': 1.2653, 'grad_norm': 0.578125, 'learning_rate': 0.00010586724766290099, 'epoch': 0.44}\n",
      "{'loss': 1.08, 'grad_norm': 0.546875, 'learning_rate': 0.00010585588528543713, 'epoch': 0.44}\n",
      "{'loss': 1.3348, 'grad_norm': 0.52734375, 'learning_rate': 0.00010584452290797327, 'epoch': 0.44}\n",
      "{'loss': 1.0742, 'grad_norm': 0.75390625, 'learning_rate': 0.0001058331605305094, 'epoch': 0.44}\n",
      "{'loss': 1.1259, 'grad_norm': 0.4921875, 'learning_rate': 0.00010582179815304556, 'epoch': 0.44}\n",
      "{'loss': 1.2585, 'grad_norm': 0.703125, 'learning_rate': 0.0001058104357755817, 'epoch': 0.44}\n",
      "{'loss': 1.1021, 'grad_norm': 0.90625, 'learning_rate': 0.00010579907339811783, 'epoch': 0.44}\n",
      "{'loss': 1.3817, 'grad_norm': 0.59375, 'learning_rate': 0.00010578771102065397, 'epoch': 0.44}\n",
      "{'loss': 1.2289, 'grad_norm': 0.6015625, 'learning_rate': 0.00010577634864319011, 'epoch': 0.44}\n",
      "{'loss': 1.1456, 'grad_norm': 0.490234375, 'learning_rate': 0.00010576498626572625, 'epoch': 0.44}\n",
      "{'loss': 1.2933, 'grad_norm': 0.5234375, 'learning_rate': 0.00010575362388826237, 'epoch': 0.44}\n",
      "{'loss': 1.1228, 'grad_norm': 1.453125, 'learning_rate': 0.00010574226151079854, 'epoch': 0.44}\n",
      "{'loss': 1.3437, 'grad_norm': 0.5703125, 'learning_rate': 0.00010573089913333467, 'epoch': 0.44}\n",
      "{'loss': 1.2218, 'grad_norm': 0.75390625, 'learning_rate': 0.00010571953675587081, 'epoch': 0.44}\n",
      "{'loss': 1.1956, 'grad_norm': 0.62109375, 'learning_rate': 0.00010570817437840695, 'epoch': 0.44}\n",
      "{'loss': 1.2122, 'grad_norm': 0.671875, 'learning_rate': 0.00010569681200094307, 'epoch': 0.44}\n",
      "{'loss': 1.0541, 'grad_norm': 0.61328125, 'learning_rate': 0.00010568544962347921, 'epoch': 0.44}\n",
      "{'loss': 1.2602, 'grad_norm': 0.5625, 'learning_rate': 0.00010567408724601535, 'epoch': 0.44}\n",
      "{'loss': 1.1699, 'grad_norm': 0.7578125, 'learning_rate': 0.00010566272486855151, 'epoch': 0.44}\n",
      "{'loss': 1.2807, 'grad_norm': 0.498046875, 'learning_rate': 0.00010565136249108765, 'epoch': 0.44}\n",
      "{'loss': 1.2067, 'grad_norm': 0.51953125, 'learning_rate': 0.00010564000011362378, 'epoch': 0.44}\n",
      "{'loss': 1.0414, 'grad_norm': 0.80078125, 'learning_rate': 0.00010562863773615991, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2691, 'grad_norm': 0.53515625, 'learning_rate': 0.00010561727535869605, 'epoch': 0.44}\n",
      "{'loss': 1.1686, 'grad_norm': 0.6875, 'learning_rate': 0.00010560591298123219, 'epoch': 0.44}\n",
      "{'loss': 1.2358, 'grad_norm': 0.54296875, 'learning_rate': 0.00010559455060376833, 'epoch': 0.44}\n",
      "{'loss': 1.2589, 'grad_norm': 0.57421875, 'learning_rate': 0.00010558318822630449, 'epoch': 0.44}\n",
      "{'loss': 1.2762, 'grad_norm': 0.62890625, 'learning_rate': 0.00010557182584884062, 'epoch': 0.44}\n",
      "{'loss': 1.3616, 'grad_norm': 0.609375, 'learning_rate': 0.00010556046347137675, 'epoch': 0.44}\n",
      "{'loss': 1.2898, 'grad_norm': 0.8125, 'learning_rate': 0.00010554910109391289, 'epoch': 0.44}\n",
      "{'loss': 1.1627, 'grad_norm': 0.703125, 'learning_rate': 0.00010553773871644903, 'epoch': 0.44}\n",
      "{'loss': 1.1976, 'grad_norm': 0.76171875, 'learning_rate': 0.00010552637633898517, 'epoch': 0.44}\n",
      "{'loss': 1.1146, 'grad_norm': 1.109375, 'learning_rate': 0.0001055150139615213, 'epoch': 0.44}\n",
      "{'loss': 1.4996, 'grad_norm': 0.55078125, 'learning_rate': 0.00010550365158405746, 'epoch': 0.44}\n",
      "{'loss': 1.2681, 'grad_norm': 0.5546875, 'learning_rate': 0.0001054922892065936, 'epoch': 0.44}\n",
      "{'loss': 1.2936, 'grad_norm': 0.4609375, 'learning_rate': 0.00010548092682912973, 'epoch': 0.44}\n",
      "{'loss': 1.2637, 'grad_norm': 0.58203125, 'learning_rate': 0.00010546956445166587, 'epoch': 0.44}\n",
      "{'loss': 1.0797, 'grad_norm': 1.3046875, 'learning_rate': 0.00010545820207420201, 'epoch': 0.44}\n",
      "{'loss': 1.4364, 'grad_norm': 0.58984375, 'learning_rate': 0.00010544683969673815, 'epoch': 0.44}\n",
      "{'loss': 1.2149, 'grad_norm': 0.9296875, 'learning_rate': 0.0001054354773192743, 'epoch': 0.44}\n",
      "{'loss': 1.1723, 'grad_norm': 0.427734375, 'learning_rate': 0.00010542411494181044, 'epoch': 0.44}\n",
      "{'loss': 1.265, 'grad_norm': 0.578125, 'learning_rate': 0.00010541275256434657, 'epoch': 0.44}\n",
      "{'loss': 1.1168, 'grad_norm': 1.140625, 'learning_rate': 0.00010540139018688271, 'epoch': 0.44}\n",
      "{'loss': 1.2656, 'grad_norm': 0.53515625, 'learning_rate': 0.00010539002780941885, 'epoch': 0.44}\n",
      "{'loss': 1.1718, 'grad_norm': 0.765625, 'learning_rate': 0.00010537866543195499, 'epoch': 0.44}\n",
      "{'loss': 1.1143, 'grad_norm': 0.34765625, 'learning_rate': 0.00010536730305449111, 'epoch': 0.44}\n",
      "{'loss': 1.187, 'grad_norm': 0.7265625, 'learning_rate': 0.00010535594067702728, 'epoch': 0.44}\n",
      "{'loss': 1.1164, 'grad_norm': 0.92578125, 'learning_rate': 0.00010534457829956341, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1765, 'grad_norm': 0.5625, 'learning_rate': 0.00010533321592209955, 'epoch': 0.44}\n",
      "{'loss': 1.1151, 'grad_norm': 0.8671875, 'learning_rate': 0.00010532185354463569, 'epoch': 0.44}\n",
      "{'loss': 1.1626, 'grad_norm': 0.44921875, 'learning_rate': 0.00010531049116717181, 'epoch': 0.44}\n",
      "{'loss': 1.2575, 'grad_norm': 0.5546875, 'learning_rate': 0.00010529912878970795, 'epoch': 0.44}\n",
      "{'loss': 1.0453, 'grad_norm': 0.828125, 'learning_rate': 0.00010528776641224409, 'epoch': 0.44}\n",
      "{'loss': 1.271, 'grad_norm': 0.64453125, 'learning_rate': 0.00010527640403478026, 'epoch': 0.44}\n",
      "{'loss': 1.2389, 'grad_norm': 0.44140625, 'learning_rate': 0.00010526504165731639, 'epoch': 0.44}\n",
      "{'loss': 1.2329, 'grad_norm': 0.4140625, 'learning_rate': 0.00010525367927985252, 'epoch': 0.44}\n",
      "{'loss': 1.18, 'grad_norm': 0.625, 'learning_rate': 0.00010524231690238866, 'epoch': 0.44}\n",
      "{'loss': 1.149, 'grad_norm': 0.9609375, 'learning_rate': 0.00010523095452492479, 'epoch': 0.44}\n",
      "{'loss': 1.4359, 'grad_norm': 0.56640625, 'learning_rate': 0.00010521959214746093, 'epoch': 0.44}\n",
      "{'loss': 1.105, 'grad_norm': 0.6953125, 'learning_rate': 0.00010520822976999707, 'epoch': 0.44}\n",
      "{'loss': 1.2001, 'grad_norm': 0.482421875, 'learning_rate': 0.00010519686739253323, 'epoch': 0.44}\n",
      "{'loss': 1.2172, 'grad_norm': 0.5859375, 'learning_rate': 0.00010518550501506936, 'epoch': 0.44}\n",
      "{'loss': 1.1893, 'grad_norm': 1.0703125, 'learning_rate': 0.0001051741426376055, 'epoch': 0.44}\n",
      "{'loss': 1.3154, 'grad_norm': 0.80859375, 'learning_rate': 0.00010516278026014163, 'epoch': 0.44}\n",
      "{'loss': 1.1011, 'grad_norm': 0.80078125, 'learning_rate': 0.00010515141788267777, 'epoch': 0.44}\n",
      "{'loss': 1.0544, 'grad_norm': 0.376953125, 'learning_rate': 0.00010514005550521391, 'epoch': 0.44}\n",
      "{'loss': 1.3131, 'grad_norm': 0.4765625, 'learning_rate': 0.00010512869312775006, 'epoch': 0.44}\n",
      "{'loss': 1.0192, 'grad_norm': 0.7578125, 'learning_rate': 0.0001051173307502862, 'epoch': 0.44}\n",
      "{'loss': 1.2533, 'grad_norm': 0.55859375, 'learning_rate': 0.00010510596837282234, 'epoch': 0.44}\n",
      "{'loss': 1.1644, 'grad_norm': 0.76953125, 'learning_rate': 0.00010509460599535847, 'epoch': 0.44}\n",
      "{'loss': 1.2106, 'grad_norm': 0.48828125, 'learning_rate': 0.00010508324361789461, 'epoch': 0.44}\n",
      "{'loss': 1.1857, 'grad_norm': 0.625, 'learning_rate': 0.00010507188124043075, 'epoch': 0.44}\n",
      "{'loss': 0.9926, 'grad_norm': 0.84765625, 'learning_rate': 0.00010506051886296689, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3311, 'grad_norm': 0.462890625, 'learning_rate': 0.00010504915648550304, 'epoch': 0.44}\n",
      "{'loss': 1.201, 'grad_norm': 0.67578125, 'learning_rate': 0.00010503779410803918, 'epoch': 0.44}\n",
      "{'loss': 1.1577, 'grad_norm': 0.38671875, 'learning_rate': 0.00010502643173057532, 'epoch': 0.44}\n",
      "{'loss': 1.1974, 'grad_norm': 0.46875, 'learning_rate': 0.00010501506935311145, 'epoch': 0.44}\n",
      "{'loss': 1.0831, 'grad_norm': 0.96875, 'learning_rate': 0.00010500370697564759, 'epoch': 0.44}\n",
      "{'loss': 1.2801, 'grad_norm': 0.51171875, 'learning_rate': 0.00010499234459818373, 'epoch': 0.44}\n",
      "{'loss': 1.2253, 'grad_norm': 0.6328125, 'learning_rate': 0.00010498098222071985, 'epoch': 0.44}\n",
      "{'loss': 1.124, 'grad_norm': 0.62890625, 'learning_rate': 0.00010496961984325602, 'epoch': 0.44}\n",
      "{'loss': 1.3083, 'grad_norm': 0.66796875, 'learning_rate': 0.00010495825746579216, 'epoch': 0.44}\n",
      "{'loss': 1.137, 'grad_norm': 0.83203125, 'learning_rate': 0.0001049468950883283, 'epoch': 0.44}\n",
      "{'loss': 1.3464, 'grad_norm': 0.55078125, 'learning_rate': 0.00010493553271086443, 'epoch': 0.44}\n",
      "{'loss': 1.1301, 'grad_norm': 0.64453125, 'learning_rate': 0.00010492417033340056, 'epoch': 0.44}\n",
      "{'loss': 1.0739, 'grad_norm': 0.490234375, 'learning_rate': 0.0001049128079559367, 'epoch': 0.44}\n",
      "{'loss': 1.1571, 'grad_norm': 0.51171875, 'learning_rate': 0.00010490144557847283, 'epoch': 0.44}\n",
      "{'loss': 1.2667, 'grad_norm': 0.77734375, 'learning_rate': 0.000104890083201009, 'epoch': 0.44}\n",
      "{'loss': 1.3303, 'grad_norm': 0.546875, 'learning_rate': 0.00010487872082354513, 'epoch': 0.44}\n",
      "{'loss': 1.073, 'grad_norm': 0.515625, 'learning_rate': 0.00010486735844608126, 'epoch': 0.44}\n",
      "{'loss': 1.114, 'grad_norm': 0.61328125, 'learning_rate': 0.0001048559960686174, 'epoch': 0.44}\n",
      "{'loss': 1.203, 'grad_norm': 0.74609375, 'learning_rate': 0.00010484463369115353, 'epoch': 0.44}\n",
      "{'loss': 1.1718, 'grad_norm': 0.89453125, 'learning_rate': 0.00010483327131368967, 'epoch': 0.44}\n",
      "{'loss': 1.1984, 'grad_norm': 0.53515625, 'learning_rate': 0.00010482190893622581, 'epoch': 0.44}\n",
      "{'loss': 1.129, 'grad_norm': 0.6796875, 'learning_rate': 0.00010481054655876197, 'epoch': 0.44}\n",
      "{'loss': 1.1927, 'grad_norm': 0.44921875, 'learning_rate': 0.0001047991841812981, 'epoch': 0.44}\n",
      "{'loss': 1.2131, 'grad_norm': 0.6328125, 'learning_rate': 0.00010478782180383424, 'epoch': 0.44}\n",
      "{'loss': 1.2277, 'grad_norm': 1.015625, 'learning_rate': 0.00010477645942637037, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3254, 'grad_norm': 0.447265625, 'learning_rate': 0.00010476509704890651, 'epoch': 0.44}\n",
      "{'loss': 1.3093, 'grad_norm': 0.7578125, 'learning_rate': 0.00010475373467144265, 'epoch': 0.44}\n",
      "{'loss': 1.2214, 'grad_norm': 0.4375, 'learning_rate': 0.0001047423722939788, 'epoch': 0.44}\n",
      "{'loss': 1.2158, 'grad_norm': 0.56640625, 'learning_rate': 0.00010473100991651494, 'epoch': 0.44}\n",
      "{'loss': 1.1466, 'grad_norm': 0.9609375, 'learning_rate': 0.00010471964753905108, 'epoch': 0.44}\n",
      "{'loss': 1.284, 'grad_norm': 0.53515625, 'learning_rate': 0.00010470828516158722, 'epoch': 0.44}\n",
      "{'loss': 1.1529, 'grad_norm': 0.76171875, 'learning_rate': 0.00010469692278412335, 'epoch': 0.44}\n",
      "{'loss': 1.2731, 'grad_norm': 0.427734375, 'learning_rate': 0.00010468556040665949, 'epoch': 0.44}\n",
      "{'loss': 1.1989, 'grad_norm': 0.65625, 'learning_rate': 0.00010467419802919563, 'epoch': 0.44}\n",
      "{'loss': 1.1343, 'grad_norm': 0.8046875, 'learning_rate': 0.00010466283565173178, 'epoch': 0.44}\n",
      "{'loss': 1.4683, 'grad_norm': 0.609375, 'learning_rate': 0.00010465147327426792, 'epoch': 0.44}\n",
      "{'loss': 1.1938, 'grad_norm': 0.6640625, 'learning_rate': 0.00010464011089680406, 'epoch': 0.44}\n",
      "{'loss': 1.2698, 'grad_norm': 0.43359375, 'learning_rate': 0.0001046287485193402, 'epoch': 0.44}\n",
      "{'loss': 1.179, 'grad_norm': 0.8828125, 'learning_rate': 0.00010461738614187633, 'epoch': 0.44}\n",
      "{'loss': 1.1651, 'grad_norm': 0.85546875, 'learning_rate': 0.00010460602376441247, 'epoch': 0.44}\n",
      "{'loss': 1.355, 'grad_norm': 0.875, 'learning_rate': 0.0001045946613869486, 'epoch': 0.44}\n",
      "{'loss': 1.1607, 'grad_norm': 0.7421875, 'learning_rate': 0.00010458329900948476, 'epoch': 0.44}\n",
      "{'loss': 1.2632, 'grad_norm': 0.3125, 'learning_rate': 0.0001045719366320209, 'epoch': 0.44}\n",
      "{'loss': 1.2848, 'grad_norm': 0.51953125, 'learning_rate': 0.00010456057425455703, 'epoch': 0.44}\n",
      "{'loss': 1.1406, 'grad_norm': 0.828125, 'learning_rate': 0.00010454921187709317, 'epoch': 0.44}\n",
      "{'loss': 1.4998, 'grad_norm': 0.64453125, 'learning_rate': 0.0001045378494996293, 'epoch': 0.44}\n",
      "{'loss': 1.2045, 'grad_norm': 0.83203125, 'learning_rate': 0.00010452648712216543, 'epoch': 0.44}\n",
      "{'loss': 1.3006, 'grad_norm': 0.47265625, 'learning_rate': 0.00010451512474470157, 'epoch': 0.44}\n",
      "{'loss': 1.1907, 'grad_norm': 0.5859375, 'learning_rate': 0.00010450376236723774, 'epoch': 0.44}\n",
      "{'loss': 1.1797, 'grad_norm': 1.2890625, 'learning_rate': 0.00010449239998977388, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2829, 'grad_norm': 0.4765625, 'learning_rate': 0.00010448103761231, 'epoch': 0.44}\n",
      "{'loss': 1.118, 'grad_norm': 0.6328125, 'learning_rate': 0.00010446967523484614, 'epoch': 0.44}\n",
      "{'loss': 1.2403, 'grad_norm': 0.48828125, 'learning_rate': 0.00010445831285738228, 'epoch': 0.44}\n",
      "{'loss': 1.1168, 'grad_norm': 0.62109375, 'learning_rate': 0.00010444695047991841, 'epoch': 0.44}\n",
      "{'loss': 1.0847, 'grad_norm': 0.78515625, 'learning_rate': 0.00010443558810245458, 'epoch': 0.44}\n",
      "{'loss': 1.277, 'grad_norm': 0.72265625, 'learning_rate': 0.00010442422572499072, 'epoch': 0.44}\n",
      "{'loss': 1.1553, 'grad_norm': 0.6953125, 'learning_rate': 0.00010441286334752684, 'epoch': 0.44}\n",
      "{'loss': 1.2727, 'grad_norm': 0.68359375, 'learning_rate': 0.00010440150097006298, 'epoch': 0.44}\n",
      "{'loss': 1.3023, 'grad_norm': 0.55078125, 'learning_rate': 0.00010439013859259912, 'epoch': 0.44}\n",
      "{'loss': 0.9807, 'grad_norm': 0.46484375, 'learning_rate': 0.00010437877621513525, 'epoch': 0.44}\n",
      "{'loss': 1.2889, 'grad_norm': 0.5078125, 'learning_rate': 0.00010436741383767139, 'epoch': 0.44}\n",
      "{'loss': 1.1636, 'grad_norm': 0.77734375, 'learning_rate': 0.00010435605146020754, 'epoch': 0.44}\n",
      "{'loss': 1.2399, 'grad_norm': 0.419921875, 'learning_rate': 0.00010434468908274368, 'epoch': 0.44}\n",
      "{'loss': 1.1106, 'grad_norm': 0.57421875, 'learning_rate': 0.00010433332670527982, 'epoch': 0.44}\n",
      "{'loss': 1.1208, 'grad_norm': 0.984375, 'learning_rate': 0.00010432196432781596, 'epoch': 0.44}\n",
      "{'loss': 1.2484, 'grad_norm': 0.455078125, 'learning_rate': 0.0001043106019503521, 'epoch': 0.44}\n",
      "{'loss': 1.0581, 'grad_norm': 0.86328125, 'learning_rate': 0.00010429923957288823, 'epoch': 0.44}\n",
      "{'loss': 1.2395, 'grad_norm': 0.435546875, 'learning_rate': 0.00010428787719542437, 'epoch': 0.44}\n",
      "{'loss': 1.2141, 'grad_norm': 0.72265625, 'learning_rate': 0.00010427651481796052, 'epoch': 0.44}\n",
      "{'loss': 1.1061, 'grad_norm': 0.875, 'learning_rate': 0.00010426515244049666, 'epoch': 0.44}\n",
      "{'loss': 1.4186, 'grad_norm': 0.76171875, 'learning_rate': 0.0001042537900630328, 'epoch': 0.44}\n",
      "{'loss': 1.0552, 'grad_norm': 0.80859375, 'learning_rate': 0.00010424242768556894, 'epoch': 0.44}\n",
      "{'loss': 1.2814, 'grad_norm': 0.4765625, 'learning_rate': 0.00010423106530810507, 'epoch': 0.45}\n",
      "{'loss': 1.3193, 'grad_norm': 0.71875, 'learning_rate': 0.00010421970293064121, 'epoch': 0.45}\n",
      "{'loss': 1.0549, 'grad_norm': 1.0390625, 'learning_rate': 0.00010420834055317734, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3328, 'grad_norm': 0.58984375, 'learning_rate': 0.0001041969781757135, 'epoch': 0.45}\n",
      "{'loss': 1.1159, 'grad_norm': 0.87890625, 'learning_rate': 0.00010418561579824964, 'epoch': 0.45}\n",
      "{'loss': 1.2159, 'grad_norm': 0.578125, 'learning_rate': 0.00010417425342078578, 'epoch': 0.45}\n",
      "{'loss': 1.2109, 'grad_norm': 0.92578125, 'learning_rate': 0.00010416289104332191, 'epoch': 0.45}\n",
      "{'loss': 1.0986, 'grad_norm': 0.71484375, 'learning_rate': 0.00010415152866585804, 'epoch': 0.45}\n",
      "{'loss': 1.3701, 'grad_norm': 0.578125, 'learning_rate': 0.00010414016628839418, 'epoch': 0.45}\n",
      "{'loss': 1.1315, 'grad_norm': 0.703125, 'learning_rate': 0.00010412880391093031, 'epoch': 0.45}\n",
      "{'loss': 1.2059, 'grad_norm': 0.478515625, 'learning_rate': 0.00010411744153346648, 'epoch': 0.45}\n",
      "{'loss': 1.3457, 'grad_norm': 0.7578125, 'learning_rate': 0.00010410607915600262, 'epoch': 0.45}\n",
      "{'loss': 1.0562, 'grad_norm': 1.265625, 'learning_rate': 0.00010409471677853874, 'epoch': 0.45}\n",
      "{'loss': 1.305, 'grad_norm': 0.52734375, 'learning_rate': 0.00010408335440107488, 'epoch': 0.45}\n",
      "{'loss': 1.246, 'grad_norm': 0.6484375, 'learning_rate': 0.00010407199202361102, 'epoch': 0.45}\n",
      "{'loss': 1.2854, 'grad_norm': 0.453125, 'learning_rate': 0.00010406062964614715, 'epoch': 0.45}\n",
      "{'loss': 1.3567, 'grad_norm': 0.66796875, 'learning_rate': 0.00010404926726868332, 'epoch': 0.45}\n",
      "{'loss': 1.1263, 'grad_norm': 0.37890625, 'learning_rate': 0.00010403790489121946, 'epoch': 0.45}\n",
      "{'loss': 1.2713, 'grad_norm': 0.66796875, 'learning_rate': 0.00010402654251375558, 'epoch': 0.45}\n",
      "{'loss': 1.1841, 'grad_norm': 0.69140625, 'learning_rate': 0.00010401518013629172, 'epoch': 0.45}\n",
      "{'loss': 1.264, 'grad_norm': 0.58203125, 'learning_rate': 0.00010400381775882786, 'epoch': 0.45}\n",
      "{'loss': 1.3926, 'grad_norm': 0.59765625, 'learning_rate': 0.000103992455381364, 'epoch': 0.45}\n",
      "{'loss': 1.0772, 'grad_norm': 0.984375, 'learning_rate': 0.00010398109300390013, 'epoch': 0.45}\n",
      "{'loss': 1.3234, 'grad_norm': 0.5625, 'learning_rate': 0.00010396973062643628, 'epoch': 0.45}\n",
      "{'loss': 1.2027, 'grad_norm': 0.96484375, 'learning_rate': 0.00010395836824897242, 'epoch': 0.45}\n",
      "{'loss': 1.3013, 'grad_norm': 0.46875, 'learning_rate': 0.00010394700587150856, 'epoch': 0.45}\n",
      "{'loss': 1.3363, 'grad_norm': 0.53515625, 'learning_rate': 0.0001039356434940447, 'epoch': 0.45}\n",
      "{'loss': 1.0247, 'grad_norm': 0.6796875, 'learning_rate': 0.00010392428111658084, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2665, 'grad_norm': 0.50390625, 'learning_rate': 0.00010391291873911697, 'epoch': 0.45}\n",
      "{'loss': 1.3194, 'grad_norm': 0.71875, 'learning_rate': 0.00010390155636165311, 'epoch': 0.45}\n",
      "{'loss': 1.222, 'grad_norm': 0.53515625, 'learning_rate': 0.00010389019398418926, 'epoch': 0.45}\n",
      "{'loss': 1.167, 'grad_norm': 0.9921875, 'learning_rate': 0.0001038788316067254, 'epoch': 0.45}\n",
      "{'loss': 0.9872, 'grad_norm': 0.91796875, 'learning_rate': 0.00010386746922926154, 'epoch': 0.45}\n",
      "{'loss': 1.4259, 'grad_norm': 0.5546875, 'learning_rate': 0.00010385610685179768, 'epoch': 0.45}\n",
      "{'loss': 1.2928, 'grad_norm': 0.59765625, 'learning_rate': 0.00010384474447433381, 'epoch': 0.45}\n",
      "{'loss': 1.2289, 'grad_norm': 0.546875, 'learning_rate': 0.00010383338209686995, 'epoch': 0.45}\n",
      "{'loss': 1.2124, 'grad_norm': 0.494140625, 'learning_rate': 0.00010382201971940608, 'epoch': 0.45}\n",
      "{'loss': 1.0271, 'grad_norm': 0.91796875, 'learning_rate': 0.00010381065734194224, 'epoch': 0.45}\n",
      "{'loss': 1.3832, 'grad_norm': 0.65234375, 'learning_rate': 0.00010379929496447838, 'epoch': 0.45}\n",
      "{'loss': 1.13, 'grad_norm': 0.84765625, 'learning_rate': 0.00010378793258701452, 'epoch': 0.45}\n",
      "{'loss': 1.1726, 'grad_norm': 0.419921875, 'learning_rate': 0.00010377657020955066, 'epoch': 0.45}\n",
      "{'loss': 1.263, 'grad_norm': 0.51171875, 'learning_rate': 0.00010376520783208678, 'epoch': 0.45}\n",
      "{'loss': 1.0361, 'grad_norm': 0.96875, 'learning_rate': 0.00010375384545462292, 'epoch': 0.45}\n",
      "{'loss': 1.3442, 'grad_norm': 0.51953125, 'learning_rate': 0.00010374248307715908, 'epoch': 0.45}\n",
      "{'loss': 1.1928, 'grad_norm': 1.2109375, 'learning_rate': 0.00010373112069969522, 'epoch': 0.45}\n",
      "{'loss': 1.1848, 'grad_norm': 0.625, 'learning_rate': 0.00010371975832223136, 'epoch': 0.45}\n",
      "{'loss': 1.2386, 'grad_norm': 0.609375, 'learning_rate': 0.00010370839594476748, 'epoch': 0.45}\n",
      "{'loss': 1.0302, 'grad_norm': 0.474609375, 'learning_rate': 0.00010369703356730362, 'epoch': 0.45}\n",
      "{'loss': 1.3299, 'grad_norm': 0.62890625, 'learning_rate': 0.00010368567118983976, 'epoch': 0.45}\n",
      "{'loss': 1.1891, 'grad_norm': 0.88671875, 'learning_rate': 0.0001036743088123759, 'epoch': 0.45}\n",
      "{'loss': 1.3472, 'grad_norm': 0.466796875, 'learning_rate': 0.00010366294643491206, 'epoch': 0.45}\n",
      "{'loss': 1.225, 'grad_norm': 0.447265625, 'learning_rate': 0.0001036515840574482, 'epoch': 0.45}\n",
      "{'loss': 1.0854, 'grad_norm': 0.78515625, 'learning_rate': 0.00010364022167998432, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3081, 'grad_norm': 0.76953125, 'learning_rate': 0.00010362885930252046, 'epoch': 0.45}\n",
      "{'loss': 1.2504, 'grad_norm': 0.921875, 'learning_rate': 0.0001036174969250566, 'epoch': 0.45}\n",
      "{'loss': 1.1833, 'grad_norm': 0.482421875, 'learning_rate': 0.00010360613454759274, 'epoch': 0.45}\n",
      "{'loss': 1.2003, 'grad_norm': 0.6484375, 'learning_rate': 0.00010359477217012887, 'epoch': 0.45}\n",
      "{'loss': 1.0031, 'grad_norm': 0.6484375, 'learning_rate': 0.00010358340979266503, 'epoch': 0.45}\n",
      "{'loss': 1.3446, 'grad_norm': 0.52734375, 'learning_rate': 0.00010357204741520116, 'epoch': 0.45}\n",
      "{'loss': 1.0766, 'grad_norm': 0.76953125, 'learning_rate': 0.0001035606850377373, 'epoch': 0.45}\n",
      "{'loss': 1.2198, 'grad_norm': 0.61328125, 'learning_rate': 0.00010354932266027344, 'epoch': 0.45}\n",
      "{'loss': 1.1486, 'grad_norm': 0.5859375, 'learning_rate': 0.00010353796028280958, 'epoch': 0.45}\n",
      "{'loss': 1.0157, 'grad_norm': 0.64453125, 'learning_rate': 0.00010352659790534572, 'epoch': 0.45}\n",
      "{'loss': 1.4707, 'grad_norm': 0.455078125, 'learning_rate': 0.00010351523552788185, 'epoch': 0.45}\n",
      "{'loss': 1.3115, 'grad_norm': 0.7734375, 'learning_rate': 0.000103503873150418, 'epoch': 0.45}\n",
      "{'loss': 1.2452, 'grad_norm': 0.3828125, 'learning_rate': 0.00010349251077295414, 'epoch': 0.45}\n",
      "{'loss': 1.2696, 'grad_norm': 0.5859375, 'learning_rate': 0.00010348114839549028, 'epoch': 0.45}\n",
      "{'loss': 1.2044, 'grad_norm': 0.7109375, 'learning_rate': 0.00010346978601802642, 'epoch': 0.45}\n",
      "{'loss': 1.3063, 'grad_norm': 0.67578125, 'learning_rate': 0.00010345842364056256, 'epoch': 0.45}\n",
      "{'loss': 1.1568, 'grad_norm': 0.57421875, 'learning_rate': 0.0001034470612630987, 'epoch': 0.45}\n",
      "{'loss': 1.1222, 'grad_norm': 0.490234375, 'learning_rate': 0.00010343569888563482, 'epoch': 0.45}\n",
      "{'loss': 1.167, 'grad_norm': 0.6328125, 'learning_rate': 0.00010342433650817098, 'epoch': 0.45}\n",
      "{'loss': 1.0236, 'grad_norm': 1.890625, 'learning_rate': 0.00010341297413070712, 'epoch': 0.45}\n",
      "{'loss': 1.2405, 'grad_norm': 0.478515625, 'learning_rate': 0.00010340161175324326, 'epoch': 0.45}\n",
      "{'loss': 1.3569, 'grad_norm': 0.88671875, 'learning_rate': 0.0001033902493757794, 'epoch': 0.45}\n",
      "{'loss': 1.2602, 'grad_norm': 0.4609375, 'learning_rate': 0.00010337888699831552, 'epoch': 0.45}\n",
      "{'loss': 1.3264, 'grad_norm': 0.48046875, 'learning_rate': 0.00010336752462085166, 'epoch': 0.45}\n",
      "{'loss': 1.1408, 'grad_norm': 0.93359375, 'learning_rate': 0.00010335616224338782, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3003, 'grad_norm': 0.58984375, 'learning_rate': 0.00010334479986592396, 'epoch': 0.45}\n",
      "{'loss': 1.1892, 'grad_norm': 0.53515625, 'learning_rate': 0.0001033334374884601, 'epoch': 0.45}\n",
      "{'loss': 1.3972, 'grad_norm': 0.5078125, 'learning_rate': 0.00010332207511099624, 'epoch': 0.45}\n",
      "{'loss': 1.2153, 'grad_norm': 0.53515625, 'learning_rate': 0.00010331071273353236, 'epoch': 0.45}\n",
      "{'loss': 1.1081, 'grad_norm': 0.71875, 'learning_rate': 0.0001032993503560685, 'epoch': 0.45}\n",
      "{'loss': 1.4633, 'grad_norm': 0.470703125, 'learning_rate': 0.00010328798797860464, 'epoch': 0.45}\n",
      "{'loss': 1.1808, 'grad_norm': 0.69921875, 'learning_rate': 0.0001032766256011408, 'epoch': 0.45}\n",
      "{'loss': 1.0888, 'grad_norm': 0.470703125, 'learning_rate': 0.00010326526322367694, 'epoch': 0.45}\n",
      "{'loss': 1.2175, 'grad_norm': 0.7421875, 'learning_rate': 0.00010325390084621306, 'epoch': 0.45}\n",
      "{'loss': 0.9763, 'grad_norm': 0.90234375, 'learning_rate': 0.0001032425384687492, 'epoch': 0.45}\n",
      "{'loss': 1.3529, 'grad_norm': 0.515625, 'learning_rate': 0.00010323117609128534, 'epoch': 0.45}\n",
      "{'loss': 1.1431, 'grad_norm': 0.60546875, 'learning_rate': 0.00010321981371382148, 'epoch': 0.45}\n",
      "{'loss': 1.1647, 'grad_norm': 0.439453125, 'learning_rate': 0.00010320845133635762, 'epoch': 0.45}\n",
      "{'loss': 1.2028, 'grad_norm': 0.53125, 'learning_rate': 0.00010319708895889377, 'epoch': 0.45}\n",
      "{'loss': 1.002, 'grad_norm': 0.7578125, 'learning_rate': 0.0001031857265814299, 'epoch': 0.45}\n",
      "{'loss': 1.2736, 'grad_norm': 0.45703125, 'learning_rate': 0.00010317436420396604, 'epoch': 0.45}\n",
      "{'loss': 1.3745, 'grad_norm': 0.462890625, 'learning_rate': 0.00010316300182650218, 'epoch': 0.45}\n",
      "{'loss': 1.2631, 'grad_norm': 0.451171875, 'learning_rate': 0.00010315163944903832, 'epoch': 0.45}\n",
      "{'loss': 1.2654, 'grad_norm': 0.67578125, 'learning_rate': 0.00010314027707157446, 'epoch': 0.45}\n",
      "{'loss': 1.0842, 'grad_norm': 0.88671875, 'learning_rate': 0.0001031289146941106, 'epoch': 0.45}\n",
      "{'loss': 1.3872, 'grad_norm': 0.625, 'learning_rate': 0.00010311755231664675, 'epoch': 0.45}\n",
      "{'loss': 1.0891, 'grad_norm': 0.59375, 'learning_rate': 0.00010310618993918288, 'epoch': 0.45}\n",
      "{'loss': 1.2826, 'grad_norm': 0.5, 'learning_rate': 0.00010309482756171902, 'epoch': 0.45}\n",
      "{'loss': 1.3333, 'grad_norm': 0.546875, 'learning_rate': 0.00010308346518425516, 'epoch': 0.45}\n",
      "{'loss': 1.0343, 'grad_norm': 0.59375, 'learning_rate': 0.0001030721028067913, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3538, 'grad_norm': 0.46875, 'learning_rate': 0.00010306074042932743, 'epoch': 0.45}\n",
      "{'loss': 1.2381, 'grad_norm': 0.58984375, 'learning_rate': 0.00010304937805186359, 'epoch': 0.45}\n",
      "{'loss': 1.2352, 'grad_norm': 0.53515625, 'learning_rate': 0.00010303801567439972, 'epoch': 0.45}\n",
      "{'loss': 1.1879, 'grad_norm': 0.8125, 'learning_rate': 0.00010302665329693586, 'epoch': 0.45}\n",
      "{'loss': 1.138, 'grad_norm': 0.81640625, 'learning_rate': 0.000103015290919472, 'epoch': 0.45}\n",
      "{'loss': 1.3265, 'grad_norm': 0.51953125, 'learning_rate': 0.00010300392854200814, 'epoch': 0.45}\n",
      "{'loss': 1.1386, 'grad_norm': 0.8515625, 'learning_rate': 0.00010299256616454426, 'epoch': 0.45}\n",
      "{'loss': 1.2928, 'grad_norm': 0.42578125, 'learning_rate': 0.0001029812037870804, 'epoch': 0.45}\n",
      "{'loss': 1.2412, 'grad_norm': 0.578125, 'learning_rate': 0.00010296984140961656, 'epoch': 0.45}\n",
      "{'loss': 1.1106, 'grad_norm': 1.265625, 'learning_rate': 0.0001029584790321527, 'epoch': 0.45}\n",
      "{'loss': 1.2753, 'grad_norm': 0.46875, 'learning_rate': 0.00010294711665468884, 'epoch': 0.45}\n",
      "{'loss': 1.3497, 'grad_norm': 0.72265625, 'learning_rate': 0.00010293575427722498, 'epoch': 0.45}\n",
      "{'loss': 1.2615, 'grad_norm': 0.52734375, 'learning_rate': 0.0001029243918997611, 'epoch': 0.45}\n",
      "{'loss': 1.1971, 'grad_norm': 0.80859375, 'learning_rate': 0.00010291302952229724, 'epoch': 0.45}\n",
      "{'loss': 0.9685, 'grad_norm': 0.96484375, 'learning_rate': 0.00010290166714483338, 'epoch': 0.45}\n",
      "{'loss': 1.2555, 'grad_norm': 0.498046875, 'learning_rate': 0.00010289030476736954, 'epoch': 0.45}\n",
      "{'loss': 1.337, 'grad_norm': 0.8671875, 'learning_rate': 0.00010287894238990568, 'epoch': 0.45}\n",
      "{'loss': 1.384, 'grad_norm': 0.453125, 'learning_rate': 0.0001028675800124418, 'epoch': 0.45}\n",
      "{'loss': 1.1849, 'grad_norm': 0.5234375, 'learning_rate': 0.00010285621763497794, 'epoch': 0.45}\n",
      "{'loss': 1.0341, 'grad_norm': 1.3984375, 'learning_rate': 0.00010284485525751408, 'epoch': 0.45}\n",
      "{'loss': 1.3436, 'grad_norm': 0.470703125, 'learning_rate': 0.00010283349288005022, 'epoch': 0.45}\n",
      "{'loss': 1.1901, 'grad_norm': 0.8125, 'learning_rate': 0.00010282213050258636, 'epoch': 0.45}\n",
      "{'loss': 1.1292, 'grad_norm': 0.48046875, 'learning_rate': 0.00010281076812512251, 'epoch': 0.45}\n",
      "{'loss': 1.3333, 'grad_norm': 0.6015625, 'learning_rate': 0.00010279940574765865, 'epoch': 0.45}\n",
      "{'loss': 1.1697, 'grad_norm': 0.490234375, 'learning_rate': 0.00010278804337019478, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4595, 'grad_norm': 0.5859375, 'learning_rate': 0.00010277668099273092, 'epoch': 0.45}\n",
      "{'loss': 1.1667, 'grad_norm': 1.1953125, 'learning_rate': 0.00010276531861526706, 'epoch': 0.45}\n",
      "{'loss': 1.0578, 'grad_norm': 0.41015625, 'learning_rate': 0.0001027539562378032, 'epoch': 0.45}\n",
      "{'loss': 1.2466, 'grad_norm': 0.6484375, 'learning_rate': 0.00010274259386033935, 'epoch': 0.45}\n",
      "{'loss': 1.2077, 'grad_norm': 0.6953125, 'learning_rate': 0.00010273123148287549, 'epoch': 0.45}\n",
      "{'loss': 1.2153, 'grad_norm': 0.5625, 'learning_rate': 0.00010271986910541162, 'epoch': 0.45}\n",
      "{'loss': 1.2792, 'grad_norm': 0.56640625, 'learning_rate': 0.00010270850672794776, 'epoch': 0.45}\n",
      "{'loss': 1.1705, 'grad_norm': 0.419921875, 'learning_rate': 0.0001026971443504839, 'epoch': 0.45}\n",
      "{'loss': 1.2852, 'grad_norm': 0.58203125, 'learning_rate': 0.00010268578197302004, 'epoch': 0.45}\n",
      "{'loss': 0.9846, 'grad_norm': 0.921875, 'learning_rate': 0.00010267441959555618, 'epoch': 0.45}\n",
      "{'loss': 1.3128, 'grad_norm': 0.451171875, 'learning_rate': 0.00010266305721809233, 'epoch': 0.45}\n",
      "{'loss': 1.2096, 'grad_norm': 0.76953125, 'learning_rate': 0.00010265169484062847, 'epoch': 0.45}\n",
      "{'loss': 1.1383, 'grad_norm': 0.54296875, 'learning_rate': 0.0001026403324631646, 'epoch': 0.45}\n",
      "{'loss': 1.1872, 'grad_norm': 0.76953125, 'learning_rate': 0.00010262897008570074, 'epoch': 0.45}\n",
      "{'loss': 1.0366, 'grad_norm': 0.75, 'learning_rate': 0.00010261760770823688, 'epoch': 0.45}\n",
      "{'loss': 1.3377, 'grad_norm': 0.44921875, 'learning_rate': 0.000102606245330773, 'epoch': 0.45}\n",
      "{'loss': 1.0701, 'grad_norm': 0.6953125, 'learning_rate': 0.00010259488295330914, 'epoch': 0.45}\n",
      "{'loss': 1.2393, 'grad_norm': 0.50390625, 'learning_rate': 0.0001025835205758453, 'epoch': 0.45}\n",
      "{'loss': 1.2434, 'grad_norm': 0.6015625, 'learning_rate': 0.00010257215819838144, 'epoch': 0.45}\n",
      "{'loss': 1.1102, 'grad_norm': 1.1640625, 'learning_rate': 0.00010256079582091758, 'epoch': 0.45}\n",
      "{'loss': 1.3894, 'grad_norm': 0.6171875, 'learning_rate': 0.00010254943344345372, 'epoch': 0.45}\n",
      "{'loss': 1.244, 'grad_norm': 0.8359375, 'learning_rate': 0.00010253807106598984, 'epoch': 0.45}\n",
      "{'loss': 1.1809, 'grad_norm': 0.373046875, 'learning_rate': 0.00010252670868852598, 'epoch': 0.45}\n",
      "{'loss': 1.2817, 'grad_norm': 0.76171875, 'learning_rate': 0.00010251534631106212, 'epoch': 0.45}\n",
      "{'loss': 1.1193, 'grad_norm': 0.9453125, 'learning_rate': 0.00010250398393359828, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3197, 'grad_norm': 0.59375, 'learning_rate': 0.00010249262155613442, 'epoch': 0.45}\n",
      "{'loss': 1.218, 'grad_norm': 0.640625, 'learning_rate': 0.00010248125917867055, 'epoch': 0.45}\n",
      "{'loss': 1.1655, 'grad_norm': 0.494140625, 'learning_rate': 0.00010246989680120668, 'epoch': 0.45}\n",
      "{'loss': 1.2644, 'grad_norm': 0.7421875, 'learning_rate': 0.00010245853442374282, 'epoch': 0.45}\n",
      "{'loss': 1.0472, 'grad_norm': 0.78125, 'learning_rate': 0.00010244717204627896, 'epoch': 0.45}\n",
      "{'loss': 1.2834, 'grad_norm': 0.625, 'learning_rate': 0.0001024358096688151, 'epoch': 0.45}\n",
      "{'loss': 1.0694, 'grad_norm': 0.61328125, 'learning_rate': 0.00010242444729135125, 'epoch': 0.45}\n",
      "{'loss': 1.1679, 'grad_norm': 0.45703125, 'learning_rate': 0.00010241308491388739, 'epoch': 0.45}\n",
      "{'loss': 1.1792, 'grad_norm': 0.484375, 'learning_rate': 0.00010240172253642353, 'epoch': 0.45}\n",
      "{'loss': 1.0211, 'grad_norm': 0.640625, 'learning_rate': 0.00010239036015895966, 'epoch': 0.45}\n",
      "{'loss': 1.4266, 'grad_norm': 0.53515625, 'learning_rate': 0.0001023789977814958, 'epoch': 0.45}\n",
      "{'loss': 1.1342, 'grad_norm': 0.77734375, 'learning_rate': 0.00010236763540403194, 'epoch': 0.45}\n",
      "{'loss': 1.2295, 'grad_norm': 0.439453125, 'learning_rate': 0.00010235627302656809, 'epoch': 0.45}\n",
      "{'loss': 1.3435, 'grad_norm': 0.5390625, 'learning_rate': 0.00010234491064910423, 'epoch': 0.45}\n",
      "{'loss': 1.0383, 'grad_norm': 0.451171875, 'learning_rate': 0.00010233354827164037, 'epoch': 0.45}\n",
      "{'loss': 1.3534, 'grad_norm': 0.53125, 'learning_rate': 0.0001023221858941765, 'epoch': 0.45}\n",
      "{'loss': 1.2039, 'grad_norm': 0.83203125, 'learning_rate': 0.00010231082351671264, 'epoch': 0.45}\n",
      "{'loss': 1.1776, 'grad_norm': 0.47265625, 'learning_rate': 0.00010229946113924878, 'epoch': 0.45}\n",
      "{'loss': 1.2177, 'grad_norm': 0.498046875, 'learning_rate': 0.00010228809876178492, 'epoch': 0.45}\n",
      "{'loss': 1.056, 'grad_norm': 1.1953125, 'learning_rate': 0.00010227673638432107, 'epoch': 0.45}\n",
      "{'loss': 1.2889, 'grad_norm': 0.66015625, 'learning_rate': 0.0001022653740068572, 'epoch': 0.45}\n",
      "{'loss': 1.1169, 'grad_norm': 0.78125, 'learning_rate': 0.00010225401162939334, 'epoch': 0.45}\n",
      "{'loss': 1.0696, 'grad_norm': 0.40625, 'learning_rate': 0.00010224264925192948, 'epoch': 0.45}\n",
      "{'loss': 1.3478, 'grad_norm': 0.5859375, 'learning_rate': 0.00010223128687446562, 'epoch': 0.45}\n",
      "{'loss': 1.1024, 'grad_norm': 1.7109375, 'learning_rate': 0.00010221992449700174, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.352, 'grad_norm': 0.4765625, 'learning_rate': 0.00010220856211953788, 'epoch': 0.45}\n",
      "{'loss': 1.1316, 'grad_norm': 0.625, 'learning_rate': 0.00010219719974207405, 'epoch': 0.45}\n",
      "{'loss': 1.2642, 'grad_norm': 0.423828125, 'learning_rate': 0.00010218583736461019, 'epoch': 0.45}\n",
      "{'loss': 1.2242, 'grad_norm': 0.8203125, 'learning_rate': 0.00010217447498714632, 'epoch': 0.45}\n",
      "{'loss': 1.1757, 'grad_norm': 0.72265625, 'learning_rate': 0.00010216311260968246, 'epoch': 0.45}\n",
      "{'loss': 1.2813, 'grad_norm': 0.478515625, 'learning_rate': 0.00010215175023221859, 'epoch': 0.45}\n",
      "{'loss': 1.2165, 'grad_norm': 1.1328125, 'learning_rate': 0.00010214038785475472, 'epoch': 0.45}\n",
      "{'loss': 1.1998, 'grad_norm': 0.51953125, 'learning_rate': 0.00010212902547729086, 'epoch': 0.45}\n",
      "{'loss': 1.2155, 'grad_norm': 0.83203125, 'learning_rate': 0.00010211766309982703, 'epoch': 0.45}\n",
      "{'loss': 1.133, 'grad_norm': 0.76171875, 'learning_rate': 0.00010210630072236316, 'epoch': 0.45}\n",
      "{'loss': 1.3923, 'grad_norm': 0.55859375, 'learning_rate': 0.00010209493834489929, 'epoch': 0.45}\n",
      "{'loss': 1.2687, 'grad_norm': 0.75390625, 'learning_rate': 0.00010208357596743543, 'epoch': 0.45}\n",
      "{'loss': 1.286, 'grad_norm': 0.466796875, 'learning_rate': 0.00010207221358997156, 'epoch': 0.45}\n",
      "{'loss': 1.3309, 'grad_norm': 0.6015625, 'learning_rate': 0.0001020608512125077, 'epoch': 0.45}\n",
      "{'loss': 1.1053, 'grad_norm': 1.171875, 'learning_rate': 0.00010204948883504387, 'epoch': 0.45}\n",
      "{'loss': 1.4614, 'grad_norm': 0.57421875, 'learning_rate': 0.00010203812645757999, 'epoch': 0.45}\n",
      "{'loss': 1.1608, 'grad_norm': 0.7109375, 'learning_rate': 0.00010202676408011613, 'epoch': 0.45}\n",
      "{'loss': 1.2356, 'grad_norm': 0.4296875, 'learning_rate': 0.00010201540170265227, 'epoch': 0.45}\n",
      "{'loss': 1.3911, 'grad_norm': 1.21875, 'learning_rate': 0.0001020040393251884, 'epoch': 0.45}\n",
      "{'loss': 1.0625, 'grad_norm': 1.2578125, 'learning_rate': 0.00010199267694772454, 'epoch': 0.45}\n",
      "{'loss': 1.2065, 'grad_norm': 0.427734375, 'learning_rate': 0.00010198131457026068, 'epoch': 0.45}\n",
      "{'loss': 1.2565, 'grad_norm': 0.51953125, 'learning_rate': 0.00010196995219279683, 'epoch': 0.45}\n",
      "{'loss': 1.2981, 'grad_norm': 0.462890625, 'learning_rate': 0.00010195858981533297, 'epoch': 0.45}\n",
      "{'loss': 1.1941, 'grad_norm': 0.59765625, 'learning_rate': 0.00010194722743786911, 'epoch': 0.46}\n",
      "{'loss': 0.9947, 'grad_norm': 0.80078125, 'learning_rate': 0.00010193586506040525, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3267, 'grad_norm': 0.69921875, 'learning_rate': 0.00010192450268294138, 'epoch': 0.46}\n",
      "{'loss': 1.1928, 'grad_norm': 0.81640625, 'learning_rate': 0.00010191314030547752, 'epoch': 0.46}\n",
      "{'loss': 1.3162, 'grad_norm': 0.455078125, 'learning_rate': 0.00010190177792801366, 'epoch': 0.46}\n",
      "{'loss': 1.2592, 'grad_norm': 0.59765625, 'learning_rate': 0.00010189041555054981, 'epoch': 0.46}\n",
      "{'loss': 1.1086, 'grad_norm': 0.7265625, 'learning_rate': 0.00010187905317308595, 'epoch': 0.46}\n",
      "{'loss': 1.2895, 'grad_norm': 0.5390625, 'learning_rate': 0.00010186769079562209, 'epoch': 0.46}\n",
      "{'loss': 1.1314, 'grad_norm': 0.9140625, 'learning_rate': 0.00010185632841815822, 'epoch': 0.46}\n",
      "{'loss': 1.1389, 'grad_norm': 0.431640625, 'learning_rate': 0.00010184496604069436, 'epoch': 0.46}\n",
      "{'loss': 1.3545, 'grad_norm': 0.765625, 'learning_rate': 0.00010183360366323049, 'epoch': 0.46}\n",
      "{'loss': 1.0053, 'grad_norm': 0.77734375, 'learning_rate': 0.00010182224128576662, 'epoch': 0.46}\n",
      "{'loss': 1.3175, 'grad_norm': 0.52734375, 'learning_rate': 0.00010181087890830279, 'epoch': 0.46}\n",
      "{'loss': 1.1457, 'grad_norm': 0.75, 'learning_rate': 0.00010179951653083893, 'epoch': 0.46}\n",
      "{'loss': 1.2131, 'grad_norm': 0.56640625, 'learning_rate': 0.00010178815415337506, 'epoch': 0.46}\n",
      "{'loss': 1.215, 'grad_norm': 0.76171875, 'learning_rate': 0.0001017767917759112, 'epoch': 0.46}\n",
      "{'loss': 1.0721, 'grad_norm': 0.79296875, 'learning_rate': 0.00010176542939844733, 'epoch': 0.46}\n",
      "{'loss': 1.4477, 'grad_norm': 0.466796875, 'learning_rate': 0.00010175406702098346, 'epoch': 0.46}\n",
      "{'loss': 1.2045, 'grad_norm': 0.9140625, 'learning_rate': 0.0001017427046435196, 'epoch': 0.46}\n",
      "{'loss': 1.2395, 'grad_norm': 0.46875, 'learning_rate': 0.00010173134226605577, 'epoch': 0.46}\n",
      "{'loss': 1.2836, 'grad_norm': 0.640625, 'learning_rate': 0.0001017199798885919, 'epoch': 0.46}\n",
      "{'loss': 1.0054, 'grad_norm': 0.75390625, 'learning_rate': 0.00010170861751112803, 'epoch': 0.46}\n",
      "{'loss': 1.4182, 'grad_norm': 0.6875, 'learning_rate': 0.00010169725513366417, 'epoch': 0.46}\n",
      "{'loss': 1.1145, 'grad_norm': 0.7890625, 'learning_rate': 0.0001016858927562003, 'epoch': 0.46}\n",
      "{'loss': 1.172, 'grad_norm': 0.578125, 'learning_rate': 0.00010167453037873644, 'epoch': 0.46}\n",
      "{'loss': 1.1895, 'grad_norm': 0.58203125, 'learning_rate': 0.00010166316800127261, 'epoch': 0.46}\n",
      "{'loss': 1.0376, 'grad_norm': 0.94921875, 'learning_rate': 0.00010165180562380873, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3117, 'grad_norm': 0.78125, 'learning_rate': 0.00010164044324634487, 'epoch': 0.46}\n",
      "{'loss': 1.235, 'grad_norm': 0.7109375, 'learning_rate': 0.00010162908086888101, 'epoch': 0.46}\n",
      "{'loss': 1.2625, 'grad_norm': 0.4140625, 'learning_rate': 0.00010161771849141715, 'epoch': 0.46}\n",
      "{'loss': 1.2669, 'grad_norm': 0.6171875, 'learning_rate': 0.00010160635611395328, 'epoch': 0.46}\n",
      "{'loss': 1.0713, 'grad_norm': 0.9765625, 'learning_rate': 0.00010159499373648942, 'epoch': 0.46}\n",
      "{'loss': 1.2951, 'grad_norm': 0.52734375, 'learning_rate': 0.00010158363135902557, 'epoch': 0.46}\n",
      "{'loss': 1.2122, 'grad_norm': 0.875, 'learning_rate': 0.00010157226898156171, 'epoch': 0.46}\n",
      "{'loss': 1.1762, 'grad_norm': 0.458984375, 'learning_rate': 0.00010156090660409785, 'epoch': 0.46}\n",
      "{'loss': 1.1846, 'grad_norm': 0.515625, 'learning_rate': 0.00010154954422663399, 'epoch': 0.46}\n",
      "{'loss': 1.0757, 'grad_norm': 0.80859375, 'learning_rate': 0.00010153818184917012, 'epoch': 0.46}\n",
      "{'loss': 1.4144, 'grad_norm': 0.66796875, 'learning_rate': 0.00010152681947170626, 'epoch': 0.46}\n",
      "{'loss': 1.1818, 'grad_norm': 0.71875, 'learning_rate': 0.0001015154570942424, 'epoch': 0.46}\n",
      "{'loss': 1.2543, 'grad_norm': 0.5390625, 'learning_rate': 0.00010150409471677855, 'epoch': 0.46}\n",
      "{'loss': 1.2191, 'grad_norm': 0.52734375, 'learning_rate': 0.00010149273233931469, 'epoch': 0.46}\n",
      "{'loss': 1.131, 'grad_norm': 1.625, 'learning_rate': 0.00010148136996185083, 'epoch': 0.46}\n",
      "{'loss': 1.3752, 'grad_norm': 0.52734375, 'learning_rate': 0.00010147000758438696, 'epoch': 0.46}\n",
      "{'loss': 1.1932, 'grad_norm': 0.7265625, 'learning_rate': 0.0001014586452069231, 'epoch': 0.46}\n",
      "{'loss': 1.1773, 'grad_norm': 0.447265625, 'learning_rate': 0.00010144728282945923, 'epoch': 0.46}\n",
      "{'loss': 1.2374, 'grad_norm': 0.70703125, 'learning_rate': 0.00010143592045199536, 'epoch': 0.46}\n",
      "{'loss': 1.2335, 'grad_norm': 1.171875, 'learning_rate': 0.00010142455807453153, 'epoch': 0.46}\n",
      "{'loss': 1.2449, 'grad_norm': 0.77734375, 'learning_rate': 0.00010141319569706767, 'epoch': 0.46}\n",
      "{'loss': 1.2792, 'grad_norm': 0.73046875, 'learning_rate': 0.0001014018333196038, 'epoch': 0.46}\n",
      "{'loss': 1.0526, 'grad_norm': 0.494140625, 'learning_rate': 0.00010139047094213994, 'epoch': 0.46}\n",
      "{'loss': 1.2847, 'grad_norm': 0.55859375, 'learning_rate': 0.00010137910856467607, 'epoch': 0.46}\n",
      "{'loss': 1.176, 'grad_norm': 0.875, 'learning_rate': 0.0001013677461872122, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3185, 'grad_norm': 0.6796875, 'learning_rate': 0.00010135638380974837, 'epoch': 0.46}\n",
      "{'loss': 1.1391, 'grad_norm': 0.65234375, 'learning_rate': 0.00010134502143228451, 'epoch': 0.46}\n",
      "{'loss': 1.2166, 'grad_norm': 0.498046875, 'learning_rate': 0.00010133365905482065, 'epoch': 0.46}\n",
      "{'loss': 1.2302, 'grad_norm': 0.59375, 'learning_rate': 0.00010132229667735677, 'epoch': 0.46}\n",
      "{'loss': 1.1137, 'grad_norm': 0.8046875, 'learning_rate': 0.00010131093429989291, 'epoch': 0.46}\n",
      "{'loss': 1.29, 'grad_norm': 0.52734375, 'learning_rate': 0.00010129957192242905, 'epoch': 0.46}\n",
      "{'loss': 1.1286, 'grad_norm': 0.63671875, 'learning_rate': 0.00010128820954496518, 'epoch': 0.46}\n",
      "{'loss': 1.1438, 'grad_norm': 0.69921875, 'learning_rate': 0.00010127684716750135, 'epoch': 0.46}\n",
      "{'loss': 1.2874, 'grad_norm': 0.5, 'learning_rate': 0.00010126548479003747, 'epoch': 0.46}\n",
      "{'loss': 1.0671, 'grad_norm': 0.75390625, 'learning_rate': 0.00010125412241257361, 'epoch': 0.46}\n",
      "{'loss': 1.2754, 'grad_norm': 0.69140625, 'learning_rate': 0.00010124276003510975, 'epoch': 0.46}\n",
      "{'loss': 1.2183, 'grad_norm': 0.6875, 'learning_rate': 0.00010123139765764589, 'epoch': 0.46}\n",
      "{'loss': 1.0996, 'grad_norm': 0.4375, 'learning_rate': 0.00010122003528018202, 'epoch': 0.46}\n",
      "{'loss': 1.241, 'grad_norm': 0.82421875, 'learning_rate': 0.00010120867290271816, 'epoch': 0.46}\n",
      "{'loss': 1.0929, 'grad_norm': 0.859375, 'learning_rate': 0.00010119731052525431, 'epoch': 0.46}\n",
      "{'loss': 1.3048, 'grad_norm': 0.4921875, 'learning_rate': 0.00010118594814779045, 'epoch': 0.46}\n",
      "{'loss': 1.1383, 'grad_norm': 0.55078125, 'learning_rate': 0.00010117458577032659, 'epoch': 0.46}\n",
      "{'loss': 1.2439, 'grad_norm': 0.5703125, 'learning_rate': 0.00010116322339286273, 'epoch': 0.46}\n",
      "{'loss': 1.297, 'grad_norm': 0.671875, 'learning_rate': 0.00010115186101539887, 'epoch': 0.46}\n",
      "{'loss': 1.098, 'grad_norm': 0.796875, 'learning_rate': 0.000101140498637935, 'epoch': 0.46}\n",
      "{'loss': 1.3431, 'grad_norm': 0.447265625, 'learning_rate': 0.00010112913626047114, 'epoch': 0.46}\n",
      "{'loss': 1.1628, 'grad_norm': 0.84765625, 'learning_rate': 0.00010111777388300729, 'epoch': 0.46}\n",
      "{'loss': 1.193, 'grad_norm': 0.61328125, 'learning_rate': 0.00010110641150554343, 'epoch': 0.46}\n",
      "{'loss': 1.1486, 'grad_norm': 0.625, 'learning_rate': 0.00010109504912807957, 'epoch': 0.46}\n",
      "{'loss': 1.2187, 'grad_norm': 0.8359375, 'learning_rate': 0.0001010836867506157, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4411, 'grad_norm': 0.546875, 'learning_rate': 0.00010107232437315184, 'epoch': 0.46}\n",
      "{'loss': 1.1244, 'grad_norm': 0.82421875, 'learning_rate': 0.00010106096199568797, 'epoch': 0.46}\n",
      "{'loss': 1.2425, 'grad_norm': 0.50390625, 'learning_rate': 0.0001010495996182241, 'epoch': 0.46}\n",
      "{'loss': 1.2958, 'grad_norm': 0.5859375, 'learning_rate': 0.00010103823724076027, 'epoch': 0.46}\n",
      "{'loss': 1.1385, 'grad_norm': 0.78125, 'learning_rate': 0.00010102687486329641, 'epoch': 0.46}\n",
      "{'loss': 1.2447, 'grad_norm': 0.51171875, 'learning_rate': 0.00010101551248583255, 'epoch': 0.46}\n",
      "{'loss': 1.1374, 'grad_norm': 0.5859375, 'learning_rate': 0.00010100415010836868, 'epoch': 0.46}\n",
      "{'loss': 1.1566, 'grad_norm': 0.419921875, 'learning_rate': 0.00010099278773090481, 'epoch': 0.46}\n",
      "{'loss': 1.1599, 'grad_norm': 0.76953125, 'learning_rate': 0.00010098142535344095, 'epoch': 0.46}\n",
      "{'loss': 1.1641, 'grad_norm': 1.046875, 'learning_rate': 0.00010097006297597711, 'epoch': 0.46}\n",
      "{'loss': 1.2495, 'grad_norm': 0.703125, 'learning_rate': 0.00010095870059851325, 'epoch': 0.46}\n",
      "{'loss': 1.0783, 'grad_norm': 0.85546875, 'learning_rate': 0.00010094733822104939, 'epoch': 0.46}\n",
      "{'loss': 1.2665, 'grad_norm': 0.439453125, 'learning_rate': 0.00010093597584358551, 'epoch': 0.46}\n",
      "{'loss': 1.2578, 'grad_norm': 0.6328125, 'learning_rate': 0.00010092461346612165, 'epoch': 0.46}\n",
      "{'loss': 0.9778, 'grad_norm': 0.89453125, 'learning_rate': 0.00010091325108865779, 'epoch': 0.46}\n",
      "{'loss': 1.4791, 'grad_norm': 0.6171875, 'learning_rate': 0.00010090188871119393, 'epoch': 0.46}\n",
      "{'loss': 1.1367, 'grad_norm': 0.953125, 'learning_rate': 0.00010089052633373009, 'epoch': 0.46}\n",
      "{'loss': 1.1492, 'grad_norm': 0.451171875, 'learning_rate': 0.00010087916395626621, 'epoch': 0.46}\n",
      "{'loss': 1.2118, 'grad_norm': 0.48046875, 'learning_rate': 0.00010086780157880235, 'epoch': 0.46}\n",
      "{'loss': 1.1357, 'grad_norm': 0.6171875, 'learning_rate': 0.00010085643920133849, 'epoch': 0.46}\n",
      "{'loss': 1.3723, 'grad_norm': 0.474609375, 'learning_rate': 0.00010084507682387463, 'epoch': 0.46}\n",
      "{'loss': 1.1122, 'grad_norm': 0.703125, 'learning_rate': 0.00010083371444641077, 'epoch': 0.46}\n",
      "{'loss': 1.1985, 'grad_norm': 0.40625, 'learning_rate': 0.0001008223520689469, 'epoch': 0.46}\n",
      "{'loss': 1.2335, 'grad_norm': 0.6796875, 'learning_rate': 0.00010081098969148306, 'epoch': 0.46}\n",
      "{'loss': 1.0817, 'grad_norm': 1.0703125, 'learning_rate': 0.00010079962731401919, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.356, 'grad_norm': 0.478515625, 'learning_rate': 0.00010078826493655533, 'epoch': 0.46}\n",
      "{'loss': 1.2092, 'grad_norm': 0.68359375, 'learning_rate': 0.00010077690255909147, 'epoch': 0.46}\n",
      "{'loss': 1.2305, 'grad_norm': 0.482421875, 'learning_rate': 0.0001007655401816276, 'epoch': 0.46}\n",
      "{'loss': 1.2107, 'grad_norm': 0.8515625, 'learning_rate': 0.00010075417780416374, 'epoch': 0.46}\n",
      "{'loss': 1.0717, 'grad_norm': 0.8046875, 'learning_rate': 0.00010074281542669988, 'epoch': 0.46}\n",
      "{'loss': 1.3294, 'grad_norm': 0.58203125, 'learning_rate': 0.00010073145304923603, 'epoch': 0.46}\n",
      "{'loss': 1.1325, 'grad_norm': 0.62109375, 'learning_rate': 0.00010072009067177217, 'epoch': 0.46}\n",
      "{'loss': 1.3207, 'grad_norm': 0.4609375, 'learning_rate': 0.00010070872829430831, 'epoch': 0.46}\n",
      "{'loss': 1.1716, 'grad_norm': 0.65234375, 'learning_rate': 0.00010069736591684445, 'epoch': 0.46}\n",
      "{'loss': 1.0952, 'grad_norm': 0.66015625, 'learning_rate': 0.00010068600353938059, 'epoch': 0.46}\n",
      "{'loss': 1.3169, 'grad_norm': 0.4765625, 'learning_rate': 0.00010067464116191671, 'epoch': 0.46}\n",
      "{'loss': 1.1789, 'grad_norm': 0.73046875, 'learning_rate': 0.00010066327878445287, 'epoch': 0.46}\n",
      "{'loss': 1.0986, 'grad_norm': 0.44921875, 'learning_rate': 0.00010065191640698901, 'epoch': 0.46}\n",
      "{'loss': 1.2056, 'grad_norm': 0.78515625, 'learning_rate': 0.00010064055402952515, 'epoch': 0.46}\n",
      "{'loss': 1.0899, 'grad_norm': 0.36328125, 'learning_rate': 0.00010062919165206129, 'epoch': 0.46}\n",
      "{'loss': 1.3473, 'grad_norm': 0.5078125, 'learning_rate': 0.00010061782927459743, 'epoch': 0.46}\n",
      "{'loss': 1.1773, 'grad_norm': 0.56640625, 'learning_rate': 0.00010060646689713355, 'epoch': 0.46}\n",
      "{'loss': 1.2113, 'grad_norm': 0.484375, 'learning_rate': 0.00010059510451966969, 'epoch': 0.46}\n",
      "{'loss': 1.3539, 'grad_norm': 0.6171875, 'learning_rate': 0.00010058374214220585, 'epoch': 0.46}\n",
      "{'loss': 1.0697, 'grad_norm': 0.78125, 'learning_rate': 0.00010057237976474199, 'epoch': 0.46}\n",
      "{'loss': 1.4643, 'grad_norm': 0.53125, 'learning_rate': 0.00010056101738727813, 'epoch': 0.46}\n",
      "{'loss': 1.021, 'grad_norm': 1.0, 'learning_rate': 0.00010054965500981425, 'epoch': 0.46}\n",
      "{'loss': 1.2785, 'grad_norm': 0.44921875, 'learning_rate': 0.00010053829263235039, 'epoch': 0.46}\n",
      "{'loss': 1.1675, 'grad_norm': 0.60546875, 'learning_rate': 0.00010052693025488653, 'epoch': 0.46}\n",
      "{'loss': 1.0714, 'grad_norm': 0.97265625, 'learning_rate': 0.00010051556787742267, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3704, 'grad_norm': 0.671875, 'learning_rate': 0.00010050420549995883, 'epoch': 0.46}\n",
      "{'loss': 1.1878, 'grad_norm': 0.66015625, 'learning_rate': 0.00010049284312249496, 'epoch': 0.46}\n",
      "{'loss': 1.2621, 'grad_norm': 0.6484375, 'learning_rate': 0.0001004814807450311, 'epoch': 0.46}\n",
      "{'loss': 1.1007, 'grad_norm': 0.62109375, 'learning_rate': 0.00010047011836756723, 'epoch': 0.46}\n",
      "{'loss': 1.1282, 'grad_norm': 0.98828125, 'learning_rate': 0.00010045875599010337, 'epoch': 0.46}\n",
      "{'loss': 1.4017, 'grad_norm': 0.8359375, 'learning_rate': 0.00010044739361263951, 'epoch': 0.46}\n",
      "{'loss': 1.1847, 'grad_norm': 0.79296875, 'learning_rate': 0.00010043603123517565, 'epoch': 0.46}\n",
      "{'loss': 1.1761, 'grad_norm': 0.51171875, 'learning_rate': 0.0001004246688577118, 'epoch': 0.46}\n",
      "{'loss': 1.2546, 'grad_norm': 0.58203125, 'learning_rate': 0.00010041330648024793, 'epoch': 0.46}\n",
      "{'loss': 1.0892, 'grad_norm': 0.70703125, 'learning_rate': 0.00010040194410278407, 'epoch': 0.46}\n",
      "{'loss': 1.2617, 'grad_norm': 0.5390625, 'learning_rate': 0.00010039058172532021, 'epoch': 0.46}\n",
      "{'loss': 1.0238, 'grad_norm': 0.71875, 'learning_rate': 0.00010037921934785635, 'epoch': 0.46}\n",
      "{'loss': 1.0962, 'grad_norm': 0.53515625, 'learning_rate': 0.00010036785697039249, 'epoch': 0.46}\n",
      "{'loss': 1.2507, 'grad_norm': 0.55078125, 'learning_rate': 0.00010035649459292862, 'epoch': 0.46}\n",
      "{'loss': 1.1151, 'grad_norm': 1.0, 'learning_rate': 0.00010034513221546478, 'epoch': 0.46}\n",
      "{'loss': 1.2583, 'grad_norm': 0.4375, 'learning_rate': 0.00010033376983800091, 'epoch': 0.46}\n",
      "{'loss': 1.2377, 'grad_norm': 0.86328125, 'learning_rate': 0.00010032240746053705, 'epoch': 0.46}\n",
      "{'loss': 1.0987, 'grad_norm': 0.4765625, 'learning_rate': 0.00010031104508307319, 'epoch': 0.46}\n",
      "{'loss': 1.4175, 'grad_norm': 0.5234375, 'learning_rate': 0.00010029968270560933, 'epoch': 0.46}\n",
      "{'loss': 1.1015, 'grad_norm': 0.80859375, 'learning_rate': 0.00010028832032814545, 'epoch': 0.46}\n",
      "{'loss': 1.3637, 'grad_norm': 1.078125, 'learning_rate': 0.00010027695795068162, 'epoch': 0.46}\n",
      "{'loss': 1.2207, 'grad_norm': 0.44140625, 'learning_rate': 0.00010026559557321775, 'epoch': 0.46}\n",
      "{'loss': 1.0168, 'grad_norm': 0.400390625, 'learning_rate': 0.00010025423319575389, 'epoch': 0.46}\n",
      "{'loss': 1.2777, 'grad_norm': 0.5546875, 'learning_rate': 0.00010024287081829003, 'epoch': 0.46}\n",
      "{'loss': 1.0329, 'grad_norm': 1.390625, 'learning_rate': 0.00010023150844082617, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3131, 'grad_norm': 0.5078125, 'learning_rate': 0.00010022014606336229, 'epoch': 0.46}\n",
      "{'loss': 1.0966, 'grad_norm': 0.6484375, 'learning_rate': 0.00010020878368589843, 'epoch': 0.46}\n",
      "{'loss': 1.297, 'grad_norm': 0.427734375, 'learning_rate': 0.0001001974213084346, 'epoch': 0.46}\n",
      "{'loss': 1.2695, 'grad_norm': 0.6171875, 'learning_rate': 0.00010018605893097073, 'epoch': 0.46}\n",
      "{'loss': 1.1212, 'grad_norm': 1.015625, 'learning_rate': 0.00010017469655350687, 'epoch': 0.46}\n",
      "{'loss': 1.2059, 'grad_norm': 0.5546875, 'learning_rate': 0.000100163334176043, 'epoch': 0.46}\n",
      "{'loss': 1.1292, 'grad_norm': 1.1953125, 'learning_rate': 0.00010015197179857913, 'epoch': 0.46}\n",
      "{'loss': 1.2742, 'grad_norm': 0.62890625, 'learning_rate': 0.00010014060942111527, 'epoch': 0.46}\n",
      "{'loss': 1.2441, 'grad_norm': 0.6015625, 'learning_rate': 0.00010012924704365141, 'epoch': 0.46}\n",
      "{'loss': 1.0218, 'grad_norm': 0.8828125, 'learning_rate': 0.00010011788466618757, 'epoch': 0.46}\n",
      "{'loss': 1.2499, 'grad_norm': 0.55859375, 'learning_rate': 0.0001001065222887237, 'epoch': 0.46}\n",
      "{'loss': 1.1614, 'grad_norm': 1.2421875, 'learning_rate': 0.00010009515991125984, 'epoch': 0.46}\n",
      "{'loss': 1.4183, 'grad_norm': 0.486328125, 'learning_rate': 0.00010008379753379597, 'epoch': 0.46}\n",
      "{'loss': 1.153, 'grad_norm': 0.90234375, 'learning_rate': 0.00010007243515633211, 'epoch': 0.46}\n",
      "{'loss': 1.1558, 'grad_norm': 1.6796875, 'learning_rate': 0.00010006107277886825, 'epoch': 0.46}\n",
      "{'loss': 1.4218, 'grad_norm': 0.490234375, 'learning_rate': 0.00010004971040140439, 'epoch': 0.46}\n",
      "{'loss': 1.0983, 'grad_norm': 0.88671875, 'learning_rate': 0.00010003834802394054, 'epoch': 0.46}\n",
      "{'loss': 1.0927, 'grad_norm': 0.50390625, 'learning_rate': 0.00010002698564647668, 'epoch': 0.46}\n",
      "{'loss': 1.2453, 'grad_norm': 0.5546875, 'learning_rate': 0.00010001562326901281, 'epoch': 0.46}\n",
      "{'loss': 1.0849, 'grad_norm': 0.73828125, 'learning_rate': 0.00010000426089154895, 'epoch': 0.46}\n",
      "{'loss': 1.2985, 'grad_norm': 0.59765625, 'learning_rate': 9.999289851408509e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2294, 'grad_norm': 0.70703125, 'learning_rate': 9.998153613662124e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2298, 'grad_norm': 0.490234375, 'learning_rate': 9.997017375915736e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0629, 'grad_norm': 0.78515625, 'learning_rate': 9.99588113816935e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1791, 'grad_norm': 0.62109375, 'learning_rate': 9.994744900422965e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3322, 'grad_norm': 0.7734375, 'learning_rate': 9.993608662676579e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1737, 'grad_norm': 0.71484375, 'learning_rate': 9.992472424930193e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2092, 'grad_norm': 0.50390625, 'learning_rate': 9.991336187183807e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3203, 'grad_norm': 0.62109375, 'learning_rate': 9.99019994943742e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1352, 'grad_norm': 0.76953125, 'learning_rate': 9.989063711691034e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3153, 'grad_norm': 0.5234375, 'learning_rate': 9.987927473944648e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2069, 'grad_norm': 0.69140625, 'learning_rate': 9.986791236198263e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1079, 'grad_norm': 0.51953125, 'learning_rate': 9.985654998451877e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2131, 'grad_norm': 0.56640625, 'learning_rate': 9.984518760705491e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0107, 'grad_norm': 0.6171875, 'learning_rate': 9.983382522959105e-05, 'epoch': 0.46}\n",
      "{'loss': 1.377, 'grad_norm': 0.6328125, 'learning_rate': 9.982246285212718e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1653, 'grad_norm': 1.0703125, 'learning_rate': 9.981110047466332e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3661, 'grad_norm': 0.46484375, 'learning_rate': 9.979973809719946e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2167, 'grad_norm': 0.56640625, 'learning_rate': 9.978837571973561e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0254, 'grad_norm': 0.9765625, 'learning_rate': 9.977701334227174e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3021, 'grad_norm': 0.44140625, 'learning_rate': 9.976565096480787e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1427, 'grad_norm': 0.640625, 'learning_rate': 9.975428858734402e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1851, 'grad_norm': 0.494140625, 'learning_rate': 9.974292620988016e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1176, 'grad_norm': 0.546875, 'learning_rate': 9.97315638324163e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0446, 'grad_norm': 0.427734375, 'learning_rate': 9.972020145495244e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2672, 'grad_norm': 0.6640625, 'learning_rate': 9.970883907748858e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2002, 'grad_norm': 0.66015625, 'learning_rate': 9.969747670002471e-05, 'epoch': 0.46}\n",
      "{'loss': 1.213, 'grad_norm': 0.45703125, 'learning_rate': 9.968611432256085e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1319, 'grad_norm': 0.74609375, 'learning_rate': 9.9674751945097e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1483, 'grad_norm': 0.96484375, 'learning_rate': 9.966338956763314e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1952, 'grad_norm': 0.625, 'learning_rate': 9.965202719016928e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1995, 'grad_norm': 1.140625, 'learning_rate': 9.964066481270542e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2641, 'grad_norm': 0.42578125, 'learning_rate': 9.962930243524155e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3105, 'grad_norm': 0.7421875, 'learning_rate': 9.961794005777769e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0891, 'grad_norm': 0.83203125, 'learning_rate': 9.960657768031383e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4802, 'grad_norm': 0.5390625, 'learning_rate': 9.959521530284998e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1613, 'grad_norm': 0.9140625, 'learning_rate': 9.95838529253861e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2254, 'grad_norm': 0.5, 'learning_rate': 9.957249054792224e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1819, 'grad_norm': 0.609375, 'learning_rate': 9.95611281704584e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0905, 'grad_norm': 0.91015625, 'learning_rate': 9.954976579299453e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4257, 'grad_norm': 0.60546875, 'learning_rate': 9.953840341553067e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1, 'grad_norm': 0.8125, 'learning_rate': 9.952704103806681e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1601, 'grad_norm': 0.416015625, 'learning_rate': 9.951567866060295e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1486, 'grad_norm': 0.5, 'learning_rate': 9.950431628313908e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2063, 'grad_norm': 1.15625, 'learning_rate': 9.949295390567522e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4008, 'grad_norm': 0.703125, 'learning_rate': 9.948159152821137e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2154, 'grad_norm': 0.828125, 'learning_rate': 9.947022915074751e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1851, 'grad_norm': 0.66796875, 'learning_rate': 9.945886677328365e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1782, 'grad_norm': 1.03125, 'learning_rate': 9.944750439581979e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1003, 'grad_norm': 1.5703125, 'learning_rate': 9.943614201835593e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3393, 'grad_norm': 0.55859375, 'learning_rate': 9.942477964089206e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2445, 'grad_norm': 0.73046875, 'learning_rate': 9.94134172634282e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2393, 'grad_norm': 0.4609375, 'learning_rate': 9.940205488596435e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1554, 'grad_norm': 0.8515625, 'learning_rate': 9.939069250850048e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0943, 'grad_norm': 0.6015625, 'learning_rate': 9.937933013103661e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3935, 'grad_norm': 0.7890625, 'learning_rate': 9.936796775357277e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1968, 'grad_norm': 0.6796875, 'learning_rate': 9.93566053761089e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3094, 'grad_norm': 0.52734375, 'learning_rate': 9.934524299864504e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1925, 'grad_norm': 0.439453125, 'learning_rate': 9.933388062118118e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1379, 'grad_norm': 1.1015625, 'learning_rate': 9.932251824371732e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2749, 'grad_norm': 0.56640625, 'learning_rate': 9.931115586625346e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2003, 'grad_norm': 0.75, 'learning_rate': 9.92997934887896e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2471, 'grad_norm': 0.47265625, 'learning_rate': 9.928843111132574e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3177, 'grad_norm': 0.640625, 'learning_rate': 9.927706873386188e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0464, 'grad_norm': 1.234375, 'learning_rate': 9.926570635639802e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3127, 'grad_norm': 0.62109375, 'learning_rate': 9.925434397893416e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2802, 'grad_norm': 0.6953125, 'learning_rate': 9.92429816014703e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1652, 'grad_norm': 0.57421875, 'learning_rate': 9.923161922400643e-05, 'epoch': 0.47}\n",
      "{'loss': 1.374, 'grad_norm': 0.75, 'learning_rate': 9.922025684654259e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1124, 'grad_norm': 1.375, 'learning_rate': 9.920889446907872e-05, 'epoch': 0.47}\n",
      "{'loss': 1.5068, 'grad_norm': 0.443359375, 'learning_rate': 9.919753209161485e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1651, 'grad_norm': 0.62109375, 'learning_rate': 9.918616971415099e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2272, 'grad_norm': 0.53515625, 'learning_rate': 9.917480733668714e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2417, 'grad_norm': 0.7734375, 'learning_rate': 9.916344495922327e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0261, 'grad_norm': 0.8671875, 'learning_rate': 9.915208258175941e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3259, 'grad_norm': 0.46875, 'learning_rate': 9.914072020429555e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1403, 'grad_norm': 0.86328125, 'learning_rate': 9.912935782683169e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2247, 'grad_norm': 0.56640625, 'learning_rate': 9.911799544936783e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1405, 'grad_norm': 0.66796875, 'learning_rate': 9.910663307190396e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1102, 'grad_norm': 0.365234375, 'learning_rate': 9.909527069444012e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5003, 'grad_norm': 0.74609375, 'learning_rate': 9.908390831697625e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2304, 'grad_norm': 0.6953125, 'learning_rate': 9.907254593951239e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1649, 'grad_norm': 0.466796875, 'learning_rate': 9.906118356204853e-05, 'epoch': 0.47}\n",
      "{'loss': 1.199, 'grad_norm': 0.90234375, 'learning_rate': 9.904982118458467e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0973, 'grad_norm': 1.0703125, 'learning_rate': 9.90384588071208e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3342, 'grad_norm': 0.546875, 'learning_rate': 9.902709642965696e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1102, 'grad_norm': 1.484375, 'learning_rate': 9.90157340521931e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2222, 'grad_norm': 0.40625, 'learning_rate': 9.900437167472922e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2988, 'grad_norm': 0.93359375, 'learning_rate': 9.899300929726536e-05, 'epoch': 0.47}\n",
      "{'loss': 0.9797, 'grad_norm': 0.625, 'learning_rate': 9.898164691980151e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4054, 'grad_norm': 0.62109375, 'learning_rate': 9.897028454233765e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1667, 'grad_norm': 0.69921875, 'learning_rate': 9.895892216487378e-05, 'epoch': 0.47}\n",
      "{'loss': 1.152, 'grad_norm': 0.68359375, 'learning_rate': 9.894755978740992e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3102, 'grad_norm': 0.54296875, 'learning_rate': 9.893619740994606e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0577, 'grad_norm': 1.0703125, 'learning_rate': 9.89248350324822e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3166, 'grad_norm': 0.546875, 'learning_rate': 9.891347265501833e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1509, 'grad_norm': 0.71484375, 'learning_rate': 9.890211027755449e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1526, 'grad_norm': 0.390625, 'learning_rate': 9.889074790009062e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3068, 'grad_norm': 0.5625, 'learning_rate': 9.887938552262676e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1019, 'grad_norm': 1.5, 'learning_rate': 9.88680231451629e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2959, 'grad_norm': 0.6484375, 'learning_rate': 9.885666076769904e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1254, 'grad_norm': 0.77734375, 'learning_rate': 9.884529839023518e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0967, 'grad_norm': 0.515625, 'learning_rate': 9.883393601277133e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2796, 'grad_norm': 0.76953125, 'learning_rate': 9.882257363530746e-05, 'epoch': 0.47}\n",
      "{'loss': 0.9323, 'grad_norm': 0.41796875, 'learning_rate': 9.881121125784359e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2896, 'grad_norm': 0.50390625, 'learning_rate': 9.879984888037973e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1473, 'grad_norm': 0.78515625, 'learning_rate': 9.878848650291588e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1946, 'grad_norm': 0.546875, 'learning_rate': 9.877712412545202e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2292, 'grad_norm': 0.59375, 'learning_rate': 9.876576174798815e-05, 'epoch': 0.47}\n",
      "{'loss': 0.9123, 'grad_norm': 0.359375, 'learning_rate': 9.875439937052429e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2984, 'grad_norm': 0.6171875, 'learning_rate': 9.874303699306043e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1592, 'grad_norm': 0.703125, 'learning_rate': 9.873167461559657e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1942, 'grad_norm': 0.51171875, 'learning_rate': 9.87203122381327e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3726, 'grad_norm': 0.494140625, 'learning_rate': 9.870894986066886e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1176, 'grad_norm': 0.92578125, 'learning_rate': 9.8697587483205e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2482, 'grad_norm': 0.4921875, 'learning_rate': 9.868622510574113e-05, 'epoch': 0.47}\n",
      "{'loss': 1.204, 'grad_norm': 0.5703125, 'learning_rate': 9.867486272827727e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2384, 'grad_norm': 0.431640625, 'learning_rate': 9.866350035081341e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3348, 'grad_norm': 0.6171875, 'learning_rate': 9.865213797334955e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1653, 'grad_norm': 0.734375, 'learning_rate': 9.86407755958857e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4302, 'grad_norm': 0.578125, 'learning_rate': 9.862941321842184e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0345, 'grad_norm': 0.921875, 'learning_rate': 9.861805084095796e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0757, 'grad_norm': 0.53515625, 'learning_rate': 9.86066884634941e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2322, 'grad_norm': 0.68359375, 'learning_rate': 9.859532608603025e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1963, 'grad_norm': 0.765625, 'learning_rate': 9.858396370856639e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4498, 'grad_norm': 0.5234375, 'learning_rate': 9.857260133110252e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1843, 'grad_norm': 0.6171875, 'learning_rate': 9.856123895363866e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2672, 'grad_norm': 0.443359375, 'learning_rate': 9.85498765761748e-05, 'epoch': 0.47}\n",
      "{'loss': 1.158, 'grad_norm': 0.65625, 'learning_rate': 9.853851419871094e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2126, 'grad_norm': 0.83203125, 'learning_rate': 9.852715182124709e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3172, 'grad_norm': 0.58984375, 'learning_rate': 9.851578944378323e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2866, 'grad_norm': 0.61328125, 'learning_rate': 9.850442706631937e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2308, 'grad_norm': 0.51171875, 'learning_rate': 9.84930646888555e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3052, 'grad_norm': 0.498046875, 'learning_rate': 9.848170231139164e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0496, 'grad_norm': 0.51953125, 'learning_rate': 9.847033993392778e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2451, 'grad_norm': 0.73046875, 'learning_rate': 9.845897755646392e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1631, 'grad_norm': 0.77734375, 'learning_rate': 9.844761517900007e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1515, 'grad_norm': 0.515625, 'learning_rate': 9.84362528015362e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3255, 'grad_norm': 0.5859375, 'learning_rate': 9.842489042407233e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0918, 'grad_norm': 0.79296875, 'learning_rate': 9.841352804660847e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2743, 'grad_norm': 0.53515625, 'learning_rate': 9.840216566914462e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2732, 'grad_norm': 0.76953125, 'learning_rate': 9.839080329168076e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1556, 'grad_norm': 0.49609375, 'learning_rate': 9.83794409142169e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2175, 'grad_norm': 0.64453125, 'learning_rate': 9.836807853675303e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1297, 'grad_norm': 0.95703125, 'learning_rate': 9.835671615928917e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3141, 'grad_norm': 0.52734375, 'learning_rate': 9.834535378182531e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1816, 'grad_norm': 0.625, 'learning_rate': 9.833399140436146e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1146, 'grad_norm': 0.447265625, 'learning_rate': 9.83226290268976e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1406, 'grad_norm': 0.55859375, 'learning_rate': 9.831126664943374e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0218, 'grad_norm': 0.5703125, 'learning_rate': 9.829990427196987e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3214, 'grad_norm': 0.6640625, 'learning_rate': 9.828854189450601e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1186, 'grad_norm': 0.6796875, 'learning_rate': 9.827717951704215e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2285, 'grad_norm': 0.66796875, 'learning_rate': 9.826581713957829e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1585, 'grad_norm': 0.5234375, 'learning_rate': 9.825445476211444e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0283, 'grad_norm': 0.83984375, 'learning_rate': 9.824309238465058e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2039, 'grad_norm': 0.7578125, 'learning_rate': 9.82317300071867e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2778, 'grad_norm': 0.72265625, 'learning_rate': 9.822036762972284e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2484, 'grad_norm': 0.70703125, 'learning_rate': 9.820900525225899e-05, 'epoch': 0.47}\n",
      "{'loss': 1.097, 'grad_norm': 0.52734375, 'learning_rate': 9.819764287479513e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0851, 'grad_norm': 0.98046875, 'learning_rate': 9.818628049733127e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3508, 'grad_norm': 0.63671875, 'learning_rate': 9.81749181198674e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2071, 'grad_norm': 1.03125, 'learning_rate': 9.816355574240354e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1709, 'grad_norm': 0.546875, 'learning_rate': 9.815219336493968e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1307, 'grad_norm': 0.5234375, 'learning_rate': 9.814083098747583e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0817, 'grad_norm': 0.86328125, 'learning_rate': 9.812946861001197e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2654, 'grad_norm': 0.609375, 'learning_rate': 9.81181062325481e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2393, 'grad_norm': 0.59765625, 'learning_rate': 9.810674385508424e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1916, 'grad_norm': 0.38671875, 'learning_rate': 9.809538147762038e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1682, 'grad_norm': 0.8046875, 'learning_rate': 9.808401910015652e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0019, 'grad_norm': 0.7421875, 'learning_rate': 9.807265672269266e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3822, 'grad_norm': 0.49609375, 'learning_rate': 9.806129434522881e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1672, 'grad_norm': 0.796875, 'learning_rate': 9.804993196776495e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1257, 'grad_norm': 0.41015625, 'learning_rate': 9.803856959030107e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2955, 'grad_norm': 0.5625, 'learning_rate': 9.802720721283722e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0141, 'grad_norm': 0.71484375, 'learning_rate': 9.801584483537336e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3108, 'grad_norm': 0.5, 'learning_rate': 9.80044824579095e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1242, 'grad_norm': 0.6171875, 'learning_rate': 9.799312008044564e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2756, 'grad_norm': 0.52734375, 'learning_rate': 9.798175770298177e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2019, 'grad_norm': 0.62890625, 'learning_rate': 9.797039532551791e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0906, 'grad_norm': 0.78125, 'learning_rate': 9.795903294805405e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4137, 'grad_norm': 0.546875, 'learning_rate': 9.79476705705902e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0693, 'grad_norm': 0.76953125, 'learning_rate': 9.793630819312634e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2202, 'grad_norm': 0.494140625, 'learning_rate': 9.792494581566248e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2013, 'grad_norm': 0.53515625, 'learning_rate': 9.791358343819861e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1267, 'grad_norm': 0.6484375, 'learning_rate': 9.790222106073475e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2867, 'grad_norm': 0.63671875, 'learning_rate': 9.789085868327089e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2099, 'grad_norm': 0.78515625, 'learning_rate': 9.787949630580703e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2152, 'grad_norm': 0.546875, 'learning_rate': 9.786813392834318e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2337, 'grad_norm': 0.515625, 'learning_rate': 9.785677155087932e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1539, 'grad_norm': 1.0, 'learning_rate': 9.784540917341544e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4174, 'grad_norm': 0.5625, 'learning_rate': 9.78340467959516e-05, 'epoch': 0.47}\n",
      "{'loss': 1.152, 'grad_norm': 0.6640625, 'learning_rate': 9.782268441848773e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1222, 'grad_norm': 0.5390625, 'learning_rate': 9.781132204102387e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3016, 'grad_norm': 0.55078125, 'learning_rate': 9.779995966356001e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0274, 'grad_norm': 0.71875, 'learning_rate': 9.778859728609614e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2808, 'grad_norm': 0.625, 'learning_rate': 9.777723490863228e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1645, 'grad_norm': 0.7578125, 'learning_rate': 9.776587253116842e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2113, 'grad_norm': 0.56640625, 'learning_rate': 9.775451015370457e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2351, 'grad_norm': 0.53125, 'learning_rate': 9.774314777624071e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1887, 'grad_norm': 0.62109375, 'learning_rate': 9.773178539877685e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2067, 'grad_norm': 0.48828125, 'learning_rate': 9.772042302131299e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1445, 'grad_norm': 0.80859375, 'learning_rate': 9.770906064384912e-05, 'epoch': 0.47}\n",
      "{'loss': 0.9865, 'grad_norm': 0.55078125, 'learning_rate': 9.769769826638526e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3048, 'grad_norm': 0.671875, 'learning_rate': 9.76863358889214e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0639, 'grad_norm': 1.0078125, 'learning_rate': 9.767497351145755e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4431, 'grad_norm': 0.53515625, 'learning_rate': 9.766361113399369e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1499, 'grad_norm': 0.5859375, 'learning_rate': 9.765224875652981e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2132, 'grad_norm': 0.53125, 'learning_rate': 9.764088637906596e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3744, 'grad_norm': 0.890625, 'learning_rate': 9.76295240016021e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0722, 'grad_norm': 1.2265625, 'learning_rate': 9.761816162413824e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3284, 'grad_norm': 0.5234375, 'learning_rate': 9.760679924667438e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1555, 'grad_norm': 0.62109375, 'learning_rate': 9.759543686921052e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2312, 'grad_norm': 0.3671875, 'learning_rate': 9.758407449174665e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2176, 'grad_norm': 0.578125, 'learning_rate': 9.757271211428279e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1123, 'grad_norm': 0.7421875, 'learning_rate': 9.756134973681894e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4862, 'grad_norm': 0.51953125, 'learning_rate': 9.754998735935508e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2112, 'grad_norm': 0.73828125, 'learning_rate': 9.753862498189122e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2635, 'grad_norm': 0.55859375, 'learning_rate': 9.752726260442736e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3121, 'grad_norm': 0.640625, 'learning_rate': 9.75159002269635e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0344, 'grad_norm': 1.546875, 'learning_rate': 9.750453784949963e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2193, 'grad_norm': 0.50390625, 'learning_rate': 9.749317547203577e-05, 'epoch': 0.47}\n",
      "{'loss': 1.184, 'grad_norm': 0.72265625, 'learning_rate': 9.748181309457192e-05, 'epoch': 0.47}\n",
      "{'loss': 1.188, 'grad_norm': 0.45703125, 'learning_rate': 9.747045071710806e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2755, 'grad_norm': 0.50390625, 'learning_rate': 9.745908833964418e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0965, 'grad_norm': 1.1171875, 'learning_rate': 9.744772596218033e-05, 'epoch': 0.47}\n",
      "{'loss': 1.3706, 'grad_norm': 0.431640625, 'learning_rate': 9.743636358471647e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1241, 'grad_norm': 0.65234375, 'learning_rate': 9.742500120725261e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2224, 'grad_norm': 0.486328125, 'learning_rate': 9.741363882978875e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2128, 'grad_norm': 0.65625, 'learning_rate': 9.740227645232489e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1089, 'grad_norm': 0.93359375, 'learning_rate': 9.739091407486102e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3003, 'grad_norm': 0.50390625, 'learning_rate': 9.737955169739716e-05, 'epoch': 0.47}\n",
      "{'loss': 1.2056, 'grad_norm': 0.50390625, 'learning_rate': 9.736818931993331e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1554, 'grad_norm': 0.5234375, 'learning_rate': 9.735682694246945e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4076, 'grad_norm': 0.6484375, 'learning_rate': 9.734546456500559e-05, 'epoch': 0.48}\n",
      "{'loss': 1.133, 'grad_norm': 1.5703125, 'learning_rate': 9.733410218754173e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3292, 'grad_norm': 1.2421875, 'learning_rate': 9.732273981007786e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1799, 'grad_norm': 0.7109375, 'learning_rate': 9.7311377432614e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2293, 'grad_norm': 0.58203125, 'learning_rate': 9.730001505515014e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1394, 'grad_norm': 0.5703125, 'learning_rate': 9.728865267768629e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1676, 'grad_norm': 0.90234375, 'learning_rate': 9.727729030022243e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3669, 'grad_norm': 0.671875, 'learning_rate': 9.726592792275855e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1699, 'grad_norm': 0.59765625, 'learning_rate': 9.72545655452947e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2401, 'grad_norm': 0.4609375, 'learning_rate': 9.724320316783084e-05, 'epoch': 0.48}\n",
      "{'loss': 1.281, 'grad_norm': 0.8046875, 'learning_rate': 9.723184079036698e-05, 'epoch': 0.48}\n",
      "{'loss': 0.9678, 'grad_norm': 0.66015625, 'learning_rate': 9.722047841290312e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2163, 'grad_norm': 0.4453125, 'learning_rate': 9.720911603543926e-05, 'epoch': 0.48}\n",
      "{'loss': 1.239, 'grad_norm': 0.609375, 'learning_rate': 9.71977536579754e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3464, 'grad_norm': 0.60546875, 'learning_rate': 9.718639128051153e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1377, 'grad_norm': 0.51171875, 'learning_rate': 9.717502890304768e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0086, 'grad_norm': 0.9609375, 'learning_rate': 9.716366652558382e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2423, 'grad_norm': 0.6171875, 'learning_rate': 9.715230414811996e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1633, 'grad_norm': 0.75390625, 'learning_rate': 9.71409417706561e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1834, 'grad_norm': 0.58984375, 'learning_rate': 9.712957939319224e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3063, 'grad_norm': 0.70703125, 'learning_rate': 9.711821701572837e-05, 'epoch': 0.48}\n",
      "{'loss': 1.03, 'grad_norm': 0.54296875, 'learning_rate': 9.710685463826451e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3256, 'grad_norm': 0.55859375, 'learning_rate': 9.709549226080066e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1583, 'grad_norm': 0.69921875, 'learning_rate': 9.70841298833368e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2042, 'grad_norm': 0.388671875, 'learning_rate': 9.707276750587292e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2546, 'grad_norm': 0.4609375, 'learning_rate': 9.706140512840908e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1577, 'grad_norm': 0.7734375, 'learning_rate': 9.705004275094521e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2032, 'grad_norm': 0.52734375, 'learning_rate': 9.703868037348135e-05, 'epoch': 0.48}\n",
      "{'loss': 1.208, 'grad_norm': 0.65625, 'learning_rate': 9.702731799601749e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1656, 'grad_norm': 0.478515625, 'learning_rate': 9.701595561855363e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2395, 'grad_norm': 0.9921875, 'learning_rate': 9.700459324108977e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0517, 'grad_norm': 0.57421875, 'learning_rate': 9.69932308636259e-05, 'epoch': 0.48}\n",
      "{'loss': 1.39, 'grad_norm': 0.74609375, 'learning_rate': 9.698186848616205e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1725, 'grad_norm': 0.6953125, 'learning_rate': 9.697050610869819e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3133, 'grad_norm': 0.5078125, 'learning_rate': 9.695914373123433e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1883, 'grad_norm': 0.69921875, 'learning_rate': 9.694778135377047e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1081, 'grad_norm': 0.7265625, 'learning_rate': 9.69364189763066e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3483, 'grad_norm': 0.47265625, 'learning_rate': 9.692505659884274e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1356, 'grad_norm': 0.96484375, 'learning_rate': 9.691369422137888e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1964, 'grad_norm': 0.515625, 'learning_rate': 9.690233184391503e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2358, 'grad_norm': 0.546875, 'learning_rate': 9.689096946645117e-05, 'epoch': 0.48}\n",
      "{'loss': 0.9977, 'grad_norm': 0.455078125, 'learning_rate': 9.68796070889873e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1997, 'grad_norm': 0.443359375, 'learning_rate': 9.686824471152345e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2829, 'grad_norm': 0.5078125, 'learning_rate': 9.685688233405958e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2798, 'grad_norm': 0.482421875, 'learning_rate': 9.684551995659572e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2545, 'grad_norm': 0.60546875, 'learning_rate': 9.683415757913186e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0678, 'grad_norm': 0.8359375, 'learning_rate': 9.6822795201668e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3323, 'grad_norm': 0.609375, 'learning_rate': 9.681143282420414e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1302, 'grad_norm': 0.8828125, 'learning_rate': 9.680007044674027e-05, 'epoch': 0.48}\n",
      "{'loss': 1.247, 'grad_norm': 0.46875, 'learning_rate': 9.678870806927642e-05, 'epoch': 0.48}\n",
      "{'loss': 1.303, 'grad_norm': 0.53125, 'learning_rate': 9.677734569181256e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0911, 'grad_norm': 1.0234375, 'learning_rate': 9.67659833143487e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2859, 'grad_norm': 0.6640625, 'learning_rate': 9.675462093688484e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1864, 'grad_norm': 0.90234375, 'learning_rate': 9.674325855942098e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1522, 'grad_norm': 0.51953125, 'learning_rate': 9.673189618195711e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2389, 'grad_norm': 0.84765625, 'learning_rate': 9.672053380449325e-05, 'epoch': 0.48}\n",
      "{'loss': 1.13, 'grad_norm': 0.5078125, 'learning_rate': 9.67091714270294e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4651, 'grad_norm': 0.4921875, 'learning_rate': 9.669780904956554e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1476, 'grad_norm': 0.7265625, 'learning_rate': 9.668644667210167e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1754, 'grad_norm': 0.56640625, 'learning_rate': 9.667508429463782e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1895, 'grad_norm': 0.5859375, 'learning_rate': 9.666372191717395e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0239, 'grad_norm': 0.56640625, 'learning_rate': 9.665235953971009e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4094, 'grad_norm': 0.55859375, 'learning_rate': 9.664099716224624e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1539, 'grad_norm': 0.82421875, 'learning_rate': 9.662963478478237e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1267, 'grad_norm': 0.47265625, 'learning_rate': 9.66182724073185e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2828, 'grad_norm': 0.5859375, 'learning_rate': 9.660691002985464e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0673, 'grad_norm': 0.8046875, 'learning_rate': 9.65955476523908e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4047, 'grad_norm': 0.68359375, 'learning_rate': 9.658418527492693e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2127, 'grad_norm': 0.9765625, 'learning_rate': 9.657282289746307e-05, 'epoch': 0.48}\n",
      "{'loss': 1.36, 'grad_norm': 0.4375, 'learning_rate': 9.656146051999921e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2709, 'grad_norm': 0.51953125, 'learning_rate': 9.655009814253535e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0902, 'grad_norm': 0.921875, 'learning_rate': 9.653873576507148e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4218, 'grad_norm': 0.5078125, 'learning_rate': 9.652737338760762e-05, 'epoch': 0.48}\n",
      "{'loss': 1.145, 'grad_norm': 0.88671875, 'learning_rate': 9.651601101014377e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2016, 'grad_norm': 0.388671875, 'learning_rate': 9.650464863267991e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1261, 'grad_norm': 0.97265625, 'learning_rate': 9.649328625521604e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1498, 'grad_norm': 0.890625, 'learning_rate': 9.648192387775219e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3336, 'grad_norm': 0.53125, 'learning_rate': 9.647056150028833e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1921, 'grad_norm': 1.125, 'learning_rate': 9.645919912282446e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2313, 'grad_norm': 0.431640625, 'learning_rate': 9.644783674536061e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2731, 'grad_norm': 0.5859375, 'learning_rate': 9.643647436789674e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0909, 'grad_norm': 0.6484375, 'learning_rate': 9.642511199043288e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4339, 'grad_norm': 1.265625, 'learning_rate': 9.641374961296901e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1823, 'grad_norm': 0.60546875, 'learning_rate': 9.640238723550517e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1002, 'grad_norm': 0.46484375, 'learning_rate': 9.63910248580413e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1884, 'grad_norm': 0.55859375, 'learning_rate': 9.637966248057744e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0813, 'grad_norm': 0.52734375, 'learning_rate': 9.636830010311358e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3932, 'grad_norm': 0.53515625, 'learning_rate': 9.635693772564972e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0893, 'grad_norm': 0.97265625, 'learning_rate': 9.634557534818586e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1788, 'grad_norm': 0.65625, 'learning_rate': 9.6334212970722e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2388, 'grad_norm': 0.5859375, 'learning_rate': 9.632285059325814e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0327, 'grad_norm': 0.703125, 'learning_rate': 9.631148821579428e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4014, 'grad_norm': 0.53515625, 'learning_rate': 9.630012583833041e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2792, 'grad_norm': 0.85546875, 'learning_rate': 9.628876346086656e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0772, 'grad_norm': 0.458984375, 'learning_rate': 9.62774010834027e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1892, 'grad_norm': 0.64453125, 'learning_rate': 9.626603870593883e-05, 'epoch': 0.48}\n",
      "{'loss': 1.019, 'grad_norm': 1.4375, 'learning_rate': 9.625467632847499e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.252, 'grad_norm': 0.5234375, 'learning_rate': 9.624331395101111e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2379, 'grad_norm': 0.6328125, 'learning_rate': 9.623195157354725e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1392, 'grad_norm': 0.47265625, 'learning_rate': 9.622058919608339e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1325, 'grad_norm': 0.76171875, 'learning_rate': 9.620922681861954e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0885, 'grad_norm': 0.83984375, 'learning_rate': 9.619786444115567e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2375, 'grad_norm': 0.375, 'learning_rate': 9.618650206369181e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1519, 'grad_norm': 0.69921875, 'learning_rate': 9.617513968622795e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2161, 'grad_norm': 0.388671875, 'learning_rate': 9.616377730876409e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2347, 'grad_norm': 0.8046875, 'learning_rate': 9.615241493130023e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0588, 'grad_norm': 0.82421875, 'learning_rate': 9.614105255383636e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3392, 'grad_norm': 0.61328125, 'learning_rate': 9.612969017637252e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1778, 'grad_norm': 0.91015625, 'learning_rate': 9.611832779890865e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2342, 'grad_norm': 0.4609375, 'learning_rate': 9.610696542144478e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3435, 'grad_norm': 0.5546875, 'learning_rate': 9.609560304398093e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1867, 'grad_norm': 0.62890625, 'learning_rate': 9.608424066651707e-05, 'epoch': 0.48}\n",
      "{'loss': 1.295, 'grad_norm': 0.61328125, 'learning_rate': 9.60728782890532e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1014, 'grad_norm': 1.0625, 'learning_rate': 9.606151591158936e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2, 'grad_norm': 0.52734375, 'learning_rate': 9.605015353412548e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3013, 'grad_norm': 0.60546875, 'learning_rate': 9.603879115666162e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1863, 'grad_norm': 0.93359375, 'learning_rate': 9.602742877919776e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3162, 'grad_norm': 0.5, 'learning_rate': 9.601606640173391e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0854, 'grad_norm': 0.71875, 'learning_rate': 9.600470402427005e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2456, 'grad_norm': 0.5625, 'learning_rate': 9.599334164680618e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2313, 'grad_norm': 0.65625, 'learning_rate': 9.598197926934232e-05, 'epoch': 0.48}\n",
      "{'loss': 0.9804, 'grad_norm': 0.88671875, 'learning_rate': 9.597061689187846e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3357, 'grad_norm': 0.66796875, 'learning_rate': 9.59592545144146e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1572, 'grad_norm': 0.60546875, 'learning_rate': 9.594789213695075e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2151, 'grad_norm': 0.52734375, 'learning_rate': 9.593652975948689e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1947, 'grad_norm': 0.7734375, 'learning_rate': 9.592516738202302e-05, 'epoch': 0.48}\n",
      "{'loss': 0.9879, 'grad_norm': 0.74609375, 'learning_rate': 9.591380500455915e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2945, 'grad_norm': 0.5078125, 'learning_rate': 9.59024426270953e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1558, 'grad_norm': 1.0234375, 'learning_rate': 9.589108024963144e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2028, 'grad_norm': 0.4765625, 'learning_rate': 9.587971787216758e-05, 'epoch': 0.48}\n",
      "{'loss': 1.19, 'grad_norm': 0.60546875, 'learning_rate': 9.586835549470373e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0574, 'grad_norm': 1.5234375, 'learning_rate': 9.585699311723985e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3183, 'grad_norm': 0.75, 'learning_rate': 9.584563073977599e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2548, 'grad_norm': 0.734375, 'learning_rate': 9.583426836231213e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1401, 'grad_norm': 0.447265625, 'learning_rate': 9.582290598484828e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2225, 'grad_norm': 0.5703125, 'learning_rate': 9.581154360738442e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0642, 'grad_norm': 0.890625, 'learning_rate': 9.580018122992055e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4173, 'grad_norm': 0.466796875, 'learning_rate': 9.578881885245669e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1702, 'grad_norm': 0.71484375, 'learning_rate': 9.577745647499283e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1599, 'grad_norm': 0.55078125, 'learning_rate': 9.576609409752897e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2515, 'grad_norm': 0.6171875, 'learning_rate': 9.575473172006512e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0671, 'grad_norm': 0.90234375, 'learning_rate': 9.574336934260126e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3405, 'grad_norm': 0.546875, 'learning_rate': 9.57320069651374e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0502, 'grad_norm': 1.3828125, 'learning_rate': 9.572064458767352e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2157, 'grad_norm': 0.578125, 'learning_rate': 9.570928221020967e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2473, 'grad_norm': 0.546875, 'learning_rate': 9.569791983274581e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0781, 'grad_norm': 0.85546875, 'learning_rate': 9.568655745528195e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1666, 'grad_norm': 0.54296875, 'learning_rate': 9.56751950778181e-05, 'epoch': 0.48}\n",
      "{'loss': 1.209, 'grad_norm': 0.70703125, 'learning_rate': 9.566383270035422e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2322, 'grad_norm': 0.400390625, 'learning_rate': 9.565247032289036e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3089, 'grad_norm': 0.6328125, 'learning_rate': 9.56411079454265e-05, 'epoch': 0.48}\n",
      "{'loss': 0.9658, 'grad_norm': 0.439453125, 'learning_rate': 9.562974556796265e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2791, 'grad_norm': 0.482421875, 'learning_rate': 9.561838319049879e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1403, 'grad_norm': 0.609375, 'learning_rate': 9.560702081303492e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2092, 'grad_norm': 0.51171875, 'learning_rate': 9.559565843557106e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3179, 'grad_norm': 0.6953125, 'learning_rate': 9.55842960581072e-05, 'epoch': 0.48}\n",
      "{'loss': 0.9814, 'grad_norm': 0.4375, 'learning_rate': 9.557293368064334e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3117, 'grad_norm': 0.51953125, 'learning_rate': 9.556157130317949e-05, 'epoch': 0.48}\n",
      "{'loss': 1.119, 'grad_norm': 1.1875, 'learning_rate': 9.555020892571563e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1069, 'grad_norm': 0.431640625, 'learning_rate': 9.553884654825177e-05, 'epoch': 0.48}\n",
      "{'loss': 1.279, 'grad_norm': 0.79296875, 'learning_rate': 9.552748417078789e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0169, 'grad_norm': 0.6953125, 'learning_rate': 9.551612179332404e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4228, 'grad_norm': 0.52734375, 'learning_rate': 9.550475941586018e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2086, 'grad_norm': 0.65234375, 'learning_rate': 9.549339703839632e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2162, 'grad_norm': 0.46875, 'learning_rate': 9.548203466093247e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1722, 'grad_norm': 0.640625, 'learning_rate': 9.547067228346859e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0098, 'grad_norm': 0.94140625, 'learning_rate': 9.545930990600473e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4311, 'grad_norm': 0.447265625, 'learning_rate': 9.544794752854088e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2317, 'grad_norm': 0.75, 'learning_rate': 9.543658515107702e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2151, 'grad_norm': 0.484375, 'learning_rate': 9.542522277361316e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2772, 'grad_norm': 0.59765625, 'learning_rate': 9.54138603961493e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0285, 'grad_norm': 0.83203125, 'learning_rate': 9.540249801868543e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2847, 'grad_norm': 0.53515625, 'learning_rate': 9.539113564122157e-05, 'epoch': 0.48}\n",
      "{'loss': 1.174, 'grad_norm': 0.73046875, 'learning_rate': 9.537977326375771e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0902, 'grad_norm': 0.451171875, 'learning_rate': 9.536841088629386e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1928, 'grad_norm': 0.59765625, 'learning_rate': 9.535704850883e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0403, 'grad_norm': 0.70703125, 'learning_rate': 9.534568613136614e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3305, 'grad_norm': 0.7578125, 'learning_rate': 9.533432375390226e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2615, 'grad_norm': 0.8046875, 'learning_rate': 9.532296137643841e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2575, 'grad_norm': 0.443359375, 'learning_rate': 9.531159899897455e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2473, 'grad_norm': 0.48828125, 'learning_rate': 9.530023662151069e-05, 'epoch': 0.48}\n",
      "{'loss': 1.025, 'grad_norm': 0.6796875, 'learning_rate': 9.528887424404684e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3203, 'grad_norm': 0.48046875, 'learning_rate': 9.527751186658296e-05, 'epoch': 0.48}\n",
      "{'loss': 1.279, 'grad_norm': 1.2109375, 'learning_rate': 9.52661494891191e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1791, 'grad_norm': 0.57421875, 'learning_rate': 9.525478711165525e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2838, 'grad_norm': 0.83203125, 'learning_rate': 9.524342473419139e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0451, 'grad_norm': 1.0703125, 'learning_rate': 9.523206235672753e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3647, 'grad_norm': 0.53125, 'learning_rate': 9.522069997926367e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1809, 'grad_norm': 1.5234375, 'learning_rate': 9.52093376017998e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2339, 'grad_norm': 0.765625, 'learning_rate': 9.519797522433594e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2139, 'grad_norm': 0.6171875, 'learning_rate': 9.518661284687208e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0339, 'grad_norm': 1.2890625, 'learning_rate': 9.517525046940823e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4129, 'grad_norm': 0.5859375, 'learning_rate': 9.516388809194437e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1386, 'grad_norm': 0.6953125, 'learning_rate': 9.51525257144805e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1669, 'grad_norm': 0.35546875, 'learning_rate': 9.514116333701663e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3616, 'grad_norm': 0.54296875, 'learning_rate': 9.512980095955278e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2219, 'grad_norm': 0.90625, 'learning_rate': 9.511843858208892e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2658, 'grad_norm': 0.546875, 'learning_rate': 9.510707620462506e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1449, 'grad_norm': 0.78515625, 'learning_rate': 9.509571382716121e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1565, 'grad_norm': 0.4921875, 'learning_rate': 9.508435144969733e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2988, 'grad_norm': 0.80859375, 'learning_rate': 9.507298907223347e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0435, 'grad_norm': 0.859375, 'learning_rate': 9.506162669476962e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3353, 'grad_norm': 0.51171875, 'learning_rate': 9.505026431730576e-05, 'epoch': 0.49}\n",
      "{'loss': 1.124, 'grad_norm': 0.59375, 'learning_rate': 9.50389019398419e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1195, 'grad_norm': 0.53515625, 'learning_rate': 9.502753956237804e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1377, 'grad_norm': 0.59375, 'learning_rate': 9.501617718491417e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0826, 'grad_norm': 0.61328125, 'learning_rate': 9.500481480745031e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2722, 'grad_norm': 0.4609375, 'learning_rate': 9.499345242998645e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1165, 'grad_norm': 1.0390625, 'learning_rate': 9.49820900525226e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1712, 'grad_norm': 0.54296875, 'learning_rate': 9.497072767505874e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2443, 'grad_norm': 0.6796875, 'learning_rate': 9.495936529759488e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0944, 'grad_norm': 0.7109375, 'learning_rate': 9.4948002920131e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2871, 'grad_norm': 0.58984375, 'learning_rate': 9.493664054266715e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0039, 'grad_norm': 0.99609375, 'learning_rate': 9.492527816520329e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2038, 'grad_norm': 0.47265625, 'learning_rate': 9.491391578773943e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2672, 'grad_norm': 0.50390625, 'learning_rate': 9.490255341027558e-05, 'epoch': 0.49}\n",
      "{'loss': 1.034, 'grad_norm': 0.8046875, 'learning_rate': 9.48911910328117e-05, 'epoch': 0.49}\n",
      "{'loss': 1.4002, 'grad_norm': 0.50390625, 'learning_rate': 9.487982865534784e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2049, 'grad_norm': 0.59375, 'learning_rate': 9.4868466277884e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1699, 'grad_norm': 0.482421875, 'learning_rate': 9.485710390042013e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2306, 'grad_norm': 0.6015625, 'learning_rate': 9.484574152295627e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1006, 'grad_norm': 0.671875, 'learning_rate': 9.483437914549241e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3557, 'grad_norm': 0.51171875, 'learning_rate': 9.482301676802854e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1254, 'grad_norm': 0.4453125, 'learning_rate': 9.481165439056468e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2592, 'grad_norm': 0.40625, 'learning_rate': 9.480029201310082e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2675, 'grad_norm': 0.75, 'learning_rate': 9.478892963563697e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1891, 'grad_norm': 0.921875, 'learning_rate': 9.477756725817311e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3592, 'grad_norm': 0.5859375, 'learning_rate': 9.476620488070925e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1425, 'grad_norm': 0.66796875, 'learning_rate': 9.475484250324539e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1444, 'grad_norm': 0.4453125, 'learning_rate': 9.474348012578152e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2028, 'grad_norm': 0.52734375, 'learning_rate': 9.473211774831766e-05, 'epoch': 0.49}\n",
      "{'loss': 0.9627, 'grad_norm': 0.8359375, 'learning_rate': 9.47207553708538e-05, 'epoch': 0.49}\n",
      "{'loss': 1.416, 'grad_norm': 0.54296875, 'learning_rate': 9.470939299338995e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1626, 'grad_norm': 0.6171875, 'learning_rate': 9.469803061592607e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0119, 'grad_norm': 0.60546875, 'learning_rate': 9.468666823846221e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3071, 'grad_norm': 0.58984375, 'learning_rate': 9.467530586099836e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0583, 'grad_norm': 0.453125, 'learning_rate': 9.46639434835345e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2528, 'grad_norm': 0.609375, 'learning_rate': 9.465258110607064e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1626, 'grad_norm': 0.76171875, 'learning_rate': 9.464121872860678e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1063, 'grad_norm': 0.53125, 'learning_rate': 9.462985635114292e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3305, 'grad_norm': 0.578125, 'learning_rate': 9.461849397367905e-05, 'epoch': 0.49}\n",
      "{'loss': 1.202, 'grad_norm': 0.73046875, 'learning_rate': 9.460713159621519e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3407, 'grad_norm': 0.52734375, 'learning_rate': 9.459576921875134e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1776, 'grad_norm': 0.921875, 'learning_rate': 9.458440684128748e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2581, 'grad_norm': 0.43359375, 'learning_rate': 9.457304446382362e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2228, 'grad_norm': 0.50390625, 'learning_rate': 9.456168208635976e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1217, 'grad_norm': 0.8515625, 'learning_rate': 9.45503197088959e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3658, 'grad_norm': 0.486328125, 'learning_rate': 9.453895733143203e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1655, 'grad_norm': 0.7265625, 'learning_rate': 9.452759495396817e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1531, 'grad_norm': 0.63671875, 'learning_rate': 9.451623257650432e-05, 'epoch': 0.49}\n",
      "{'loss': 1.304, 'grad_norm': 0.5859375, 'learning_rate': 9.450487019904045e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0327, 'grad_norm': 0.7734375, 'learning_rate': 9.449350782157658e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3124, 'grad_norm': 0.484375, 'learning_rate': 9.448214544411273e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2323, 'grad_norm': 0.75390625, 'learning_rate': 9.447078306664887e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1453, 'grad_norm': 0.484375, 'learning_rate': 9.445942068918501e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2446, 'grad_norm': 0.498046875, 'learning_rate': 9.444805831172115e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1746, 'grad_norm': 0.97265625, 'learning_rate': 9.443669593425729e-05, 'epoch': 0.49}\n",
      "{'loss': 1.4246, 'grad_norm': 0.6328125, 'learning_rate': 9.442533355679342e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3011, 'grad_norm': 0.7578125, 'learning_rate': 9.441397117932956e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2084, 'grad_norm': 0.46484375, 'learning_rate': 9.440260880186571e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2187, 'grad_norm': 0.6796875, 'learning_rate': 9.439124642440185e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0574, 'grad_norm': 1.078125, 'learning_rate': 9.437988404693799e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2851, 'grad_norm': 0.609375, 'learning_rate': 9.436852166947413e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1055, 'grad_norm': 0.59765625, 'learning_rate': 9.435715929201026e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1298, 'grad_norm': 0.59765625, 'learning_rate': 9.43457969145464e-05, 'epoch': 0.49}\n",
      "{'loss': 1.118, 'grad_norm': 0.68359375, 'learning_rate': 9.433443453708254e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0845, 'grad_norm': 0.80078125, 'learning_rate': 9.432307215961869e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2658, 'grad_norm': 0.5078125, 'learning_rate': 9.431170978215482e-05, 'epoch': 0.49}\n",
      "{'loss': 0.9657, 'grad_norm': 0.6484375, 'learning_rate': 9.430034740469095e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1966, 'grad_norm': 0.48828125, 'learning_rate': 9.42889850272271e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2526, 'grad_norm': 0.53125, 'learning_rate': 9.427762264976324e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1285, 'grad_norm': 1.1875, 'learning_rate': 9.426626027229938e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.388, 'grad_norm': 0.73828125, 'learning_rate': 9.425489789483552e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1005, 'grad_norm': 0.63671875, 'learning_rate': 9.424353551737166e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2505, 'grad_norm': 0.44921875, 'learning_rate': 9.42321731399078e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2064, 'grad_norm': 0.546875, 'learning_rate': 9.422081076244393e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0593, 'grad_norm': 0.5390625, 'learning_rate': 9.420944838498008e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3621, 'grad_norm': 0.71484375, 'learning_rate': 9.419808600751622e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1164, 'grad_norm': 0.609375, 'learning_rate': 9.418672363005236e-05, 'epoch': 0.49}\n",
      "{'loss': 1.237, 'grad_norm': 0.453125, 'learning_rate': 9.41753612525885e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1891, 'grad_norm': 0.6171875, 'learning_rate': 9.416399887512464e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1836, 'grad_norm': 1.375, 'learning_rate': 9.415263649766077e-05, 'epoch': 0.49}\n",
      "{'loss': 1.265, 'grad_norm': 0.5390625, 'learning_rate': 9.414127412019691e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2831, 'grad_norm': 0.69140625, 'learning_rate': 9.412991174273306e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1461, 'grad_norm': 0.640625, 'learning_rate': 9.411854936526919e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1773, 'grad_norm': 0.671875, 'learning_rate': 9.410718698780532e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0933, 'grad_norm': 0.69921875, 'learning_rate': 9.409582461034148e-05, 'epoch': 0.49}\n",
      "{'loss': 1.4039, 'grad_norm': 0.5859375, 'learning_rate': 9.408446223287761e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1832, 'grad_norm': 0.62109375, 'learning_rate': 9.407309985541375e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2468, 'grad_norm': 0.63671875, 'learning_rate': 9.406173747794989e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1467, 'grad_norm': 0.78125, 'learning_rate': 9.405037510048603e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0441, 'grad_norm': 1.0546875, 'learning_rate': 9.403901272302217e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3144, 'grad_norm': 0.6875, 'learning_rate': 9.40276503455583e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1381, 'grad_norm': 0.76171875, 'learning_rate': 9.401628796809445e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3155, 'grad_norm': 0.53125, 'learning_rate': 9.400492559063059e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3328, 'grad_norm': 0.671875, 'learning_rate': 9.399356321316673e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1506, 'grad_norm': 0.83984375, 'learning_rate': 9.398220083570287e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.38, 'grad_norm': 0.515625, 'learning_rate': 9.3970838458239e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1105, 'grad_norm': 0.86328125, 'learning_rate': 9.395947608077514e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1735, 'grad_norm': 0.63671875, 'learning_rate': 9.394811370331128e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2314, 'grad_norm': 0.6328125, 'learning_rate': 9.393675132584743e-05, 'epoch': 0.49}\n",
      "{'loss': 0.9534, 'grad_norm': 0.53125, 'learning_rate': 9.392538894838356e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3206, 'grad_norm': 0.6484375, 'learning_rate': 9.39140265709197e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1797, 'grad_norm': 0.50390625, 'learning_rate': 9.390266419345585e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1286, 'grad_norm': 0.734375, 'learning_rate': 9.389130181599198e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1671, 'grad_norm': 1.296875, 'learning_rate': 9.387993943852812e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1536, 'grad_norm': 0.82421875, 'learning_rate': 9.386857706106426e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3514, 'grad_norm': 1.1796875, 'learning_rate': 9.38572146836004e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0443, 'grad_norm': 0.76171875, 'learning_rate': 9.384585230613654e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3739, 'grad_norm': 0.609375, 'learning_rate': 9.383448992867267e-05, 'epoch': 0.49}\n",
      "{'loss': 1.198, 'grad_norm': 0.640625, 'learning_rate': 9.382312755120883e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0449, 'grad_norm': 0.3203125, 'learning_rate': 9.381176517374496e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3075, 'grad_norm': 0.44921875, 'learning_rate': 9.38004027962811e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3014, 'grad_norm': 0.66015625, 'learning_rate': 9.378904041881724e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1633, 'grad_norm': 0.55078125, 'learning_rate': 9.377767804135338e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2322, 'grad_norm': 0.60546875, 'learning_rate': 9.376631566388951e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2069, 'grad_norm': 0.921875, 'learning_rate': 9.375495328642565e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2834, 'grad_norm': 0.6640625, 'learning_rate': 9.37435909089618e-05, 'epoch': 0.49}\n",
      "{'loss': 1.092, 'grad_norm': 0.7421875, 'learning_rate': 9.373222853149793e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2177, 'grad_norm': 0.466796875, 'learning_rate': 9.372086615403407e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1749, 'grad_norm': 0.56640625, 'learning_rate': 9.370950377657022e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0917, 'grad_norm': 1.4296875, 'learning_rate': 9.369814139910636e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2756, 'grad_norm': 0.4609375, 'learning_rate': 9.368677902164249e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2024, 'grad_norm': 0.703125, 'learning_rate': 9.367541664417863e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2566, 'grad_norm': 0.6328125, 'learning_rate': 9.366405426671477e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2213, 'grad_norm': 0.66796875, 'learning_rate': 9.36526918892509e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1112, 'grad_norm': 1.140625, 'learning_rate': 9.364132951178704e-05, 'epoch': 0.49}\n",
      "{'loss': 1.331, 'grad_norm': 0.5703125, 'learning_rate': 9.36299671343232e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2623, 'grad_norm': 0.76953125, 'learning_rate': 9.361860475685933e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2549, 'grad_norm': 0.494140625, 'learning_rate': 9.360724237939547e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2733, 'grad_norm': 0.6171875, 'learning_rate': 9.359588000193161e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0214, 'grad_norm': 0.6484375, 'learning_rate': 9.358451762446775e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3138, 'grad_norm': 0.470703125, 'learning_rate': 9.357315524700388e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1716, 'grad_norm': 0.77734375, 'learning_rate': 9.356179286954004e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2155, 'grad_norm': 0.5625, 'learning_rate': 9.355043049207617e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1799, 'grad_norm': 0.546875, 'learning_rate': 9.35390681146123e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0368, 'grad_norm': 0.99609375, 'learning_rate': 9.352770573714844e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3661, 'grad_norm': 0.55859375, 'learning_rate': 9.351634335968459e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2344, 'grad_norm': 0.921875, 'learning_rate': 9.350498098222073e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1996, 'grad_norm': 0.53125, 'learning_rate': 9.349361860475686e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2574, 'grad_norm': 0.6171875, 'learning_rate': 9.3482256227293e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1735, 'grad_norm': 0.89453125, 'learning_rate': 9.347089384982914e-05, 'epoch': 0.49}\n",
      "{'loss': 1.4002, 'grad_norm': 0.59375, 'learning_rate': 9.345953147236528e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0507, 'grad_norm': 0.74609375, 'learning_rate': 9.344816909490141e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2798, 'grad_norm': 0.41796875, 'learning_rate': 9.343680671743757e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3412, 'grad_norm': 0.45703125, 'learning_rate': 9.34254443399737e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0694, 'grad_norm': 0.66796875, 'learning_rate': 9.341408196250984e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3982, 'grad_norm': 0.5390625, 'learning_rate': 9.340271958504598e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2835, 'grad_norm': 0.79296875, 'learning_rate': 9.339135720758212e-05, 'epoch': 0.49}\n",
      "{'loss': 1.255, 'grad_norm': 0.52734375, 'learning_rate': 9.337999483011826e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2207, 'grad_norm': 0.5703125, 'learning_rate': 9.336863245265441e-05, 'epoch': 0.49}\n",
      "{'loss': 1.044, 'grad_norm': 0.984375, 'learning_rate': 9.335727007519054e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2383, 'grad_norm': 0.44140625, 'learning_rate': 9.334590769772667e-05, 'epoch': 0.49}\n",
      "{'loss': 1.136, 'grad_norm': 0.69921875, 'learning_rate': 9.333454532026281e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2653, 'grad_norm': 0.42578125, 'learning_rate': 9.332318294279896e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2614, 'grad_norm': 0.625, 'learning_rate': 9.33118205653351e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1711, 'grad_norm': 0.796875, 'learning_rate': 9.330045818787123e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3152, 'grad_norm': 0.6796875, 'learning_rate': 9.328909581040737e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1858, 'grad_norm': 1.9140625, 'learning_rate': 9.327773343294351e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1742, 'grad_norm': 0.42578125, 'learning_rate': 9.326637105547965e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2535, 'grad_norm': 0.87890625, 'learning_rate': 9.325500867801579e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0856, 'grad_norm': 0.8515625, 'learning_rate': 9.324364630055194e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3299, 'grad_norm': 0.703125, 'learning_rate': 9.323228392308807e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1708, 'grad_norm': 1.1875, 'learning_rate': 9.322092154562421e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2778, 'grad_norm': 0.62109375, 'learning_rate': 9.320955916816035e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0711, 'grad_norm': 0.625, 'learning_rate': 9.319819679069649e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0578, 'grad_norm': 1.0, 'learning_rate': 9.318683441323263e-05, 'epoch': 0.49}\n",
      "{'loss': 1.4051, 'grad_norm': 0.484375, 'learning_rate': 9.317547203576878e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1842, 'grad_norm': 0.95703125, 'learning_rate': 9.316410965830492e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1594, 'grad_norm': 0.578125, 'learning_rate': 9.315274728084104e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2696, 'grad_norm': 0.84765625, 'learning_rate': 9.314138490337718e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1966, 'grad_norm': 0.9296875, 'learning_rate': 9.313002252591333e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2866, 'grad_norm': 0.6328125, 'learning_rate': 9.311866014844947e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1797, 'grad_norm': 0.671875, 'learning_rate': 9.31072977709856e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2111, 'grad_norm': 0.4140625, 'learning_rate': 9.309593539352174e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2178, 'grad_norm': 0.486328125, 'learning_rate': 9.308457301605788e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0984, 'grad_norm': 0.69140625, 'learning_rate': 9.307321063859402e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2872, 'grad_norm': 0.55859375, 'learning_rate': 9.306184826113016e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2254, 'grad_norm': 0.76953125, 'learning_rate': 9.305048588366631e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2963, 'grad_norm': 0.5, 'learning_rate': 9.303912350620245e-05, 'epoch': 0.49}\n",
      "{'loss': 1.347, 'grad_norm': 0.62890625, 'learning_rate': 9.302776112873858e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0078, 'grad_norm': 1.6171875, 'learning_rate': 9.301639875127472e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3747, 'grad_norm': 0.46484375, 'learning_rate': 9.300503637381086e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2249, 'grad_norm': 0.61328125, 'learning_rate': 9.2993673996347e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2148, 'grad_norm': 0.431640625, 'learning_rate': 9.298231161888315e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2678, 'grad_norm': 1.0859375, 'learning_rate': 9.297094924141929e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0603, 'grad_norm': 0.796875, 'learning_rate': 9.295958686395541e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1983, 'grad_norm': 0.47265625, 'learning_rate': 9.294822448649155e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2398, 'grad_norm': 0.671875, 'learning_rate': 9.29368621090277e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1557, 'grad_norm': 0.447265625, 'learning_rate': 9.292549973156384e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2694, 'grad_norm': 0.5859375, 'learning_rate': 9.291413735409998e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1271, 'grad_norm': 0.48046875, 'learning_rate': 9.290277497663611e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2084, 'grad_norm': 0.7265625, 'learning_rate': 9.289141259917225e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0994, 'grad_norm': 0.62890625, 'learning_rate': 9.288005022170839e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2801, 'grad_norm': 0.439453125, 'learning_rate': 9.286868784424454e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1814, 'grad_norm': 0.6796875, 'learning_rate': 9.285732546678068e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0439, 'grad_norm': 0.94921875, 'learning_rate': 9.284596308931682e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3145, 'grad_norm': 0.58203125, 'learning_rate': 9.283460071185295e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1853, 'grad_norm': 0.8203125, 'learning_rate': 9.282323833438909e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1315, 'grad_norm': 0.5390625, 'learning_rate': 9.281187595692523e-05, 'epoch': 0.49}\n",
      "{'loss': 1.2553, 'grad_norm': 0.80078125, 'learning_rate': 9.280051357946137e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1456, 'grad_norm': 0.953125, 'learning_rate': 9.278915120199752e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3604, 'grad_norm': 1.0625, 'learning_rate': 9.277778882453366e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1675, 'grad_norm': 0.625, 'learning_rate': 9.276642644706978e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1831, 'grad_norm': 0.44140625, 'learning_rate': 9.275506406960592e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2906, 'grad_norm': 0.66015625, 'learning_rate': 9.274370169214207e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1032, 'grad_norm': 0.86328125, 'learning_rate': 9.273233931467821e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2765, 'grad_norm': 0.515625, 'learning_rate': 9.272097693721435e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1725, 'grad_norm': 0.74609375, 'learning_rate': 9.270961455975048e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1837, 'grad_norm': 0.45703125, 'learning_rate': 9.269825218228662e-05, 'epoch': 0.5}\n",
      "{'loss': 1.178, 'grad_norm': 0.765625, 'learning_rate': 9.268688980482276e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0778, 'grad_norm': 1.2421875, 'learning_rate': 9.267552742735891e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2919, 'grad_norm': 0.55859375, 'learning_rate': 9.266416504989505e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0516, 'grad_norm': 0.69921875, 'learning_rate': 9.265280267243119e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2283, 'grad_norm': 0.490234375, 'learning_rate': 9.264144029496732e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2596, 'grad_norm': 0.52734375, 'learning_rate': 9.263007791750346e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1485, 'grad_norm': 1.0, 'learning_rate': 9.26187155400396e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3589, 'grad_norm': 0.60546875, 'learning_rate': 9.260735316257574e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1036, 'grad_norm': 0.65625, 'learning_rate': 9.259599078511189e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0884, 'grad_norm': 0.462890625, 'learning_rate': 9.258462840764803e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3187, 'grad_norm': 0.64453125, 'learning_rate': 9.257326603018415e-05, 'epoch': 0.5}\n",
      "{'loss': 0.9533, 'grad_norm': 0.85546875, 'learning_rate': 9.256190365272029e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4273, 'grad_norm': 0.578125, 'learning_rate': 9.255054127525644e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2055, 'grad_norm': 1.1953125, 'learning_rate': 9.253917889779258e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2702, 'grad_norm': 0.427734375, 'learning_rate': 9.252781652032872e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1687, 'grad_norm': 0.498046875, 'learning_rate': 9.251645414286485e-05, 'epoch': 0.5}\n",
      "{'loss': 1.015, 'grad_norm': 0.8125, 'learning_rate': 9.250509176540099e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2806, 'grad_norm': 0.48046875, 'learning_rate': 9.249372938793713e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1743, 'grad_norm': 0.73046875, 'learning_rate': 9.248236701047328e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1427, 'grad_norm': 0.447265625, 'learning_rate': 9.247100463300942e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2174, 'grad_norm': 0.87890625, 'learning_rate': 9.245964225554556e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1932, 'grad_norm': 0.80859375, 'learning_rate': 9.24482798780817e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4746, 'grad_norm': 0.83984375, 'learning_rate': 9.243691750061783e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1677, 'grad_norm': 0.7421875, 'learning_rate': 9.242555512315397e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2543, 'grad_norm': 0.408203125, 'learning_rate': 9.241419274569011e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3014, 'grad_norm': 0.51953125, 'learning_rate': 9.240283036822626e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1268, 'grad_norm': 0.8203125, 'learning_rate': 9.23914679907624e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3837, 'grad_norm': 0.5234375, 'learning_rate': 9.238010561329852e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1314, 'grad_norm': 0.8203125, 'learning_rate': 9.236874323583466e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2188, 'grad_norm': 0.408203125, 'learning_rate': 9.235738085837081e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2236, 'grad_norm': 0.88671875, 'learning_rate': 9.234601848090695e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2181, 'grad_norm': 0.90625, 'learning_rate': 9.233465610344309e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3498, 'grad_norm': 0.53125, 'learning_rate': 9.232329372597923e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1675, 'grad_norm': 1.109375, 'learning_rate': 9.231193134851536e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2352, 'grad_norm': 0.462890625, 'learning_rate': 9.23005689710515e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1836, 'grad_norm': 0.8203125, 'learning_rate': 9.228920659358765e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1462, 'grad_norm': 0.8359375, 'learning_rate': 9.227784421612379e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4053, 'grad_norm': 0.5390625, 'learning_rate': 9.226648183865993e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1449, 'grad_norm': 0.5703125, 'learning_rate': 9.225511946119607e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3148, 'grad_norm': 0.5078125, 'learning_rate': 9.22437570837322e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2642, 'grad_norm': 0.66796875, 'learning_rate': 9.223239470626834e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0747, 'grad_norm': 1.1015625, 'learning_rate': 9.222103232880448e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2837, 'grad_norm': 0.55078125, 'learning_rate': 9.220966995134063e-05, 'epoch': 0.5}\n",
      "{'loss': 1.179, 'grad_norm': 0.73046875, 'learning_rate': 9.219830757387677e-05, 'epoch': 0.5}\n",
      "{'loss': 1.061, 'grad_norm': 0.408203125, 'learning_rate': 9.218694519641289e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1815, 'grad_norm': 0.5859375, 'learning_rate': 9.217558281894904e-05, 'epoch': 0.5}\n",
      "{'loss': 1.035, 'grad_norm': 0.875, 'learning_rate': 9.216422044148518e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3293, 'grad_norm': 0.5625, 'learning_rate': 9.215285806402132e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1943, 'grad_norm': 0.61328125, 'learning_rate': 9.214149568655746e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0423, 'grad_norm': 0.51953125, 'learning_rate': 9.21301333090936e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2754, 'grad_norm': 0.48828125, 'learning_rate': 9.211877093162973e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0243, 'grad_norm': 0.97265625, 'learning_rate': 9.210740855416587e-05, 'epoch': 0.5}\n",
      "{'loss': 1.328, 'grad_norm': 0.58203125, 'learning_rate': 9.209604617670202e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2503, 'grad_norm': 0.9375, 'learning_rate': 9.208468379923816e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2539, 'grad_norm': 0.5546875, 'learning_rate': 9.20733214217743e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2579, 'grad_norm': 0.5234375, 'learning_rate': 9.206195904431044e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1778, 'grad_norm': 0.765625, 'learning_rate': 9.205059666684657e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2639, 'grad_norm': 0.50390625, 'learning_rate': 9.203923428938271e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1323, 'grad_norm': 0.6796875, 'learning_rate': 9.202787191191885e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2366, 'grad_norm': 0.38671875, 'learning_rate': 9.2016509534455e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3512, 'grad_norm': 1.09375, 'learning_rate': 9.200514715699114e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0733, 'grad_norm': 0.7734375, 'learning_rate': 9.199378477952726e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3185, 'grad_norm': 0.7109375, 'learning_rate': 9.198242240206341e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3808, 'grad_norm': 0.57421875, 'learning_rate': 9.197106002459955e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2277, 'grad_norm': 0.455078125, 'learning_rate': 9.195969764713569e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1561, 'grad_norm': 0.5625, 'learning_rate': 9.194833526967183e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1686, 'grad_norm': 0.95703125, 'learning_rate': 9.193697289220797e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3378, 'grad_norm': 0.828125, 'learning_rate': 9.19256105147441e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2052, 'grad_norm': 0.65234375, 'learning_rate': 9.191424813728024e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2263, 'grad_norm': 0.396484375, 'learning_rate': 9.19028857598164e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1952, 'grad_norm': 0.6484375, 'learning_rate': 9.189152338235253e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1135, 'grad_norm': 0.953125, 'learning_rate': 9.188016100488867e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2092, 'grad_norm': 0.5390625, 'learning_rate': 9.186879862742481e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1633, 'grad_norm': 0.796875, 'learning_rate': 9.185743624996094e-05, 'epoch': 0.5}\n",
      "{'loss': 1.309, 'grad_norm': 0.453125, 'learning_rate': 9.184607387249708e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1454, 'grad_norm': 0.66796875, 'learning_rate': 9.183471149503322e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1167, 'grad_norm': 0.6328125, 'learning_rate': 9.182334911756937e-05, 'epoch': 0.5}\n",
      "{'loss': 1.337, 'grad_norm': 0.546875, 'learning_rate': 9.181198674010551e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0609, 'grad_norm': 0.68359375, 'learning_rate': 9.180062436264163e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2768, 'grad_norm': 0.4296875, 'learning_rate': 9.178926198517779e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2531, 'grad_norm': 0.65625, 'learning_rate': 9.177789960771392e-05, 'epoch': 0.5}\n",
      "{'loss': 0.9904, 'grad_norm': 1.28125, 'learning_rate': 9.176653723025006e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2663, 'grad_norm': 0.50390625, 'learning_rate': 9.17551748527862e-05, 'epoch': 0.5}\n",
      "{'loss': 1.086, 'grad_norm': 0.5234375, 'learning_rate': 9.174381247532234e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1016, 'grad_norm': 0.53515625, 'learning_rate': 9.173245009785847e-05, 'epoch': 0.5}\n",
      "{'loss': 1.107, 'grad_norm': 0.55859375, 'learning_rate': 9.172108772039461e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1007, 'grad_norm': 0.73828125, 'learning_rate': 9.170972534293076e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3045, 'grad_norm': 0.55859375, 'learning_rate': 9.16983629654669e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3055, 'grad_norm': 0.96484375, 'learning_rate': 9.168700058800304e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2711, 'grad_norm': 0.94921875, 'learning_rate': 9.167563821053918e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2205, 'grad_norm': 0.6328125, 'learning_rate': 9.166427583307532e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0702, 'grad_norm': 0.85546875, 'learning_rate': 9.165291345561145e-05, 'epoch': 0.5}\n",
      "{'loss': 1.309, 'grad_norm': 0.53125, 'learning_rate': 9.164155107814759e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3168, 'grad_norm': 0.62109375, 'learning_rate': 9.163018870068374e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2188, 'grad_norm': 0.50390625, 'learning_rate': 9.161882632321988e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3409, 'grad_norm': 0.6328125, 'learning_rate': 9.1607463945756e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2486, 'grad_norm': 0.8828125, 'learning_rate': 9.159610156829216e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3132, 'grad_norm': 0.482421875, 'learning_rate': 9.15847391908283e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2743, 'grad_norm': 0.70703125, 'learning_rate': 9.157337681336443e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1559, 'grad_norm': 0.60546875, 'learning_rate': 9.156201443590057e-05, 'epoch': 0.5}\n",
      "{'loss': 1.259, 'grad_norm': 0.41796875, 'learning_rate': 9.155065205843671e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1789, 'grad_norm': 1.203125, 'learning_rate': 9.153928968097285e-05, 'epoch': 0.5}\n",
      "{'loss': 1.512, 'grad_norm': 0.482421875, 'learning_rate': 9.152792730350898e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2015, 'grad_norm': 0.66796875, 'learning_rate': 9.151656492604513e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1547, 'grad_norm': 0.486328125, 'learning_rate': 9.150520254858127e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1536, 'grad_norm': 0.6015625, 'learning_rate': 9.149384017111741e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1579, 'grad_norm': 1.015625, 'learning_rate': 9.148247779365355e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4875, 'grad_norm': 0.46875, 'learning_rate': 9.147111541618969e-05, 'epoch': 0.5}\n",
      "{'loss': 1.106, 'grad_norm': 0.74609375, 'learning_rate': 9.145975303872582e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1345, 'grad_norm': 0.37890625, 'learning_rate': 9.144839066126196e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2291, 'grad_norm': 0.609375, 'learning_rate': 9.143702828379811e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1057, 'grad_norm': 1.171875, 'learning_rate': 9.142566590633425e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3563, 'grad_norm': 0.462890625, 'learning_rate': 9.141430352887038e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1392, 'grad_norm': 0.75, 'learning_rate': 9.140294115140653e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2658, 'grad_norm': 0.7109375, 'learning_rate': 9.139157877394266e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3554, 'grad_norm': 0.984375, 'learning_rate': 9.13802163964788e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1262, 'grad_norm': 0.90625, 'learning_rate': 9.136885401901494e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2954, 'grad_norm': 0.50390625, 'learning_rate': 9.135749164155109e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0962, 'grad_norm': 0.79296875, 'learning_rate': 9.134612926408722e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1342, 'grad_norm': 0.5, 'learning_rate': 9.133476688662335e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1754, 'grad_norm': 0.8828125, 'learning_rate': 9.13234045091595e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2737, 'grad_norm': 1.0546875, 'learning_rate': 9.131204213169564e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3173, 'grad_norm': 0.5546875, 'learning_rate': 9.130067975423178e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0804, 'grad_norm': 0.58984375, 'learning_rate': 9.128931737676792e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2444, 'grad_norm': 0.5625, 'learning_rate': 9.127795499930406e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2456, 'grad_norm': 0.6328125, 'learning_rate': 9.12665926218402e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0579, 'grad_norm': 0.68359375, 'learning_rate': 9.125523024437633e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2966, 'grad_norm': 0.48828125, 'learning_rate': 9.124386786691248e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3215, 'grad_norm': 0.66015625, 'learning_rate': 9.123250548944862e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1792, 'grad_norm': 0.486328125, 'learning_rate': 9.122114311198475e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1501, 'grad_norm': 0.48828125, 'learning_rate': 9.12097807345209e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0872, 'grad_norm': 0.67578125, 'learning_rate': 9.119841835705704e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3207, 'grad_norm': 0.453125, 'learning_rate': 9.118705597959317e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2692, 'grad_norm': 0.8359375, 'learning_rate': 9.117569360212931e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2107, 'grad_norm': 0.51953125, 'learning_rate': 9.116433122466546e-05, 'epoch': 0.5}\n",
      "{'loss': 1.198, 'grad_norm': 0.765625, 'learning_rate': 9.115296884720159e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0708, 'grad_norm': 0.7734375, 'learning_rate': 9.114160646973772e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3459, 'grad_norm': 0.73828125, 'learning_rate': 9.113024409227388e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1112, 'grad_norm': 0.458984375, 'learning_rate': 9.111888171481001e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1517, 'grad_norm': 0.4375, 'learning_rate': 9.110751933734615e-05, 'epoch': 0.5}\n",
      "{'loss': 1.302, 'grad_norm': 0.5234375, 'learning_rate': 9.109615695988229e-05, 'epoch': 0.5}\n",
      "{'loss': 0.9951, 'grad_norm': 0.91796875, 'learning_rate': 9.108479458241843e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2765, 'grad_norm': 0.64453125, 'learning_rate': 9.107343220495457e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1815, 'grad_norm': 0.83984375, 'learning_rate': 9.10620698274907e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2578, 'grad_norm': 0.4375, 'learning_rate': 9.105070745002685e-05, 'epoch': 0.5}\n",
      "{'loss': 1.229, 'grad_norm': 0.61328125, 'learning_rate': 9.103934507256299e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1039, 'grad_norm': 0.9140625, 'learning_rate': 9.102798269509912e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4653, 'grad_norm': 0.5078125, 'learning_rate': 9.101662031763527e-05, 'epoch': 0.5}\n",
      "{'loss': 1.217, 'grad_norm': 0.73828125, 'learning_rate': 9.10052579401714e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0628, 'grad_norm': 0.486328125, 'learning_rate': 9.099389556270754e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2982, 'grad_norm': 0.609375, 'learning_rate': 9.09825331852437e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0552, 'grad_norm': 0.84375, 'learning_rate': 9.097117080777983e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2724, 'grad_norm': 0.59375, 'learning_rate': 9.095980843031596e-05, 'epoch': 0.5}\n",
      "{'loss': 1.102, 'grad_norm': 0.82421875, 'learning_rate': 9.09484460528521e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2474, 'grad_norm': 0.474609375, 'learning_rate': 9.093708367538825e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1673, 'grad_norm': 1.125, 'learning_rate': 9.092572129792438e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0699, 'grad_norm': 0.8515625, 'learning_rate': 9.091435892046052e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2639, 'grad_norm': 0.61328125, 'learning_rate': 9.090299654299666e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0744, 'grad_norm': 0.765625, 'learning_rate': 9.08916341655328e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0347, 'grad_norm': 0.498046875, 'learning_rate': 9.088027178806894e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0903, 'grad_norm': 0.52734375, 'learning_rate': 9.086890941060507e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1115, 'grad_norm': 1.5546875, 'learning_rate': 9.085754703314123e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3468, 'grad_norm': 0.515625, 'learning_rate': 9.084618465567736e-05, 'epoch': 0.5}\n",
      "{'loss': 1.142, 'grad_norm': 0.578125, 'learning_rate': 9.083482227821349e-05, 'epoch': 0.5}\n",
      "{'loss': 1.17, 'grad_norm': 0.4453125, 'learning_rate': 9.082345990074964e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1765, 'grad_norm': 0.64453125, 'learning_rate': 9.081209752328578e-05, 'epoch': 0.5}\n",
      "{'loss': 0.9749, 'grad_norm': 0.6640625, 'learning_rate': 9.080073514582191e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3887, 'grad_norm': 0.5625, 'learning_rate': 9.078937276835807e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0456, 'grad_norm': 0.6484375, 'learning_rate': 9.07780103908942e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1599, 'grad_norm': 0.58984375, 'learning_rate': 9.076664801343033e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3027, 'grad_norm': 1.28125, 'learning_rate': 9.075528563596647e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1223, 'grad_norm': 0.93359375, 'learning_rate': 9.074392325850262e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2915, 'grad_norm': 0.69921875, 'learning_rate': 9.073256088103876e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2505, 'grad_norm': 0.8125, 'learning_rate': 9.072119850357489e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3375, 'grad_norm': 0.3984375, 'learning_rate': 9.070983612611103e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1758, 'grad_norm': 0.53515625, 'learning_rate': 9.069847374864717e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1925, 'grad_norm': 0.79296875, 'learning_rate': 9.06871113711833e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2842, 'grad_norm': 0.59375, 'learning_rate': 9.067574899371944e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1842, 'grad_norm': 0.765625, 'learning_rate': 9.06643866162556e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1888, 'grad_norm': 0.53515625, 'learning_rate': 9.065302423879173e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2291, 'grad_norm': 0.796875, 'learning_rate': 9.064166186132786e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0539, 'grad_norm': 0.76953125, 'learning_rate': 9.063029948386401e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3037, 'grad_norm': 0.54296875, 'learning_rate': 9.061893710640015e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1913, 'grad_norm': 0.58203125, 'learning_rate': 9.060757472893629e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1483, 'grad_norm': 0.3671875, 'learning_rate': 9.059621235147244e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3347, 'grad_norm': 0.59765625, 'learning_rate': 9.058484997400857e-05, 'epoch': 0.5}\n",
      "{'loss': 0.9685, 'grad_norm': 0.65625, 'learning_rate': 9.05734875965447e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2282, 'grad_norm': 0.66015625, 'learning_rate': 9.056212521908084e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0299, 'grad_norm': 0.82421875, 'learning_rate': 9.055076284161699e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1784, 'grad_norm': 0.4453125, 'learning_rate': 9.053940046415313e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2954, 'grad_norm': 0.81640625, 'learning_rate': 9.052803808668926e-05, 'epoch': 0.5}\n",
      "{'loss': 1.089, 'grad_norm': 0.6015625, 'learning_rate': 9.05166757092254e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3356, 'grad_norm': 0.75, 'learning_rate': 9.050531333176154e-05, 'epoch': 0.5}\n",
      "{'loss': 1.023, 'grad_norm': 0.625, 'learning_rate': 9.049395095429768e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1138, 'grad_norm': 0.5625, 'learning_rate': 9.048258857683382e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2539, 'grad_norm': 0.76953125, 'learning_rate': 9.047122619936997e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0402, 'grad_norm': 0.8828125, 'learning_rate': 9.04598638219061e-05, 'epoch': 0.51}\n",
      "{'loss': 1.363, 'grad_norm': 0.5234375, 'learning_rate': 9.044850144444223e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0957, 'grad_norm': 0.671875, 'learning_rate': 9.043713906697838e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1985, 'grad_norm': 0.5390625, 'learning_rate': 9.042577668951452e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2501, 'grad_norm': 0.64453125, 'learning_rate': 9.041441431205066e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1086, 'grad_norm': 1.0546875, 'learning_rate': 9.040305193458681e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2345, 'grad_norm': 0.578125, 'learning_rate': 9.039168955712294e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1112, 'grad_norm': 0.64453125, 'learning_rate': 9.038032717965907e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2646, 'grad_norm': 0.49609375, 'learning_rate': 9.036896480219521e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2835, 'grad_norm': 0.74609375, 'learning_rate': 9.035760242473136e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9646, 'grad_norm': 0.91796875, 'learning_rate': 9.03462400472675e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3687, 'grad_norm': 0.53125, 'learning_rate': 9.033487766980363e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2062, 'grad_norm': 1.328125, 'learning_rate': 9.032351529233977e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3037, 'grad_norm': 0.4140625, 'learning_rate': 9.031215291487591e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2518, 'grad_norm': 0.72265625, 'learning_rate': 9.030079053741205e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9278, 'grad_norm': 0.54296875, 'learning_rate': 9.02894281599482e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1792, 'grad_norm': 0.5546875, 'learning_rate': 9.027806578248434e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1398, 'grad_norm': 0.68359375, 'learning_rate': 9.026670340502047e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1094, 'grad_norm': 0.51171875, 'learning_rate': 9.02553410275566e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3003, 'grad_norm': 0.60546875, 'learning_rate': 9.024397865009275e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9343, 'grad_norm': 1.140625, 'learning_rate': 9.023261627262889e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2454, 'grad_norm': 0.58984375, 'learning_rate': 9.022125389516503e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2143, 'grad_norm': 0.6796875, 'learning_rate': 9.020989151770118e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2119, 'grad_norm': 0.71484375, 'learning_rate': 9.019852914023732e-05, 'epoch': 0.51}\n",
      "{'loss': 1.179, 'grad_norm': 0.578125, 'learning_rate': 9.018716676277344e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1995, 'grad_norm': 1.15625, 'learning_rate': 9.017580438530958e-05, 'epoch': 0.51}\n",
      "{'loss': 1.4413, 'grad_norm': 0.53125, 'learning_rate': 9.016444200784573e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1454, 'grad_norm': 0.546875, 'learning_rate': 9.015307963038187e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2068, 'grad_norm': 0.9375, 'learning_rate': 9.0141717252918e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2128, 'grad_norm': 0.62109375, 'learning_rate': 9.013035487545414e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0034, 'grad_norm': 1.5546875, 'learning_rate': 9.011899249799028e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2213, 'grad_norm': 0.447265625, 'learning_rate': 9.010763012052642e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1862, 'grad_norm': 0.7265625, 'learning_rate': 9.009626774306257e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0381, 'grad_norm': 0.44140625, 'learning_rate': 9.008490536559871e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1421, 'grad_norm': 0.54296875, 'learning_rate': 9.007354298813485e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9368, 'grad_norm': 0.45703125, 'learning_rate': 9.006218061067097e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3213, 'grad_norm': 0.59765625, 'learning_rate': 9.005081823320712e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1222, 'grad_norm': 0.6953125, 'learning_rate': 9.003945585574326e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1485, 'grad_norm': 0.515625, 'learning_rate': 9.00280934782794e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1537, 'grad_norm': 0.58984375, 'learning_rate': 9.001673110081555e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1771, 'grad_norm': 1.0625, 'learning_rate': 9.000536872335169e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2843, 'grad_norm': 0.64453125, 'learning_rate': 8.999400634588781e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1711, 'grad_norm': 0.68359375, 'learning_rate': 8.998264396842395e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1851, 'grad_norm': 0.498046875, 'learning_rate': 8.99712815909601e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2053, 'grad_norm': 0.52734375, 'learning_rate': 8.995991921349624e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0552, 'grad_norm': 0.75, 'learning_rate': 8.994855683603238e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2648, 'grad_norm': 0.546875, 'learning_rate': 8.993719445856851e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1966, 'grad_norm': 0.51171875, 'learning_rate': 8.992583208110465e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3109, 'grad_norm': 0.412109375, 'learning_rate': 8.991446970364079e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2548, 'grad_norm': 0.9609375, 'learning_rate': 8.990310732617694e-05, 'epoch': 0.51}\n",
      "{'loss': 1.049, 'grad_norm': 0.71484375, 'learning_rate': 8.989174494871308e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2816, 'grad_norm': 0.486328125, 'learning_rate': 8.988038257124922e-05, 'epoch': 0.51}\n",
      "{'loss': 1.139, 'grad_norm': 0.64453125, 'learning_rate': 8.986902019378534e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1554, 'grad_norm': 0.4765625, 'learning_rate': 8.985765781632149e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3556, 'grad_norm': 0.625, 'learning_rate': 8.984629543885763e-05, 'epoch': 0.51}\n",
      "{'loss': 0.995, 'grad_norm': 1.0390625, 'learning_rate': 8.983493306139377e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3186, 'grad_norm': 0.55078125, 'learning_rate': 8.982357068392992e-05, 'epoch': 0.51}\n",
      "{'loss': 1.082, 'grad_norm': 0.6796875, 'learning_rate': 8.981220830646606e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0947, 'grad_norm': 0.55078125, 'learning_rate': 8.980084592900218e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2027, 'grad_norm': 0.76171875, 'learning_rate': 8.978948355153833e-05, 'epoch': 0.51}\n",
      "{'loss': 1.186, 'grad_norm': 0.921875, 'learning_rate': 8.977812117407447e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1955, 'grad_norm': 0.5390625, 'learning_rate': 8.976675879661061e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1769, 'grad_norm': 0.61328125, 'learning_rate': 8.975539641914675e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2513, 'grad_norm': 0.44140625, 'learning_rate': 8.974403404168288e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2818, 'grad_norm': 0.6328125, 'learning_rate': 8.973267166421902e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0201, 'grad_norm': 1.078125, 'learning_rate': 8.972130928675516e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3053, 'grad_norm': 0.9453125, 'learning_rate': 8.970994690929131e-05, 'epoch': 0.51}\n",
      "{'loss': 1.126, 'grad_norm': 1.3515625, 'learning_rate': 8.969858453182745e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2616, 'grad_norm': 0.40625, 'learning_rate': 8.968722215436359e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2378, 'grad_norm': 0.75390625, 'learning_rate': 8.967585977689971e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0129, 'grad_norm': 0.6796875, 'learning_rate': 8.966449739943586e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3937, 'grad_norm': 0.62890625, 'learning_rate': 8.9653135021972e-05, 'epoch': 0.51}\n",
      "{'loss': 1.193, 'grad_norm': 0.74609375, 'learning_rate': 8.964177264450814e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2831, 'grad_norm': 0.455078125, 'learning_rate': 8.963041026704429e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2842, 'grad_norm': 0.7265625, 'learning_rate': 8.961904788958043e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0782, 'grad_norm': 0.8046875, 'learning_rate': 8.960768551211655e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3003, 'grad_norm': 0.7421875, 'learning_rate': 8.95963231346527e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1346, 'grad_norm': 1.21875, 'learning_rate': 8.958496075718884e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2927, 'grad_norm': 0.8984375, 'learning_rate': 8.957359837972498e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3011, 'grad_norm': 0.5703125, 'learning_rate': 8.956223600226112e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0522, 'grad_norm': 0.82421875, 'learning_rate': 8.955087362479725e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3576, 'grad_norm': 0.5078125, 'learning_rate': 8.953951124733339e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2031, 'grad_norm': 0.66796875, 'learning_rate': 8.952814886986953e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2326, 'grad_norm': 0.400390625, 'learning_rate': 8.951678649240568e-05, 'epoch': 0.51}\n",
      "{'loss': 1.291, 'grad_norm': 0.8125, 'learning_rate': 8.950542411494182e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0481, 'grad_norm': 0.71875, 'learning_rate': 8.949406173747796e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3175, 'grad_norm': 0.58203125, 'learning_rate': 8.948269936001408e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1337, 'grad_norm': 0.94140625, 'learning_rate': 8.947133698255023e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2441, 'grad_norm': 0.546875, 'learning_rate': 8.945997460508637e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1351, 'grad_norm': 0.6328125, 'learning_rate': 8.944861222762251e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0363, 'grad_norm': 0.57421875, 'learning_rate': 8.943724985015866e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1967, 'grad_norm': 0.69140625, 'learning_rate': 8.94258874726948e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0898, 'grad_norm': 0.703125, 'learning_rate': 8.941452509523092e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1231, 'grad_norm': 0.5390625, 'learning_rate': 8.940316271776707e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2183, 'grad_norm': 0.6640625, 'learning_rate': 8.939180034030321e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0347, 'grad_norm': 0.70703125, 'learning_rate': 8.938043796283935e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2291, 'grad_norm': 0.5625, 'learning_rate': 8.936907558537549e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1175, 'grad_norm': 0.5390625, 'learning_rate': 8.935771320791163e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2394, 'grad_norm': 0.474609375, 'learning_rate': 8.934635083044776e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2515, 'grad_norm': 0.53515625, 'learning_rate': 8.93349884529839e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1118, 'grad_norm': 1.2734375, 'learning_rate': 8.932362607552005e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1789, 'grad_norm': 0.5390625, 'learning_rate': 8.931226369805619e-05, 'epoch': 0.51}\n",
      "{'loss': 1.224, 'grad_norm': 0.68359375, 'learning_rate': 8.930090132059233e-05, 'epoch': 0.51}\n",
      "{'loss': 1.235, 'grad_norm': 0.4296875, 'learning_rate': 8.928953894312845e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2697, 'grad_norm': 0.7421875, 'learning_rate': 8.92781765656646e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1304, 'grad_norm': 0.82421875, 'learning_rate': 8.926681418820074e-05, 'epoch': 0.51}\n",
      "{'loss': 1.4959, 'grad_norm': 0.58203125, 'learning_rate': 8.925545181073688e-05, 'epoch': 0.51}\n",
      "{'loss': 1.147, 'grad_norm': 0.77734375, 'learning_rate': 8.924408943327303e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2088, 'grad_norm': 0.494140625, 'learning_rate': 8.923272705580917e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2301, 'grad_norm': 0.53515625, 'learning_rate': 8.922136467834529e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0356, 'grad_norm': 0.470703125, 'learning_rate': 8.921000230088144e-05, 'epoch': 0.51}\n",
      "{'loss': 1.5025, 'grad_norm': 0.4921875, 'learning_rate': 8.919863992341758e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1534, 'grad_norm': 1.1484375, 'learning_rate': 8.918727754595372e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3252, 'grad_norm': 0.455078125, 'learning_rate': 8.917591516848986e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3262, 'grad_norm': 0.6875, 'learning_rate': 8.9164552791026e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1008, 'grad_norm': 0.376953125, 'learning_rate': 8.915319041356213e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3114, 'grad_norm': 0.5, 'learning_rate': 8.914182803609827e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0588, 'grad_norm': 0.73046875, 'learning_rate': 8.913046565863442e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1726, 'grad_norm': 0.42578125, 'learning_rate': 8.911910328117056e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2073, 'grad_norm': 0.79296875, 'learning_rate': 8.91077409037067e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0412, 'grad_norm': 0.69921875, 'learning_rate': 8.909637852624284e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2882, 'grad_norm': 0.765625, 'learning_rate': 8.908501614877897e-05, 'epoch': 0.51}\n",
      "{'loss': 1.265, 'grad_norm': 0.8203125, 'learning_rate': 8.907365377131511e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1308, 'grad_norm': 0.431640625, 'learning_rate': 8.906229139385125e-05, 'epoch': 0.51}\n",
      "{'loss': 1.368, 'grad_norm': 0.55859375, 'learning_rate': 8.90509290163874e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1432, 'grad_norm': 0.87890625, 'learning_rate': 8.903956663892354e-05, 'epoch': 0.51}\n",
      "{'loss': 1.201, 'grad_norm': 0.49609375, 'learning_rate': 8.902820426145966e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2789, 'grad_norm': 0.7578125, 'learning_rate': 8.901684188399582e-05, 'epoch': 0.51}\n",
      "{'loss': 1.292, 'grad_norm': 0.51953125, 'learning_rate': 8.900547950653195e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1764, 'grad_norm': 0.578125, 'learning_rate': 8.899411712906809e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0553, 'grad_norm': 0.4453125, 'learning_rate': 8.898275475160423e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3505, 'grad_norm': 0.6484375, 'learning_rate': 8.897139237414037e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1203, 'grad_norm': 0.70703125, 'learning_rate': 8.89600299966765e-05, 'epoch': 0.51}\n",
      "{'loss': 1.314, 'grad_norm': 0.4609375, 'learning_rate': 8.894866761921264e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2554, 'grad_norm': 0.6640625, 'learning_rate': 8.89373052417488e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1279, 'grad_norm': 1.28125, 'learning_rate': 8.892594286428493e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3269, 'grad_norm': 0.490234375, 'learning_rate': 8.891458048682107e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1352, 'grad_norm': 0.94921875, 'learning_rate': 8.890321810935721e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2285, 'grad_norm': 0.484375, 'learning_rate': 8.889185573189335e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2336, 'grad_norm': 0.73828125, 'learning_rate': 8.888049335442948e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1799, 'grad_norm': 0.921875, 'learning_rate': 8.886913097696562e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3194, 'grad_norm': 0.60546875, 'learning_rate': 8.885776859950177e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1125, 'grad_norm': 0.6953125, 'learning_rate': 8.884640622203791e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2119, 'grad_norm': 0.50390625, 'learning_rate': 8.883504384457403e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2636, 'grad_norm': 0.6328125, 'learning_rate': 8.882368146711019e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0957, 'grad_norm': 1.2421875, 'learning_rate': 8.881231908964632e-05, 'epoch': 0.51}\n",
      "{'loss': 1.242, 'grad_norm': 0.79296875, 'learning_rate': 8.880095671218246e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1486, 'grad_norm': 0.546875, 'learning_rate': 8.87895943347186e-05, 'epoch': 0.51}\n",
      "{'loss': 1.204, 'grad_norm': 0.419921875, 'learning_rate': 8.877823195725474e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1926, 'grad_norm': 0.53125, 'learning_rate': 8.876686957979088e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0566, 'grad_norm': 0.373046875, 'learning_rate': 8.875550720232701e-05, 'epoch': 0.51}\n",
      "{'loss': 1.4456, 'grad_norm': 0.48046875, 'learning_rate': 8.874414482486316e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1071, 'grad_norm': 0.76953125, 'learning_rate': 8.87327824473993e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1319, 'grad_norm': 0.59765625, 'learning_rate': 8.872142006993544e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1765, 'grad_norm': 0.5703125, 'learning_rate': 8.871005769247158e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2371, 'grad_norm': 0.94921875, 'learning_rate': 8.869869531500772e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2052, 'grad_norm': 0.50390625, 'learning_rate': 8.868733293754385e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1258, 'grad_norm': 0.48828125, 'learning_rate': 8.867597056007999e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2838, 'grad_norm': 0.494140625, 'learning_rate': 8.866460818261614e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2116, 'grad_norm': 0.6015625, 'learning_rate': 8.865324580515228e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0816, 'grad_norm': 1.3828125, 'learning_rate': 8.86418834276884e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2775, 'grad_norm': 0.5234375, 'learning_rate': 8.863052105022456e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2026, 'grad_norm': 0.62890625, 'learning_rate': 8.86191586727607e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2328, 'grad_norm': 0.447265625, 'learning_rate': 8.860779629529683e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1792, 'grad_norm': 0.55859375, 'learning_rate': 8.859643391783297e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0953, 'grad_norm': 0.8515625, 'learning_rate': 8.858507154036911e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3831, 'grad_norm': 0.61328125, 'learning_rate': 8.857370916290525e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2224, 'grad_norm': 0.67578125, 'learning_rate': 8.856234678544138e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2745, 'grad_norm': 0.578125, 'learning_rate': 8.855098440797753e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1944, 'grad_norm': 0.703125, 'learning_rate': 8.853962203051367e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0614, 'grad_norm': 0.765625, 'learning_rate': 8.852825965304981e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3097, 'grad_norm': 0.5546875, 'learning_rate': 8.851689727558595e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1908, 'grad_norm': 0.90625, 'learning_rate': 8.850553489812209e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3599, 'grad_norm': 0.5234375, 'learning_rate': 8.849417252065822e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2412, 'grad_norm': 0.5625, 'learning_rate': 8.848281014319436e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0864, 'grad_norm': 0.94140625, 'learning_rate': 8.847144776573051e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3304, 'grad_norm': 0.59375, 'learning_rate': 8.846008538826665e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1819, 'grad_norm': 0.703125, 'learning_rate': 8.844872301080278e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2257, 'grad_norm': 0.4296875, 'learning_rate': 8.843736063333893e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1592, 'grad_norm': 0.7109375, 'learning_rate': 8.842599825587506e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1379, 'grad_norm': 0.6640625, 'learning_rate': 8.84146358784112e-05, 'epoch': 0.51}\n",
      "{'loss': 1.374, 'grad_norm': 0.60546875, 'learning_rate': 8.840327350094735e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0533, 'grad_norm': 0.69921875, 'learning_rate': 8.839191112348348e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3112, 'grad_norm': 0.45703125, 'learning_rate': 8.838054874601962e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1816, 'grad_norm': 0.5625, 'learning_rate': 8.836918636855575e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0336, 'grad_norm': 0.578125, 'learning_rate': 8.83578239910919e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2761, 'grad_norm': 0.490234375, 'learning_rate': 8.834646161362804e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0998, 'grad_norm': 0.8046875, 'learning_rate': 8.833509923616418e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1861, 'grad_norm': 0.5546875, 'learning_rate': 8.832373685870032e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2836, 'grad_norm': 0.58984375, 'learning_rate': 8.831237448123646e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0914, 'grad_norm': 0.96484375, 'learning_rate': 8.83010121037726e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2813, 'grad_norm': 0.578125, 'learning_rate': 8.828964972630873e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2036, 'grad_norm': 0.83203125, 'learning_rate': 8.827828734884488e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2096, 'grad_norm': 0.76171875, 'learning_rate': 8.826692497138102e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2488, 'grad_norm': 0.67578125, 'learning_rate': 8.825556259391715e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0657, 'grad_norm': 0.8359375, 'learning_rate': 8.82442002164533e-05, 'epoch': 0.51}\n",
      "{'loss': 1.3024, 'grad_norm': 0.55859375, 'learning_rate': 8.823283783898944e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2172, 'grad_norm': 1.2265625, 'learning_rate': 8.822147546152557e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1312, 'grad_norm': 0.5078125, 'learning_rate': 8.821011308406172e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1518, 'grad_norm': 0.67578125, 'learning_rate': 8.819875070659785e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9906, 'grad_norm': 0.7890625, 'learning_rate': 8.818738832913399e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2966, 'grad_norm': 0.5234375, 'learning_rate': 8.817602595167012e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2302, 'grad_norm': 0.66015625, 'learning_rate': 8.816466357420628e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1285, 'grad_norm': 0.4453125, 'learning_rate': 8.815330119674241e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3057, 'grad_norm': 0.63671875, 'learning_rate': 8.814193881927855e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0254, 'grad_norm': 0.490234375, 'learning_rate': 8.813057644181469e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3077, 'grad_norm': 0.59765625, 'learning_rate': 8.811921406435083e-05, 'epoch': 0.52}\n",
      "{'loss': 1.155, 'grad_norm': 0.75390625, 'learning_rate': 8.810785168688697e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1427, 'grad_norm': 0.466796875, 'learning_rate': 8.80964893094231e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2721, 'grad_norm': 0.609375, 'learning_rate': 8.808512693195925e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0759, 'grad_norm': 0.828125, 'learning_rate': 8.807376455449539e-05, 'epoch': 0.52}\n",
      "{'loss': 1.393, 'grad_norm': 0.57421875, 'learning_rate': 8.806240217703152e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2507, 'grad_norm': 1.015625, 'learning_rate': 8.805103979956767e-05, 'epoch': 0.52}\n",
      "{'loss': 1.282, 'grad_norm': 0.5625, 'learning_rate': 8.80396774221038e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1989, 'grad_norm': 0.640625, 'learning_rate': 8.802831504463994e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0517, 'grad_norm': 1.0859375, 'learning_rate': 8.80169526671761e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2745, 'grad_norm': 0.7734375, 'learning_rate': 8.800559028971222e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1634, 'grad_norm': 0.73828125, 'learning_rate': 8.799422791224836e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0974, 'grad_norm': 0.5546875, 'learning_rate': 8.79828655347845e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2249, 'grad_norm': 0.63671875, 'learning_rate': 8.797150315732065e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2093, 'grad_norm': 1.0546875, 'learning_rate': 8.796014077985678e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2974, 'grad_norm': 0.66015625, 'learning_rate': 8.794877840239292e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2655, 'grad_norm': 1.0859375, 'learning_rate': 8.793741602492906e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1994, 'grad_norm': 0.46875, 'learning_rate': 8.79260536474652e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3739, 'grad_norm': 0.953125, 'learning_rate': 8.791469127000134e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0907, 'grad_norm': 0.81640625, 'learning_rate': 8.790332889253749e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3295, 'grad_norm': 0.75, 'learning_rate': 8.789196651507363e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0882, 'grad_norm': 0.9609375, 'learning_rate': 8.788060413760976e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0629, 'grad_norm': 0.478515625, 'learning_rate': 8.786924176014589e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2726, 'grad_norm': 0.5703125, 'learning_rate': 8.785787938268204e-05, 'epoch': 0.52}\n",
      "{'loss': 1.06, 'grad_norm': 0.7890625, 'learning_rate': 8.784651700521818e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2815, 'grad_norm': 0.734375, 'learning_rate': 8.783515462775431e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2023, 'grad_norm': 0.72265625, 'learning_rate': 8.782379225029047e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0844, 'grad_norm': 0.59375, 'learning_rate': 8.781242987282659e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2477, 'grad_norm': 1.109375, 'learning_rate': 8.780106749536273e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0004, 'grad_norm': 0.546875, 'learning_rate': 8.778970511789887e-05, 'epoch': 0.52}\n",
      "{'loss': 1.26, 'grad_norm': 0.51953125, 'learning_rate': 8.777834274043502e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9868, 'grad_norm': 0.90625, 'learning_rate': 8.776698036297116e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3317, 'grad_norm': 0.53515625, 'learning_rate': 8.775561798550729e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1838, 'grad_norm': 0.61328125, 'learning_rate': 8.774425560804343e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0211, 'grad_norm': 0.6640625, 'learning_rate': 8.773289323057957e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4329, 'grad_norm': 0.578125, 'learning_rate': 8.77215308531157e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2101, 'grad_norm': 0.640625, 'learning_rate': 8.771016847565186e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2306, 'grad_norm': 0.53515625, 'learning_rate': 8.7698806098188e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2516, 'grad_norm': 0.828125, 'learning_rate': 8.768744372072413e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1256, 'grad_norm': 1.421875, 'learning_rate': 8.767608134326026e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1768, 'grad_norm': 0.67578125, 'learning_rate': 8.766471896579641e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1212, 'grad_norm': 0.91796875, 'learning_rate': 8.765335658833255e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3066, 'grad_norm': 0.4921875, 'learning_rate': 8.764199421086869e-05, 'epoch': 0.52}\n",
      "{'loss': 1.307, 'grad_norm': 0.98828125, 'learning_rate': 8.763063183340484e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1189, 'grad_norm': 0.83203125, 'learning_rate': 8.761926945594096e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3663, 'grad_norm': 0.5859375, 'learning_rate': 8.76079070784771e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1937, 'grad_norm': 0.8046875, 'learning_rate': 8.759654470101324e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2549, 'grad_norm': 0.859375, 'learning_rate': 8.758518232354939e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2222, 'grad_norm': 0.5234375, 'learning_rate': 8.757381994608553e-05, 'epoch': 0.52}\n",
      "{'loss': 0.948, 'grad_norm': 0.9140625, 'learning_rate': 8.756245756862166e-05, 'epoch': 0.52}\n",
      "{'loss': 1.391, 'grad_norm': 0.490234375, 'learning_rate': 8.75510951911578e-05, 'epoch': 0.52}\n",
      "{'loss': 1.219, 'grad_norm': 0.7109375, 'learning_rate': 8.753973281369394e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1756, 'grad_norm': 0.47265625, 'learning_rate': 8.752837043623008e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3048, 'grad_norm': 0.5, 'learning_rate': 8.751700805876623e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0548, 'grad_norm': 1.2578125, 'learning_rate': 8.750564568130237e-05, 'epoch': 0.52}\n",
      "{'loss': 1.377, 'grad_norm': 0.4921875, 'learning_rate': 8.74942833038385e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1213, 'grad_norm': 0.8125, 'learning_rate': 8.748292092637463e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2771, 'grad_norm': 0.423828125, 'learning_rate': 8.747155854891078e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2614, 'grad_norm': 0.96484375, 'learning_rate': 8.746019617144692e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0624, 'grad_norm': 0.8984375, 'learning_rate': 8.744883379398306e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3574, 'grad_norm': 0.6875, 'learning_rate': 8.743747141651921e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1657, 'grad_norm': 0.859375, 'learning_rate': 8.742610903905533e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0942, 'grad_norm': 0.4296875, 'learning_rate': 8.741474666159147e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2409, 'grad_norm': 0.6875, 'learning_rate': 8.740338428412761e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0779, 'grad_norm': 1.25, 'learning_rate': 8.739202190666376e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2535, 'grad_norm': 0.640625, 'learning_rate': 8.73806595291999e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1779, 'grad_norm': 2.09375, 'learning_rate': 8.736929715173603e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2204, 'grad_norm': 0.421875, 'learning_rate': 8.735793477427217e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3367, 'grad_norm': 0.68359375, 'learning_rate': 8.734657239680831e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1251, 'grad_norm': 1.125, 'learning_rate': 8.733521001934445e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4176, 'grad_norm': 0.5234375, 'learning_rate': 8.73238476418806e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1479, 'grad_norm': 0.79296875, 'learning_rate': 8.731248526441674e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2215, 'grad_norm': 0.5078125, 'learning_rate': 8.730112288695288e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1931, 'grad_norm': 0.69140625, 'learning_rate': 8.7289760509489e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0608, 'grad_norm': 0.609375, 'learning_rate': 8.727839813202515e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3405, 'grad_norm': 0.73046875, 'learning_rate': 8.726703575456129e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1825, 'grad_norm': 1.0390625, 'learning_rate': 8.725567337709743e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1276, 'grad_norm': 0.56640625, 'learning_rate': 8.724431099963358e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1663, 'grad_norm': 0.66015625, 'learning_rate': 8.72329486221697e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2145, 'grad_norm': 0.7890625, 'learning_rate': 8.722158624470584e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3791, 'grad_norm': 0.6953125, 'learning_rate': 8.721022386724199e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1939, 'grad_norm': 0.7421875, 'learning_rate': 8.719886148977813e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2063, 'grad_norm': 0.62109375, 'learning_rate': 8.718749911231427e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2429, 'grad_norm': 0.51171875, 'learning_rate': 8.71761367348504e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0814, 'grad_norm': 0.609375, 'learning_rate': 8.716477435738654e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4218, 'grad_norm': 0.6015625, 'learning_rate': 8.715341197992268e-05, 'epoch': 0.52}\n",
      "{'loss': 1.175, 'grad_norm': 0.6796875, 'learning_rate': 8.714204960245882e-05, 'epoch': 0.52}\n",
      "{'loss': 1.225, 'grad_norm': 0.458984375, 'learning_rate': 8.713068722499497e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1743, 'grad_norm': 0.734375, 'learning_rate': 8.711932484753111e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0922, 'grad_norm': 0.8984375, 'learning_rate': 8.710796247006725e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2478, 'grad_norm': 0.451171875, 'learning_rate': 8.709660009260337e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9959, 'grad_norm': 0.73046875, 'learning_rate': 8.708523771513952e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2632, 'grad_norm': 0.44140625, 'learning_rate': 8.707387533767566e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0626, 'grad_norm': 0.71875, 'learning_rate': 8.70625129602118e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1293, 'grad_norm': 0.31640625, 'learning_rate': 8.705115058274795e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3337, 'grad_norm': 0.54296875, 'learning_rate': 8.703978820528407e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1926, 'grad_norm': 0.609375, 'learning_rate': 8.702842582782021e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1752, 'grad_norm': 0.6640625, 'learning_rate': 8.701706345035636e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1793, 'grad_norm': 0.5859375, 'learning_rate': 8.70057010728925e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9104, 'grad_norm': 0.94140625, 'learning_rate': 8.699433869542864e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3698, 'grad_norm': 0.5625, 'learning_rate': 8.698297631796478e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2243, 'grad_norm': 0.60546875, 'learning_rate': 8.697161394050091e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0792, 'grad_norm': 0.67578125, 'learning_rate': 8.696025156303705e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3029, 'grad_norm': 0.82421875, 'learning_rate': 8.694888918557319e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0257, 'grad_norm': 0.52734375, 'learning_rate': 8.693752680810934e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3344, 'grad_norm': 0.5625, 'learning_rate': 8.692616443064548e-05, 'epoch': 0.52}\n",
      "{'loss': 1.231, 'grad_norm': 0.85546875, 'learning_rate': 8.691480205318162e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2887, 'grad_norm': 0.451171875, 'learning_rate': 8.690343967571774e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2317, 'grad_norm': 0.65625, 'learning_rate': 8.689207729825389e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9917, 'grad_norm': 0.84765625, 'learning_rate': 8.688071492079003e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3621, 'grad_norm': 0.67578125, 'learning_rate': 8.686935254332617e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0507, 'grad_norm': 0.734375, 'learning_rate': 8.685799016586232e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2306, 'grad_norm': 0.455078125, 'learning_rate': 8.684662778839844e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2561, 'grad_norm': 0.88671875, 'learning_rate': 8.683526541093458e-05, 'epoch': 0.52}\n",
      "{'loss': 1.195, 'grad_norm': 0.578125, 'learning_rate': 8.682390303347073e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2652, 'grad_norm': 0.5234375, 'learning_rate': 8.681254065600687e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0349, 'grad_norm': 0.875, 'learning_rate': 8.680117827854301e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1836, 'grad_norm': 0.62109375, 'learning_rate': 8.678981590107915e-05, 'epoch': 0.52}\n",
      "{'loss': 1.317, 'grad_norm': 0.59375, 'learning_rate': 8.677845352361528e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1418, 'grad_norm': 0.9921875, 'learning_rate': 8.676709114615142e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4232, 'grad_norm': 0.8515625, 'learning_rate': 8.675572876868756e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1889, 'grad_norm': 1.0078125, 'learning_rate': 8.674436639122371e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1011, 'grad_norm': 0.484375, 'learning_rate': 8.673300401375985e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2235, 'grad_norm': 0.546875, 'learning_rate': 8.672164163629599e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0995, 'grad_norm': 0.68359375, 'learning_rate': 8.671027925883211e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3023, 'grad_norm': 0.56640625, 'learning_rate': 8.669891688136826e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1272, 'grad_norm': 0.66796875, 'learning_rate': 8.66875545039044e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1754, 'grad_norm': 0.50390625, 'learning_rate': 8.667619212644054e-05, 'epoch': 0.52}\n",
      "{'loss': 1.227, 'grad_norm': 0.64453125, 'learning_rate': 8.666482974897669e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1555, 'grad_norm': 0.70703125, 'learning_rate': 8.665346737151281e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3028, 'grad_norm': 0.453125, 'learning_rate': 8.664210499404895e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1534, 'grad_norm': 0.8671875, 'learning_rate': 8.66307426165851e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1737, 'grad_norm': 0.455078125, 'learning_rate': 8.661938023912124e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2282, 'grad_norm': 0.71875, 'learning_rate': 8.660801786165738e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0181, 'grad_norm': 1.0703125, 'learning_rate': 8.659665548419352e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3396, 'grad_norm': 0.53125, 'learning_rate': 8.658529310672965e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2032, 'grad_norm': 0.77734375, 'learning_rate': 8.657393072926579e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2558, 'grad_norm': 0.400390625, 'learning_rate': 8.656256835180193e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1747, 'grad_norm': 0.68359375, 'learning_rate': 8.655120597433808e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2154, 'grad_norm': 0.7734375, 'learning_rate': 8.653984359687422e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4196, 'grad_norm': 0.5078125, 'learning_rate': 8.652848121941036e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1194, 'grad_norm': 0.91796875, 'learning_rate': 8.65171188419465e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2996, 'grad_norm': 0.486328125, 'learning_rate': 8.650575646448263e-05, 'epoch': 0.52}\n",
      "{'loss': 1.286, 'grad_norm': 0.53515625, 'learning_rate': 8.649439408701877e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9713, 'grad_norm': 1.0078125, 'learning_rate': 8.648303170955491e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2507, 'grad_norm': 0.703125, 'learning_rate': 8.647166933209106e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1477, 'grad_norm': 1.015625, 'learning_rate': 8.646030695462718e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2142, 'grad_norm': 0.43359375, 'learning_rate': 8.644894457716332e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2266, 'grad_norm': 0.56640625, 'learning_rate': 8.643758219969947e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1637, 'grad_norm': 0.88671875, 'learning_rate': 8.642621982223561e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3545, 'grad_norm': 0.56640625, 'learning_rate': 8.641485744477175e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3233, 'grad_norm': 0.86328125, 'learning_rate': 8.640349506730789e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1016, 'grad_norm': 0.44921875, 'learning_rate': 8.639213268984403e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1891, 'grad_norm': 0.609375, 'learning_rate': 8.638077031238016e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0605, 'grad_norm': 1.0859375, 'learning_rate': 8.63694079349163e-05, 'epoch': 0.52}\n",
      "{'loss': 1.348, 'grad_norm': 0.7109375, 'learning_rate': 8.635804555745245e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0473, 'grad_norm': 0.6171875, 'learning_rate': 8.634668317998859e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1118, 'grad_norm': 0.416015625, 'learning_rate': 8.633532080252473e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1938, 'grad_norm': 0.8046875, 'learning_rate': 8.632395842506087e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9375, 'grad_norm': 0.61328125, 'learning_rate': 8.6312596047597e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4109, 'grad_norm': 0.7734375, 'learning_rate': 8.630123367013314e-05, 'epoch': 0.52}\n",
      "{'loss': 1.152, 'grad_norm': 0.7421875, 'learning_rate': 8.628987129266928e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2324, 'grad_norm': 0.474609375, 'learning_rate': 8.627850891520543e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1896, 'grad_norm': 0.5234375, 'learning_rate': 8.626714653774156e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9395, 'grad_norm': 0.546875, 'learning_rate': 8.625578416027769e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2549, 'grad_norm': 0.62890625, 'learning_rate': 8.624442178281384e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2707, 'grad_norm': 0.76171875, 'learning_rate': 8.623305940534998e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2177, 'grad_norm': 0.462890625, 'learning_rate': 8.622169702788612e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1248, 'grad_norm': 0.65625, 'learning_rate': 8.621033465042226e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0902, 'grad_norm': 0.455078125, 'learning_rate': 8.61989722729584e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3809, 'grad_norm': 0.57421875, 'learning_rate': 8.618760989549453e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1252, 'grad_norm': 0.98828125, 'learning_rate': 8.617624751803067e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2907, 'grad_norm': 0.4453125, 'learning_rate': 8.616488514056682e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1658, 'grad_norm': 0.6484375, 'learning_rate': 8.615352276310296e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1316, 'grad_norm': 1.0078125, 'learning_rate': 8.61421603856391e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2618, 'grad_norm': 0.54296875, 'learning_rate': 8.613079800817524e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0365, 'grad_norm': 0.71484375, 'learning_rate': 8.611943563071137e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2122, 'grad_norm': 0.484375, 'learning_rate': 8.610807325324751e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0393, 'grad_norm': 0.6796875, 'learning_rate': 8.609671087578365e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1571, 'grad_norm': 0.9765625, 'learning_rate': 8.60853484983198e-05, 'epoch': 0.52}\n",
      "{'loss': 1.3582, 'grad_norm': 0.640625, 'learning_rate': 8.607398612085593e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2053, 'grad_norm': 0.8984375, 'learning_rate': 8.606262374339206e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2257, 'grad_norm': 0.431640625, 'learning_rate': 8.605126136592822e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1032, 'grad_norm': 0.6015625, 'learning_rate': 8.603989898846435e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9754, 'grad_norm': 0.828125, 'learning_rate': 8.602853661100049e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2712, 'grad_norm': 0.58203125, 'learning_rate': 8.601717423353663e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1802, 'grad_norm': 0.609375, 'learning_rate': 8.600581185607277e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2866, 'grad_norm': 0.47265625, 'learning_rate': 8.59944494786089e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1994, 'grad_norm': 0.859375, 'learning_rate': 8.598308710114504e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1447, 'grad_norm': 1.046875, 'learning_rate': 8.59717247236812e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2958, 'grad_norm': 0.5, 'learning_rate': 8.596036234621733e-05, 'epoch': 0.52}\n",
      "{'loss': 1.19, 'grad_norm': 0.90234375, 'learning_rate': 8.594899996875347e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2264, 'grad_norm': 0.396484375, 'learning_rate': 8.593763759128961e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1811, 'grad_norm': 0.5859375, 'learning_rate': 8.592627521382575e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0523, 'grad_norm': 1.0546875, 'learning_rate': 8.591491283636188e-05, 'epoch': 0.52}\n",
      "{'loss': 1.361, 'grad_norm': 0.4921875, 'learning_rate': 8.590355045889802e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2363, 'grad_norm': 0.6875, 'learning_rate': 8.589218808143417e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2851, 'grad_norm': 0.494140625, 'learning_rate': 8.58808257039703e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2996, 'grad_norm': 0.83984375, 'learning_rate': 8.586946332650643e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1864, 'grad_norm': 1.25, 'learning_rate': 8.585810094904259e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3419, 'grad_norm': 0.6171875, 'learning_rate': 8.584673857157872e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2841, 'grad_norm': 0.62109375, 'learning_rate': 8.583537619411486e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2983, 'grad_norm': 0.54296875, 'learning_rate': 8.5824013816651e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1203, 'grad_norm': 0.6015625, 'learning_rate': 8.581265143918714e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0701, 'grad_norm': 0.66015625, 'learning_rate': 8.580128906172328e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2831, 'grad_norm': 0.51953125, 'learning_rate': 8.578992668425941e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1234, 'grad_norm': 0.62890625, 'learning_rate': 8.577856430679556e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1571, 'grad_norm': 0.44140625, 'learning_rate': 8.57672019293317e-05, 'epoch': 0.53}\n",
      "{'loss': 1.236, 'grad_norm': 0.70703125, 'learning_rate': 8.575583955186784e-05, 'epoch': 0.53}\n",
      "{'loss': 1.099, 'grad_norm': 1.1640625, 'learning_rate': 8.574447717440398e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4637, 'grad_norm': 0.7578125, 'learning_rate': 8.573311479694012e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0821, 'grad_norm': 0.62109375, 'learning_rate': 8.572175241947625e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2241, 'grad_norm': 0.484375, 'learning_rate': 8.571039004201239e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2532, 'grad_norm': 0.4921875, 'learning_rate': 8.569902766454854e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0745, 'grad_norm': 0.9296875, 'learning_rate': 8.568766528708467e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3987, 'grad_norm': 0.51953125, 'learning_rate': 8.56763029096208e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0756, 'grad_norm': 0.828125, 'learning_rate': 8.566494053215696e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2102, 'grad_norm': 0.52734375, 'learning_rate': 8.56535781546931e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2147, 'grad_norm': 0.59765625, 'learning_rate': 8.564221577722923e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1342, 'grad_norm': 4.0, 'learning_rate': 8.563085339976537e-05, 'epoch': 0.53}\n",
      "{'loss': 1.31, 'grad_norm': 0.53515625, 'learning_rate': 8.561949102230151e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0976, 'grad_norm': 0.53125, 'learning_rate': 8.560812864483765e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1423, 'grad_norm': 0.462890625, 'learning_rate': 8.559676626737378e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1297, 'grad_norm': 0.6796875, 'learning_rate': 8.558540388990994e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0864, 'grad_norm': 1.21875, 'learning_rate': 8.557404151244607e-05, 'epoch': 0.53}\n",
      "{'loss': 1.274, 'grad_norm': 0.47265625, 'learning_rate': 8.556267913498221e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1225, 'grad_norm': 1.1953125, 'learning_rate': 8.555131675751835e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3295, 'grad_norm': 0.45703125, 'learning_rate': 8.553995438005449e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2335, 'grad_norm': 0.890625, 'learning_rate': 8.552859200259062e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0571, 'grad_norm': 1.0625, 'learning_rate': 8.551722962512676e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1783, 'grad_norm': 0.64453125, 'learning_rate': 8.550586724766291e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1908, 'grad_norm': 0.65625, 'learning_rate': 8.549450487019904e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1862, 'grad_norm': 0.443359375, 'learning_rate': 8.548314249273518e-05, 'epoch': 0.53}\n",
      "{'loss': 1.251, 'grad_norm': 0.51953125, 'learning_rate': 8.547178011527133e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0059, 'grad_norm': 0.71875, 'learning_rate': 8.546041773780746e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2833, 'grad_norm': 0.578125, 'learning_rate': 8.54490553603436e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2937, 'grad_norm': 0.84765625, 'learning_rate': 8.543769298287974e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1343, 'grad_norm': 0.412109375, 'learning_rate': 8.542633060541588e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1832, 'grad_norm': 0.71484375, 'learning_rate': 8.541496822795202e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0995, 'grad_norm': 1.015625, 'learning_rate': 8.540360585048815e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2856, 'grad_norm': 0.466796875, 'learning_rate': 8.53922434730243e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2167, 'grad_norm': 0.76171875, 'learning_rate': 8.538088109556044e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1541, 'grad_norm': 0.5078125, 'learning_rate': 8.536951871809658e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2268, 'grad_norm': 0.93359375, 'learning_rate': 8.535815634063272e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0663, 'grad_norm': 1.0234375, 'learning_rate': 8.534679396316886e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3582, 'grad_norm': 0.76171875, 'learning_rate': 8.5335431585705e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2275, 'grad_norm': 0.60546875, 'learning_rate': 8.532406920824115e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2996, 'grad_norm': 0.494140625, 'learning_rate': 8.531270683077728e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2289, 'grad_norm': 0.73828125, 'learning_rate': 8.530134445331341e-05, 'epoch': 0.53}\n",
      "{'loss': 1.072, 'grad_norm': 0.494140625, 'learning_rate': 8.528998207584955e-05, 'epoch': 0.53}\n",
      "{'loss': 1.388, 'grad_norm': 0.5546875, 'learning_rate': 8.52786196983857e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1059, 'grad_norm': 0.859375, 'learning_rate': 8.526725732092184e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2623, 'grad_norm': 0.5625, 'learning_rate': 8.525589494345797e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2071, 'grad_norm': 0.69921875, 'learning_rate': 8.524453256599411e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1557, 'grad_norm': 0.78515625, 'learning_rate': 8.523317018853025e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3112, 'grad_norm': 0.458984375, 'learning_rate': 8.522180781106639e-05, 'epoch': 0.53}\n",
      "{'loss': 1.132, 'grad_norm': 0.73828125, 'learning_rate': 8.521044543360252e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1344, 'grad_norm': 0.5, 'learning_rate': 8.519908305613868e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3194, 'grad_norm': 0.83203125, 'learning_rate': 8.518772067867481e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0517, 'grad_norm': 1.421875, 'learning_rate': 8.517635830121095e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3479, 'grad_norm': 0.49609375, 'learning_rate': 8.516499592374709e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2364, 'grad_norm': 0.67578125, 'learning_rate': 8.515363354628323e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2878, 'grad_norm': 0.482421875, 'learning_rate': 8.514227116881937e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2527, 'grad_norm': 0.6171875, 'learning_rate': 8.513090879135552e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9772, 'grad_norm': 0.431640625, 'learning_rate': 8.511954641389165e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2565, 'grad_norm': 0.66015625, 'learning_rate': 8.510818403642778e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1249, 'grad_norm': 0.53515625, 'learning_rate': 8.509682165896392e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2279, 'grad_norm': 0.5859375, 'learning_rate': 8.508545928150007e-05, 'epoch': 0.53}\n",
      "{'loss': 1.194, 'grad_norm': 0.474609375, 'learning_rate': 8.50740969040362e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0354, 'grad_norm': 0.91015625, 'learning_rate': 8.506273452657234e-05, 'epoch': 0.53}\n",
      "{'loss': 1.5205, 'grad_norm': 0.4453125, 'learning_rate': 8.505137214910848e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3339, 'grad_norm': 0.74609375, 'learning_rate': 8.504000977164462e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1663, 'grad_norm': 0.55859375, 'learning_rate': 8.502864739418076e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2048, 'grad_norm': 0.58203125, 'learning_rate': 8.50172850167169e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0918, 'grad_norm': 1.3359375, 'learning_rate': 8.500592263925305e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3134, 'grad_norm': 0.478515625, 'learning_rate': 8.499456026178918e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1452, 'grad_norm': 1.109375, 'learning_rate': 8.498319788432532e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2144, 'grad_norm': 0.54296875, 'learning_rate': 8.497183550686146e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2265, 'grad_norm': 0.71484375, 'learning_rate': 8.49604731293976e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0068, 'grad_norm': 1.4765625, 'learning_rate': 8.494911075193374e-05, 'epoch': 0.53}\n",
      "{'loss': 1.406, 'grad_norm': 0.5859375, 'learning_rate': 8.493774837446989e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1191, 'grad_norm': 0.67578125, 'learning_rate': 8.492638599700603e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2575, 'grad_norm': 0.6328125, 'learning_rate': 8.491502361954215e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3507, 'grad_norm': 0.5234375, 'learning_rate': 8.490366124207829e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0012, 'grad_norm': 0.9296875, 'learning_rate': 8.489229886461444e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.273, 'grad_norm': 0.45703125, 'learning_rate': 8.488093648715058e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0801, 'grad_norm': 0.80078125, 'learning_rate': 8.486957410968671e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1891, 'grad_norm': 0.49609375, 'learning_rate': 8.485821173222285e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3394, 'grad_norm': 0.640625, 'learning_rate': 8.484684935475899e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0926, 'grad_norm': 0.6015625, 'learning_rate': 8.483548697729513e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2552, 'grad_norm': 0.55859375, 'learning_rate': 8.482412459983127e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1058, 'grad_norm': 0.62890625, 'learning_rate': 8.481276222236742e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1178, 'grad_norm': 0.42578125, 'learning_rate': 8.480139984490356e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2527, 'grad_norm': 0.498046875, 'learning_rate': 8.47900374674397e-05, 'epoch': 0.53}\n",
      "{'loss': 1.195, 'grad_norm': 0.703125, 'learning_rate': 8.477867508997583e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2734, 'grad_norm': 0.486328125, 'learning_rate': 8.476731271251197e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1771, 'grad_norm': 0.77734375, 'learning_rate': 8.475595033504811e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1627, 'grad_norm': 0.39453125, 'learning_rate': 8.474458795758426e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2114, 'grad_norm': 0.52734375, 'learning_rate': 8.47332255801204e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9581, 'grad_norm': 0.392578125, 'learning_rate': 8.472186320265652e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2808, 'grad_norm': 0.5546875, 'learning_rate': 8.471050082519266e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1792, 'grad_norm': 0.6796875, 'learning_rate': 8.469913844772881e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1946, 'grad_norm': 0.5234375, 'learning_rate': 8.468777607026495e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1401, 'grad_norm': 0.46875, 'learning_rate': 8.467641369280109e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0745, 'grad_norm': 1.171875, 'learning_rate': 8.466505131533722e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3172, 'grad_norm': 0.74609375, 'learning_rate': 8.465368893787336e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0684, 'grad_norm': 0.703125, 'learning_rate': 8.46423265604095e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1288, 'grad_norm': 0.515625, 'learning_rate': 8.463096418294565e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1827, 'grad_norm': 0.59375, 'learning_rate': 8.461960180548179e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1677, 'grad_norm': 0.51171875, 'learning_rate': 8.460823942801793e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2063, 'grad_norm': 0.7265625, 'learning_rate': 8.459687705055406e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1271, 'grad_norm': 0.68359375, 'learning_rate': 8.45855146730902e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1376, 'grad_norm': 0.50390625, 'learning_rate': 8.457415229562634e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2279, 'grad_norm': 0.51171875, 'learning_rate': 8.456278991816248e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0432, 'grad_norm': 0.92578125, 'learning_rate': 8.455142754069863e-05, 'epoch': 0.53}\n",
      "{'loss': 1.259, 'grad_norm': 0.68359375, 'learning_rate': 8.454006516323477e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1592, 'grad_norm': 0.94140625, 'learning_rate': 8.452870278577089e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1781, 'grad_norm': 0.53515625, 'learning_rate': 8.451734040830703e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1698, 'grad_norm': 0.56640625, 'learning_rate': 8.450597803084318e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9974, 'grad_norm': 0.400390625, 'learning_rate': 8.449461565337932e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2005, 'grad_norm': 0.55078125, 'learning_rate': 8.448325327591546e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2138, 'grad_norm': 0.796875, 'learning_rate': 8.44718908984516e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4099, 'grad_norm': 0.58984375, 'learning_rate': 8.446052852098773e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1448, 'grad_norm': 0.60546875, 'learning_rate': 8.444916614352387e-05, 'epoch': 0.53}\n",
      "{'loss': 0.922, 'grad_norm': 0.89453125, 'learning_rate': 8.443780376606002e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3378, 'grad_norm': 0.515625, 'learning_rate': 8.442644138859616e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1943, 'grad_norm': 1.4921875, 'learning_rate': 8.44150790111323e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2916, 'grad_norm': 0.44921875, 'learning_rate': 8.440371663366843e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0872, 'grad_norm': 0.5390625, 'learning_rate': 8.439235425620457e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1311, 'grad_norm': 1.265625, 'learning_rate': 8.438099187874071e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3368, 'grad_norm': 0.5859375, 'learning_rate': 8.436962950127685e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1383, 'grad_norm': 0.58203125, 'learning_rate': 8.4358267123813e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2227, 'grad_norm': 0.53125, 'learning_rate': 8.434690474634914e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2048, 'grad_norm': 1.0078125, 'learning_rate': 8.433554236888526e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1362, 'grad_norm': 0.92578125, 'learning_rate': 8.43241799914214e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1412, 'grad_norm': 0.51953125, 'learning_rate': 8.431281761395755e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0877, 'grad_norm': 0.73046875, 'learning_rate': 8.430145523649369e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1809, 'grad_norm': 0.439453125, 'learning_rate': 8.429009285902983e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2581, 'grad_norm': 0.5859375, 'learning_rate': 8.427873048156596e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0466, 'grad_norm': 1.6328125, 'learning_rate': 8.42673681041021e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3074, 'grad_norm': 0.490234375, 'learning_rate': 8.425600572663824e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1842, 'grad_norm': 0.62890625, 'learning_rate': 8.424464334917439e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1562, 'grad_norm': 0.412109375, 'learning_rate': 8.423328097171053e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2039, 'grad_norm': 0.6015625, 'learning_rate': 8.422191859424667e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0124, 'grad_norm': 0.828125, 'learning_rate': 8.42105562167828e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2637, 'grad_norm': 0.546875, 'learning_rate': 8.419919383931894e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1002, 'grad_norm': 1.03125, 'learning_rate': 8.418783146185508e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2147, 'grad_norm': 0.484375, 'learning_rate': 8.417646908439122e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2403, 'grad_norm': 0.546875, 'learning_rate': 8.416510670692737e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0907, 'grad_norm': 1.1015625, 'learning_rate': 8.415374432946351e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2822, 'grad_norm': 0.498046875, 'learning_rate': 8.414238195199963e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1744, 'grad_norm': 1.15625, 'learning_rate': 8.413101957453578e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0626, 'grad_norm': 0.4765625, 'learning_rate': 8.411965719707192e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2332, 'grad_norm': 0.59375, 'learning_rate': 8.410829481960806e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0016, 'grad_norm': 0.79296875, 'learning_rate': 8.40969324421442e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2949, 'grad_norm': 0.515625, 'learning_rate': 8.408557006468034e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1611, 'grad_norm': 0.7109375, 'learning_rate': 8.407420768721647e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0859, 'grad_norm': 0.56640625, 'learning_rate': 8.406284530975261e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1888, 'grad_norm': 0.51953125, 'learning_rate': 8.405148293228876e-05, 'epoch': 0.53}\n",
      "{'loss': 1.04, 'grad_norm': 0.98046875, 'learning_rate': 8.40401205548249e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3254, 'grad_norm': 0.5078125, 'learning_rate': 8.402875817736104e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0231, 'grad_norm': 0.83984375, 'learning_rate': 8.401739579989718e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2635, 'grad_norm': 0.490234375, 'learning_rate': 8.400603342243331e-05, 'epoch': 0.53}\n",
      "{'loss': 1.221, 'grad_norm': 0.5546875, 'learning_rate': 8.399467104496945e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0552, 'grad_norm': 0.75, 'learning_rate': 8.398330866750559e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4078, 'grad_norm': 0.609375, 'learning_rate': 8.397194629004174e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1989, 'grad_norm': 0.7109375, 'learning_rate': 8.396058391257788e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2169, 'grad_norm': 0.46875, 'learning_rate': 8.3949221535114e-05, 'epoch': 0.53}\n",
      "{'loss': 1.315, 'grad_norm': 0.55078125, 'learning_rate': 8.393785915765015e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0276, 'grad_norm': 0.76953125, 'learning_rate': 8.392649678018629e-05, 'epoch': 0.53}\n",
      "{'loss': 1.416, 'grad_norm': 0.6484375, 'learning_rate': 8.391513440272243e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2181, 'grad_norm': 0.66015625, 'learning_rate': 8.390377202525857e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1942, 'grad_norm': 0.423828125, 'learning_rate': 8.38924096477947e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0213, 'grad_norm': 0.6015625, 'learning_rate': 8.388104727033084e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0178, 'grad_norm': 0.7109375, 'learning_rate': 8.386968489286698e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2586, 'grad_norm': 0.58984375, 'learning_rate': 8.385832251540313e-05, 'epoch': 0.53}\n",
      "{'loss': 1.072, 'grad_norm': 0.6171875, 'learning_rate': 8.384696013793927e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2625, 'grad_norm': 0.51171875, 'learning_rate': 8.383559776047541e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1773, 'grad_norm': 0.58203125, 'learning_rate': 8.382423538301155e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1594, 'grad_norm': 0.7734375, 'learning_rate': 8.381287300554768e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4197, 'grad_norm': 0.486328125, 'learning_rate': 8.380151062808382e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2328, 'grad_norm': 0.6328125, 'learning_rate': 8.379014825061996e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2017, 'grad_norm': 0.482421875, 'learning_rate': 8.377878587315611e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1042, 'grad_norm': 0.68359375, 'learning_rate': 8.376742349569225e-05, 'epoch': 0.53}\n",
      "{'loss': 1.105, 'grad_norm': 1.59375, 'learning_rate': 8.375606111822837e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4188, 'grad_norm': 0.455078125, 'learning_rate': 8.374469874076452e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1002, 'grad_norm': 0.6171875, 'learning_rate': 8.373333636330066e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1847, 'grad_norm': 0.48828125, 'learning_rate': 8.37219739858368e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3124, 'grad_norm': 0.6015625, 'learning_rate': 8.371061160837294e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0425, 'grad_norm': 1.3828125, 'learning_rate': 8.369924923090908e-05, 'epoch': 0.53}\n",
      "{'loss': 1.26, 'grad_norm': 0.5234375, 'learning_rate': 8.368788685344521e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0918, 'grad_norm': 0.796875, 'learning_rate': 8.367652447598135e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2348, 'grad_norm': 0.5, 'learning_rate': 8.36651620985175e-05, 'epoch': 0.53}\n",
      "{'loss': 1.238, 'grad_norm': 0.66015625, 'learning_rate': 8.365379972105364e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0305, 'grad_norm': 0.66796875, 'learning_rate': 8.364243734358978e-05, 'epoch': 0.53}\n",
      "{'loss': 1.264, 'grad_norm': 0.59375, 'learning_rate': 8.363107496612592e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1478, 'grad_norm': 0.88671875, 'learning_rate': 8.361971258866205e-05, 'epoch': 0.53}\n",
      "{'loss': 1.2387, 'grad_norm': 0.447265625, 'learning_rate': 8.360835021119819e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2272, 'grad_norm': 0.5625, 'learning_rate': 8.359698783373433e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9998, 'grad_norm': 1.0625, 'learning_rate': 8.358562545627048e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2935, 'grad_norm': 0.5078125, 'learning_rate': 8.357426307880662e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1205, 'grad_norm': 0.76953125, 'learning_rate': 8.356290070134274e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0764, 'grad_norm': 0.5234375, 'learning_rate': 8.35515383238789e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1636, 'grad_norm': 0.74609375, 'learning_rate': 8.354017594641503e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0954, 'grad_norm': 0.8046875, 'learning_rate': 8.352881356895117e-05, 'epoch': 0.54}\n",
      "{'loss': 1.485, 'grad_norm': 0.53515625, 'learning_rate': 8.351745119148731e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2421, 'grad_norm': 0.56640625, 'learning_rate': 8.350608881402345e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1731, 'grad_norm': 0.38671875, 'learning_rate': 8.349472643655958e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2779, 'grad_norm': 0.75390625, 'learning_rate': 8.348336405909572e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0081, 'grad_norm': 0.7734375, 'learning_rate': 8.347200168163187e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3625, 'grad_norm': 0.478515625, 'learning_rate': 8.346063930416801e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1485, 'grad_norm': 0.71484375, 'learning_rate': 8.344927692670415e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1312, 'grad_norm': 0.427734375, 'learning_rate': 8.343791454924029e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1462, 'grad_norm': 0.546875, 'learning_rate': 8.342655217177643e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1554, 'grad_norm': 0.87109375, 'learning_rate': 8.341518979431256e-05, 'epoch': 0.54}\n",
      "{'loss': 1.214, 'grad_norm': 0.6171875, 'learning_rate': 8.34038274168487e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1652, 'grad_norm': 0.59375, 'learning_rate': 8.339246503938485e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2596, 'grad_norm': 0.55078125, 'learning_rate': 8.338110266192099e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1987, 'grad_norm': 0.6015625, 'learning_rate': 8.336974028445711e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1052, 'grad_norm': 0.9765625, 'learning_rate': 8.335837790699327e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3719, 'grad_norm': 0.79296875, 'learning_rate': 8.33470155295294e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1823, 'grad_norm': 0.78125, 'learning_rate': 8.333565315206554e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0644, 'grad_norm': 0.55078125, 'learning_rate': 8.332429077460168e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1083, 'grad_norm': 0.60546875, 'learning_rate': 8.331292839713782e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0563, 'grad_norm': 0.98828125, 'learning_rate': 8.330156601967396e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3173, 'grad_norm': 0.93359375, 'learning_rate': 8.32902036422101e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2444, 'grad_norm': 0.609375, 'learning_rate': 8.327884126474624e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1115, 'grad_norm': 0.443359375, 'learning_rate': 8.326747888728238e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2034, 'grad_norm': 0.625, 'learning_rate': 8.325611650981852e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1425, 'grad_norm': 0.8359375, 'learning_rate': 8.324475413235466e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2681, 'grad_norm': 0.66015625, 'learning_rate': 8.32333917548908e-05, 'epoch': 0.54}\n",
      "{'loss': 1.286, 'grad_norm': 0.7578125, 'learning_rate': 8.322202937742693e-05, 'epoch': 0.54}\n",
      "{'loss': 1.284, 'grad_norm': 0.47265625, 'learning_rate': 8.321066699996307e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2907, 'grad_norm': 0.67578125, 'learning_rate': 8.319930462249922e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0645, 'grad_norm': 1.0078125, 'learning_rate': 8.318794224503536e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3439, 'grad_norm': 0.54296875, 'learning_rate': 8.317657986757149e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1837, 'grad_norm': 0.6796875, 'learning_rate': 8.316521749010764e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0746, 'grad_norm': 0.5546875, 'learning_rate': 8.315385511264377e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3663, 'grad_norm': 0.54296875, 'learning_rate': 8.314249273517991e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0914, 'grad_norm': 0.53125, 'learning_rate': 8.313113035771605e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3738, 'grad_norm': 0.462890625, 'learning_rate': 8.311976798025219e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1356, 'grad_norm': 0.57421875, 'learning_rate': 8.310840560278833e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1444, 'grad_norm': 0.455078125, 'learning_rate': 8.309704322532446e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3016, 'grad_norm': 0.69921875, 'learning_rate': 8.308568084786062e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0453, 'grad_norm': 0.56640625, 'learning_rate': 8.307431847039675e-05, 'epoch': 0.54}\n",
      "{'loss': 1.369, 'grad_norm': 0.98828125, 'learning_rate': 8.306295609293289e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1216, 'grad_norm': 0.98046875, 'learning_rate': 8.305159371546903e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0522, 'grad_norm': 0.85546875, 'learning_rate': 8.304023133800517e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2105, 'grad_norm': 0.64453125, 'learning_rate': 8.30288689605413e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0111, 'grad_norm': 0.8125, 'learning_rate': 8.301750658307744e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2083, 'grad_norm': 0.9375, 'learning_rate': 8.30061442056136e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2005, 'grad_norm': 0.59765625, 'learning_rate': 8.299478182814973e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1203, 'grad_norm': 0.44921875, 'learning_rate': 8.298341945068586e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2672, 'grad_norm': 0.73828125, 'learning_rate': 8.297205707322201e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1096, 'grad_norm': 0.56640625, 'learning_rate': 8.296069469575815e-05, 'epoch': 0.54}\n",
      "{'loss': 1.29, 'grad_norm': 0.8046875, 'learning_rate': 8.294933231829428e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2821, 'grad_norm': 0.65234375, 'learning_rate': 8.293796994083042e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2813, 'grad_norm': 0.4296875, 'learning_rate': 8.292660756336656e-05, 'epoch': 0.54}\n",
      "{'loss': 1.178, 'grad_norm': 1.1171875, 'learning_rate': 8.29152451859027e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1156, 'grad_norm': 0.73046875, 'learning_rate': 8.290388280843883e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2991, 'grad_norm': 0.578125, 'learning_rate': 8.289252043097499e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0645, 'grad_norm': 0.84375, 'learning_rate': 8.288115805351112e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2507, 'grad_norm': 0.470703125, 'learning_rate': 8.286979567604726e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2107, 'grad_norm': 0.5390625, 'learning_rate': 8.28584332985834e-05, 'epoch': 0.54}\n",
      "{'loss': 1.157, 'grad_norm': 0.478515625, 'learning_rate': 8.284707092111954e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3762, 'grad_norm': 0.6015625, 'learning_rate': 8.283570854365568e-05, 'epoch': 0.54}\n",
      "{'loss': 1.066, 'grad_norm': 0.9453125, 'learning_rate': 8.282434616619181e-05, 'epoch': 0.54}\n",
      "{'loss': 1.112, 'grad_norm': 0.48828125, 'learning_rate': 8.281298378872796e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1871, 'grad_norm': 0.5859375, 'learning_rate': 8.28016214112641e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1371, 'grad_norm': 2.1875, 'learning_rate': 8.279025903380023e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3054, 'grad_norm': 0.5, 'learning_rate': 8.277889665633638e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2513, 'grad_norm': 0.81640625, 'learning_rate': 8.276753427887252e-05, 'epoch': 0.54}\n",
      "{'loss': 1.294, 'grad_norm': 0.62890625, 'learning_rate': 8.275617190140865e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3103, 'grad_norm': 0.53515625, 'learning_rate': 8.27448095239448e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1568, 'grad_norm': 0.96484375, 'learning_rate': 8.273344714648093e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4003, 'grad_norm': 0.8984375, 'learning_rate': 8.272208476901707e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1156, 'grad_norm': 0.71875, 'learning_rate': 8.27107223915532e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1806, 'grad_norm': 0.53515625, 'learning_rate': 8.269936001408936e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2751, 'grad_norm': 0.5625, 'learning_rate': 8.26879976366255e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0754, 'grad_norm': 0.5859375, 'learning_rate': 8.267663525916163e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3561, 'grad_norm': 0.57421875, 'learning_rate': 8.266527288169777e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1143, 'grad_norm': 0.67578125, 'learning_rate': 8.265391050423391e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1706, 'grad_norm': 0.451171875, 'learning_rate': 8.264254812677005e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2471, 'grad_norm': 0.96875, 'learning_rate': 8.263118574930618e-05, 'epoch': 0.54}\n",
      "{'loss': 1.106, 'grad_norm': 0.82421875, 'learning_rate': 8.261982337184234e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2589, 'grad_norm': 0.6484375, 'learning_rate': 8.260846099437847e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1485, 'grad_norm': 1.234375, 'learning_rate': 8.25970986169146e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2761, 'grad_norm': 0.462890625, 'learning_rate': 8.258573623945075e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2434, 'grad_norm': 0.59375, 'learning_rate': 8.257437386198689e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1003, 'grad_norm': 0.86328125, 'learning_rate': 8.256301148452302e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3731, 'grad_norm': 0.54296875, 'learning_rate': 8.255164910705918e-05, 'epoch': 0.54}\n",
      "{'loss': 1.08, 'grad_norm': 0.5546875, 'learning_rate': 8.25402867295953e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2069, 'grad_norm': 0.439453125, 'learning_rate': 8.252892435213144e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1677, 'grad_norm': 0.52734375, 'learning_rate': 8.251756197466758e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0837, 'grad_norm': 0.58984375, 'learning_rate': 8.250619959720373e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3004, 'grad_norm': 0.515625, 'learning_rate': 8.249483721973987e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1935, 'grad_norm': 0.68359375, 'learning_rate': 8.2483474842276e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1121, 'grad_norm': 0.765625, 'learning_rate': 8.247211246481214e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2636, 'grad_norm': 0.53125, 'learning_rate': 8.246075008734828e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9942, 'grad_norm': 0.953125, 'learning_rate': 8.244938770988442e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4655, 'grad_norm': 0.50390625, 'learning_rate': 8.243802533242055e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2819, 'grad_norm': 0.9375, 'learning_rate': 8.24266629549567e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1304, 'grad_norm': 0.62890625, 'learning_rate': 8.241530057749284e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1659, 'grad_norm': 0.609375, 'learning_rate': 8.240393820002897e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1883, 'grad_norm': 0.640625, 'learning_rate': 8.239257582256512e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3795, 'grad_norm': 0.546875, 'learning_rate': 8.238121344510126e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0455, 'grad_norm': 0.76953125, 'learning_rate': 8.23698510676374e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2819, 'grad_norm': 0.4609375, 'learning_rate': 8.235848869017355e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2796, 'grad_norm': 0.625, 'learning_rate': 8.234712631270967e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0357, 'grad_norm': 0.6484375, 'learning_rate': 8.233576393524581e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3005, 'grad_norm': 0.609375, 'learning_rate': 8.232440155778195e-05, 'epoch': 0.54}\n",
      "{'loss': 1.07, 'grad_norm': 1.3671875, 'learning_rate': 8.23130391803181e-05, 'epoch': 0.54}\n",
      "{'loss': 1.155, 'grad_norm': 0.53515625, 'learning_rate': 8.230167680285424e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2515, 'grad_norm': 0.8828125, 'learning_rate': 8.229031442539037e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1137, 'grad_norm': 0.82421875, 'learning_rate': 8.227895204792651e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2407, 'grad_norm': 0.5390625, 'learning_rate': 8.226758967046265e-05, 'epoch': 0.54}\n",
      "{'loss': 1.089, 'grad_norm': 0.91015625, 'learning_rate': 8.225622729299879e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0022, 'grad_norm': 0.4296875, 'learning_rate': 8.224486491553494e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3203, 'grad_norm': 0.640625, 'learning_rate': 8.223350253807108e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1161, 'grad_norm': 0.66015625, 'learning_rate': 8.222214016060721e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4356, 'grad_norm': 0.55859375, 'learning_rate': 8.221077778314334e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0955, 'grad_norm': 0.69921875, 'learning_rate': 8.219941540567949e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2078, 'grad_norm': 0.5078125, 'learning_rate': 8.218805302821563e-05, 'epoch': 0.54}\n",
      "{'loss': 1.342, 'grad_norm': 0.578125, 'learning_rate': 8.217669065075177e-05, 'epoch': 0.54}\n",
      "{'loss': 1.007, 'grad_norm': 0.71484375, 'learning_rate': 8.216532827328792e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3692, 'grad_norm': 0.67578125, 'learning_rate': 8.215396589582404e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1898, 'grad_norm': 1.1015625, 'learning_rate': 8.214260351836018e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1755, 'grad_norm': 0.484375, 'learning_rate': 8.213124114089632e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2822, 'grad_norm': 1.1015625, 'learning_rate': 8.211987876343247e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1428, 'grad_norm': 0.77734375, 'learning_rate': 8.21085163859686e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3592, 'grad_norm': 0.46875, 'learning_rate': 8.209715400850474e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2433, 'grad_norm': 0.66015625, 'learning_rate': 8.208579163104088e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1795, 'grad_norm': 0.5390625, 'learning_rate': 8.207442925357702e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4007, 'grad_norm': 0.53515625, 'learning_rate': 8.206306687611316e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2509, 'grad_norm': 0.7890625, 'learning_rate': 8.205170449864931e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3341, 'grad_norm': 0.54296875, 'learning_rate': 8.204034212118545e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1331, 'grad_norm': 0.76953125, 'learning_rate': 8.202897974372158e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1995, 'grad_norm': 0.5078125, 'learning_rate': 8.201761736625771e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2562, 'grad_norm': 0.62109375, 'learning_rate': 8.200625498879386e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9814, 'grad_norm': 0.419921875, 'learning_rate': 8.199489261133e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3605, 'grad_norm': 0.470703125, 'learning_rate': 8.198353023386614e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1807, 'grad_norm': 0.609375, 'learning_rate': 8.197216785640229e-05, 'epoch': 0.54}\n",
      "{'loss': 1.139, 'grad_norm': 0.5078125, 'learning_rate': 8.196080547893841e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2226, 'grad_norm': 0.67578125, 'learning_rate': 8.194944310147455e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0768, 'grad_norm': 1.359375, 'learning_rate': 8.193808072401069e-05, 'epoch': 0.54}\n",
      "{'loss': 1.286, 'grad_norm': 0.69140625, 'learning_rate': 8.192671834654684e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2618, 'grad_norm': 0.97265625, 'learning_rate': 8.191535596908298e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1766, 'grad_norm': 0.50390625, 'learning_rate': 8.190399359161911e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2683, 'grad_norm': 0.59375, 'learning_rate': 8.189263121415525e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9935, 'grad_norm': 0.5859375, 'learning_rate': 8.188126883669139e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3626, 'grad_norm': 0.490234375, 'learning_rate': 8.186990645922753e-05, 'epoch': 0.54}\n",
      "{'loss': 1.113, 'grad_norm': 0.484375, 'learning_rate': 8.185854408176368e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3402, 'grad_norm': 0.51171875, 'learning_rate': 8.184718170429982e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2053, 'grad_norm': 0.96484375, 'learning_rate': 8.183581932683596e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0815, 'grad_norm': 0.7109375, 'learning_rate': 8.182445694937208e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3132, 'grad_norm': 0.48046875, 'learning_rate': 8.181309457190823e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1907, 'grad_norm': 0.83203125, 'learning_rate': 8.180173219444437e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1281, 'grad_norm': 0.44140625, 'learning_rate': 8.179036981698051e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3497, 'grad_norm': 0.5390625, 'learning_rate': 8.177900743951666e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1214, 'grad_norm': 0.92578125, 'learning_rate': 8.176764506205278e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4013, 'grad_norm': 0.58984375, 'learning_rate': 8.175628268458892e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1865, 'grad_norm': 0.6484375, 'learning_rate': 8.174492030712506e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2276, 'grad_norm': 0.5703125, 'learning_rate': 8.173355792966121e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1922, 'grad_norm': 0.8359375, 'learning_rate': 8.172219555219735e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1805, 'grad_norm': 0.95703125, 'learning_rate': 8.171083317473349e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3514, 'grad_norm': 0.46484375, 'learning_rate': 8.169947079726962e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2364, 'grad_norm': 0.6796875, 'learning_rate': 8.168810841980576e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2608, 'grad_norm': 0.44140625, 'learning_rate': 8.16767460423419e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3035, 'grad_norm': 0.8203125, 'learning_rate': 8.166538366487805e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9889, 'grad_norm': 0.44921875, 'learning_rate': 8.165402128741419e-05, 'epoch': 0.54}\n",
      "{'loss': 1.35, 'grad_norm': 0.458984375, 'learning_rate': 8.164265890995033e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2426, 'grad_norm': 0.66015625, 'learning_rate': 8.163129653248645e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3584, 'grad_norm': 0.455078125, 'learning_rate': 8.16199341550226e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2132, 'grad_norm': 0.625, 'learning_rate': 8.160857177755874e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1787, 'grad_norm': 0.796875, 'learning_rate': 8.159720940009488e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2168, 'grad_norm': 0.578125, 'learning_rate': 8.158584702263103e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0654, 'grad_norm': 0.7578125, 'learning_rate': 8.157448464516715e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1142, 'grad_norm': 0.5234375, 'learning_rate': 8.156312226770329e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2718, 'grad_norm': 0.58984375, 'learning_rate': 8.155175989023944e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0748, 'grad_norm': 0.5859375, 'learning_rate': 8.154039751277558e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3287, 'grad_norm': 0.5234375, 'learning_rate': 8.152903513531172e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2444, 'grad_norm': 0.640625, 'learning_rate': 8.151767275784786e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1105, 'grad_norm': 0.8203125, 'learning_rate': 8.1506310380384e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1975, 'grad_norm': 0.6015625, 'learning_rate': 8.149494800292013e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1174, 'grad_norm': 1.0859375, 'learning_rate': 8.148358562545627e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2855, 'grad_norm': 0.6171875, 'learning_rate': 8.147222324799242e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1334, 'grad_norm': 0.7578125, 'learning_rate': 8.146086087052856e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1244, 'grad_norm': 0.5, 'learning_rate': 8.14494984930647e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3075, 'grad_norm': 0.6953125, 'learning_rate': 8.143813611560082e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1398, 'grad_norm': 0.81640625, 'learning_rate': 8.142677373813697e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2985, 'grad_norm': 0.58984375, 'learning_rate': 8.141541136067311e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1388, 'grad_norm': 0.62890625, 'learning_rate': 8.140404898320925e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2104, 'grad_norm': 0.447265625, 'learning_rate': 8.13926866057454e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1169, 'grad_norm': 0.59765625, 'learning_rate': 8.138132422828152e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1343, 'grad_norm': 0.66015625, 'learning_rate': 8.136996185081766e-05, 'epoch': 0.54}\n",
      "{'loss': 1.3856, 'grad_norm': 0.8984375, 'learning_rate': 8.135859947335381e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0876, 'grad_norm': 0.609375, 'learning_rate': 8.134723709588995e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1782, 'grad_norm': 0.466796875, 'learning_rate': 8.133587471842609e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2574, 'grad_norm': 0.5859375, 'learning_rate': 8.132451234096223e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9924, 'grad_norm': 0.953125, 'learning_rate': 8.131314996349836e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2615, 'grad_norm': 0.64453125, 'learning_rate': 8.13017875860345e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1576, 'grad_norm': 0.7421875, 'learning_rate': 8.129042520857064e-05, 'epoch': 0.55}\n",
      "{'loss': 1.222, 'grad_norm': 0.58984375, 'learning_rate': 8.127906283110679e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2029, 'grad_norm': 1.4921875, 'learning_rate': 8.126770045364293e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0025, 'grad_norm': 0.8125, 'learning_rate': 8.125633807617907e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1981, 'grad_norm': 0.5625, 'learning_rate': 8.124497569871519e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1811, 'grad_norm': 0.734375, 'learning_rate': 8.123361332125134e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0966, 'grad_norm': 0.484375, 'learning_rate': 8.122225094378748e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1908, 'grad_norm': 0.5390625, 'learning_rate': 8.121088856632362e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0525, 'grad_norm': 0.8515625, 'learning_rate': 8.119952618885977e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3214, 'grad_norm': 0.52734375, 'learning_rate': 8.11881638113959e-05, 'epoch': 0.55}\n",
      "{'loss': 1.112, 'grad_norm': 0.7109375, 'learning_rate': 8.117680143393203e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1573, 'grad_norm': 0.62109375, 'learning_rate': 8.116543905646818e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2646, 'grad_norm': 0.6484375, 'learning_rate': 8.115407667900432e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0256, 'grad_norm': 1.1484375, 'learning_rate': 8.114271430154046e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3166, 'grad_norm': 0.47265625, 'learning_rate': 8.11313519240766e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1931, 'grad_norm': 0.71484375, 'learning_rate': 8.111998954661274e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2004, 'grad_norm': 0.4765625, 'learning_rate': 8.110862716914887e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1744, 'grad_norm': 0.56640625, 'learning_rate': 8.109726479168501e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1363, 'grad_norm': 0.98046875, 'learning_rate': 8.108590241422116e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3497, 'grad_norm': 0.6328125, 'learning_rate': 8.10745400367573e-05, 'epoch': 0.55}\n",
      "{'loss': 0.981, 'grad_norm': 0.67578125, 'learning_rate': 8.106317765929344e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2465, 'grad_norm': 1.078125, 'learning_rate': 8.105181528182956e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3732, 'grad_norm': 0.65625, 'learning_rate': 8.104045290436571e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0206, 'grad_norm': 1.59375, 'learning_rate': 8.102909052690185e-05, 'epoch': 0.55}\n",
      "{'loss': 1.294, 'grad_norm': 0.50390625, 'learning_rate': 8.101772814943799e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0823, 'grad_norm': 0.90234375, 'learning_rate': 8.100636577197414e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1663, 'grad_norm': 0.484375, 'learning_rate': 8.099500339451027e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1365, 'grad_norm': 0.6015625, 'learning_rate': 8.09836410170464e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1302, 'grad_norm': 0.83984375, 'learning_rate': 8.097227863958255e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3276, 'grad_norm': 0.63671875, 'learning_rate': 8.096091626211869e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1475, 'grad_norm': 0.72265625, 'learning_rate': 8.094955388465483e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1599, 'grad_norm': 0.55078125, 'learning_rate': 8.093819150719097e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2844, 'grad_norm': 0.73046875, 'learning_rate': 8.09268291297271e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0995, 'grad_norm': 1.03125, 'learning_rate': 8.091546675226324e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4138, 'grad_norm': 0.5078125, 'learning_rate': 8.090410437479938e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2599, 'grad_norm': 0.91015625, 'learning_rate': 8.089274199733553e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2152, 'grad_norm': 0.8828125, 'learning_rate': 8.088137961987167e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1614, 'grad_norm': 0.69921875, 'learning_rate': 8.087001724240781e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0197, 'grad_norm': 0.97265625, 'learning_rate': 8.085865486494395e-05, 'epoch': 0.55}\n",
      "{'loss': 1.263, 'grad_norm': 0.65234375, 'learning_rate': 8.084729248748008e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1945, 'grad_norm': 0.65234375, 'learning_rate': 8.083593011001622e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0892, 'grad_norm': 0.52734375, 'learning_rate': 8.082456773255236e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1997, 'grad_norm': 0.55859375, 'learning_rate': 8.081320535508851e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0329, 'grad_norm': 0.859375, 'learning_rate': 8.080184297762464e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3618, 'grad_norm': 0.48828125, 'learning_rate': 8.079048060016077e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2289, 'grad_norm': 0.65234375, 'learning_rate': 8.077911822269693e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1982, 'grad_norm': 0.58984375, 'learning_rate': 8.076775584523306e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2859, 'grad_norm': 0.50390625, 'learning_rate': 8.07563934677692e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9888, 'grad_norm': 1.546875, 'learning_rate': 8.074503109030534e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2691, 'grad_norm': 0.546875, 'learning_rate': 8.073366871284148e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1238, 'grad_norm': 0.76953125, 'learning_rate': 8.072230633537761e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2327, 'grad_norm': 0.421875, 'learning_rate': 8.071094395791375e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2833, 'grad_norm': 0.5703125, 'learning_rate': 8.06995815804499e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0918, 'grad_norm': 1.2109375, 'learning_rate': 8.068821920298604e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1718, 'grad_norm': 0.57421875, 'learning_rate': 8.067685682552218e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0856, 'grad_norm': 0.71484375, 'learning_rate': 8.066549444805832e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2547, 'grad_norm': 0.44140625, 'learning_rate': 8.065413207059445e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2426, 'grad_norm': 0.6953125, 'learning_rate': 8.064276969313059e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2345, 'grad_norm': 1.015625, 'learning_rate': 8.063140731566673e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3179, 'grad_norm': 0.6171875, 'learning_rate': 8.062004493820288e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2748, 'grad_norm': 0.56640625, 'learning_rate': 8.0608682560739e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2197, 'grad_norm': 0.4765625, 'learning_rate': 8.059732018327514e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2063, 'grad_norm': 0.50390625, 'learning_rate': 8.05859578058113e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1699, 'grad_norm': 0.8828125, 'learning_rate': 8.057459542834743e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3447, 'grad_norm': 0.46484375, 'learning_rate': 8.056323305088357e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1873, 'grad_norm': 0.7265625, 'learning_rate': 8.055187067341971e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1659, 'grad_norm': 0.55859375, 'learning_rate': 8.054050829595585e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2826, 'grad_norm': 0.60546875, 'learning_rate': 8.052914591849198e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2123, 'grad_norm': 0.65234375, 'learning_rate': 8.051778354102812e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2572, 'grad_norm': 0.48046875, 'learning_rate': 8.050642116356427e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0677, 'grad_norm': 0.671875, 'learning_rate': 8.049505878610041e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1376, 'grad_norm': 0.5078125, 'learning_rate': 8.048369640863655e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2835, 'grad_norm': 0.57421875, 'learning_rate': 8.047233403117269e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9853, 'grad_norm': 1.078125, 'learning_rate': 8.046097165370883e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2675, 'grad_norm': 0.466796875, 'learning_rate': 8.044960927624496e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1706, 'grad_norm': 0.65625, 'learning_rate': 8.04382468987811e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2953, 'grad_norm': 0.443359375, 'learning_rate': 8.042688452131725e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2274, 'grad_norm': 0.6484375, 'learning_rate': 8.041552214385338e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1336, 'grad_norm': 8.75, 'learning_rate': 8.040415976638951e-05, 'epoch': 0.55}\n",
      "{'loss': 1.5251, 'grad_norm': 0.53125, 'learning_rate': 8.039279738892567e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1722, 'grad_norm': 0.8359375, 'learning_rate': 8.03814350114618e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1444, 'grad_norm': 0.49609375, 'learning_rate': 8.037007263399794e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3059, 'grad_norm': 0.9140625, 'learning_rate': 8.035871025653408e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1454, 'grad_norm': 0.796875, 'learning_rate': 8.034734787907022e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3908, 'grad_norm': 0.53515625, 'learning_rate': 8.033598550160636e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1103, 'grad_norm': 0.76953125, 'learning_rate': 8.03246231241425e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1825, 'grad_norm': 0.53125, 'learning_rate': 8.031326074667864e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1098, 'grad_norm': 0.671875, 'learning_rate': 8.030189836921478e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0356, 'grad_norm': 0.5078125, 'learning_rate': 8.029053599175092e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3532, 'grad_norm': 0.53125, 'learning_rate': 8.027917361428706e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1236, 'grad_norm': 0.74609375, 'learning_rate': 8.02678112368232e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3474, 'grad_norm': 0.44921875, 'learning_rate': 8.025644885935933e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1848, 'grad_norm': 0.62109375, 'learning_rate': 8.024508648189547e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9946, 'grad_norm': 0.490234375, 'learning_rate': 8.023372410443162e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3317, 'grad_norm': 0.55078125, 'learning_rate': 8.022236172696775e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1755, 'grad_norm': 0.84765625, 'learning_rate': 8.021099934950389e-05, 'epoch': 0.55}\n",
      "{'loss': 1.199, 'grad_norm': 0.41015625, 'learning_rate': 8.019963697204004e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3341, 'grad_norm': 0.796875, 'learning_rate': 8.018827459457617e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0372, 'grad_norm': 1.109375, 'learning_rate': 8.017691221711231e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3243, 'grad_norm': 0.515625, 'learning_rate': 8.016554983964845e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2502, 'grad_norm': 0.6640625, 'learning_rate': 8.015418746218459e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3155, 'grad_norm': 0.57421875, 'learning_rate': 8.014282508472073e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2336, 'grad_norm': 0.9140625, 'learning_rate': 8.013146270725686e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1427, 'grad_norm': 0.6796875, 'learning_rate': 8.012010032979302e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2076, 'grad_norm': 0.62109375, 'learning_rate': 8.010873795232915e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1249, 'grad_norm': 0.67578125, 'learning_rate': 8.009737557486529e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0399, 'grad_norm': 0.52734375, 'learning_rate': 8.008601319740143e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3241, 'grad_norm': 0.73046875, 'learning_rate': 8.007465081993757e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1768, 'grad_norm': 1.0390625, 'learning_rate': 8.00632884424737e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4565, 'grad_norm': 0.53125, 'learning_rate': 8.005192606500984e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0728, 'grad_norm': 0.75, 'learning_rate': 8.0040563687546e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9691, 'grad_norm': 0.57421875, 'learning_rate': 8.002920131008212e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2303, 'grad_norm': 0.83203125, 'learning_rate': 8.001783893261826e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1177, 'grad_norm': 1.1796875, 'learning_rate': 8.000647655515441e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3305, 'grad_norm': 0.6640625, 'learning_rate': 7.999511417769055e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1142, 'grad_norm': 0.6015625, 'learning_rate': 7.998375180022668e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0092, 'grad_norm': 0.609375, 'learning_rate': 7.997238942276282e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2366, 'grad_norm': 0.51171875, 'learning_rate': 7.996102704529896e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1097, 'grad_norm': 0.90625, 'learning_rate': 7.99496646678351e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2385, 'grad_norm': 0.5078125, 'learning_rate': 7.993830229037123e-05, 'epoch': 0.55}\n",
      "{'loss': 1.225, 'grad_norm': 0.7578125, 'learning_rate': 7.992693991290739e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2339, 'grad_norm': 0.48046875, 'learning_rate': 7.991557753544352e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1843, 'grad_norm': 0.625, 'learning_rate': 7.990421515797966e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0556, 'grad_norm': 0.66015625, 'learning_rate': 7.98928527805158e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2956, 'grad_norm': 0.5625, 'learning_rate': 7.988149040305194e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1418, 'grad_norm': 0.8046875, 'learning_rate': 7.987012802558808e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1274, 'grad_norm': 0.390625, 'learning_rate': 7.985876564812421e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2713, 'grad_norm': 0.51953125, 'learning_rate': 7.984740327066036e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9841, 'grad_norm': 0.48828125, 'learning_rate': 7.983604089319649e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3556, 'grad_norm': 0.57421875, 'learning_rate': 7.982467851573263e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1814, 'grad_norm': 0.94140625, 'learning_rate': 7.981331613826878e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1616, 'grad_norm': 0.515625, 'learning_rate': 7.980195376080492e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2436, 'grad_norm': 0.4921875, 'learning_rate': 7.979059138334105e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1292, 'grad_norm': 1.203125, 'learning_rate': 7.977922900587719e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3317, 'grad_norm': 0.66796875, 'learning_rate': 7.976786662841333e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0912, 'grad_norm': 0.64453125, 'learning_rate': 7.975650425094947e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1701, 'grad_norm': 0.50390625, 'learning_rate': 7.97451418734856e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2711, 'grad_norm': 0.59765625, 'learning_rate': 7.973377949602176e-05, 'epoch': 0.55}\n",
      "{'loss': 1.084, 'grad_norm': 0.71484375, 'learning_rate': 7.97224171185579e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3456, 'grad_norm': 0.51171875, 'learning_rate': 7.971105474109403e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0965, 'grad_norm': 0.6796875, 'learning_rate': 7.969969236363017e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1697, 'grad_norm': 0.62890625, 'learning_rate': 7.968832998616631e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2373, 'grad_norm': 0.48046875, 'learning_rate': 7.967696760870245e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1352, 'grad_norm': 0.7890625, 'learning_rate': 7.96656052312386e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2874, 'grad_norm': 0.53125, 'learning_rate': 7.965424285377474e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1289, 'grad_norm': 0.56640625, 'learning_rate': 7.964288047631086e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0813, 'grad_norm': 0.36328125, 'learning_rate': 7.9631518098847e-05, 'epoch': 0.55}\n",
      "{'loss': 1.233, 'grad_norm': 0.5703125, 'learning_rate': 7.962015572138315e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9361, 'grad_norm': 0.6484375, 'learning_rate': 7.960879334391929e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3715, 'grad_norm': 0.70703125, 'learning_rate': 7.959743096645542e-05, 'epoch': 0.55}\n",
      "{'loss': 1.243, 'grad_norm': 0.75, 'learning_rate': 7.958606858899156e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1807, 'grad_norm': 0.4296875, 'learning_rate': 7.95747062115277e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3065, 'grad_norm': 0.8828125, 'learning_rate': 7.956334383406384e-05, 'epoch': 0.55}\n",
      "{'loss': 1.111, 'grad_norm': 0.78515625, 'learning_rate': 7.955198145659998e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2268, 'grad_norm': 0.6171875, 'learning_rate': 7.954061907913613e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1114, 'grad_norm': 0.71875, 'learning_rate': 7.952925670167227e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2784, 'grad_norm': 0.515625, 'learning_rate': 7.95178943242084e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3091, 'grad_norm': 0.53125, 'learning_rate': 7.950653194674454e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2791, 'grad_norm': 0.91796875, 'learning_rate': 7.949516956928068e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2626, 'grad_norm': 0.5703125, 'learning_rate': 7.948380719181682e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1923, 'grad_norm': 0.71875, 'learning_rate': 7.947244481435297e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1547, 'grad_norm': 0.52734375, 'learning_rate': 7.94610824368891e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2042, 'grad_norm': 0.49609375, 'learning_rate': 7.944972005942523e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0286, 'grad_norm': 0.8203125, 'learning_rate': 7.943835768196137e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2741, 'grad_norm': 0.72265625, 'learning_rate': 7.942699530449752e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1817, 'grad_norm': 0.73046875, 'learning_rate': 7.941563292703366e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0513, 'grad_norm': 0.373046875, 'learning_rate': 7.94042705495698e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1919, 'grad_norm': 0.546875, 'learning_rate': 7.939290817210593e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1131, 'grad_norm': 0.71484375, 'learning_rate': 7.938154579464207e-05, 'epoch': 0.55}\n",
      "{'loss': 1.376, 'grad_norm': 0.5859375, 'learning_rate': 7.937018341717821e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1357, 'grad_norm': 0.5625, 'learning_rate': 7.935882103971435e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2559, 'grad_norm': 0.65234375, 'learning_rate': 7.93474586622505e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3361, 'grad_norm': 0.609375, 'learning_rate': 7.933609628478664e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0029, 'grad_norm': 0.76953125, 'learning_rate': 7.932473390732277e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2668, 'grad_norm': 0.494140625, 'learning_rate': 7.931337152985891e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0924, 'grad_norm': 0.7578125, 'learning_rate': 7.930200915239505e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2233, 'grad_norm': 0.46875, 'learning_rate': 7.929064677493119e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2571, 'grad_norm': 0.765625, 'learning_rate': 7.927928439746734e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9928, 'grad_norm': 0.66015625, 'learning_rate': 7.926792202000348e-05, 'epoch': 0.55}\n",
      "{'loss': 1.5195, 'grad_norm': 0.6875, 'learning_rate': 7.92565596425396e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1436, 'grad_norm': 0.81640625, 'learning_rate': 7.924519726507574e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0933, 'grad_norm': 0.54296875, 'learning_rate': 7.923383488761189e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2407, 'grad_norm': 0.6953125, 'learning_rate': 7.922247251014803e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1636, 'grad_norm': 1.1328125, 'learning_rate': 7.921111013268417e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2457, 'grad_norm': 0.51953125, 'learning_rate': 7.919974775522032e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1476, 'grad_norm': 0.8828125, 'learning_rate': 7.918838537775644e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2221, 'grad_norm': 0.38671875, 'learning_rate': 7.917702300029258e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1741, 'grad_norm': 0.5234375, 'learning_rate': 7.916566062282872e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0538, 'grad_norm': 0.859375, 'learning_rate': 7.915429824536487e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3042, 'grad_norm': 0.48046875, 'learning_rate': 7.9142935867901e-05, 'epoch': 0.55}\n",
      "{'loss': 1.156, 'grad_norm': 0.5859375, 'learning_rate': 7.913157349043714e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1238, 'grad_norm': 0.5625, 'learning_rate': 7.912021111297328e-05, 'epoch': 0.55}\n",
      "{'loss': 1.3776, 'grad_norm': 0.5703125, 'learning_rate': 7.910884873550942e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0072, 'grad_norm': 0.78125, 'learning_rate': 7.909748635804556e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2601, 'grad_norm': 0.5703125, 'learning_rate': 7.908612398058171e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0239, 'grad_norm': 0.92578125, 'learning_rate': 7.907476160311785e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1964, 'grad_norm': 0.58203125, 'learning_rate': 7.906339922565397e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2451, 'grad_norm': 0.6953125, 'learning_rate': 7.905203684819011e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0991, 'grad_norm': 0.80859375, 'learning_rate': 7.904067447072626e-05, 'epoch': 0.55}\n",
      "{'loss': 1.4503, 'grad_norm': 0.52734375, 'learning_rate': 7.90293120932624e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1315, 'grad_norm': 0.6953125, 'learning_rate': 7.901794971579854e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2256, 'grad_norm': 0.46484375, 'learning_rate': 7.900658733833469e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2345, 'grad_norm': 0.4921875, 'learning_rate': 7.899522496087081e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0708, 'grad_norm': 0.70703125, 'learning_rate': 7.898386258340695e-05, 'epoch': 0.56}\n",
      "{'loss': 1.349, 'grad_norm': 0.5546875, 'learning_rate': 7.89725002059431e-05, 'epoch': 0.56}\n",
      "{'loss': 1.258, 'grad_norm': 0.9453125, 'learning_rate': 7.896113782847924e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1823, 'grad_norm': 0.4453125, 'learning_rate': 7.894977545101538e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2145, 'grad_norm': 0.55078125, 'learning_rate': 7.893841307355151e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0832, 'grad_norm': 1.0, 'learning_rate': 7.892705069608765e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3454, 'grad_norm': 0.490234375, 'learning_rate': 7.891568831862379e-05, 'epoch': 0.56}\n",
      "{'loss': 1.131, 'grad_norm': 0.73828125, 'learning_rate': 7.890432594115993e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2977, 'grad_norm': 0.546875, 'learning_rate': 7.889296356369608e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3488, 'grad_norm': 0.62890625, 'learning_rate': 7.888160118623222e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0886, 'grad_norm': 0.8359375, 'learning_rate': 7.887023880876834e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2773, 'grad_norm': 0.828125, 'learning_rate': 7.885887643130448e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4032, 'grad_norm': 0.625, 'learning_rate': 7.884751405384063e-05, 'epoch': 0.56}\n",
      "{'loss': 1.278, 'grad_norm': 0.486328125, 'learning_rate': 7.883615167637677e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1236, 'grad_norm': 0.59765625, 'learning_rate': 7.882478929891291e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1285, 'grad_norm': 1.5703125, 'learning_rate': 7.881342692144906e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3614, 'grad_norm': 0.66796875, 'learning_rate': 7.880206454398518e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2027, 'grad_norm': 0.6484375, 'learning_rate': 7.879070216652132e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2409, 'grad_norm': 0.578125, 'learning_rate': 7.877933978905747e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3645, 'grad_norm': 1.078125, 'learning_rate': 7.876797741159361e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0155, 'grad_norm': 0.55859375, 'learning_rate': 7.875661503412975e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3723, 'grad_norm': 0.439453125, 'learning_rate': 7.874525265666589e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2176, 'grad_norm': 0.66015625, 'learning_rate': 7.873389027920202e-05, 'epoch': 0.56}\n",
      "{'loss': 1.126, 'grad_norm': 0.51953125, 'learning_rate': 7.872252790173816e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2238, 'grad_norm': 0.76171875, 'learning_rate': 7.87111655242743e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0796, 'grad_norm': 1.1953125, 'learning_rate': 7.869980314681045e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2524, 'grad_norm': 0.53515625, 'learning_rate': 7.868844076934659e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1163, 'grad_norm': 0.78515625, 'learning_rate': 7.867707839188271e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1716, 'grad_norm': 0.52734375, 'learning_rate': 7.866571601441885e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3146, 'grad_norm': 0.7578125, 'learning_rate': 7.8654353636955e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1583, 'grad_norm': 1.203125, 'learning_rate': 7.864299125949114e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3888, 'grad_norm': 0.54296875, 'learning_rate': 7.863162888202728e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1979, 'grad_norm': 0.55078125, 'learning_rate': 7.862026650456343e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2757, 'grad_norm': 0.439453125, 'learning_rate': 7.860890412709955e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1258, 'grad_norm': 0.5625, 'learning_rate': 7.859754174963569e-05, 'epoch': 0.56}\n",
      "{'loss': 0.9834, 'grad_norm': 0.47265625, 'learning_rate': 7.858617937217184e-05, 'epoch': 0.56}\n",
      "{'loss': 1.5165, 'grad_norm': 0.55859375, 'learning_rate': 7.857481699470798e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2093, 'grad_norm': 0.68359375, 'learning_rate': 7.856345461724412e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2005, 'grad_norm': 0.482421875, 'learning_rate': 7.855209223978026e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1594, 'grad_norm': 0.5234375, 'learning_rate': 7.85407298623164e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1634, 'grad_norm': 0.78515625, 'learning_rate': 7.852936748485253e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3011, 'grad_norm': 0.58203125, 'learning_rate': 7.851800510738867e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1242, 'grad_norm': 0.6796875, 'learning_rate': 7.850664272992482e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2077, 'grad_norm': 0.5, 'learning_rate': 7.849528035246096e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2622, 'grad_norm': 0.90625, 'learning_rate': 7.848391797499708e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1234, 'grad_norm': 0.625, 'learning_rate': 7.847255559753323e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4525, 'grad_norm': 0.4609375, 'learning_rate': 7.846119322006937e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3271, 'grad_norm': 0.83984375, 'learning_rate': 7.844983084260551e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2196, 'grad_norm': 0.5234375, 'learning_rate': 7.843846846514165e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1941, 'grad_norm': 0.5625, 'learning_rate': 7.84271060876778e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0833, 'grad_norm': 0.75, 'learning_rate': 7.841574371021392e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3243, 'grad_norm': 0.6328125, 'learning_rate': 7.840438133275006e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1737, 'grad_norm': 0.72265625, 'learning_rate': 7.839301895528621e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2125, 'grad_norm': 0.6015625, 'learning_rate': 7.838165657782235e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2299, 'grad_norm': 0.828125, 'learning_rate': 7.837029420035849e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0549, 'grad_norm': 0.828125, 'learning_rate': 7.835893182289463e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3082, 'grad_norm': 0.53515625, 'learning_rate': 7.834756944543076e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2588, 'grad_norm': 0.703125, 'learning_rate': 7.83362070679669e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2246, 'grad_norm': 0.4453125, 'learning_rate': 7.832484469050304e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3401, 'grad_norm': 0.6015625, 'learning_rate': 7.831348231303919e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1846, 'grad_norm': 0.8515625, 'learning_rate': 7.830211993557533e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3166, 'grad_norm': 0.54296875, 'learning_rate': 7.829075755811145e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1482, 'grad_norm': 0.7421875, 'learning_rate': 7.82793951806476e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2927, 'grad_norm': 0.484375, 'learning_rate': 7.826803280318374e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2243, 'grad_norm': 0.64453125, 'learning_rate': 7.825667042571988e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0338, 'grad_norm': 1.2421875, 'learning_rate': 7.824530804825602e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3055, 'grad_norm': 0.6171875, 'learning_rate': 7.823394567079217e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1548, 'grad_norm': 0.56640625, 'learning_rate': 7.82225832933283e-05, 'epoch': 0.56}\n",
      "{'loss': 1.151, 'grad_norm': 0.453125, 'learning_rate': 7.821122091586443e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2156, 'grad_norm': 0.6640625, 'learning_rate': 7.819985853840058e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0894, 'grad_norm': 0.99609375, 'learning_rate': 7.818849616093672e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4094, 'grad_norm': 0.58203125, 'learning_rate': 7.817713378347286e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1743, 'grad_norm': 0.66015625, 'learning_rate': 7.8165771406009e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2106, 'grad_norm': 0.4609375, 'learning_rate': 7.815440902854514e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2065, 'grad_norm': 0.6171875, 'learning_rate': 7.814304665108127e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0563, 'grad_norm': 0.74609375, 'learning_rate': 7.813168427361741e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4053, 'grad_norm': 0.515625, 'learning_rate': 7.812032189615356e-05, 'epoch': 0.56}\n",
      "{'loss': 1.106, 'grad_norm': 0.69140625, 'learning_rate': 7.81089595186897e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2731, 'grad_norm': 0.58203125, 'learning_rate': 7.809759714122582e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2227, 'grad_norm': 0.578125, 'learning_rate': 7.808623476376198e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1884, 'grad_norm': 0.87109375, 'learning_rate': 7.807487238629811e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3116, 'grad_norm': 0.51953125, 'learning_rate': 7.806351000883425e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1335, 'grad_norm': 0.9921875, 'learning_rate': 7.805214763137039e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1983, 'grad_norm': 0.412109375, 'learning_rate': 7.804078525390654e-05, 'epoch': 0.56}\n",
      "{'loss': 1.243, 'grad_norm': 0.6328125, 'learning_rate': 7.802942287644267e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0251, 'grad_norm': 0.62890625, 'learning_rate': 7.80180604989788e-05, 'epoch': 0.56}\n",
      "{'loss': 1.33, 'grad_norm': 0.5, 'learning_rate': 7.800669812151495e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1434, 'grad_norm': 0.69921875, 'learning_rate': 7.799533574405109e-05, 'epoch': 0.56}\n",
      "{'loss': 1.223, 'grad_norm': 0.39453125, 'learning_rate': 7.798397336658723e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3682, 'grad_norm': 0.59375, 'learning_rate': 7.797261098912337e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1841, 'grad_norm': 0.93359375, 'learning_rate': 7.79612486116595e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4507, 'grad_norm': 1.03125, 'learning_rate': 7.794988623419564e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1891, 'grad_norm': 0.875, 'learning_rate': 7.793852385673178e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1601, 'grad_norm': 0.44140625, 'learning_rate': 7.792716147926793e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1861, 'grad_norm': 0.85546875, 'learning_rate': 7.791579910180407e-05, 'epoch': 0.56}\n",
      "{'loss': 1.052, 'grad_norm': 0.82421875, 'learning_rate': 7.79044367243402e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3902, 'grad_norm': 0.6796875, 'learning_rate': 7.789307434687635e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1005, 'grad_norm': 1.0234375, 'learning_rate': 7.788171196941248e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2602, 'grad_norm': 0.431640625, 'learning_rate': 7.787034959194862e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4039, 'grad_norm': 0.54296875, 'learning_rate': 7.785898721448476e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0938, 'grad_norm': 0.431640625, 'learning_rate': 7.784762483702091e-05, 'epoch': 0.56}\n",
      "{'loss': 1.365, 'grad_norm': 0.4921875, 'learning_rate': 7.783626245955704e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0792, 'grad_norm': 0.5859375, 'learning_rate': 7.782490008209317e-05, 'epoch': 0.56}\n",
      "{'loss': 1.139, 'grad_norm': 0.40625, 'learning_rate': 7.781353770462933e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2364, 'grad_norm': 0.55859375, 'learning_rate': 7.780217532716546e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1222, 'grad_norm': 0.77734375, 'learning_rate': 7.77908129497016e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3202, 'grad_norm': 0.60546875, 'learning_rate': 7.777945057223774e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2147, 'grad_norm': 0.80859375, 'learning_rate': 7.776808819477388e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2065, 'grad_norm': 0.447265625, 'learning_rate': 7.775672581731001e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1905, 'grad_norm': 0.67578125, 'learning_rate': 7.774536343984615e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1447, 'grad_norm': 0.796875, 'learning_rate': 7.77340010623823e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2143, 'grad_norm': 0.6171875, 'learning_rate': 7.772263868491844e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1434, 'grad_norm': 0.76953125, 'learning_rate': 7.771127630745457e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2915, 'grad_norm': 0.421875, 'learning_rate': 7.769991392999072e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2473, 'grad_norm': 0.73046875, 'learning_rate': 7.768855155252686e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0568, 'grad_norm': 1.078125, 'learning_rate': 7.767718917506299e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4727, 'grad_norm': 0.53125, 'learning_rate': 7.766582679759913e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1111, 'grad_norm': 0.875, 'learning_rate': 7.765446442013528e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2473, 'grad_norm': 0.462890625, 'learning_rate': 7.76431020426714e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3198, 'grad_norm': 0.55078125, 'learning_rate': 7.763173966520754e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0528, 'grad_norm': 0.8046875, 'learning_rate': 7.76203772877437e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2777, 'grad_norm': 0.71875, 'learning_rate': 7.760901491027983e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0577, 'grad_norm': 0.84765625, 'learning_rate': 7.759765253281597e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2323, 'grad_norm': 0.54296875, 'learning_rate': 7.758629015535211e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2795, 'grad_norm': 0.8046875, 'learning_rate': 7.757492777788825e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0933, 'grad_norm': 0.953125, 'learning_rate': 7.756356540042439e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4427, 'grad_norm': 0.4609375, 'learning_rate': 7.755220302296052e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1201, 'grad_norm': 0.73046875, 'learning_rate': 7.754084064549667e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3266, 'grad_norm': 0.52734375, 'learning_rate': 7.752947826803281e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2861, 'grad_norm': 0.75390625, 'learning_rate': 7.751811589056894e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2019, 'grad_norm': 1.203125, 'learning_rate': 7.750675351310509e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4388, 'grad_norm': 0.51953125, 'learning_rate': 7.749539113564123e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1719, 'grad_norm': 0.7265625, 'learning_rate': 7.748402875817736e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2144, 'grad_norm': 0.478515625, 'learning_rate': 7.74726663807135e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3509, 'grad_norm': 0.6015625, 'learning_rate': 7.746130400324965e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1408, 'grad_norm': 0.73828125, 'learning_rate': 7.744994162578578e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3026, 'grad_norm': 0.83984375, 'learning_rate': 7.743857924832192e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2196, 'grad_norm': 0.765625, 'learning_rate': 7.742721687085807e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1454, 'grad_norm': 0.359375, 'learning_rate': 7.74158544933942e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2987, 'grad_norm': 0.55859375, 'learning_rate': 7.740449211593034e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1946, 'grad_norm': 2.96875, 'learning_rate': 7.739312973846648e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3414, 'grad_norm': 0.71484375, 'learning_rate': 7.738176736100262e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2223, 'grad_norm': 0.94140625, 'learning_rate': 7.737040498353876e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2675, 'grad_norm': 0.48828125, 'learning_rate': 7.73590426060749e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2661, 'grad_norm': 0.921875, 'learning_rate': 7.734768022861104e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1838, 'grad_norm': 0.75390625, 'learning_rate': 7.733631785114718e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3689, 'grad_norm': 0.49609375, 'learning_rate': 7.732495547368331e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2538, 'grad_norm': 0.8359375, 'learning_rate': 7.731359309621946e-05, 'epoch': 0.56}\n",
      "{'loss': 1.165, 'grad_norm': 0.46875, 'learning_rate': 7.73022307187556e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1368, 'grad_norm': 0.7578125, 'learning_rate': 7.729086834129173e-05, 'epoch': 0.56}\n",
      "{'loss': 1.013, 'grad_norm': 1.0234375, 'learning_rate': 7.727950596382787e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2444, 'grad_norm': 0.73046875, 'learning_rate': 7.726814358636402e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1665, 'grad_norm': 0.76953125, 'learning_rate': 7.725678120890015e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2219, 'grad_norm': 0.5, 'learning_rate': 7.724541883143629e-05, 'epoch': 0.56}\n",
      "{'loss': 1.206, 'grad_norm': 0.60546875, 'learning_rate': 7.723405645397244e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0275, 'grad_norm': 0.75, 'learning_rate': 7.722269407650857e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2585, 'grad_norm': 0.63671875, 'learning_rate': 7.721133169904471e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0916, 'grad_norm': 0.8046875, 'learning_rate': 7.719996932158085e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4476, 'grad_norm': 0.4375, 'learning_rate': 7.718860694411699e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2028, 'grad_norm': 0.53515625, 'learning_rate': 7.717724456665313e-05, 'epoch': 0.56}\n",
      "{'loss': 1.117, 'grad_norm': 0.7265625, 'learning_rate': 7.716588218918926e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3065, 'grad_norm': 0.546875, 'learning_rate': 7.715451981172542e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0902, 'grad_norm': 0.72265625, 'learning_rate': 7.714315743426155e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1609, 'grad_norm': 0.62109375, 'learning_rate': 7.713179505679768e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2874, 'grad_norm': 0.62890625, 'learning_rate': 7.712043267933383e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2359, 'grad_norm': 2.03125, 'learning_rate': 7.710907030186997e-05, 'epoch': 0.56}\n",
      "{'loss': 1.235, 'grad_norm': 0.58984375, 'learning_rate': 7.70977079244061e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2218, 'grad_norm': 0.75, 'learning_rate': 7.708634554694226e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1736, 'grad_norm': 0.4609375, 'learning_rate': 7.70749831694784e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1748, 'grad_norm': 0.578125, 'learning_rate': 7.706362079201452e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0838, 'grad_norm': 1.25, 'learning_rate': 7.705225841455066e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3715, 'grad_norm': 0.66015625, 'learning_rate': 7.704089603708681e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1183, 'grad_norm': 0.7578125, 'learning_rate': 7.702953365962295e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2152, 'grad_norm': 0.455078125, 'learning_rate': 7.701817128215908e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3142, 'grad_norm': 0.609375, 'learning_rate': 7.700680890469522e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1047, 'grad_norm': 1.046875, 'learning_rate': 7.699544652723136e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2102, 'grad_norm': 0.58984375, 'learning_rate': 7.69840841497675e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0888, 'grad_norm': 0.7265625, 'learning_rate': 7.697272177230363e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2899, 'grad_norm': 0.43359375, 'learning_rate': 7.696135939483979e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1403, 'grad_norm': 0.703125, 'learning_rate': 7.694999701737592e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0844, 'grad_norm': 1.3359375, 'learning_rate': 7.693863463991205e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4233, 'grad_norm': 0.478515625, 'learning_rate': 7.69272722624482e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2128, 'grad_norm': 1.296875, 'learning_rate': 7.691590988498434e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2367, 'grad_norm': 1.3203125, 'learning_rate': 7.690454750752048e-05, 'epoch': 0.56}\n",
      "{'loss': 1.156, 'grad_norm': 0.8359375, 'learning_rate': 7.689318513005663e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0677, 'grad_norm': 1.09375, 'learning_rate': 7.688182275259276e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4062, 'grad_norm': 0.71484375, 'learning_rate': 7.687046037512889e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1369, 'grad_norm': 1.0078125, 'learning_rate': 7.685909799766503e-05, 'epoch': 0.56}\n",
      "{'loss': 1.139, 'grad_norm': 0.4765625, 'learning_rate': 7.684773562020118e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4321, 'grad_norm': 0.61328125, 'learning_rate': 7.683637324273732e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1171, 'grad_norm': 0.68359375, 'learning_rate': 7.682501086527345e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2161, 'grad_norm': 0.875, 'learning_rate': 7.681364848780959e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1319, 'grad_norm': 0.73046875, 'learning_rate': 7.680228611034573e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1902, 'grad_norm': 0.494140625, 'learning_rate': 7.679092373288187e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2094, 'grad_norm': 0.765625, 'learning_rate': 7.6779561355418e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0899, 'grad_norm': 1.0703125, 'learning_rate': 7.676819897795416e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2423, 'grad_norm': 0.91796875, 'learning_rate': 7.67568366004903e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2306, 'grad_norm': 0.64453125, 'learning_rate': 7.674547422302642e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1129, 'grad_norm': 0.61328125, 'learning_rate': 7.673411184556257e-05, 'epoch': 0.57}\n",
      "{'loss': 1.253, 'grad_norm': 0.5390625, 'learning_rate': 7.672274946809871e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0115, 'grad_norm': 0.53515625, 'learning_rate': 7.671138709063485e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3978, 'grad_norm': 0.53125, 'learning_rate': 7.6700024713171e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0225, 'grad_norm': 0.62890625, 'learning_rate': 7.668866233570714e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1575, 'grad_norm': 0.490234375, 'learning_rate': 7.667729995824326e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1633, 'grad_norm': 0.57421875, 'learning_rate': 7.66659375807794e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1088, 'grad_norm': 0.8203125, 'learning_rate': 7.665457520331555e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.288, 'grad_norm': 0.51171875, 'learning_rate': 7.664321282585169e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2011, 'grad_norm': 0.7890625, 'learning_rate': 7.663185044838782e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2465, 'grad_norm': 0.53515625, 'learning_rate': 7.662048807092396e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3258, 'grad_norm': 0.61328125, 'learning_rate': 7.66091256934601e-05, 'epoch': 0.57}\n",
      "{'loss': 0.9667, 'grad_norm': 0.78515625, 'learning_rate': 7.659776331599624e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2782, 'grad_norm': 0.67578125, 'learning_rate': 7.658640093853239e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0076, 'grad_norm': 0.76953125, 'learning_rate': 7.657503856106853e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1374, 'grad_norm': 0.47265625, 'learning_rate': 7.656367618360467e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2749, 'grad_norm': 0.64453125, 'learning_rate': 7.655231380614079e-05, 'epoch': 0.57}\n",
      "{'loss': 0.9916, 'grad_norm': 0.71875, 'learning_rate': 7.654095142867694e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1597, 'grad_norm': 0.6171875, 'learning_rate': 7.652958905121308e-05, 'epoch': 0.57}\n",
      "{'loss': 1.095, 'grad_norm': 0.7109375, 'learning_rate': 7.651822667374922e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2299, 'grad_norm': 0.458984375, 'learning_rate': 7.650686429628537e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2709, 'grad_norm': 0.59375, 'learning_rate': 7.64955019188215e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1668, 'grad_norm': 1.1015625, 'learning_rate': 7.648413954135763e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3493, 'grad_norm': 0.486328125, 'learning_rate': 7.647277716389377e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1496, 'grad_norm': 0.90234375, 'learning_rate': 7.646141478642992e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1143, 'grad_norm': 0.41796875, 'learning_rate': 7.645005240896606e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2039, 'grad_norm': 0.5234375, 'learning_rate': 7.64386900315022e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0849, 'grad_norm': 0.98046875, 'learning_rate': 7.642732765403833e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3183, 'grad_norm': 0.52734375, 'learning_rate': 7.641596527657447e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2714, 'grad_norm': 0.89453125, 'learning_rate': 7.640460289911061e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3042, 'grad_norm': 0.4453125, 'learning_rate': 7.639324052164676e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1466, 'grad_norm': 0.78515625, 'learning_rate': 7.63818781441829e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0362, 'grad_norm': 0.9296875, 'learning_rate': 7.637051576671904e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2724, 'grad_norm': 0.60546875, 'learning_rate': 7.635915338925516e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2956, 'grad_norm': 0.6328125, 'learning_rate': 7.634779101179131e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1173, 'grad_norm': 0.494140625, 'learning_rate': 7.633642863432745e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2619, 'grad_norm': 0.67578125, 'learning_rate': 7.632506625686359e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2246, 'grad_norm': 0.859375, 'learning_rate': 7.631370387939974e-05, 'epoch': 0.57}\n",
      "{'loss': 1.4162, 'grad_norm': 0.609375, 'learning_rate': 7.630234150193588e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2964, 'grad_norm': 0.66796875, 'learning_rate': 7.6290979124472e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1594, 'grad_norm': 0.73828125, 'learning_rate': 7.627961674700814e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1589, 'grad_norm': 0.69921875, 'learning_rate': 7.626825436954429e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0679, 'grad_norm': 1.15625, 'learning_rate': 7.625689199208043e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3776, 'grad_norm': 0.498046875, 'learning_rate': 7.624552961461657e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0877, 'grad_norm': 0.6875, 'learning_rate': 7.62341672371527e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1952, 'grad_norm': 0.462890625, 'learning_rate': 7.622280485968884e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1898, 'grad_norm': 0.52734375, 'learning_rate': 7.621144248222498e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1491, 'grad_norm': 0.9921875, 'learning_rate': 7.620008010476113e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2602, 'grad_norm': 0.51953125, 'learning_rate': 7.618871772729727e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1789, 'grad_norm': 0.75390625, 'learning_rate': 7.61773553498334e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2241, 'grad_norm': 0.51953125, 'learning_rate': 7.616599297236953e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1982, 'grad_norm': 0.68359375, 'learning_rate': 7.615463059490568e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1274, 'grad_norm': 0.9140625, 'learning_rate': 7.614326821744182e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3056, 'grad_norm': 0.443359375, 'learning_rate': 7.613190583997796e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1138, 'grad_norm': 1.0703125, 'learning_rate': 7.612054346251411e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2168, 'grad_norm': 0.546875, 'learning_rate': 7.610918108505025e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2211, 'grad_norm': 0.578125, 'learning_rate': 7.609781870758637e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0811, 'grad_norm': 0.94140625, 'learning_rate': 7.608645633012251e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.32, 'grad_norm': 0.62109375, 'learning_rate': 7.607509395265866e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0139, 'grad_norm': 0.546875, 'learning_rate': 7.60637315751948e-05, 'epoch': 0.57}\n",
      "{'loss': 1.172, 'grad_norm': 0.43359375, 'learning_rate': 7.605236919773094e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1739, 'grad_norm': 0.69921875, 'learning_rate': 7.604100682026707e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0211, 'grad_norm': 0.5390625, 'learning_rate': 7.602964444280321e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2924, 'grad_norm': 0.51953125, 'learning_rate': 7.601828206533935e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1345, 'grad_norm': 0.7109375, 'learning_rate': 7.60069196878755e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3053, 'grad_norm': 0.427734375, 'learning_rate': 7.599555731041164e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2543, 'grad_norm': 0.6796875, 'learning_rate': 7.598419493294778e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1005, 'grad_norm': 0.51171875, 'learning_rate': 7.59728325554839e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3254, 'grad_norm': 0.58203125, 'learning_rate': 7.596147017802005e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1929, 'grad_norm': 0.67578125, 'learning_rate': 7.595010780055619e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2115, 'grad_norm': 0.59375, 'learning_rate': 7.593874542309233e-05, 'epoch': 0.57}\n",
      "{'loss': 1.211, 'grad_norm': 0.5625, 'learning_rate': 7.592738304562848e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0618, 'grad_norm': 1.109375, 'learning_rate': 7.591602066816462e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2935, 'grad_norm': 0.7109375, 'learning_rate': 7.590465829070074e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1019, 'grad_norm': 1.015625, 'learning_rate': 7.58932959132369e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2311, 'grad_norm': 0.4296875, 'learning_rate': 7.588193353577303e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2496, 'grad_norm': 0.609375, 'learning_rate': 7.587057115830917e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0963, 'grad_norm': 0.5703125, 'learning_rate': 7.585920878084531e-05, 'epoch': 0.57}\n",
      "{'loss': 1.4643, 'grad_norm': 0.51171875, 'learning_rate': 7.584784640338145e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1962, 'grad_norm': 0.7109375, 'learning_rate': 7.583648402591758e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2389, 'grad_norm': 0.53125, 'learning_rate': 7.582512164845372e-05, 'epoch': 0.57}\n",
      "{'loss': 1.272, 'grad_norm': 0.66796875, 'learning_rate': 7.581375927098987e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1375, 'grad_norm': 1.1640625, 'learning_rate': 7.580239689352601e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4728, 'grad_norm': 0.5546875, 'learning_rate': 7.579103451606215e-05, 'epoch': 0.57}\n",
      "{'loss': 1.198, 'grad_norm': 0.7578125, 'learning_rate': 7.577967213859827e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2861, 'grad_norm': 0.43359375, 'learning_rate': 7.576830976113442e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2275, 'grad_norm': 0.609375, 'learning_rate': 7.575694738367056e-05, 'epoch': 0.57}\n",
      "{'loss': 0.9572, 'grad_norm': 0.498046875, 'learning_rate': 7.57455850062067e-05, 'epoch': 0.57}\n",
      "{'loss': 1.359, 'grad_norm': 0.70703125, 'learning_rate': 7.573422262874285e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0973, 'grad_norm': 0.8125, 'learning_rate': 7.572286025127899e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1587, 'grad_norm': 0.427734375, 'learning_rate': 7.571149787381511e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1479, 'grad_norm': 0.61328125, 'learning_rate': 7.570013549635126e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0943, 'grad_norm': 1.0, 'learning_rate': 7.56887731188874e-05, 'epoch': 0.57}\n",
      "{'loss': 1.4015, 'grad_norm': 0.56640625, 'learning_rate': 7.567741074142354e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1937, 'grad_norm': 0.94140625, 'learning_rate': 7.566604836395968e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1099, 'grad_norm': 0.5390625, 'learning_rate': 7.565468598649582e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0768, 'grad_norm': 0.515625, 'learning_rate': 7.564332360903195e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1355, 'grad_norm': 0.73828125, 'learning_rate': 7.563196123156809e-05, 'epoch': 0.57}\n",
      "{'loss': 1.4791, 'grad_norm': 0.6015625, 'learning_rate': 7.562059885410424e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2161, 'grad_norm': 1.1015625, 'learning_rate': 7.560923647664038e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3793, 'grad_norm': 0.56640625, 'learning_rate': 7.559787409917652e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3043, 'grad_norm': 0.6015625, 'learning_rate': 7.558651172171264e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2307, 'grad_norm': 0.9609375, 'learning_rate': 7.55751493442488e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2696, 'grad_norm': 0.65625, 'learning_rate': 7.556378696678493e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1088, 'grad_norm': 1.03125, 'learning_rate': 7.555242458932107e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2244, 'grad_norm': 0.625, 'learning_rate': 7.554106221185722e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1306, 'grad_norm': 0.609375, 'learning_rate': 7.552969983439336e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0877, 'grad_norm': 1.859375, 'learning_rate': 7.551833745692948e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3612, 'grad_norm': 0.53515625, 'learning_rate': 7.550697507946563e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2378, 'grad_norm': 0.7890625, 'learning_rate': 7.549561270200177e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2316, 'grad_norm': 0.55078125, 'learning_rate': 7.548425032453791e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2544, 'grad_norm': 0.84375, 'learning_rate': 7.547288794707405e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0766, 'grad_norm': 0.6640625, 'learning_rate': 7.546152556961019e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2466, 'grad_norm': 0.5546875, 'learning_rate': 7.545016319214632e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0604, 'grad_norm': 0.90234375, 'learning_rate': 7.543880081468246e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2218, 'grad_norm': 0.52734375, 'learning_rate': 7.542743843721861e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2864, 'grad_norm': 1.0078125, 'learning_rate': 7.541607605975475e-05, 'epoch': 0.57}\n",
      "{'loss': 1.122, 'grad_norm': 0.7890625, 'learning_rate': 7.540471368229089e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3779, 'grad_norm': 0.859375, 'learning_rate': 7.539335130482701e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2401, 'grad_norm': 0.75390625, 'learning_rate': 7.538198892736316e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0894, 'grad_norm': 0.462890625, 'learning_rate': 7.53706265498993e-05, 'epoch': 0.57}\n",
      "{'loss': 1.398, 'grad_norm': 0.7109375, 'learning_rate': 7.535926417243544e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1509, 'grad_norm': 0.9375, 'learning_rate': 7.534790179497159e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3016, 'grad_norm': 0.515625, 'learning_rate': 7.533653941750773e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1435, 'grad_norm': 0.9140625, 'learning_rate': 7.532517704004385e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1755, 'grad_norm': 0.443359375, 'learning_rate': 7.531381466258e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2067, 'grad_norm': 0.73828125, 'learning_rate': 7.530245228511614e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0328, 'grad_norm': 0.87890625, 'learning_rate': 7.529108990765228e-05, 'epoch': 0.57}\n",
      "{'loss': 1.5823, 'grad_norm': 0.6171875, 'learning_rate': 7.527972753018842e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1245, 'grad_norm': 0.609375, 'learning_rate': 7.526836515272456e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2312, 'grad_norm': 0.61328125, 'learning_rate': 7.52570027752607e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2725, 'grad_norm': 0.671875, 'learning_rate': 7.524564039779683e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0931, 'grad_norm': 0.57421875, 'learning_rate': 7.523427802033298e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2736, 'grad_norm': 0.60546875, 'learning_rate': 7.522291564286912e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1445, 'grad_norm': 0.95703125, 'learning_rate': 7.521155326540526e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1988, 'grad_norm': 0.62890625, 'learning_rate': 7.52001908879414e-05, 'epoch': 0.57}\n",
      "{'loss': 1.161, 'grad_norm': 0.57421875, 'learning_rate': 7.518882851047754e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0543, 'grad_norm': 1.125, 'learning_rate': 7.517746613301367e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1893, 'grad_norm': 0.5234375, 'learning_rate': 7.516610375554981e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1922, 'grad_norm': 0.6015625, 'learning_rate': 7.515474137808596e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2064, 'grad_norm': 0.490234375, 'learning_rate': 7.51433790006221e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3311, 'grad_norm': 0.58203125, 'learning_rate': 7.513201662315822e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0015, 'grad_norm': 1.6015625, 'learning_rate': 7.512065424569438e-05, 'epoch': 0.57}\n",
      "{'loss': 1.4354, 'grad_norm': 0.58984375, 'learning_rate': 7.510929186823051e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1568, 'grad_norm': 0.703125, 'learning_rate': 7.509792949076665e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2035, 'grad_norm': 0.5546875, 'learning_rate': 7.508656711330279e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1949, 'grad_norm': 0.58984375, 'learning_rate': 7.507520473583893e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1018, 'grad_norm': 0.6875, 'learning_rate': 7.506384235837507e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1971, 'grad_norm': 0.5234375, 'learning_rate': 7.50524799809112e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1583, 'grad_norm': 0.83984375, 'learning_rate': 7.504111760344735e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1234, 'grad_norm': 0.4375, 'learning_rate': 7.502975522598349e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1927, 'grad_norm': 0.98046875, 'learning_rate': 7.501839284851963e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0656, 'grad_norm': 0.41796875, 'learning_rate': 7.500703047105577e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2583, 'grad_norm': 0.5390625, 'learning_rate': 7.49956680935919e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1256, 'grad_norm': 1.1171875, 'learning_rate': 7.498430571612804e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2428, 'grad_norm': 0.51953125, 'learning_rate': 7.497294333866418e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2249, 'grad_norm': 0.62890625, 'learning_rate': 7.496158096120033e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0576, 'grad_norm': 1.34375, 'learning_rate': 7.495021858373647e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2968, 'grad_norm': 0.80859375, 'learning_rate': 7.49388562062726e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1457, 'grad_norm': 1.0078125, 'learning_rate': 7.492749382880875e-05, 'epoch': 0.57}\n",
      "{'loss': 1.206, 'grad_norm': 0.451171875, 'learning_rate': 7.491613145134488e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2043, 'grad_norm': 0.57421875, 'learning_rate': 7.490476907388102e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1495, 'grad_norm': 0.6640625, 'learning_rate': 7.489340669641716e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3302, 'grad_norm': 0.4296875, 'learning_rate': 7.48820443189533e-05, 'epoch': 0.57}\n",
      "{'loss': 1.153, 'grad_norm': 0.8125, 'learning_rate': 7.487068194148944e-05, 'epoch': 0.57}\n",
      "{'loss': 1.073, 'grad_norm': 0.5078125, 'learning_rate': 7.485931956402557e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1845, 'grad_norm': 0.94140625, 'learning_rate': 7.484795718656173e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0927, 'grad_norm': 1.328125, 'learning_rate': 7.483659480909786e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3969, 'grad_norm': 0.486328125, 'learning_rate': 7.4825232431634e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1363, 'grad_norm': 1.2734375, 'learning_rate': 7.481387005417014e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3471, 'grad_norm': 0.48828125, 'learning_rate': 7.480250767670628e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2832, 'grad_norm': 0.65234375, 'learning_rate': 7.479114529924241e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0351, 'grad_norm': 1.015625, 'learning_rate': 7.477978292177855e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3927, 'grad_norm': 0.51171875, 'learning_rate': 7.47684205443147e-05, 'epoch': 0.57}\n",
      "{'loss': 1.145, 'grad_norm': 0.875, 'learning_rate': 7.475705816685084e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1237, 'grad_norm': 0.5, 'learning_rate': 7.474569578938697e-05, 'epoch': 0.57}\n",
      "{'loss': 1.194, 'grad_norm': 0.59375, 'learning_rate': 7.473433341192312e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1137, 'grad_norm': 0.79296875, 'learning_rate': 7.472297103445926e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3961, 'grad_norm': 0.51953125, 'learning_rate': 7.471160865699539e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2216, 'grad_norm': 0.67578125, 'learning_rate': 7.470024627953153e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3059, 'grad_norm': 0.7890625, 'learning_rate': 7.468888390206767e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3032, 'grad_norm': 0.6328125, 'learning_rate': 7.46775215246038e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0168, 'grad_norm': 0.4921875, 'learning_rate': 7.466615914713994e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2937, 'grad_norm': 0.625, 'learning_rate': 7.46547967696761e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0286, 'grad_norm': 0.74609375, 'learning_rate': 7.464343439221223e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3307, 'grad_norm': 0.71875, 'learning_rate': 7.463207201474837e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1896, 'grad_norm': 0.70703125, 'learning_rate': 7.462070963728451e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0892, 'grad_norm': 0.51953125, 'learning_rate': 7.460934725982065e-05, 'epoch': 0.57}\n",
      "{'loss': 1.445, 'grad_norm': 0.56640625, 'learning_rate': 7.459798488235679e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0636, 'grad_norm': 0.5859375, 'learning_rate': 7.458662250489292e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2104, 'grad_norm': 0.4453125, 'learning_rate': 7.457526012742907e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1853, 'grad_norm': 0.7421875, 'learning_rate': 7.456389774996521e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0557, 'grad_norm': 0.875, 'learning_rate': 7.455253537250134e-05, 'epoch': 0.57}\n",
      "{'loss': 1.3229, 'grad_norm': 0.58203125, 'learning_rate': 7.454117299503749e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0904, 'grad_norm': 0.71875, 'learning_rate': 7.452981061757363e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2722, 'grad_norm': 0.40234375, 'learning_rate': 7.451844824010976e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1577, 'grad_norm': 0.56640625, 'learning_rate': 7.450708586264592e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1139, 'grad_norm': 1.0078125, 'learning_rate': 7.449572348518204e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2053, 'grad_norm': 0.44140625, 'learning_rate': 7.448436110771818e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1434, 'grad_norm': 0.78515625, 'learning_rate': 7.447299873025432e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0104, 'grad_norm': 0.50390625, 'learning_rate': 7.446163635279047e-05, 'epoch': 0.57}\n",
      "{'loss': 1.2264, 'grad_norm': 0.55078125, 'learning_rate': 7.44502739753266e-05, 'epoch': 0.58}\n",
      "{'loss': 1.153, 'grad_norm': 0.703125, 'learning_rate': 7.443891159786274e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3736, 'grad_norm': 0.62109375, 'learning_rate': 7.442754922039888e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1905, 'grad_norm': 0.5390625, 'learning_rate': 7.441618684293502e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2319, 'grad_norm': 0.59375, 'learning_rate': 7.440482446547116e-05, 'epoch': 0.58}\n",
      "{'loss': 1.285, 'grad_norm': 0.470703125, 'learning_rate': 7.43934620880073e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0921, 'grad_norm': 1.3046875, 'learning_rate': 7.438209971054345e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3624, 'grad_norm': 0.4609375, 'learning_rate': 7.437073733307958e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1651, 'grad_norm': 0.80078125, 'learning_rate': 7.435937495561571e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2006, 'grad_norm': 0.392578125, 'learning_rate': 7.434801257815186e-05, 'epoch': 0.58}\n",
      "{'loss': 1.238, 'grad_norm': 0.62890625, 'learning_rate': 7.4336650200688e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0539, 'grad_norm': 1.09375, 'learning_rate': 7.432528782322413e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1832, 'grad_norm': 0.54296875, 'learning_rate': 7.431392544576029e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1944, 'grad_norm': 0.984375, 'learning_rate': 7.430256306829641e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1409, 'grad_norm': 0.40625, 'learning_rate': 7.429120069083255e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1674, 'grad_norm': 0.51953125, 'learning_rate': 7.427983831336869e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1226, 'grad_norm': 0.8125, 'learning_rate': 7.426847593590484e-05, 'epoch': 0.58}\n",
      "{'loss': 1.254, 'grad_norm': 0.6484375, 'learning_rate': 7.425711355844098e-05, 'epoch': 0.58}\n",
      "{'loss': 1.174, 'grad_norm': 0.5390625, 'learning_rate': 7.424575118097711e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3392, 'grad_norm': 0.458984375, 'learning_rate': 7.423438880351325e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1866, 'grad_norm': 1.0625, 'learning_rate': 7.422302642604939e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0426, 'grad_norm': 0.99609375, 'learning_rate': 7.421166404858553e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2527, 'grad_norm': 0.51953125, 'learning_rate': 7.420030167112166e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1434, 'grad_norm': 0.74609375, 'learning_rate': 7.418893929365782e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3626, 'grad_norm': 0.4921875, 'learning_rate': 7.417757691619395e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3122, 'grad_norm': 0.84765625, 'learning_rate': 7.416621453873008e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0512, 'grad_norm': 1.25, 'learning_rate': 7.415485216126623e-05, 'epoch': 0.58}\n",
      "{'loss': 1.339, 'grad_norm': 0.453125, 'learning_rate': 7.414348978380237e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0329, 'grad_norm': 0.65625, 'learning_rate': 7.41321274063385e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2071, 'grad_norm': 0.51953125, 'learning_rate': 7.412076502887466e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2493, 'grad_norm': 0.66796875, 'learning_rate': 7.410940265141078e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9937, 'grad_norm': 1.25, 'learning_rate': 7.409804027394692e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4125, 'grad_norm': 0.7890625, 'learning_rate': 7.408667789648306e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1397, 'grad_norm': 0.56640625, 'learning_rate': 7.407531551901921e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1752, 'grad_norm': 0.4375, 'learning_rate': 7.406395314155535e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1383, 'grad_norm': 0.55859375, 'learning_rate': 7.405259076409148e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0539, 'grad_norm': 0.6484375, 'learning_rate': 7.404122838662762e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3365, 'grad_norm': 0.53515625, 'learning_rate': 7.402986600916376e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0855, 'grad_norm': 0.59765625, 'learning_rate': 7.40185036316999e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2353, 'grad_norm': 0.478515625, 'learning_rate': 7.400714125423605e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2706, 'grad_norm': 0.6484375, 'learning_rate': 7.399577887677219e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0693, 'grad_norm': 0.98828125, 'learning_rate': 7.398441649930832e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3948, 'grad_norm': 0.5625, 'learning_rate': 7.397305412184445e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1811, 'grad_norm': 0.87890625, 'learning_rate': 7.39616917443806e-05, 'epoch': 0.58}\n",
      "{'loss': 1.25, 'grad_norm': 0.482421875, 'learning_rate': 7.395032936691674e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2603, 'grad_norm': 0.62890625, 'learning_rate': 7.393896698945288e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0689, 'grad_norm': 0.6484375, 'learning_rate': 7.392760461198903e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2673, 'grad_norm': 0.51171875, 'learning_rate': 7.391624223452515e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2602, 'grad_norm': 1.125, 'learning_rate': 7.390487985706129e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2463, 'grad_norm': 0.455078125, 'learning_rate': 7.389351747959743e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2283, 'grad_norm': 0.81640625, 'learning_rate': 7.388215510213358e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1039, 'grad_norm': 0.578125, 'learning_rate': 7.387079272466972e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2514, 'grad_norm': 0.6015625, 'learning_rate': 7.385943034720585e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1837, 'grad_norm': 0.69140625, 'learning_rate': 7.384806796974199e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2124, 'grad_norm': 0.474609375, 'learning_rate': 7.383670559227813e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1985, 'grad_norm': 0.7734375, 'learning_rate': 7.382534321481427e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0766, 'grad_norm': 0.7578125, 'learning_rate': 7.381398083735042e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3509, 'grad_norm': 0.55078125, 'learning_rate': 7.380261845988656e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1688, 'grad_norm': 0.80859375, 'learning_rate': 7.37912560824227e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1962, 'grad_norm': 0.51171875, 'learning_rate': 7.377989370495882e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2333, 'grad_norm': 0.6953125, 'learning_rate': 7.376853132749497e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0511, 'grad_norm': 0.8046875, 'learning_rate': 7.375716895003111e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2736, 'grad_norm': 0.58984375, 'learning_rate': 7.374580657256725e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0814, 'grad_norm': 0.75390625, 'learning_rate': 7.37344441951034e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0729, 'grad_norm': 0.7109375, 'learning_rate': 7.372308181763952e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2837, 'grad_norm': 0.8046875, 'learning_rate': 7.371171944017566e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2264, 'grad_norm': 1.1328125, 'learning_rate': 7.37003570627118e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3275, 'grad_norm': 0.52734375, 'learning_rate': 7.368899468524795e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0866, 'grad_norm': 0.73046875, 'learning_rate': 7.367763230778409e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1852, 'grad_norm': 0.44140625, 'learning_rate': 7.366626993032022e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1834, 'grad_norm': 0.71875, 'learning_rate': 7.365490755285636e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1026, 'grad_norm': 0.7734375, 'learning_rate': 7.36435451753925e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3418, 'grad_norm': 0.609375, 'learning_rate': 7.363218279792864e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0759, 'grad_norm': 0.6484375, 'learning_rate': 7.362082042046479e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2937, 'grad_norm': 0.50390625, 'learning_rate': 7.360945804300093e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2521, 'grad_norm': 0.5234375, 'learning_rate': 7.359809566553707e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0728, 'grad_norm': 0.78125, 'learning_rate': 7.358673328807319e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3664, 'grad_norm': 0.63671875, 'learning_rate': 7.357537091060934e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2474, 'grad_norm': 0.703125, 'learning_rate': 7.356400853314548e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1031, 'grad_norm': 0.478515625, 'learning_rate': 7.355264615568162e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1009, 'grad_norm': 0.578125, 'learning_rate': 7.354128377821777e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0862, 'grad_norm': 0.52734375, 'learning_rate': 7.352992140075389e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3359, 'grad_norm': 0.52734375, 'learning_rate': 7.351855902329003e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1578, 'grad_norm': 0.466796875, 'learning_rate': 7.350719664582617e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1766, 'grad_norm': 0.49609375, 'learning_rate': 7.349583426836232e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2404, 'grad_norm': 0.9609375, 'learning_rate': 7.348447189089846e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9319, 'grad_norm': 0.37890625, 'learning_rate': 7.34731095134346e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2801, 'grad_norm': 0.5625, 'learning_rate': 7.346174713597073e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2027, 'grad_norm': 0.62890625, 'learning_rate': 7.345038475850687e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3008, 'grad_norm': 0.45703125, 'learning_rate': 7.343902238104301e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2012, 'grad_norm': 0.6328125, 'learning_rate': 7.342766000357916e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0574, 'grad_norm': 1.1953125, 'learning_rate': 7.34162976261153e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2682, 'grad_norm': 0.46484375, 'learning_rate': 7.340493524865144e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2999, 'grad_norm': 0.671875, 'learning_rate': 7.339357287118756e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2276, 'grad_norm': 0.44921875, 'learning_rate': 7.338221049372371e-05, 'epoch': 0.58}\n",
      "{'loss': 1.345, 'grad_norm': 0.515625, 'learning_rate': 7.337084811625985e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1338, 'grad_norm': 0.96484375, 'learning_rate': 7.335948573879599e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2222, 'grad_norm': 0.59765625, 'learning_rate': 7.334812336133214e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1594, 'grad_norm': 0.77734375, 'learning_rate': 7.333676098386826e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0952, 'grad_norm': 0.5234375, 'learning_rate': 7.33253986064044e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1349, 'grad_norm': 0.5546875, 'learning_rate': 7.331403622894055e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0703, 'grad_norm': 0.66796875, 'learning_rate': 7.330267385147669e-05, 'epoch': 0.58}\n",
      "{'loss': 1.4145, 'grad_norm': 0.54296875, 'learning_rate': 7.329131147401283e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2357, 'grad_norm': 0.6640625, 'learning_rate': 7.327994909654897e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1011, 'grad_norm': 0.431640625, 'learning_rate': 7.32685867190851e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2647, 'grad_norm': 0.625, 'learning_rate': 7.325722434162124e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0949, 'grad_norm': 0.640625, 'learning_rate': 7.324586196415738e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2734, 'grad_norm': 0.546875, 'learning_rate': 7.323449958669353e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1367, 'grad_norm': 1.0, 'learning_rate': 7.322313720922967e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0433, 'grad_norm': 0.49609375, 'learning_rate': 7.32117748317658e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2996, 'grad_norm': 0.578125, 'learning_rate': 7.320041245430193e-05, 'epoch': 0.58}\n",
      "{'loss': 0.958, 'grad_norm': 1.28125, 'learning_rate': 7.318905007683808e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2078, 'grad_norm': 0.625, 'learning_rate': 7.317768769937422e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0768, 'grad_norm': 0.71875, 'learning_rate': 7.316632532191036e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0983, 'grad_norm': 0.470703125, 'learning_rate': 7.315496294444651e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2849, 'grad_norm': 0.55859375, 'learning_rate': 7.314360056698263e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0728, 'grad_norm': 1.140625, 'learning_rate': 7.313223818951877e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3963, 'grad_norm': 0.6328125, 'learning_rate': 7.312087581205492e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0969, 'grad_norm': 0.984375, 'learning_rate': 7.310951343459106e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2249, 'grad_norm': 0.5234375, 'learning_rate': 7.30981510571272e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2422, 'grad_norm': 0.54296875, 'learning_rate': 7.308678867966334e-05, 'epoch': 0.58}\n",
      "{'loss': 1.064, 'grad_norm': 0.61328125, 'learning_rate': 7.307542630219947e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3775, 'grad_norm': 0.55078125, 'learning_rate': 7.306406392473561e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1755, 'grad_norm': 0.6328125, 'learning_rate': 7.305270154727175e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2061, 'grad_norm': 0.609375, 'learning_rate': 7.30413391698079e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3194, 'grad_norm': 0.6484375, 'learning_rate': 7.302997679234404e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0348, 'grad_norm': 1.078125, 'learning_rate': 7.301861441488018e-05, 'epoch': 0.58}\n",
      "{'loss': 1.4015, 'grad_norm': 0.5546875, 'learning_rate': 7.30072520374163e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2091, 'grad_norm': 1.453125, 'learning_rate': 7.299588965995245e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0906, 'grad_norm': 0.5703125, 'learning_rate': 7.298452728248859e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2489, 'grad_norm': 0.73828125, 'learning_rate': 7.297316490502473e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0802, 'grad_norm': 0.90234375, 'learning_rate': 7.296180252756088e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3088, 'grad_norm': 0.52734375, 'learning_rate': 7.2950440150097e-05, 'epoch': 0.58}\n",
      "{'loss': 1.293, 'grad_norm': 0.91796875, 'learning_rate': 7.293907777263314e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3309, 'grad_norm': 0.44140625, 'learning_rate': 7.29277153951693e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2372, 'grad_norm': 0.484375, 'learning_rate': 7.291635301770543e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0379, 'grad_norm': 0.7265625, 'learning_rate': 7.290499064024157e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3196, 'grad_norm': 0.48046875, 'learning_rate': 7.289362826277771e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1073, 'grad_norm': 0.640625, 'learning_rate': 7.288226588531385e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2566, 'grad_norm': 0.458984375, 'learning_rate': 7.287090350784998e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1972, 'grad_norm': 1.25, 'learning_rate': 7.285954113038612e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1462, 'grad_norm': 0.7578125, 'learning_rate': 7.284817875292227e-05, 'epoch': 0.58}\n",
      "{'loss': 1.5144, 'grad_norm': 0.58984375, 'learning_rate': 7.283681637545841e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0963, 'grad_norm': 0.96484375, 'learning_rate': 7.282545399799455e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2859, 'grad_norm': 0.56640625, 'learning_rate': 7.281409162053067e-05, 'epoch': 0.58}\n",
      "{'loss': 1.268, 'grad_norm': 0.63671875, 'learning_rate': 7.280272924306682e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0598, 'grad_norm': 0.98046875, 'learning_rate': 7.279136686560296e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2492, 'grad_norm': 0.578125, 'learning_rate': 7.27800044881391e-05, 'epoch': 0.58}\n",
      "{'loss': 1.168, 'grad_norm': 0.66015625, 'learning_rate': 7.276864211067525e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1561, 'grad_norm': 0.44140625, 'learning_rate': 7.275727973321138e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2512, 'grad_norm': 0.875, 'learning_rate': 7.274591735574751e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9805, 'grad_norm': 0.7265625, 'learning_rate': 7.273455497828366e-05, 'epoch': 0.58}\n",
      "{'loss': 1.4196, 'grad_norm': 0.6015625, 'learning_rate': 7.27231926008198e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1239, 'grad_norm': 0.75390625, 'learning_rate': 7.271183022335594e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1854, 'grad_norm': 0.486328125, 'learning_rate': 7.270046784589208e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2064, 'grad_norm': 0.482421875, 'learning_rate': 7.268910546842822e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0573, 'grad_norm': 0.78515625, 'learning_rate': 7.267774309096435e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2899, 'grad_norm': 0.48828125, 'learning_rate': 7.266638071350049e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1827, 'grad_norm': 0.765625, 'learning_rate': 7.265501833603664e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2626, 'grad_norm': 0.451171875, 'learning_rate': 7.264365595857278e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3149, 'grad_norm': 0.55078125, 'learning_rate': 7.263229358110892e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1541, 'grad_norm': 0.859375, 'learning_rate': 7.262093120364506e-05, 'epoch': 0.58}\n",
      "{'loss': 1.271, 'grad_norm': 0.64453125, 'learning_rate': 7.26095688261812e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1443, 'grad_norm': 0.73046875, 'learning_rate': 7.259820644871733e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1499, 'grad_norm': 0.59375, 'learning_rate': 7.258684407125347e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3272, 'grad_norm': 0.5859375, 'learning_rate': 7.257548169378962e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0829, 'grad_norm': 1.21875, 'learning_rate': 7.256411931632575e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2578, 'grad_norm': 0.61328125, 'learning_rate': 7.255275693886188e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2878, 'grad_norm': 0.8671875, 'learning_rate': 7.254139456139803e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0399, 'grad_norm': 0.470703125, 'learning_rate': 7.253003218393417e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1888, 'grad_norm': 0.56640625, 'learning_rate': 7.251866980647031e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9914, 'grad_norm': 1.1484375, 'learning_rate': 7.250730742900645e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3281, 'grad_norm': 0.83984375, 'learning_rate': 7.249594505154259e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1688, 'grad_norm': 0.671875, 'learning_rate': 7.248458267407872e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3119, 'grad_norm': 0.625, 'learning_rate': 7.247322029661486e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1696, 'grad_norm': 0.6953125, 'learning_rate': 7.246185791915101e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9409, 'grad_norm': 0.423828125, 'learning_rate': 7.245049554168715e-05, 'epoch': 0.58}\n",
      "{'loss': 1.296, 'grad_norm': 0.51953125, 'learning_rate': 7.243913316422329e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2007, 'grad_norm': 0.92578125, 'learning_rate': 7.242777078675943e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1035, 'grad_norm': 0.470703125, 'learning_rate': 7.241640840929556e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2861, 'grad_norm': 0.62109375, 'learning_rate': 7.24050460318317e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1109, 'grad_norm': 0.8671875, 'learning_rate': 7.239368365436784e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3064, 'grad_norm': 0.91796875, 'learning_rate': 7.238232127690399e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1615, 'grad_norm': 0.796875, 'learning_rate': 7.237095889944012e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1077, 'grad_norm': 0.51171875, 'learning_rate': 7.235959652197625e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1934, 'grad_norm': 0.68359375, 'learning_rate': 7.23482341445124e-05, 'epoch': 0.58}\n",
      "{'loss': 1.122, 'grad_norm': 0.828125, 'learning_rate': 7.233687176704854e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2212, 'grad_norm': 0.515625, 'learning_rate': 7.232550938958468e-05, 'epoch': 0.58}\n",
      "{'loss': 1.225, 'grad_norm': 0.78125, 'learning_rate': 7.231414701212082e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1695, 'grad_norm': 0.447265625, 'learning_rate': 7.230278463465696e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2272, 'grad_norm': 0.69921875, 'learning_rate': 7.22914222571931e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0733, 'grad_norm': 0.51171875, 'learning_rate': 7.228005987972923e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3055, 'grad_norm': 0.53515625, 'learning_rate': 7.226869750226538e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0787, 'grad_norm': 0.6484375, 'learning_rate': 7.225733512480152e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1129, 'grad_norm': 0.453125, 'learning_rate': 7.224597274733766e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3635, 'grad_norm': 0.60546875, 'learning_rate': 7.22346103698738e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1394, 'grad_norm': 1.0234375, 'learning_rate': 7.222324799240994e-05, 'epoch': 0.58}\n",
      "{'loss': 1.226, 'grad_norm': 0.5390625, 'learning_rate': 7.221188561494607e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1652, 'grad_norm': 0.8125, 'learning_rate': 7.220052323748221e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2026, 'grad_norm': 0.40234375, 'learning_rate': 7.218916086001836e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1762, 'grad_norm': 0.62890625, 'learning_rate': 7.217779848255449e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0473, 'grad_norm': 0.796875, 'learning_rate': 7.216643610509062e-05, 'epoch': 0.58}\n",
      "{'loss': 1.3799, 'grad_norm': 0.625, 'learning_rate': 7.215507372762678e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2022, 'grad_norm': 0.9140625, 'learning_rate': 7.214371135016291e-05, 'epoch': 0.59}\n",
      "{'loss': 1.21, 'grad_norm': 0.44140625, 'learning_rate': 7.213234897269905e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2173, 'grad_norm': 0.8984375, 'learning_rate': 7.212098659523519e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1216, 'grad_norm': 1.0078125, 'learning_rate': 7.210962421777133e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2934, 'grad_norm': 0.609375, 'learning_rate': 7.209826184030747e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1814, 'grad_norm': 1.0078125, 'learning_rate': 7.20868994628436e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2845, 'grad_norm': 0.404296875, 'learning_rate': 7.207553708537975e-05, 'epoch': 0.59}\n",
      "{'loss': 1.38, 'grad_norm': 0.58984375, 'learning_rate': 7.206417470791589e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9516, 'grad_norm': 0.6328125, 'learning_rate': 7.205281233045203e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2646, 'grad_norm': 0.5390625, 'learning_rate': 7.204144995298817e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2841, 'grad_norm': 1.5234375, 'learning_rate': 7.20300875755243e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2004, 'grad_norm': 0.5234375, 'learning_rate': 7.201872519806044e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1155, 'grad_norm': 0.62890625, 'learning_rate': 7.200736282059658e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0076, 'grad_norm': 1.0, 'learning_rate': 7.199600044313273e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3744, 'grad_norm': 0.51171875, 'learning_rate': 7.198463806566886e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2088, 'grad_norm': 0.7734375, 'learning_rate': 7.1973275688205e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0962, 'grad_norm': 0.609375, 'learning_rate': 7.196191331074115e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3106, 'grad_norm': 0.70703125, 'learning_rate': 7.195055093327728e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9832, 'grad_norm': 0.7265625, 'learning_rate': 7.193918855581342e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3558, 'grad_norm': 0.64453125, 'learning_rate': 7.192782617834956e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2187, 'grad_norm': 0.72265625, 'learning_rate': 7.19164638008857e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2338, 'grad_norm': 0.640625, 'learning_rate': 7.190510142342184e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2372, 'grad_norm': 0.62890625, 'learning_rate': 7.189373904595797e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9822, 'grad_norm': 0.60546875, 'learning_rate': 7.188237666849413e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3788, 'grad_norm': 0.50390625, 'learning_rate': 7.187101429103026e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2252, 'grad_norm': 0.86328125, 'learning_rate': 7.18596519135664e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2231, 'grad_norm': 0.4140625, 'learning_rate': 7.184828953610254e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2545, 'grad_norm': 0.53515625, 'learning_rate': 7.183692715863868e-05, 'epoch': 0.59}\n",
      "{'loss': 0.983, 'grad_norm': 0.625, 'learning_rate': 7.182556478117481e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3408, 'grad_norm': 0.51953125, 'learning_rate': 7.181420240371095e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1977, 'grad_norm': 0.7890625, 'learning_rate': 7.18028400262471e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1794, 'grad_norm': 0.5, 'learning_rate': 7.179147764878323e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3156, 'grad_norm': 0.49609375, 'learning_rate': 7.178011527131937e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0494, 'grad_norm': 0.462890625, 'learning_rate': 7.176875289385552e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3413, 'grad_norm': 0.66015625, 'learning_rate': 7.175739051639166e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2252, 'grad_norm': 0.87890625, 'learning_rate': 7.174602813892779e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1995, 'grad_norm': 0.48828125, 'learning_rate': 7.173466576146393e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2755, 'grad_norm': 0.8203125, 'learning_rate': 7.172330338400007e-05, 'epoch': 0.59}\n",
      "{'loss': 1.062, 'grad_norm': 0.77734375, 'learning_rate': 7.17119410065362e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3386, 'grad_norm': 0.474609375, 'learning_rate': 7.170057862907234e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1628, 'grad_norm': 0.75390625, 'learning_rate': 7.16892162516085e-05, 'epoch': 0.59}\n",
      "{'loss': 1.18, 'grad_norm': 0.55078125, 'learning_rate': 7.167785387414463e-05, 'epoch': 0.59}\n",
      "{'loss': 1.171, 'grad_norm': 0.62109375, 'learning_rate': 7.166649149668077e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1088, 'grad_norm': 1.0703125, 'learning_rate': 7.165512911921691e-05, 'epoch': 0.59}\n",
      "{'loss': 1.4626, 'grad_norm': 0.56640625, 'learning_rate': 7.164376674175305e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1532, 'grad_norm': 0.73046875, 'learning_rate': 7.163240436428919e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2536, 'grad_norm': 0.5234375, 'learning_rate': 7.162104198682532e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3843, 'grad_norm': 0.5703125, 'learning_rate': 7.160967960936147e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0945, 'grad_norm': 0.62890625, 'learning_rate': 7.15983172318976e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2815, 'grad_norm': 0.47265625, 'learning_rate': 7.158695485443374e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1605, 'grad_norm': 0.734375, 'learning_rate': 7.157559247696989e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1574, 'grad_norm': 0.482421875, 'learning_rate': 7.156423009950603e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1409, 'grad_norm': 0.64453125, 'learning_rate': 7.155286772204216e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0502, 'grad_norm': 0.83203125, 'learning_rate': 7.15415053445783e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.439, 'grad_norm': 0.69140625, 'learning_rate': 7.153014296711444e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2355, 'grad_norm': 0.69921875, 'learning_rate': 7.151878058965058e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0788, 'grad_norm': 0.44921875, 'learning_rate': 7.150741821218672e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1987, 'grad_norm': 0.58203125, 'learning_rate': 7.149605583472287e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1359, 'grad_norm': 1.1796875, 'learning_rate': 7.1484693457259e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3212, 'grad_norm': 0.5703125, 'learning_rate': 7.147333107979514e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1692, 'grad_norm': 0.69140625, 'learning_rate': 7.146196870233128e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1698, 'grad_norm': 0.6015625, 'learning_rate': 7.145060632486742e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2053, 'grad_norm': 0.56640625, 'learning_rate': 7.143924394740356e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1451, 'grad_norm': 1.0078125, 'learning_rate': 7.142788156993971e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2919, 'grad_norm': 0.80859375, 'learning_rate': 7.141651919247585e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1099, 'grad_norm': 1.3046875, 'learning_rate': 7.140515681501197e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1715, 'grad_norm': 0.470703125, 'learning_rate': 7.139379443754811e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3236, 'grad_norm': 0.58984375, 'learning_rate': 7.138243206008426e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0698, 'grad_norm': 0.828125, 'learning_rate': 7.13710696826204e-05, 'epoch': 0.59}\n",
      "{'loss': 1.4175, 'grad_norm': 0.59375, 'learning_rate': 7.135970730515653e-05, 'epoch': 0.59}\n",
      "{'loss': 1.079, 'grad_norm': 0.66015625, 'learning_rate': 7.134834492769267e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3825, 'grad_norm': 0.50390625, 'learning_rate': 7.133698255022881e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1135, 'grad_norm': 0.55859375, 'learning_rate': 7.132562017276495e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9922, 'grad_norm': 1.15625, 'learning_rate': 7.131425779530109e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3011, 'grad_norm': 0.46875, 'learning_rate': 7.130289541783724e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0894, 'grad_norm': 0.67578125, 'learning_rate': 7.129153304037338e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2164, 'grad_norm': 0.546875, 'learning_rate': 7.128017066290951e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2667, 'grad_norm': 0.5859375, 'learning_rate': 7.126880828544565e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2135, 'grad_norm': 0.734375, 'learning_rate': 7.125744590798179e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1952, 'grad_norm': 0.4765625, 'learning_rate': 7.124608353051793e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1746, 'grad_norm': 0.9375, 'learning_rate': 7.123472115305408e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2107, 'grad_norm': 0.625, 'learning_rate': 7.122335877559022e-05, 'epoch': 0.59}\n",
      "{'loss': 1.19, 'grad_norm': 0.546875, 'learning_rate': 7.121199639812634e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9553, 'grad_norm': 0.82421875, 'learning_rate': 7.120063402066248e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3091, 'grad_norm': 0.482421875, 'learning_rate': 7.118927164319863e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1859, 'grad_norm': 0.53125, 'learning_rate': 7.117790926573477e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2516, 'grad_norm': 0.4140625, 'learning_rate': 7.11665468882709e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1369, 'grad_norm': 0.625, 'learning_rate': 7.115518451080704e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1217, 'grad_norm': 1.3203125, 'learning_rate': 7.114382213334318e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2883, 'grad_norm': 0.6015625, 'learning_rate': 7.113245975587932e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1282, 'grad_norm': 0.69140625, 'learning_rate': 7.112109737841546e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2033, 'grad_norm': 0.482421875, 'learning_rate': 7.110973500095161e-05, 'epoch': 0.59}\n",
      "{'loss': 1.142, 'grad_norm': 0.91015625, 'learning_rate': 7.109837262348775e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0396, 'grad_norm': 0.97265625, 'learning_rate': 7.108701024602388e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3624, 'grad_norm': 0.58984375, 'learning_rate': 7.107564786856002e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1276, 'grad_norm': 0.59765625, 'learning_rate': 7.106428549109616e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2574, 'grad_norm': 0.486328125, 'learning_rate': 7.10529231136323e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1663, 'grad_norm': 0.671875, 'learning_rate': 7.104156073616845e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0179, 'grad_norm': 0.78125, 'learning_rate': 7.103019835870459e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3964, 'grad_norm': 0.46484375, 'learning_rate': 7.101883598124071e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0572, 'grad_norm': 1.4609375, 'learning_rate': 7.100747360377685e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0886, 'grad_norm': 0.65234375, 'learning_rate': 7.0996111226313e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3036, 'grad_norm': 0.66796875, 'learning_rate': 7.098474884884914e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1603, 'grad_norm': 1.125, 'learning_rate': 7.097338647138528e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3072, 'grad_norm': 0.625, 'learning_rate': 7.096202409392141e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0733, 'grad_norm': 0.74609375, 'learning_rate': 7.095066171645755e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0919, 'grad_norm': 0.55078125, 'learning_rate': 7.093929933899369e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2485, 'grad_norm': 0.6875, 'learning_rate': 7.092793696152983e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0965, 'grad_norm': 1.1875, 'learning_rate': 7.091657458406598e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3445, 'grad_norm': 0.5546875, 'learning_rate': 7.090521220660212e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3035, 'grad_norm': 1.0625, 'learning_rate': 7.089384982913825e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2977, 'grad_norm': 0.396484375, 'learning_rate': 7.088248745167439e-05, 'epoch': 0.59}\n",
      "{'loss': 1.277, 'grad_norm': 0.59375, 'learning_rate': 7.087112507421053e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1498, 'grad_norm': 0.8671875, 'learning_rate': 7.085976269674667e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3957, 'grad_norm': 0.55078125, 'learning_rate': 7.084840031928282e-05, 'epoch': 0.59}\n",
      "{'loss': 1.157, 'grad_norm': 0.6640625, 'learning_rate': 7.083703794181896e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0138, 'grad_norm': 0.54296875, 'learning_rate': 7.082567556435508e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2423, 'grad_norm': 0.7265625, 'learning_rate': 7.081431318689122e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9542, 'grad_norm': 0.87890625, 'learning_rate': 7.080295080942737e-05, 'epoch': 0.59}\n",
      "{'loss': 1.223, 'grad_norm': 0.546875, 'learning_rate': 7.079158843196351e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1838, 'grad_norm': 0.89453125, 'learning_rate': 7.078022605449965e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2863, 'grad_norm': 0.455078125, 'learning_rate': 7.076886367703578e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1485, 'grad_norm': 0.60546875, 'learning_rate': 7.075750129957192e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0345, 'grad_norm': 0.69140625, 'learning_rate': 7.074613892210806e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1855, 'grad_norm': 0.75390625, 'learning_rate': 7.073477654464421e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3194, 'grad_norm': 0.73046875, 'learning_rate': 7.072341416718035e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0655, 'grad_norm': 0.4765625, 'learning_rate': 7.071205178971649e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1356, 'grad_norm': 0.6640625, 'learning_rate': 7.070068941225262e-05, 'epoch': 0.59}\n",
      "{'loss': 0.988, 'grad_norm': 1.2890625, 'learning_rate': 7.068932703478876e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2554, 'grad_norm': 0.6953125, 'learning_rate': 7.06779646573249e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1242, 'grad_norm': 1.140625, 'learning_rate': 7.066660227986104e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2234, 'grad_norm': 0.54296875, 'learning_rate': 7.065523990239719e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3705, 'grad_norm': 0.6171875, 'learning_rate': 7.064387752493333e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0248, 'grad_norm': 0.396484375, 'learning_rate': 7.063251514746945e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3015, 'grad_norm': 0.52734375, 'learning_rate': 7.062115277000559e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1751, 'grad_norm': 0.75, 'learning_rate': 7.060979039254174e-05, 'epoch': 0.59}\n",
      "{'loss': 1.241, 'grad_norm': 0.7578125, 'learning_rate': 7.059842801507788e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2989, 'grad_norm': 0.6328125, 'learning_rate': 7.058706563761402e-05, 'epoch': 0.59}\n",
      "{'loss': 0.997, 'grad_norm': 1.1953125, 'learning_rate': 7.057570326015015e-05, 'epoch': 0.59}\n",
      "{'loss': 1.4323, 'grad_norm': 0.55859375, 'learning_rate': 7.056434088268629e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2208, 'grad_norm': 0.78125, 'learning_rate': 7.055297850522243e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2087, 'grad_norm': 0.427734375, 'learning_rate': 7.054161612775858e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2179, 'grad_norm': 0.5703125, 'learning_rate': 7.053025375029472e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9954, 'grad_norm': 1.078125, 'learning_rate': 7.051889137283086e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3213, 'grad_norm': 0.53515625, 'learning_rate': 7.0507528995367e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1628, 'grad_norm': 0.5859375, 'learning_rate': 7.049616661790313e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2619, 'grad_norm': 0.57421875, 'learning_rate': 7.048480424043927e-05, 'epoch': 0.59}\n",
      "{'loss': 1.249, 'grad_norm': 0.56640625, 'learning_rate': 7.047344186297541e-05, 'epoch': 0.59}\n",
      "{'loss': 1.178, 'grad_norm': 0.859375, 'learning_rate': 7.046207948551156e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3438, 'grad_norm': 0.68359375, 'learning_rate': 7.04507171080477e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1922, 'grad_norm': 0.63671875, 'learning_rate': 7.043935473058382e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1576, 'grad_norm': 0.4296875, 'learning_rate': 7.042799235311996e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2854, 'grad_norm': 0.61328125, 'learning_rate': 7.041662997565611e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0516, 'grad_norm': 1.203125, 'learning_rate': 7.040526759819225e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3744, 'grad_norm': 0.5078125, 'learning_rate': 7.039390522072839e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1194, 'grad_norm': 0.75, 'learning_rate': 7.038254284326453e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1957, 'grad_norm': 0.4296875, 'learning_rate': 7.037118046580066e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3075, 'grad_norm': 0.79296875, 'learning_rate': 7.03598180883368e-05, 'epoch': 0.59}\n",
      "{'loss': 1.16, 'grad_norm': 0.859375, 'learning_rate': 7.034845571087295e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2285, 'grad_norm': 0.55859375, 'learning_rate': 7.033709333340909e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0303, 'grad_norm': 0.474609375, 'learning_rate': 7.032573095594523e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2895, 'grad_norm': 0.5625, 'learning_rate': 7.031436857848137e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1962, 'grad_norm': 0.671875, 'learning_rate': 7.03030062010175e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0562, 'grad_norm': 1.046875, 'learning_rate': 7.029164382355364e-05, 'epoch': 0.59}\n",
      "{'loss': 1.4639, 'grad_norm': 0.455078125, 'learning_rate': 7.028028144608978e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1841, 'grad_norm': 0.85546875, 'learning_rate': 7.026891906862593e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2481, 'grad_norm': 0.4921875, 'learning_rate': 7.025755669116207e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2615, 'grad_norm': 0.96484375, 'learning_rate': 7.02461943136982e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0631, 'grad_norm': 0.7109375, 'learning_rate': 7.023483193623434e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3347, 'grad_norm': 0.53515625, 'learning_rate': 7.022346955877048e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1503, 'grad_norm': 0.828125, 'learning_rate': 7.021210718130662e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1629, 'grad_norm': 0.58203125, 'learning_rate': 7.020074480384276e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2762, 'grad_norm': 0.515625, 'learning_rate': 7.01893824263789e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1883, 'grad_norm': 0.7578125, 'learning_rate': 7.017802004891503e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3289, 'grad_norm': 0.515625, 'learning_rate': 7.016665767145117e-05, 'epoch': 0.59}\n",
      "{'loss': 1.109, 'grad_norm': 0.828125, 'learning_rate': 7.015529529398732e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0907, 'grad_norm': 0.5234375, 'learning_rate': 7.014393291652346e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2368, 'grad_norm': 0.48828125, 'learning_rate': 7.01325705390596e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9814, 'grad_norm': 0.828125, 'learning_rate': 7.012120816159574e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3069, 'grad_norm': 0.54296875, 'learning_rate': 7.010984578413187e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1372, 'grad_norm': 0.9453125, 'learning_rate': 7.009848340666801e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1865, 'grad_norm': 0.43359375, 'learning_rate': 7.008712102920415e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2372, 'grad_norm': 0.55859375, 'learning_rate': 7.00757586517403e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9978, 'grad_norm': 0.91796875, 'learning_rate': 7.006439627427644e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2225, 'grad_norm': 0.60546875, 'learning_rate': 7.005303389681256e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2063, 'grad_norm': 0.90234375, 'learning_rate': 7.004167151934872e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2561, 'grad_norm': 0.421875, 'learning_rate': 7.003030914188485e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2659, 'grad_norm': 0.83203125, 'learning_rate': 7.001894676442099e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9756, 'grad_norm': 1.171875, 'learning_rate': 7.000758438695713e-05, 'epoch': 0.59}\n",
      "{'loss': 1.316, 'grad_norm': 0.640625, 'learning_rate': 6.999622200949327e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0985, 'grad_norm': 0.65234375, 'learning_rate': 6.99848596320294e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2492, 'grad_norm': 0.486328125, 'learning_rate': 6.997349725456554e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1476, 'grad_norm': 0.6953125, 'learning_rate': 6.99621348771017e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0503, 'grad_norm': 1.3046875, 'learning_rate': 6.995077249963783e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3031, 'grad_norm': 0.470703125, 'learning_rate': 6.993941012217397e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2323, 'grad_norm': 0.62890625, 'learning_rate': 6.992804774471011e-05, 'epoch': 0.59}\n",
      "{'loss': 1.1897, 'grad_norm': 0.5546875, 'learning_rate': 6.991668536724625e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2927, 'grad_norm': 0.53125, 'learning_rate': 6.990532298978238e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0249, 'grad_norm': 0.515625, 'learning_rate': 6.989396061231852e-05, 'epoch': 0.59}\n",
      "{'loss': 1.3658, 'grad_norm': 0.53125, 'learning_rate': 6.988259823485467e-05, 'epoch': 0.59}\n",
      "{'loss': 1.2598, 'grad_norm': 0.66796875, 'learning_rate': 6.987123585739081e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1451, 'grad_norm': 0.45703125, 'learning_rate': 6.985987347992693e-05, 'epoch': 0.6}\n",
      "{'loss': 1.188, 'grad_norm': 0.96484375, 'learning_rate': 6.984851110246309e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1541, 'grad_norm': 0.625, 'learning_rate': 6.983714872499922e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4069, 'grad_norm': 0.734375, 'learning_rate': 6.982578634753536e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1494, 'grad_norm': 0.59375, 'learning_rate': 6.98144239700715e-05, 'epoch': 0.6}\n",
      "{'loss': 1.266, 'grad_norm': 0.53515625, 'learning_rate': 6.980306159260764e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1419, 'grad_norm': 0.74609375, 'learning_rate': 6.979169921514378e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1186, 'grad_norm': 1.890625, 'learning_rate': 6.978033683767991e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3356, 'grad_norm': 0.703125, 'learning_rate': 6.976897446021606e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1285, 'grad_norm': 0.71484375, 'learning_rate': 6.97576120827522e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2713, 'grad_norm': 0.7265625, 'learning_rate': 6.974624970528834e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3287, 'grad_norm': 0.61328125, 'learning_rate': 6.973488732782448e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0227, 'grad_norm': 0.423828125, 'learning_rate': 6.972352495036062e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3148, 'grad_norm': 0.55859375, 'learning_rate': 6.971216257289675e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0735, 'grad_norm': 0.7109375, 'learning_rate': 6.970080019543289e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1984, 'grad_norm': 0.51171875, 'learning_rate': 6.968943781796904e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1181, 'grad_norm': 0.6015625, 'learning_rate': 6.967807544050518e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0475, 'grad_norm': 1.65625, 'learning_rate': 6.96667130630413e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2395, 'grad_norm': 0.70703125, 'learning_rate': 6.965535068557746e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2872, 'grad_norm': 0.73828125, 'learning_rate': 6.96439883081136e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1633, 'grad_norm': 0.498046875, 'learning_rate': 6.963262593064973e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3158, 'grad_norm': 0.8125, 'learning_rate': 6.962126355318587e-05, 'epoch': 0.6}\n",
      "{'loss': 0.9539, 'grad_norm': 0.458984375, 'learning_rate': 6.960990117572201e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3613, 'grad_norm': 0.65625, 'learning_rate': 6.959853879825815e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0747, 'grad_norm': 0.52734375, 'learning_rate': 6.958717642079428e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2193, 'grad_norm': 0.451171875, 'learning_rate': 6.957581404333044e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2564, 'grad_norm': 0.65234375, 'learning_rate': 6.956445166586657e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0394, 'grad_norm': 0.9296875, 'learning_rate': 6.955308928840271e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.261, 'grad_norm': 0.6171875, 'learning_rate': 6.954172691093885e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0883, 'grad_norm': 1.1484375, 'learning_rate': 6.953036453347499e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1375, 'grad_norm': 0.5390625, 'learning_rate': 6.951900215601112e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3199, 'grad_norm': 0.765625, 'learning_rate': 6.950763977854726e-05, 'epoch': 0.6}\n",
      "{'loss': 1.046, 'grad_norm': 0.546875, 'learning_rate': 6.949627740108341e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2968, 'grad_norm': 0.6953125, 'learning_rate': 6.948491502361955e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2199, 'grad_norm': 1.1015625, 'learning_rate': 6.947355264615568e-05, 'epoch': 0.6}\n",
      "{'loss': 1.26, 'grad_norm': 0.412109375, 'learning_rate': 6.946219026869183e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1797, 'grad_norm': 0.76171875, 'learning_rate': 6.945082789122797e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0662, 'grad_norm': 1.0234375, 'learning_rate': 6.94394655137641e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3258, 'grad_norm': 0.55859375, 'learning_rate': 6.942810313630024e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1735, 'grad_norm': 0.7734375, 'learning_rate': 6.941674075883638e-05, 'epoch': 0.6}\n",
      "{'loss': 1.155, 'grad_norm': 0.443359375, 'learning_rate': 6.940537838137252e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2919, 'grad_norm': 0.6171875, 'learning_rate': 6.939401600390865e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0896, 'grad_norm': 0.6875, 'learning_rate': 6.93826536264448e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3602, 'grad_norm': 0.546875, 'learning_rate': 6.937129124898094e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1751, 'grad_norm': 0.72265625, 'learning_rate': 6.935992887151708e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3532, 'grad_norm': 0.5390625, 'learning_rate': 6.934856649405322e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2115, 'grad_norm': 0.6171875, 'learning_rate': 6.933720411658936e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0219, 'grad_norm': 0.69921875, 'learning_rate': 6.93258417391255e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3453, 'grad_norm': 0.6171875, 'learning_rate': 6.931447936166163e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1868, 'grad_norm': 0.62890625, 'learning_rate': 6.930311698419778e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2873, 'grad_norm': 0.41796875, 'learning_rate': 6.929175460673392e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2799, 'grad_norm': 0.63671875, 'learning_rate': 6.928039222927005e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0576, 'grad_norm': 0.8203125, 'learning_rate': 6.92690298518062e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3933, 'grad_norm': 0.69921875, 'learning_rate': 6.925766747434234e-05, 'epoch': 0.6}\n",
      "{'loss': 1.192, 'grad_norm': 0.78125, 'learning_rate': 6.924630509687847e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3064, 'grad_norm': 0.458984375, 'learning_rate': 6.923494271941461e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2997, 'grad_norm': 0.84375, 'learning_rate': 6.922358034195075e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1157, 'grad_norm': 0.9140625, 'learning_rate': 6.921221796448689e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3728, 'grad_norm': 0.5, 'learning_rate': 6.920085558702302e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0222, 'grad_norm': 0.62890625, 'learning_rate': 6.918949320955918e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2117, 'grad_norm': 0.439453125, 'learning_rate': 6.917813083209531e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1842, 'grad_norm': 0.6484375, 'learning_rate': 6.916676845463145e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0488, 'grad_norm': 0.60546875, 'learning_rate': 6.915540607716759e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2295, 'grad_norm': 0.53125, 'learning_rate': 6.914404369970373e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1524, 'grad_norm': 0.77734375, 'learning_rate': 6.913268132223987e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2176, 'grad_norm': 0.40234375, 'learning_rate': 6.9121318944776e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3505, 'grad_norm': 0.65625, 'learning_rate': 6.910995656731215e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0783, 'grad_norm': 0.6875, 'learning_rate': 6.909859418984829e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2566, 'grad_norm': 0.59375, 'learning_rate': 6.908723181238442e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2217, 'grad_norm': 0.8671875, 'learning_rate': 6.907586943492057e-05, 'epoch': 0.6}\n",
      "{'loss': 1.156, 'grad_norm': 0.486328125, 'learning_rate': 6.90645070574567e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2265, 'grad_norm': 0.515625, 'learning_rate': 6.905314467999284e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1963, 'grad_norm': 1.625, 'learning_rate': 6.904178230252898e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2826, 'grad_norm': 0.6640625, 'learning_rate': 6.903041992506512e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1324, 'grad_norm': 0.6171875, 'learning_rate': 6.901905754760126e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1903, 'grad_norm': 0.6328125, 'learning_rate': 6.90076951701374e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1799, 'grad_norm': 0.57421875, 'learning_rate': 6.899633279267355e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1452, 'grad_norm': 0.75390625, 'learning_rate': 6.898497041520968e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3066, 'grad_norm': 0.71484375, 'learning_rate': 6.897360803774582e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2093, 'grad_norm': 0.48828125, 'learning_rate': 6.896224566028196e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2995, 'grad_norm': 0.3984375, 'learning_rate': 6.89508832828181e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2334, 'grad_norm': 0.87890625, 'learning_rate': 6.893952090535424e-05, 'epoch': 0.6}\n",
      "{'loss': 0.9084, 'grad_norm': 0.421875, 'learning_rate': 6.892815852789037e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3264, 'grad_norm': 0.5, 'learning_rate': 6.891679615042653e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1222, 'grad_norm': 0.9453125, 'learning_rate': 6.890543377296266e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1986, 'grad_norm': 0.439453125, 'learning_rate': 6.889407139549879e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2996, 'grad_norm': 0.6015625, 'learning_rate': 6.888270901803494e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1365, 'grad_norm': 1.09375, 'learning_rate': 6.887134664057108e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3034, 'grad_norm': 0.423828125, 'learning_rate': 6.885998426310721e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2494, 'grad_norm': 0.66015625, 'learning_rate': 6.884862188564337e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1202, 'grad_norm': 0.66015625, 'learning_rate': 6.883725950817949e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1965, 'grad_norm': 0.84765625, 'learning_rate': 6.882589713071563e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1238, 'grad_norm': 1.078125, 'learning_rate': 6.881453475325177e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3272, 'grad_norm': 0.494140625, 'learning_rate': 6.880317237578792e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1013, 'grad_norm': 0.80859375, 'learning_rate': 6.879180999832406e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2443, 'grad_norm': 0.875, 'learning_rate': 6.87804476208602e-05, 'epoch': 0.6}\n",
      "{'loss': 1.221, 'grad_norm': 0.64453125, 'learning_rate': 6.876908524339633e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1007, 'grad_norm': 0.92578125, 'learning_rate': 6.875772286593247e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2797, 'grad_norm': 0.51171875, 'learning_rate': 6.874636048846861e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1468, 'grad_norm': 0.91015625, 'learning_rate': 6.873499811100474e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1537, 'grad_norm': 0.50390625, 'learning_rate': 6.87236357335409e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1143, 'grad_norm': 0.6640625, 'learning_rate': 6.871227335607703e-05, 'epoch': 0.6}\n",
      "{'loss': 1.138, 'grad_norm': 1.015625, 'learning_rate': 6.870091097861316e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4088, 'grad_norm': 0.61328125, 'learning_rate': 6.868954860114931e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1354, 'grad_norm': 0.7734375, 'learning_rate': 6.867818622368545e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2332, 'grad_norm': 0.41796875, 'learning_rate': 6.866682384622159e-05, 'epoch': 0.6}\n",
      "{'loss': 1.297, 'grad_norm': 0.65234375, 'learning_rate': 6.865546146875774e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0725, 'grad_norm': 0.97265625, 'learning_rate': 6.864409909129386e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2716, 'grad_norm': 0.55078125, 'learning_rate': 6.863273671383e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2174, 'grad_norm': 0.69921875, 'learning_rate': 6.862137433636614e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3418, 'grad_norm': 0.3828125, 'learning_rate': 6.861001195890229e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3114, 'grad_norm': 0.59765625, 'learning_rate': 6.859864958143843e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0672, 'grad_norm': 0.83984375, 'learning_rate': 6.858728720397456e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1803, 'grad_norm': 0.53515625, 'learning_rate': 6.85759248265107e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0975, 'grad_norm': 0.8828125, 'learning_rate': 6.856456244904684e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3253, 'grad_norm': 0.42578125, 'learning_rate': 6.855320007158298e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2277, 'grad_norm': 0.55078125, 'learning_rate': 6.854183769411912e-05, 'epoch': 0.6}\n",
      "{'loss': 0.9734, 'grad_norm': 0.77734375, 'learning_rate': 6.853047531665527e-05, 'epoch': 0.6}\n",
      "{'loss': 1.4019, 'grad_norm': 0.55078125, 'learning_rate': 6.85191129391914e-05, 'epoch': 0.6}\n",
      "{'loss': 1.071, 'grad_norm': 0.77734375, 'learning_rate': 6.850775056172753e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3167, 'grad_norm': 0.4453125, 'learning_rate': 6.849638818426368e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1655, 'grad_norm': 0.62890625, 'learning_rate': 6.848502580679982e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1278, 'grad_norm': 1.0234375, 'learning_rate': 6.847366342933596e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3078, 'grad_norm': 0.65625, 'learning_rate': 6.846230105187211e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2795, 'grad_norm': 0.7578125, 'learning_rate': 6.845093867440823e-05, 'epoch': 0.6}\n",
      "{'loss': 1.12, 'grad_norm': 0.578125, 'learning_rate': 6.843957629694437e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2258, 'grad_norm': 0.609375, 'learning_rate': 6.842821391948051e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0557, 'grad_norm': 0.79296875, 'learning_rate': 6.841685154201666e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4649, 'grad_norm': 0.5546875, 'learning_rate': 6.84054891645528e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1968, 'grad_norm': 0.79296875, 'learning_rate': 6.839412678708893e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3337, 'grad_norm': 0.55078125, 'learning_rate': 6.838276440962507e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1892, 'grad_norm': 0.73046875, 'learning_rate': 6.837140203216121e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2281, 'grad_norm': 0.98046875, 'learning_rate': 6.836003965469735e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3803, 'grad_norm': 0.6640625, 'learning_rate': 6.83486772772335e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2259, 'grad_norm': 0.7578125, 'learning_rate': 6.833731489976964e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3023, 'grad_norm': 0.53125, 'learning_rate': 6.832595252230578e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1457, 'grad_norm': 0.51953125, 'learning_rate': 6.83145901448419e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0389, 'grad_norm': 0.875, 'learning_rate': 6.830322776737805e-05, 'epoch': 0.6}\n",
      "{'loss': 1.243, 'grad_norm': 0.6015625, 'learning_rate': 6.829186538991419e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2806, 'grad_norm': 0.80078125, 'learning_rate': 6.828050301245033e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2197, 'grad_norm': 0.6484375, 'learning_rate': 6.826914063498648e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1816, 'grad_norm': 0.76953125, 'learning_rate': 6.82577782575226e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0318, 'grad_norm': 0.95703125, 'learning_rate': 6.824641588005874e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3331, 'grad_norm': 0.5546875, 'learning_rate': 6.823505350259488e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2059, 'grad_norm': 0.7578125, 'learning_rate': 6.822369112513103e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0817, 'grad_norm': 0.58984375, 'learning_rate': 6.821232874766717e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2346, 'grad_norm': 0.5859375, 'learning_rate': 6.82009663702033e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2009, 'grad_norm': 2.0625, 'learning_rate': 6.818960399273944e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3367, 'grad_norm': 0.458984375, 'learning_rate': 6.817824161527558e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1612, 'grad_norm': 0.68359375, 'learning_rate': 6.816687923781172e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1839, 'grad_norm': 0.4609375, 'learning_rate': 6.815551686034787e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1404, 'grad_norm': 0.72265625, 'learning_rate': 6.814415448288401e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2005, 'grad_norm': 1.2578125, 'learning_rate': 6.813279210542015e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3576, 'grad_norm': 0.490234375, 'learning_rate': 6.812142972795627e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1927, 'grad_norm': 0.5, 'learning_rate': 6.811006735049242e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2315, 'grad_norm': 0.4765625, 'learning_rate': 6.809870497302856e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2176, 'grad_norm': 0.6171875, 'learning_rate': 6.80873425955647e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1588, 'grad_norm': 1.671875, 'learning_rate': 6.807598021810085e-05, 'epoch': 0.6}\n",
      "{'loss': 1.388, 'grad_norm': 0.53125, 'learning_rate': 6.806461784063697e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1584, 'grad_norm': 0.61328125, 'learning_rate': 6.805325546317311e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1511, 'grad_norm': 0.56640625, 'learning_rate': 6.804189308570925e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2774, 'grad_norm': 0.51171875, 'learning_rate': 6.80305307082454e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1095, 'grad_norm': 0.73828125, 'learning_rate': 6.801916833078154e-05, 'epoch': 0.6}\n",
      "{'loss': 1.364, 'grad_norm': 0.53515625, 'learning_rate': 6.800780595331768e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3042, 'grad_norm': 0.921875, 'learning_rate': 6.799644357585381e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2154, 'grad_norm': 0.462890625, 'learning_rate': 6.798508119838995e-05, 'epoch': 0.6}\n",
      "{'loss': 1.286, 'grad_norm': 0.71875, 'learning_rate': 6.797371882092609e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0944, 'grad_norm': 0.859375, 'learning_rate': 6.796235644346224e-05, 'epoch': 0.6}\n",
      "{'loss': 1.498, 'grad_norm': 0.68359375, 'learning_rate': 6.795099406599838e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1321, 'grad_norm': 0.7109375, 'learning_rate': 6.793963168853452e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1922, 'grad_norm': 0.515625, 'learning_rate': 6.792826931107064e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2238, 'grad_norm': 0.58203125, 'learning_rate': 6.791690693360679e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0487, 'grad_norm': 0.431640625, 'learning_rate': 6.790554455614293e-05, 'epoch': 0.6}\n",
      "{'loss': 1.223, 'grad_norm': 0.75390625, 'learning_rate': 6.789418217867907e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3014, 'grad_norm': 0.77734375, 'learning_rate': 6.788281980121522e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1447, 'grad_norm': 0.4375, 'learning_rate': 6.787145742375134e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2712, 'grad_norm': 0.67578125, 'learning_rate': 6.786009504628748e-05, 'epoch': 0.6}\n",
      "{'loss': 0.9923, 'grad_norm': 1.0625, 'learning_rate': 6.784873266882362e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3165, 'grad_norm': 0.69140625, 'learning_rate': 6.783737029135977e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2116, 'grad_norm': 0.73828125, 'learning_rate': 6.782600791389591e-05, 'epoch': 0.6}\n",
      "{'loss': 1.227, 'grad_norm': 0.4609375, 'learning_rate': 6.781464553643205e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2969, 'grad_norm': 0.56640625, 'learning_rate': 6.780328315896818e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1105, 'grad_norm': 1.921875, 'learning_rate': 6.779192078150432e-05, 'epoch': 0.6}\n",
      "{'loss': 1.5052, 'grad_norm': 0.58984375, 'learning_rate': 6.778055840404046e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3734, 'grad_norm': 0.65234375, 'learning_rate': 6.776919602657661e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0889, 'grad_norm': 0.474609375, 'learning_rate': 6.775783364911275e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1128, 'grad_norm': 0.6015625, 'learning_rate': 6.774647127164889e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0503, 'grad_norm': 0.94921875, 'learning_rate': 6.773510889418501e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3827, 'grad_norm': 0.5078125, 'learning_rate': 6.772374651672116e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1354, 'grad_norm': 0.67578125, 'learning_rate': 6.77123841392573e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1762, 'grad_norm': 0.482421875, 'learning_rate': 6.770102176179344e-05, 'epoch': 0.6}\n",
      "{'loss': 1.25, 'grad_norm': 0.51953125, 'learning_rate': 6.768965938432959e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0669, 'grad_norm': 0.94140625, 'learning_rate': 6.767829700686571e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3754, 'grad_norm': 0.61328125, 'learning_rate': 6.766693462940185e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1111, 'grad_norm': 0.59765625, 'learning_rate': 6.7655572251938e-05, 'epoch': 0.6}\n",
      "{'loss': 1.141, 'grad_norm': 0.42578125, 'learning_rate': 6.764420987447414e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2442, 'grad_norm': 0.62109375, 'learning_rate': 6.763284749701028e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0109, 'grad_norm': 0.8515625, 'learning_rate': 6.762148511954642e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3634, 'grad_norm': 0.51953125, 'learning_rate': 6.761012274208255e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1543, 'grad_norm': 0.921875, 'learning_rate': 6.759876036461869e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2193, 'grad_norm': 0.58203125, 'learning_rate': 6.758739798715483e-05, 'epoch': 0.6}\n",
      "{'loss': 1.24, 'grad_norm': 0.61328125, 'learning_rate': 6.757603560969098e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0877, 'grad_norm': 1.25, 'learning_rate': 6.756467323222712e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2538, 'grad_norm': 0.7578125, 'learning_rate': 6.755331085476326e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1834, 'grad_norm': 0.61328125, 'learning_rate': 6.754194847729938e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2794, 'grad_norm': 0.48828125, 'learning_rate': 6.753058609983553e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1744, 'grad_norm': 0.4921875, 'learning_rate': 6.751922372237167e-05, 'epoch': 0.61}\n",
      "{'loss': 1.124, 'grad_norm': 0.88671875, 'learning_rate': 6.750786134490781e-05, 'epoch': 0.61}\n",
      "{'loss': 1.4898, 'grad_norm': 1.3671875, 'learning_rate': 6.749649896744396e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1956, 'grad_norm': 0.71875, 'learning_rate': 6.748513658998008e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1682, 'grad_norm': 0.494140625, 'learning_rate': 6.747377421251622e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1959, 'grad_norm': 0.70703125, 'learning_rate': 6.746241183505237e-05, 'epoch': 0.61}\n",
      "{'loss': 1.139, 'grad_norm': 0.65625, 'learning_rate': 6.745104945758851e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3506, 'grad_norm': 0.59375, 'learning_rate': 6.743968708012465e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2738, 'grad_norm': 1.296875, 'learning_rate': 6.742832470266079e-05, 'epoch': 0.61}\n",
      "{'loss': 1.235, 'grad_norm': 0.7265625, 'learning_rate': 6.741696232519693e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2856, 'grad_norm': 0.578125, 'learning_rate': 6.740559994773306e-05, 'epoch': 0.61}\n",
      "{'loss': 1.014, 'grad_norm': 1.078125, 'learning_rate': 6.73942375702692e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3608, 'grad_norm': 0.93359375, 'learning_rate': 6.738287519280535e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1663, 'grad_norm': 0.7421875, 'learning_rate': 6.737151281534149e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1469, 'grad_norm': 0.515625, 'learning_rate': 6.736015043787763e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2889, 'grad_norm': 0.76953125, 'learning_rate': 6.734878806041375e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0786, 'grad_norm': 1.140625, 'learning_rate': 6.73374256829499e-05, 'epoch': 0.61}\n",
      "{'loss': 1.445, 'grad_norm': 0.75, 'learning_rate': 6.732606330548604e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0705, 'grad_norm': 0.86328125, 'learning_rate': 6.731470092802218e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2076, 'grad_norm': 0.58203125, 'learning_rate': 6.730333855055833e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2178, 'grad_norm': 0.72265625, 'learning_rate': 6.729197617309446e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0404, 'grad_norm': 1.2421875, 'learning_rate': 6.72806137956306e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4411, 'grad_norm': 0.5234375, 'learning_rate': 6.726925141816674e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3418, 'grad_norm': 0.796875, 'learning_rate': 6.725788904070288e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1937, 'grad_norm': 0.4140625, 'learning_rate': 6.724652666323902e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2716, 'grad_norm': 0.53515625, 'learning_rate': 6.723516428577516e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1621, 'grad_norm': 0.97265625, 'learning_rate': 6.72238019083113e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3186, 'grad_norm': 0.451171875, 'learning_rate': 6.721243953084743e-05, 'epoch': 0.61}\n",
      "{'loss': 1.129, 'grad_norm': 0.61328125, 'learning_rate': 6.720107715338357e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2753, 'grad_norm': 0.5625, 'learning_rate': 6.718971477591972e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2667, 'grad_norm': 0.71484375, 'learning_rate': 6.717835239845586e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0679, 'grad_norm': 0.76953125, 'learning_rate': 6.7166990020992e-05, 'epoch': 0.61}\n",
      "{'loss': 1.399, 'grad_norm': 0.4609375, 'learning_rate': 6.715562764352812e-05, 'epoch': 0.61}\n",
      "{'loss': 1.121, 'grad_norm': 0.83203125, 'learning_rate': 6.714426526606427e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2331, 'grad_norm': 0.484375, 'learning_rate': 6.713290288860041e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2693, 'grad_norm': 0.64453125, 'learning_rate': 6.712154051113655e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1158, 'grad_norm': 1.078125, 'learning_rate': 6.71101781336727e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3509, 'grad_norm': 0.5546875, 'learning_rate': 6.709881575620883e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1075, 'grad_norm': 0.76953125, 'learning_rate': 6.708745337874496e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2316, 'grad_norm': 0.427734375, 'learning_rate': 6.707609100128112e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3351, 'grad_norm': 0.609375, 'learning_rate': 6.706472862381725e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1472, 'grad_norm': 0.80078125, 'learning_rate': 6.705336624635339e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2262, 'grad_norm': 0.55078125, 'learning_rate': 6.704200386888953e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1892, 'grad_norm': 0.73828125, 'learning_rate': 6.703064149142567e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3044, 'grad_norm': 0.6484375, 'learning_rate': 6.70192791139618e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2219, 'grad_norm': 0.55078125, 'learning_rate': 6.700791673649794e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1193, 'grad_norm': 0.99609375, 'learning_rate': 6.69965543590341e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3016, 'grad_norm': 0.7421875, 'learning_rate': 6.698519198157023e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1553, 'grad_norm': 0.78125, 'learning_rate': 6.697382960410637e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0779, 'grad_norm': 0.5546875, 'learning_rate': 6.696246722664251e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2475, 'grad_norm': 0.5390625, 'learning_rate': 6.695110484917865e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0975, 'grad_norm': 0.91015625, 'learning_rate': 6.693974247171478e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2521, 'grad_norm': 0.515625, 'learning_rate': 6.692838009425092e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1766, 'grad_norm': 0.9765625, 'learning_rate': 6.691701771678707e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1821, 'grad_norm': 0.45703125, 'learning_rate': 6.69056553393232e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1272, 'grad_norm': 0.5, 'learning_rate': 6.689429296185933e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0745, 'grad_norm': 0.90234375, 'learning_rate': 6.688293058439549e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2967, 'grad_norm': 0.671875, 'learning_rate': 6.687156820693162e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1393, 'grad_norm': 0.92578125, 'learning_rate': 6.686020582946776e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1709, 'grad_norm': 0.5703125, 'learning_rate': 6.68488434520039e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1885, 'grad_norm': 0.6015625, 'learning_rate': 6.683748107454004e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1008, 'grad_norm': 0.99609375, 'learning_rate': 6.682611869707618e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2348, 'grad_norm': 0.54296875, 'learning_rate': 6.681475631961231e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0476, 'grad_norm': 0.59765625, 'learning_rate': 6.680339394214846e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2395, 'grad_norm': 0.5, 'learning_rate': 6.67920315646846e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2548, 'grad_norm': 0.67578125, 'learning_rate': 6.678066918722074e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1746, 'grad_norm': 0.7578125, 'learning_rate': 6.676930680975688e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3275, 'grad_norm': 0.59765625, 'learning_rate': 6.675794443229302e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1772, 'grad_norm': 0.91796875, 'learning_rate': 6.674658205482915e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1497, 'grad_norm': 0.408203125, 'learning_rate': 6.673521967736529e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1188, 'grad_norm': 0.5625, 'learning_rate': 6.672385729990144e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1199, 'grad_norm': 1.3828125, 'learning_rate': 6.671249492243757e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.214, 'grad_norm': 0.72265625, 'learning_rate': 6.67011325449737e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2165, 'grad_norm': 0.8203125, 'learning_rate': 6.668977016750986e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0534, 'grad_norm': 0.5, 'learning_rate': 6.6678407790046e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1551, 'grad_norm': 0.57421875, 'learning_rate': 6.666704541258213e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0318, 'grad_norm': 0.69140625, 'learning_rate': 6.665568303511827e-05, 'epoch': 0.61}\n",
      "{'loss': 1.442, 'grad_norm': 0.5625, 'learning_rate': 6.664432065765441e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0437, 'grad_norm': 0.6953125, 'learning_rate': 6.663295828019055e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2121, 'grad_norm': 0.546875, 'learning_rate': 6.662159590272668e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2307, 'grad_norm': 0.84765625, 'learning_rate': 6.661023352526284e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2055, 'grad_norm': 1.046875, 'learning_rate': 6.659887114779897e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3948, 'grad_norm': 0.55859375, 'learning_rate': 6.658750877033511e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2983, 'grad_norm': 0.65625, 'learning_rate': 6.657614639287125e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0849, 'grad_norm': 0.453125, 'learning_rate': 6.656478401540739e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2709, 'grad_norm': 0.63671875, 'learning_rate': 6.655342163794352e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9766, 'grad_norm': 1.1875, 'learning_rate': 6.654205926047966e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2043, 'grad_norm': 0.6171875, 'learning_rate': 6.653069688301581e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1393, 'grad_norm': 0.80859375, 'learning_rate': 6.651933450555194e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2351, 'grad_norm': 0.3984375, 'learning_rate': 6.650797212808808e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2275, 'grad_norm': 0.578125, 'learning_rate': 6.649660975062423e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1208, 'grad_norm': 0.8984375, 'learning_rate': 6.648524737316037e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2169, 'grad_norm': 0.470703125, 'learning_rate': 6.64738849956965e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1167, 'grad_norm': 0.83203125, 'learning_rate': 6.646252261823265e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2328, 'grad_norm': 0.45703125, 'learning_rate': 6.645116024076878e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1761, 'grad_norm': 0.703125, 'learning_rate': 6.643979786330492e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1259, 'grad_norm': 1.09375, 'learning_rate': 6.642843548584105e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.384, 'grad_norm': 0.51953125, 'learning_rate': 6.64170731083772e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9928, 'grad_norm': 0.83984375, 'learning_rate': 6.640571073091334e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1971, 'grad_norm': 0.462890625, 'learning_rate': 6.639434835344948e-05, 'epoch': 0.61}\n",
      "{'loss': 1.164, 'grad_norm': 0.52734375, 'learning_rate': 6.638298597598562e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1713, 'grad_norm': 1.1640625, 'learning_rate': 6.637162359852176e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3759, 'grad_norm': 0.51953125, 'learning_rate': 6.63602612210579e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2756, 'grad_norm': 0.62890625, 'learning_rate': 6.634889884359403e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1908, 'grad_norm': 0.5390625, 'learning_rate': 6.633753646613018e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1634, 'grad_norm': 0.56640625, 'learning_rate': 6.632617408866631e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1163, 'grad_norm': 1.125, 'learning_rate': 6.631481171120245e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2989, 'grad_norm': 0.52734375, 'learning_rate': 6.63034493337386e-05, 'epoch': 0.61}\n",
      "{'loss': 1.105, 'grad_norm': 0.6953125, 'learning_rate': 6.629208695627474e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0199, 'grad_norm': 0.43359375, 'learning_rate': 6.628072457881087e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2294, 'grad_norm': 1.203125, 'learning_rate': 6.626936220134703e-05, 'epoch': 0.61}\n",
      "{'loss': 1.161, 'grad_norm': 0.78515625, 'learning_rate': 6.625799982388315e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2671, 'grad_norm': 0.9375, 'learning_rate': 6.624663744641929e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1674, 'grad_norm': 0.734375, 'learning_rate': 6.623527506895543e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2647, 'grad_norm': 0.609375, 'learning_rate': 6.622391269149158e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3091, 'grad_norm': 0.51953125, 'learning_rate': 6.621255031402771e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0717, 'grad_norm': 0.71484375, 'learning_rate': 6.620118793656385e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2535, 'grad_norm': 0.61328125, 'learning_rate': 6.618982555909999e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1573, 'grad_norm': 0.7734375, 'learning_rate': 6.617846318163613e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0457, 'grad_norm': 0.443359375, 'learning_rate': 6.616710080417227e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2074, 'grad_norm': 0.7578125, 'learning_rate': 6.61557384267084e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1796, 'grad_norm': 0.7265625, 'learning_rate': 6.614437604924456e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2544, 'grad_norm': 0.5, 'learning_rate': 6.613301367178068e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0559, 'grad_norm': 0.76171875, 'learning_rate': 6.612165129431682e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2615, 'grad_norm': 0.515625, 'learning_rate': 6.611028891685297e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3574, 'grad_norm': 0.9765625, 'learning_rate': 6.60989265393891e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0102, 'grad_norm': 1.3828125, 'learning_rate': 6.608756416192524e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2865, 'grad_norm': 0.47265625, 'learning_rate': 6.60762017844614e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3126, 'grad_norm': 0.66015625, 'learning_rate': 6.606483940699752e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3418, 'grad_norm': 0.640625, 'learning_rate': 6.605347702953366e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2524, 'grad_norm': 0.5625, 'learning_rate': 6.60421146520698e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9324, 'grad_norm': 0.578125, 'learning_rate': 6.603075227460595e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3499, 'grad_norm': 0.515625, 'learning_rate': 6.601938989714208e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1937, 'grad_norm': 0.8359375, 'learning_rate': 6.600802751967822e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0799, 'grad_norm': 0.51953125, 'learning_rate': 6.599666514221436e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2384, 'grad_norm': 0.54296875, 'learning_rate': 6.59853027647505e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9564, 'grad_norm': 1.03125, 'learning_rate': 6.597394038728664e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2455, 'grad_norm': 0.59375, 'learning_rate': 6.596257800982277e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1321, 'grad_norm': 0.859375, 'learning_rate': 6.595121563235893e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3074, 'grad_norm': 0.42578125, 'learning_rate': 6.593985325489505e-05, 'epoch': 0.61}\n",
      "{'loss': 1.189, 'grad_norm': 0.53515625, 'learning_rate': 6.592849087743119e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1323, 'grad_norm': 1.3359375, 'learning_rate': 6.591712849996734e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3205, 'grad_norm': 0.486328125, 'learning_rate': 6.590576612250348e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2338, 'grad_norm': 0.48046875, 'learning_rate': 6.589440374503961e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2206, 'grad_norm': 0.50390625, 'learning_rate': 6.588304136757577e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2615, 'grad_norm': 0.73046875, 'learning_rate': 6.587167899011189e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1395, 'grad_norm': 1.265625, 'learning_rate': 6.586031661264803e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2941, 'grad_norm': 0.68359375, 'learning_rate': 6.584895423518417e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1427, 'grad_norm': 0.71875, 'learning_rate': 6.583759185772032e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2441, 'grad_norm': 0.50390625, 'learning_rate': 6.582622948025646e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3195, 'grad_norm': 0.6484375, 'learning_rate': 6.58148671027926e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0155, 'grad_norm': 0.92578125, 'learning_rate': 6.580350472532873e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1864, 'grad_norm': 0.7265625, 'learning_rate': 6.579214234786487e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1455, 'grad_norm': 0.99609375, 'learning_rate': 6.578077997040101e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2461, 'grad_norm': 0.66015625, 'learning_rate': 6.576941759293716e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2429, 'grad_norm': 0.7109375, 'learning_rate': 6.57580552154733e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0601, 'grad_norm': 0.7578125, 'learning_rate': 6.574669283800942e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3096, 'grad_norm': 0.71875, 'learning_rate': 6.573533046054556e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1163, 'grad_norm': 0.83984375, 'learning_rate': 6.572396808308171e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1887, 'grad_norm': 0.515625, 'learning_rate': 6.571260570561785e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2561, 'grad_norm': 0.51953125, 'learning_rate': 6.570124332815399e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9369, 'grad_norm': 1.0625, 'learning_rate': 6.568988095069014e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2711, 'grad_norm': 0.7890625, 'learning_rate': 6.567851857322626e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1351, 'grad_norm': 0.54296875, 'learning_rate': 6.56671561957624e-05, 'epoch': 0.61}\n",
      "{'loss': 1.261, 'grad_norm': 0.6640625, 'learning_rate': 6.565579381829854e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2837, 'grad_norm': 0.81640625, 'learning_rate': 6.564443144083469e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1144, 'grad_norm': 0.9296875, 'learning_rate': 6.563306906337083e-05, 'epoch': 0.61}\n",
      "{'loss': 1.283, 'grad_norm': 0.6171875, 'learning_rate': 6.562170668590696e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1905, 'grad_norm': 0.85546875, 'learning_rate': 6.56103443084431e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1117, 'grad_norm': 0.5546875, 'learning_rate': 6.559898193097924e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1468, 'grad_norm': 0.609375, 'learning_rate': 6.558761955351538e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0701, 'grad_norm': 0.6953125, 'learning_rate': 6.557625717605153e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2213, 'grad_norm': 0.52734375, 'learning_rate': 6.556489479858767e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2258, 'grad_norm': 0.91796875, 'learning_rate': 6.555353242112379e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0536, 'grad_norm': 0.5390625, 'learning_rate': 6.554217004365993e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2878, 'grad_norm': 0.60546875, 'learning_rate': 6.553080766619608e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1633, 'grad_norm': 0.78515625, 'learning_rate': 6.551944528873222e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3887, 'grad_norm': 0.5, 'learning_rate': 6.550808291126836e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0872, 'grad_norm': 0.65625, 'learning_rate': 6.549672053380451e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3522, 'grad_norm': 0.53515625, 'learning_rate': 6.548535815634063e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1002, 'grad_norm': 0.703125, 'learning_rate': 6.547399577887677e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2029, 'grad_norm': 0.734375, 'learning_rate': 6.546263340141291e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2888, 'grad_norm': 0.5390625, 'learning_rate': 6.545127102394906e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2375, 'grad_norm': 0.85546875, 'learning_rate': 6.54399086464852e-05, 'epoch': 0.61}\n",
      "{'loss': 1.244, 'grad_norm': 0.482421875, 'learning_rate': 6.542854626902133e-05, 'epoch': 0.61}\n",
      "{'loss': 1.306, 'grad_norm': 0.59375, 'learning_rate': 6.541718389155747e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0395, 'grad_norm': 0.7109375, 'learning_rate': 6.540582151409361e-05, 'epoch': 0.61}\n",
      "{'loss': 1.3328, 'grad_norm': 0.6875, 'learning_rate': 6.539445913662975e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1576, 'grad_norm': 0.796875, 'learning_rate': 6.53830967591659e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2247, 'grad_norm': 0.75, 'learning_rate': 6.537173438170204e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2976, 'grad_norm': 0.5859375, 'learning_rate': 6.536037200423816e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0389, 'grad_norm': 1.1484375, 'learning_rate': 6.53490096267743e-05, 'epoch': 0.61}\n",
      "{'loss': 1.2821, 'grad_norm': 0.6015625, 'learning_rate': 6.533764724931045e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1715, 'grad_norm': 0.71875, 'learning_rate': 6.532628487184659e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1287, 'grad_norm': 0.546875, 'learning_rate': 6.531492249438273e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0579, 'grad_norm': 0.9453125, 'learning_rate': 6.530356011691888e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1347, 'grad_norm': 0.84375, 'learning_rate': 6.5292197739455e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2246, 'grad_norm': 0.54296875, 'learning_rate': 6.528083536199114e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1028, 'grad_norm': 0.63671875, 'learning_rate': 6.526947298452728e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2117, 'grad_norm': 0.484375, 'learning_rate': 6.525811060706343e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1026, 'grad_norm': 0.578125, 'learning_rate': 6.524674822959957e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0527, 'grad_norm': 1.03125, 'learning_rate': 6.52353858521357e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2226, 'grad_norm': 0.5546875, 'learning_rate': 6.522402347467184e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2293, 'grad_norm': 0.7421875, 'learning_rate': 6.521266109720798e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1385, 'grad_norm': 0.6484375, 'learning_rate': 6.520129871974412e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1966, 'grad_norm': 0.53515625, 'learning_rate': 6.518993634228027e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0131, 'grad_norm': 1.0078125, 'learning_rate': 6.517857396481641e-05, 'epoch': 0.62}\n",
      "{'loss': 1.4037, 'grad_norm': 0.5546875, 'learning_rate': 6.516721158735253e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1368, 'grad_norm': 0.9453125, 'learning_rate': 6.515584920988867e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2901, 'grad_norm': 0.578125, 'learning_rate': 6.514448683242482e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2021, 'grad_norm': 0.8046875, 'learning_rate': 6.513312445496096e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0888, 'grad_norm': 0.859375, 'learning_rate': 6.51217620774971e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3543, 'grad_norm': 0.5859375, 'learning_rate': 6.511039970003325e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1091, 'grad_norm': 0.6484375, 'learning_rate': 6.509903732256937e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2687, 'grad_norm': 0.5625, 'learning_rate': 6.508767494510551e-05, 'epoch': 0.62}\n",
      "{'loss': 1.162, 'grad_norm': 0.62109375, 'learning_rate': 6.507631256764166e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1021, 'grad_norm': 0.78515625, 'learning_rate': 6.50649501901778e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2891, 'grad_norm': 0.64453125, 'learning_rate': 6.505358781271394e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2015, 'grad_norm': 0.75, 'learning_rate': 6.504222543525008e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2933, 'grad_norm': 0.5390625, 'learning_rate': 6.503086305778621e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2082, 'grad_norm': 0.609375, 'learning_rate': 6.501950068032235e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9389, 'grad_norm': 0.86328125, 'learning_rate': 6.500813830285849e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3492, 'grad_norm': 0.609375, 'learning_rate': 6.499677592539464e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2318, 'grad_norm': 0.9765625, 'learning_rate': 6.498541354793078e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1466, 'grad_norm': 0.453125, 'learning_rate': 6.49740511704669e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2213, 'grad_norm': 0.73046875, 'learning_rate': 6.496268879300304e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0706, 'grad_norm': 1.1328125, 'learning_rate': 6.495132641553919e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3612, 'grad_norm': 0.55078125, 'learning_rate': 6.493996403807533e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0864, 'grad_norm': 0.8359375, 'learning_rate': 6.492860166061147e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1665, 'grad_norm': 0.47265625, 'learning_rate': 6.491723928314762e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2296, 'grad_norm': 0.56640625, 'learning_rate': 6.490587690568374e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9707, 'grad_norm': 0.88671875, 'learning_rate': 6.489451452821988e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1972, 'grad_norm': 0.77734375, 'learning_rate': 6.488315215075603e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2436, 'grad_norm': 0.953125, 'learning_rate': 6.487178977329217e-05, 'epoch': 0.62}\n",
      "{'loss': 1.279, 'grad_norm': 0.486328125, 'learning_rate': 6.486042739582831e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2574, 'grad_norm': 0.62890625, 'learning_rate': 6.484906501836445e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1854, 'grad_norm': 0.95703125, 'learning_rate': 6.483770264090058e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1821, 'grad_norm': 0.640625, 'learning_rate': 6.482634026343672e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2187, 'grad_norm': 0.6484375, 'learning_rate': 6.481497788597286e-05, 'epoch': 0.62}\n",
      "{'loss': 1.159, 'grad_norm': 0.515625, 'learning_rate': 6.480361550850901e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1635, 'grad_norm': 0.546875, 'learning_rate': 6.479225313104515e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9781, 'grad_norm': 0.6015625, 'learning_rate': 6.478089075358127e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3737, 'grad_norm': 0.5546875, 'learning_rate': 6.476952837611741e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1038, 'grad_norm': 0.63671875, 'learning_rate': 6.475816599865356e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1332, 'grad_norm': 0.46875, 'learning_rate': 6.47468036211897e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3005, 'grad_norm': 0.546875, 'learning_rate': 6.473544124372584e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9903, 'grad_norm': 1.171875, 'learning_rate': 6.472407886626199e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4763, 'grad_norm': 0.546875, 'learning_rate': 6.471271648879811e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2626, 'grad_norm': 0.6796875, 'learning_rate': 6.470135411133425e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1454, 'grad_norm': 0.55078125, 'learning_rate': 6.46899917338704e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2588, 'grad_norm': 0.6015625, 'learning_rate': 6.467862935640654e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9836, 'grad_norm': 0.431640625, 'learning_rate': 6.466726697894268e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1951, 'grad_norm': 0.57421875, 'learning_rate': 6.465590460147882e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2013, 'grad_norm': 0.68359375, 'learning_rate': 6.464454222401496e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1943, 'grad_norm': 0.63671875, 'learning_rate': 6.463317984655109e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2272, 'grad_norm': 0.5546875, 'learning_rate': 6.462181746908723e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9717, 'grad_norm': 0.640625, 'learning_rate': 6.461045509162338e-05, 'epoch': 0.62}\n",
      "{'loss': 1.362, 'grad_norm': 0.53125, 'learning_rate': 6.459909271415952e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0566, 'grad_norm': 0.80078125, 'learning_rate': 6.458773033669564e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2258, 'grad_norm': 0.6015625, 'learning_rate': 6.45763679592318e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1457, 'grad_norm': 0.5703125, 'learning_rate': 6.456500558176793e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1098, 'grad_norm': 0.94921875, 'learning_rate': 6.455364320430407e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2735, 'grad_norm': 0.59375, 'learning_rate': 6.454228082684021e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2355, 'grad_norm': 0.62109375, 'learning_rate': 6.453091844937636e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1048, 'grad_norm': 0.482421875, 'learning_rate': 6.451955607191249e-05, 'epoch': 0.62}\n",
      "{'loss': 1.126, 'grad_norm': 0.7890625, 'learning_rate': 6.450819369444862e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0707, 'grad_norm': 0.7734375, 'learning_rate': 6.449683131698477e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3304, 'grad_norm': 0.482421875, 'learning_rate': 6.448546893952091e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1227, 'grad_norm': 0.6015625, 'learning_rate': 6.447410656205705e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2889, 'grad_norm': 0.52734375, 'learning_rate': 6.446274418459319e-05, 'epoch': 0.62}\n",
      "{'loss': 1.237, 'grad_norm': 0.66015625, 'learning_rate': 6.445138180712933e-05, 'epoch': 0.62}\n",
      "{'loss': 1.078, 'grad_norm': 0.56640625, 'learning_rate': 6.444001942966546e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3567, 'grad_norm': 0.7421875, 'learning_rate': 6.44286570522016e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1946, 'grad_norm': 0.82421875, 'learning_rate': 6.441729467473775e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1665, 'grad_norm': 0.5546875, 'learning_rate': 6.440593229727389e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2485, 'grad_norm': 0.51171875, 'learning_rate': 6.439456991981001e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1087, 'grad_norm': 0.92578125, 'learning_rate': 6.438320754234617e-05, 'epoch': 0.62}\n",
      "{'loss': 1.4632, 'grad_norm': 0.5703125, 'learning_rate': 6.43718451648823e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3285, 'grad_norm': 0.71484375, 'learning_rate': 6.436048278741844e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1111, 'grad_norm': 0.5859375, 'learning_rate': 6.434912040995458e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2723, 'grad_norm': 0.55859375, 'learning_rate': 6.433775803249073e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0408, 'grad_norm': 0.625, 'learning_rate': 6.432639565502686e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3633, 'grad_norm': 0.466796875, 'learning_rate': 6.4315033277563e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1967, 'grad_norm': 0.5859375, 'learning_rate': 6.430367090009914e-05, 'epoch': 0.62}\n",
      "{'loss': 1.354, 'grad_norm': 0.439453125, 'learning_rate': 6.429230852263528e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1768, 'grad_norm': 0.62109375, 'learning_rate': 6.428094614517142e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0984, 'grad_norm': 0.64453125, 'learning_rate': 6.426958376770756e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3761, 'grad_norm': 0.58984375, 'learning_rate': 6.42582213902437e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2001, 'grad_norm': 0.65234375, 'learning_rate': 6.424685901277983e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1663, 'grad_norm': 0.4140625, 'learning_rate': 6.423549663531597e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1483, 'grad_norm': 0.578125, 'learning_rate': 6.422413425785212e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0777, 'grad_norm': 1.2265625, 'learning_rate': 6.421277188038826e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1817, 'grad_norm': 0.68359375, 'learning_rate': 6.420140950292439e-05, 'epoch': 0.62}\n",
      "{'loss': 1.168, 'grad_norm': 0.703125, 'learning_rate': 6.419004712546054e-05, 'epoch': 0.62}\n",
      "{'loss': 1.09, 'grad_norm': 0.52734375, 'learning_rate': 6.417868474799667e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2518, 'grad_norm': 0.63671875, 'learning_rate': 6.416732237053281e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0941, 'grad_norm': 1.0859375, 'learning_rate': 6.415595999306895e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3826, 'grad_norm': 0.66015625, 'learning_rate': 6.41445976156051e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1769, 'grad_norm': 0.73046875, 'learning_rate': 6.413323523814123e-05, 'epoch': 0.62}\n",
      "{'loss': 1.132, 'grad_norm': 0.671875, 'learning_rate': 6.412187286067736e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2054, 'grad_norm': 0.5234375, 'learning_rate': 6.411051048321352e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0586, 'grad_norm': 0.8359375, 'learning_rate': 6.409914810574965e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3318, 'grad_norm': 0.5390625, 'learning_rate': 6.408778572828579e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1524, 'grad_norm': 0.68359375, 'learning_rate': 6.407642335082193e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2609, 'grad_norm': 0.5, 'learning_rate': 6.406506097335807e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2256, 'grad_norm': 0.7421875, 'learning_rate': 6.40536985958942e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0151, 'grad_norm': 0.94140625, 'learning_rate': 6.404233621843034e-05, 'epoch': 0.62}\n",
      "{'loss': 1.4284, 'grad_norm': 0.56640625, 'learning_rate': 6.40309738409665e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1897, 'grad_norm': 0.63671875, 'learning_rate': 6.401961146350263e-05, 'epoch': 0.62}\n",
      "{'loss': 1.137, 'grad_norm': 0.6875, 'learning_rate': 6.400824908603876e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3148, 'grad_norm': 0.59375, 'learning_rate': 6.399688670857491e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9637, 'grad_norm': 0.79296875, 'learning_rate': 6.398552433111105e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2242, 'grad_norm': 0.6328125, 'learning_rate': 6.397416195364718e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1025, 'grad_norm': 0.5625, 'learning_rate': 6.396279957618332e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2249, 'grad_norm': 0.56640625, 'learning_rate': 6.395143719871947e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2756, 'grad_norm': 0.7578125, 'learning_rate': 6.39400748212556e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1502, 'grad_norm': 1.4296875, 'learning_rate': 6.392871244379173e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3617, 'grad_norm': 0.64453125, 'learning_rate': 6.391735006632789e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2302, 'grad_norm': 0.65625, 'learning_rate': 6.390598768886402e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1869, 'grad_norm': 0.443359375, 'learning_rate': 6.389462531140016e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2795, 'grad_norm': 0.66015625, 'learning_rate': 6.38832629339363e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9693, 'grad_norm': 0.8828125, 'learning_rate': 6.387190055647244e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2944, 'grad_norm': 0.5078125, 'learning_rate': 6.386053817900858e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1868, 'grad_norm': 1.0078125, 'learning_rate': 6.384917580154471e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2235, 'grad_norm': 0.55859375, 'learning_rate': 6.383781342408086e-05, 'epoch': 0.62}\n",
      "{'loss': 1.283, 'grad_norm': 0.55859375, 'learning_rate': 6.3826451046617e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0974, 'grad_norm': 1.3046875, 'learning_rate': 6.381508866915313e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2964, 'grad_norm': 0.5078125, 'learning_rate': 6.380372629168928e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1874, 'grad_norm': 0.67578125, 'learning_rate': 6.379236391422542e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1284, 'grad_norm': 0.388671875, 'learning_rate': 6.378100153676155e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1316, 'grad_norm': 0.7890625, 'learning_rate': 6.376963915929769e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0924, 'grad_norm': 0.80078125, 'learning_rate': 6.375827678183384e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3785, 'grad_norm': 0.640625, 'learning_rate': 6.374691440436997e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2868, 'grad_norm': 0.92578125, 'learning_rate': 6.37355520269061e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2807, 'grad_norm': 0.490234375, 'learning_rate': 6.372418964944226e-05, 'epoch': 0.62}\n",
      "{'loss': 1.284, 'grad_norm': 0.59375, 'learning_rate': 6.37128272719784e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0443, 'grad_norm': 0.60546875, 'learning_rate': 6.370146489451453e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3442, 'grad_norm': 0.65625, 'learning_rate': 6.369010251705067e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1927, 'grad_norm': 1.0859375, 'learning_rate': 6.367874013958681e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1873, 'grad_norm': 0.56640625, 'learning_rate': 6.366737776212295e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1874, 'grad_norm': 0.64453125, 'learning_rate': 6.365601538465908e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1315, 'grad_norm': 1.0859375, 'learning_rate': 6.364465300719524e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2501, 'grad_norm': 0.703125, 'learning_rate': 6.363329062973137e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2338, 'grad_norm': 0.765625, 'learning_rate': 6.36219282522675e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0318, 'grad_norm': 0.44140625, 'learning_rate': 6.361056587480365e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2254, 'grad_norm': 0.7109375, 'learning_rate': 6.359920349733979e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0345, 'grad_norm': 0.640625, 'learning_rate': 6.358784111987592e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4489, 'grad_norm': 0.5390625, 'learning_rate': 6.357647874241206e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2375, 'grad_norm': 0.73828125, 'learning_rate': 6.356511636494821e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2227, 'grad_norm': 0.48828125, 'learning_rate': 6.355375398748434e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2649, 'grad_norm': 0.65625, 'learning_rate': 6.354239161002048e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1853, 'grad_norm': 0.59375, 'learning_rate': 6.353102923255663e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3469, 'grad_norm': 0.494140625, 'learning_rate': 6.351966685509277e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1905, 'grad_norm': 0.7265625, 'learning_rate': 6.35083044776289e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0571, 'grad_norm': 0.447265625, 'learning_rate': 6.349694210016504e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2462, 'grad_norm': 0.58203125, 'learning_rate': 6.348557972270118e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0232, 'grad_norm': 0.609375, 'learning_rate': 6.347421734523732e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3149, 'grad_norm': 0.46875, 'learning_rate': 6.346285496777345e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1231, 'grad_norm': 1.5078125, 'learning_rate': 6.34514925903096e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1318, 'grad_norm': 0.3984375, 'learning_rate': 6.344013021284574e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1803, 'grad_norm': 0.60546875, 'learning_rate': 6.342876783538187e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0159, 'grad_norm': 0.9375, 'learning_rate': 6.341740545791802e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3059, 'grad_norm': 0.5703125, 'learning_rate': 6.340604308045416e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1676, 'grad_norm': 0.82421875, 'learning_rate': 6.33946807029903e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2268, 'grad_norm': 0.625, 'learning_rate': 6.338331832552643e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3201, 'grad_norm': 0.60546875, 'learning_rate': 6.337195594806258e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0481, 'grad_norm': 0.859375, 'learning_rate': 6.336059357059871e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2522, 'grad_norm': 0.53515625, 'learning_rate': 6.334923119313485e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1688, 'grad_norm': 0.828125, 'learning_rate': 6.3337868815671e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2144, 'grad_norm': 0.435546875, 'learning_rate': 6.332650643820714e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3105, 'grad_norm': 0.625, 'learning_rate': 6.331514406074327e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2236, 'grad_norm': 0.88671875, 'learning_rate': 6.330378168327941e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3869, 'grad_norm': 0.58203125, 'learning_rate': 6.329241930581555e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0413, 'grad_norm': 0.84765625, 'learning_rate': 6.328105692835169e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2227, 'grad_norm': 0.416015625, 'learning_rate': 6.326969455088783e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2998, 'grad_norm': 0.609375, 'learning_rate': 6.325833217342398e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0402, 'grad_norm': 0.8046875, 'learning_rate': 6.324696979596011e-05, 'epoch': 0.62}\n",
      "{'loss': 1.372, 'grad_norm': 0.64453125, 'learning_rate': 6.323560741849624e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1651, 'grad_norm': 0.55078125, 'learning_rate': 6.322424504103239e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2405, 'grad_norm': 0.40234375, 'learning_rate': 6.321288266356853e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3193, 'grad_norm': 0.640625, 'learning_rate': 6.320152028610467e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0679, 'grad_norm': 0.65625, 'learning_rate': 6.319015790864082e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2627, 'grad_norm': 0.50390625, 'learning_rate': 6.317879553117696e-05, 'epoch': 0.62}\n",
      "{'loss': 1.281, 'grad_norm': 0.69140625, 'learning_rate': 6.316743315371308e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2496, 'grad_norm': 0.7578125, 'learning_rate': 6.315607077624922e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2329, 'grad_norm': 0.609375, 'learning_rate': 6.314470839878537e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0841, 'grad_norm': 0.41796875, 'learning_rate': 6.31333460213215e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2532, 'grad_norm': 0.6328125, 'learning_rate': 6.312198364385764e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1107, 'grad_norm': 0.703125, 'learning_rate': 6.311062126639378e-05, 'epoch': 0.62}\n",
      "{'loss': 1.374, 'grad_norm': 0.43359375, 'learning_rate': 6.309925888892992e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1698, 'grad_norm': 0.6328125, 'learning_rate': 6.308789651146606e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0427, 'grad_norm': 0.79296875, 'learning_rate': 6.30765341340022e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3285, 'grad_norm': 0.52734375, 'learning_rate': 6.306517175653835e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0859, 'grad_norm': 0.61328125, 'learning_rate': 6.305380937907449e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2697, 'grad_norm': 0.44140625, 'learning_rate': 6.304244700161061e-05, 'epoch': 0.62}\n",
      "{'loss': 1.3035, 'grad_norm': 0.53125, 'learning_rate': 6.303108462414676e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0779, 'grad_norm': 0.65234375, 'learning_rate': 6.30197222466829e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4025, 'grad_norm': 0.5703125, 'learning_rate': 6.300835986921904e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1288, 'grad_norm': 0.953125, 'learning_rate': 6.299699749175519e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2647, 'grad_norm': 0.5546875, 'learning_rate': 6.298563511429133e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1627, 'grad_norm': 0.703125, 'learning_rate': 6.297427273682745e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0151, 'grad_norm': 0.95703125, 'learning_rate': 6.296291035936359e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2735, 'grad_norm': 0.5390625, 'learning_rate': 6.295154798189974e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3088, 'grad_norm': 0.71484375, 'learning_rate': 6.294018560443588e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1377, 'grad_norm': 0.5078125, 'learning_rate': 6.292882322697202e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1672, 'grad_norm': 0.65625, 'learning_rate': 6.291746084950815e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0218, 'grad_norm': 0.55859375, 'learning_rate': 6.290609847204429e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3776, 'grad_norm': 0.58984375, 'learning_rate': 6.289473609458043e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1325, 'grad_norm': 0.60546875, 'learning_rate': 6.288337371711657e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1117, 'grad_norm': 0.83203125, 'learning_rate': 6.287201133965272e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1272, 'grad_norm': 0.6875, 'learning_rate': 6.286064896218886e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0041, 'grad_norm': 1.03125, 'learning_rate': 6.284928658472498e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2899, 'grad_norm': 0.52734375, 'learning_rate': 6.283792420726113e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1505, 'grad_norm': 1.1328125, 'learning_rate': 6.282656182979727e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2328, 'grad_norm': 0.5, 'learning_rate': 6.281519945233341e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2986, 'grad_norm': 0.49609375, 'learning_rate': 6.280383707486956e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9387, 'grad_norm': 0.64453125, 'learning_rate': 6.27924746974057e-05, 'epoch': 0.63}\n",
      "{'loss': 1.4729, 'grad_norm': 0.70703125, 'learning_rate': 6.278111231994182e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1049, 'grad_norm': 0.734375, 'learning_rate': 6.276974994247796e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1329, 'grad_norm': 0.5, 'learning_rate': 6.275838756501411e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2143, 'grad_norm': 0.6875, 'learning_rate': 6.274702518755025e-05, 'epoch': 0.63}\n",
      "{'loss': 1.061, 'grad_norm': 1.234375, 'learning_rate': 6.273566281008639e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2747, 'grad_norm': 0.625, 'learning_rate': 6.272430043262252e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2164, 'grad_norm': 0.74609375, 'learning_rate': 6.271293805515866e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2342, 'grad_norm': 0.42578125, 'learning_rate': 6.27015756776948e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1718, 'grad_norm': 0.95703125, 'learning_rate': 6.269021330023095e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1585, 'grad_norm': 0.74609375, 'learning_rate': 6.267885092276709e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3825, 'grad_norm': 0.66796875, 'learning_rate': 6.266748854530323e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1437, 'grad_norm': 0.59765625, 'learning_rate': 6.265612616783935e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3206, 'grad_norm': 0.5234375, 'learning_rate': 6.26447637903755e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1257, 'grad_norm': 0.5390625, 'learning_rate': 6.263340141291164e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2873, 'grad_norm': 0.78125, 'learning_rate': 6.262203903544778e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2852, 'grad_norm': 0.6015625, 'learning_rate': 6.261067665798393e-05, 'epoch': 0.63}\n",
      "{'loss': 1.228, 'grad_norm': 0.6484375, 'learning_rate': 6.259931428052007e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1622, 'grad_norm': 0.5234375, 'learning_rate': 6.258795190305619e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2053, 'grad_norm': 0.72265625, 'learning_rate': 6.257658952559233e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9231, 'grad_norm': 0.8046875, 'learning_rate': 6.256522714812848e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2632, 'grad_norm': 0.66796875, 'learning_rate': 6.255386477066462e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2046, 'grad_norm': 0.74609375, 'learning_rate': 6.254250239320076e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2789, 'grad_norm': 0.470703125, 'learning_rate': 6.25311400157369e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2679, 'grad_norm': 0.69140625, 'learning_rate': 6.251977763827303e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9926, 'grad_norm': 0.7109375, 'learning_rate': 6.250841526080917e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2352, 'grad_norm': 0.5625, 'learning_rate': 6.249705288334532e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0991, 'grad_norm': 0.74609375, 'learning_rate': 6.248569050588146e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0214, 'grad_norm': 0.453125, 'learning_rate': 6.24743281284176e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2771, 'grad_norm': 0.515625, 'learning_rate': 6.246296575095373e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2048, 'grad_norm': 1.1875, 'learning_rate': 6.245160337348987e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1238, 'grad_norm': 0.5859375, 'learning_rate': 6.244024099602601e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0941, 'grad_norm': 0.9296875, 'learning_rate': 6.242887861856215e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1332, 'grad_norm': 0.50390625, 'learning_rate': 6.24175162410983e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2391, 'grad_norm': 0.57421875, 'learning_rate': 6.240615386363444e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9, 'grad_norm': 0.5078125, 'learning_rate': 6.239479148617056e-05, 'epoch': 0.63}\n",
      "{'loss': 1.5506, 'grad_norm': 0.5625, 'learning_rate': 6.23834291087067e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1564, 'grad_norm': 0.8984375, 'learning_rate': 6.237206673124285e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0449, 'grad_norm': 0.578125, 'learning_rate': 6.236070435377899e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1981, 'grad_norm': 0.59375, 'learning_rate': 6.234934197631513e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0549, 'grad_norm': 2.671875, 'learning_rate': 6.233797959885126e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2819, 'grad_norm': 0.6484375, 'learning_rate': 6.23266172213874e-05, 'epoch': 0.63}\n",
      "{'loss': 1.103, 'grad_norm': 0.63671875, 'learning_rate': 6.231525484392354e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1362, 'grad_norm': 0.388671875, 'learning_rate': 6.230389246645969e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2082, 'grad_norm': 0.67578125, 'learning_rate': 6.229253008899583e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0009, 'grad_norm': 0.78125, 'learning_rate': 6.228116771153197e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1821, 'grad_norm': 0.8515625, 'learning_rate': 6.22698053340681e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1431, 'grad_norm': 0.59765625, 'learning_rate': 6.225844295660424e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2022, 'grad_norm': 0.4453125, 'learning_rate': 6.224708057914038e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0728, 'grad_norm': 0.61328125, 'learning_rate': 6.223571820167652e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0331, 'grad_norm': 0.77734375, 'learning_rate': 6.222435582421267e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3859, 'grad_norm': 0.58203125, 'learning_rate': 6.221299344674881e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1764, 'grad_norm': 0.65625, 'learning_rate': 6.220163106928493e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1985, 'grad_norm': 0.640625, 'learning_rate': 6.219026869182107e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2767, 'grad_norm': 0.70703125, 'learning_rate': 6.217890631435722e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1018, 'grad_norm': 1.1875, 'learning_rate': 6.216754393689336e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2739, 'grad_norm': 0.52734375, 'learning_rate': 6.21561815594295e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1475, 'grad_norm': 0.62890625, 'learning_rate': 6.214481918196564e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2331, 'grad_norm': 0.515625, 'learning_rate': 6.213345680450177e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2757, 'grad_norm': 0.6640625, 'learning_rate': 6.212209442703791e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1695, 'grad_norm': 0.88671875, 'learning_rate': 6.211073204957406e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2785, 'grad_norm': 0.50390625, 'learning_rate': 6.20993696721102e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2306, 'grad_norm': 0.6015625, 'learning_rate': 6.208800729464634e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1721, 'grad_norm': 0.52734375, 'learning_rate': 6.207664491718248e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1736, 'grad_norm': 0.55078125, 'learning_rate': 6.206528253971861e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0248, 'grad_norm': 1.03125, 'learning_rate': 6.205392016225475e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3784, 'grad_norm': 0.734375, 'learning_rate': 6.204255778479089e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1289, 'grad_norm': 0.6875, 'learning_rate': 6.203119540732704e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2129, 'grad_norm': 0.466796875, 'learning_rate': 6.201983302986318e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3242, 'grad_norm': 0.59765625, 'learning_rate': 6.20084706523993e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1814, 'grad_norm': 0.68359375, 'learning_rate': 6.199710827493545e-05, 'epoch': 0.63}\n",
      "{'loss': 1.284, 'grad_norm': 0.48828125, 'learning_rate': 6.198574589747159e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1291, 'grad_norm': 0.7109375, 'learning_rate': 6.197438352000773e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1152, 'grad_norm': 0.5234375, 'learning_rate': 6.196302114254387e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1988, 'grad_norm': 0.62109375, 'learning_rate': 6.195165876508e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1501, 'grad_norm': 0.93359375, 'learning_rate': 6.194029638761614e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1692, 'grad_norm': 0.46875, 'learning_rate': 6.192893401015228e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1492, 'grad_norm': 0.79296875, 'learning_rate': 6.191757163268843e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2789, 'grad_norm': 0.515625, 'learning_rate': 6.190620925522457e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3066, 'grad_norm': 0.56640625, 'learning_rate': 6.189484687776071e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9766, 'grad_norm': 0.408203125, 'learning_rate': 6.188348450029685e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3851, 'grad_norm': 0.5546875, 'learning_rate': 6.187212212283298e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1622, 'grad_norm': 0.86328125, 'learning_rate': 6.186075974536912e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1125, 'grad_norm': 0.5703125, 'learning_rate': 6.184939736790526e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3785, 'grad_norm': 0.53125, 'learning_rate': 6.183803499044141e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9272, 'grad_norm': 0.79296875, 'learning_rate': 6.182667261297755e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2978, 'grad_norm': 0.71875, 'learning_rate': 6.181531023551367e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1817, 'grad_norm': 0.66796875, 'learning_rate': 6.180394785804983e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1467, 'grad_norm': 0.65234375, 'learning_rate': 6.179258548058596e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2131, 'grad_norm': 0.58203125, 'learning_rate': 6.17812231031221e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1659, 'grad_norm': 1.296875, 'learning_rate': 6.176986072565824e-05, 'epoch': 0.63}\n",
      "{'loss': 1.255, 'grad_norm': 0.49609375, 'learning_rate': 6.175849834819438e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1705, 'grad_norm': 0.65625, 'learning_rate': 6.174713597073051e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3389, 'grad_norm': 6.125, 'learning_rate': 6.173577359326665e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1702, 'grad_norm': 0.68359375, 'learning_rate': 6.17244112158028e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9887, 'grad_norm': 1.2109375, 'learning_rate': 6.171304883833894e-05, 'epoch': 0.63}\n",
      "{'loss': 1.372, 'grad_norm': 0.54296875, 'learning_rate': 6.170168646087508e-05, 'epoch': 0.63}\n",
      "{'loss': 1.229, 'grad_norm': 0.81640625, 'learning_rate': 6.169032408341122e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0787, 'grad_norm': 0.53515625, 'learning_rate': 6.167896170594736e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1479, 'grad_norm': 0.7109375, 'learning_rate': 6.166759932848349e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0359, 'grad_norm': 1.328125, 'learning_rate': 6.165623695101963e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3832, 'grad_norm': 0.671875, 'learning_rate': 6.164487457355578e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1393, 'grad_norm': 0.73828125, 'learning_rate': 6.163351219609192e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1277, 'grad_norm': 0.490234375, 'learning_rate': 6.162214981862804e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2462, 'grad_norm': 0.70703125, 'learning_rate': 6.16107874411642e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2507, 'grad_norm': 0.78515625, 'learning_rate': 6.159942506370033e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5337, 'grad_norm': 0.59765625, 'learning_rate': 6.158806268623647e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0791, 'grad_norm': 0.7890625, 'learning_rate': 6.157670030877261e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3069, 'grad_norm': 0.400390625, 'learning_rate': 6.156533793130875e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2164, 'grad_norm': 0.6171875, 'learning_rate': 6.155397555384489e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0556, 'grad_norm': 1.453125, 'learning_rate': 6.154261317638102e-05, 'epoch': 0.63}\n",
      "{'loss': 1.4597, 'grad_norm': 0.51953125, 'learning_rate': 6.153125079891717e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1601, 'grad_norm': 0.85546875, 'learning_rate': 6.151988842145331e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1449, 'grad_norm': 0.44921875, 'learning_rate': 6.150852604398945e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1831, 'grad_norm': 0.66796875, 'learning_rate': 6.149716366652559e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0929, 'grad_norm': 0.84375, 'learning_rate': 6.148580128906173e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3998, 'grad_norm': 0.75390625, 'learning_rate': 6.147443891159786e-05, 'epoch': 0.63}\n",
      "{'loss': 1.158, 'grad_norm': 0.8828125, 'learning_rate': 6.1463076534134e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1458, 'grad_norm': 0.52734375, 'learning_rate': 6.145171415667015e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1331, 'grad_norm': 0.58984375, 'learning_rate': 6.144035177920629e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0554, 'grad_norm': 0.69921875, 'learning_rate': 6.142898940174242e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3718, 'grad_norm': 0.5390625, 'learning_rate': 6.141762702427857e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1914, 'grad_norm': 0.8203125, 'learning_rate': 6.14062646468147e-05, 'epoch': 0.63}\n",
      "{'loss': 1.327, 'grad_norm': 0.52734375, 'learning_rate': 6.139490226935084e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1429, 'grad_norm': 0.55859375, 'learning_rate': 6.138353989188698e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0209, 'grad_norm': 0.8984375, 'learning_rate': 6.137217751442312e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3185, 'grad_norm': 0.63671875, 'learning_rate': 6.136081513695926e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1413, 'grad_norm': 0.859375, 'learning_rate': 6.13494527594954e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1614, 'grad_norm': 0.50390625, 'learning_rate': 6.133809038203155e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3168, 'grad_norm': 0.96484375, 'learning_rate': 6.132672800456768e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0365, 'grad_norm': 0.75, 'learning_rate': 6.131536562710382e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.227, 'grad_norm': 0.515625, 'learning_rate': 6.130400324963996e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1295, 'grad_norm': 0.6875, 'learning_rate': 6.12926408721761e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2083, 'grad_norm': 0.65625, 'learning_rate': 6.128127849471223e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2168, 'grad_norm': 0.67578125, 'learning_rate': 6.126991611724837e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1939, 'grad_norm': 0.953125, 'learning_rate': 6.125855373978452e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3007, 'grad_norm': 0.78515625, 'learning_rate': 6.124719136232066e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1393, 'grad_norm': 0.8515625, 'learning_rate': 6.123582898485679e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1688, 'grad_norm': 0.59375, 'learning_rate': 6.122446660739294e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2905, 'grad_norm': 0.77734375, 'learning_rate': 6.121310422992907e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0921, 'grad_norm': 0.9453125, 'learning_rate': 6.120174185246521e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3999, 'grad_norm': 0.55859375, 'learning_rate': 6.119037947500135e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1493, 'grad_norm': 0.85546875, 'learning_rate': 6.117901709753749e-05, 'epoch': 0.63}\n",
      "{'loss': 1.144, 'grad_norm': 0.396484375, 'learning_rate': 6.116765472007363e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2642, 'grad_norm': 0.55859375, 'learning_rate': 6.115629234260976e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9593, 'grad_norm': 0.50390625, 'learning_rate': 6.114492996514592e-05, 'epoch': 0.63}\n",
      "{'loss': 1.4197, 'grad_norm': 0.57421875, 'learning_rate': 6.113356758768205e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1438, 'grad_norm': 0.83984375, 'learning_rate': 6.112220521021819e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2799, 'grad_norm': 0.49609375, 'learning_rate': 6.111084283275433e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1911, 'grad_norm': 0.6328125, 'learning_rate': 6.109948045529047e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1108, 'grad_norm': 1.0546875, 'learning_rate': 6.10881180778266e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3115, 'grad_norm': 0.609375, 'learning_rate': 6.107675570036274e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1961, 'grad_norm': 0.6484375, 'learning_rate': 6.10653933228989e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0653, 'grad_norm': 0.435546875, 'learning_rate': 6.105403094543503e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2253, 'grad_norm': 0.61328125, 'learning_rate': 6.104266856797116e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1215, 'grad_norm': 0.76171875, 'learning_rate': 6.103130619050731e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2214, 'grad_norm': 0.5234375, 'learning_rate': 6.1019943813043446e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1695, 'grad_norm': 0.79296875, 'learning_rate': 6.1008581435579583e-05, 'epoch': 0.63}\n",
      "{'loss': 1.28, 'grad_norm': 0.478515625, 'learning_rate': 6.0997219058115715e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1878, 'grad_norm': 0.6328125, 'learning_rate': 6.0985856680651866e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0853, 'grad_norm': 0.8984375, 'learning_rate': 6.0974494303188004e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3896, 'grad_norm': 0.5234375, 'learning_rate': 6.0963131925724135e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0941, 'grad_norm': 1.0234375, 'learning_rate': 6.0951769548260286e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2476, 'grad_norm': 0.423828125, 'learning_rate': 6.0940407170796424e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2485, 'grad_norm': 0.73046875, 'learning_rate': 6.0929044793332555e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9677, 'grad_norm': 0.63671875, 'learning_rate': 6.091768241586871e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2608, 'grad_norm': 0.59765625, 'learning_rate': 6.090632003840484e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0968, 'grad_norm': 0.65625, 'learning_rate': 6.0894957660940976e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1269, 'grad_norm': 0.5546875, 'learning_rate': 6.0883595283477113e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2235, 'grad_norm': 0.81640625, 'learning_rate': 6.087223290601326e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1366, 'grad_norm': 0.98828125, 'learning_rate': 6.0860870528549396e-05, 'epoch': 0.63}\n",
      "{'loss': 1.4468, 'grad_norm': 0.55859375, 'learning_rate': 6.0849508151085534e-05, 'epoch': 0.63}\n",
      "{'loss': 1.132, 'grad_norm': 0.63671875, 'learning_rate': 6.083814577362168e-05, 'epoch': 0.63}\n",
      "{'loss': 1.201, 'grad_norm': 0.443359375, 'learning_rate': 6.0826783396157816e-05, 'epoch': 0.63}\n",
      "{'loss': 1.258, 'grad_norm': 0.66015625, 'learning_rate': 6.0815421018693954e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9667, 'grad_norm': 1.234375, 'learning_rate': 6.08040586412301e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3001, 'grad_norm': 0.45703125, 'learning_rate': 6.0792696263766237e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1889, 'grad_norm': 1.15625, 'learning_rate': 6.0781333886302374e-05, 'epoch': 0.63}\n",
      "{'loss': 1.174, 'grad_norm': 0.78125, 'learning_rate': 6.0769971508838506e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3003, 'grad_norm': 0.63671875, 'learning_rate': 6.075860913137466e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0323, 'grad_norm': 0.921875, 'learning_rate': 6.0747246753910795e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3475, 'grad_norm': 0.50390625, 'learning_rate': 6.0735884376446926e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1689, 'grad_norm': 0.59375, 'learning_rate': 6.072452199898308e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1293, 'grad_norm': 0.52734375, 'learning_rate': 6.071315962151921e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2781, 'grad_norm': 0.59765625, 'learning_rate': 6.0701797244055346e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1545, 'grad_norm': 0.4609375, 'learning_rate': 6.0690434866591484e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3087, 'grad_norm': 0.5625, 'learning_rate': 6.067907248912763e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2216, 'grad_norm': 0.6484375, 'learning_rate': 6.0667710111663767e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2464, 'grad_norm': 0.48828125, 'learning_rate': 6.0656347734199904e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0771, 'grad_norm': 0.52734375, 'learning_rate': 6.064498535673605e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9518, 'grad_norm': 0.671875, 'learning_rate': 6.063362297927219e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3169, 'grad_norm': 0.58984375, 'learning_rate': 6.0622260601808325e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2198, 'grad_norm': 0.65234375, 'learning_rate': 6.061089822434447e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2154, 'grad_norm': 0.443359375, 'learning_rate': 6.059953584688061e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2071, 'grad_norm': 0.56640625, 'learning_rate': 6.0588173469416745e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1406, 'grad_norm': 0.92578125, 'learning_rate': 6.0576811091952876e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2966, 'grad_norm': 0.5390625, 'learning_rate': 6.056544871448903e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1497, 'grad_norm': 1.4296875, 'learning_rate': 6.0554086337025165e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1528, 'grad_norm': 0.466796875, 'learning_rate': 6.0542723959561296e-05, 'epoch': 0.64}\n",
      "{'loss': 1.17, 'grad_norm': 0.6328125, 'learning_rate': 6.053136158209745e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0409, 'grad_norm': 0.796875, 'learning_rate': 6.051999920463358e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2989, 'grad_norm': 0.58203125, 'learning_rate': 6.050863682716972e-05, 'epoch': 0.64}\n",
      "{'loss': 1.131, 'grad_norm': 0.6875, 'learning_rate': 6.0497274449705855e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1308, 'grad_norm': 0.4609375, 'learning_rate': 6.0485912072242e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2001, 'grad_norm': 0.49609375, 'learning_rate': 6.047454969477814e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0358, 'grad_norm': 0.9140625, 'learning_rate': 6.0463187317314275e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3909, 'grad_norm': 0.484375, 'learning_rate': 6.045182493985042e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0708, 'grad_norm': 0.8125, 'learning_rate': 6.044046256238656e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2407, 'grad_norm': 0.5546875, 'learning_rate': 6.0429100184922695e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2318, 'grad_norm': 0.63671875, 'learning_rate': 6.041773780745884e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9885, 'grad_norm': 0.57421875, 'learning_rate': 6.040637542999498e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2892, 'grad_norm': 0.54296875, 'learning_rate': 6.0395013052531116e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2073, 'grad_norm': 0.7734375, 'learning_rate': 6.038365067506725e-05, 'epoch': 0.64}\n",
      "{'loss': 1.26, 'grad_norm': 0.50390625, 'learning_rate': 6.03722882976034e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1685, 'grad_norm': 0.640625, 'learning_rate': 6.0360925920139536e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0877, 'grad_norm': 0.59375, 'learning_rate': 6.034956354267567e-05, 'epoch': 0.64}\n",
      "{'loss': 1.336, 'grad_norm': 0.51171875, 'learning_rate': 6.033820116521182e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2377, 'grad_norm': 0.78515625, 'learning_rate': 6.032683878774795e-05, 'epoch': 0.64}\n",
      "{'loss': 1.056, 'grad_norm': 0.50390625, 'learning_rate': 6.031547641028409e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2224, 'grad_norm': 0.640625, 'learning_rate': 6.0304114032820225e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0391, 'grad_norm': 1.328125, 'learning_rate': 6.029275165535637e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3923, 'grad_norm': 0.53515625, 'learning_rate': 6.028138927789251e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2361, 'grad_norm': 0.55859375, 'learning_rate': 6.0270026900428646e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2243, 'grad_norm': 0.546875, 'learning_rate': 6.025866452296479e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2093, 'grad_norm': 0.671875, 'learning_rate': 6.024730214550093e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9811, 'grad_norm': 1.265625, 'learning_rate': 6.0235939768037066e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3066, 'grad_norm': 0.50390625, 'learning_rate': 6.022457739057321e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2787, 'grad_norm': 0.671875, 'learning_rate': 6.021321501310935e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1947, 'grad_norm': 0.44140625, 'learning_rate': 6.0201852635645486e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2592, 'grad_norm': 0.70703125, 'learning_rate': 6.019049025818162e-05, 'epoch': 0.64}\n",
      "{'loss': 0.969, 'grad_norm': 0.79296875, 'learning_rate': 6.017912788071777e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3125, 'grad_norm': 0.51953125, 'learning_rate': 6.016776550325391e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1329, 'grad_norm': 1.1015625, 'learning_rate': 6.015640312579004e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9443, 'grad_norm': 0.6953125, 'learning_rate': 6.014504074832619e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1712, 'grad_norm': 0.90625, 'learning_rate': 6.013367837086232e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0775, 'grad_norm': 1.5078125, 'learning_rate': 6.012231599339846e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1226, 'grad_norm': 0.6171875, 'learning_rate': 6.011095361593461e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1186, 'grad_norm': 0.76171875, 'learning_rate': 6.009959123847074e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2216, 'grad_norm': 0.3984375, 'learning_rate': 6.008822886100688e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2264, 'grad_norm': 0.69140625, 'learning_rate': 6.0076866483543016e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1993, 'grad_norm': 0.89453125, 'learning_rate': 6.006550410607916e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2728, 'grad_norm': 0.451171875, 'learning_rate': 6.00541417286153e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1188, 'grad_norm': 0.765625, 'learning_rate': 6.0042779351151437e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2554, 'grad_norm': 0.48828125, 'learning_rate': 6.003141697368758e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2859, 'grad_norm': 0.75390625, 'learning_rate': 6.002005459622372e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0962, 'grad_norm': 0.66015625, 'learning_rate': 6.000869221875986e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4985, 'grad_norm': 0.7265625, 'learning_rate': 5.999732984129599e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0486, 'grad_norm': 0.5859375, 'learning_rate': 5.998596746383214e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2028, 'grad_norm': 0.52734375, 'learning_rate': 5.997460508636828e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2859, 'grad_norm': 0.56640625, 'learning_rate': 5.996324270890441e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1205, 'grad_norm': 0.80859375, 'learning_rate': 5.995188033144056e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4017, 'grad_norm': 0.490234375, 'learning_rate': 5.994051795397669e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2264, 'grad_norm': 0.66015625, 'learning_rate': 5.992915557651283e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1277, 'grad_norm': 0.65625, 'learning_rate': 5.991779319904898e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2352, 'grad_norm': 0.72265625, 'learning_rate': 5.990643082158511e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9755, 'grad_norm': 0.79296875, 'learning_rate': 5.989506844412125e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4082, 'grad_norm': 0.5078125, 'learning_rate': 5.988370606665739e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1613, 'grad_norm': 0.61328125, 'learning_rate': 5.987234368919353e-05, 'epoch': 0.64}\n",
      "{'loss': 1.302, 'grad_norm': 0.498046875, 'learning_rate': 5.986098131172967e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2208, 'grad_norm': 1.109375, 'learning_rate': 5.984961893426581e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0804, 'grad_norm': 0.8359375, 'learning_rate': 5.983825655680195e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3649, 'grad_norm': 0.69140625, 'learning_rate': 5.982689417933809e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1639, 'grad_norm': 0.578125, 'learning_rate': 5.981553180187423e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1628, 'grad_norm': 0.65234375, 'learning_rate': 5.980416942441036e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2231, 'grad_norm': 0.609375, 'learning_rate': 5.979280704694651e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9923, 'grad_norm': 0.83203125, 'learning_rate': 5.978144466948265e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4125, 'grad_norm': 0.5546875, 'learning_rate': 5.977008229201878e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0905, 'grad_norm': 0.58984375, 'learning_rate': 5.975871991455493e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3141, 'grad_norm': 0.59375, 'learning_rate': 5.974735753709106e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1159, 'grad_norm': 0.8671875, 'learning_rate': 5.97359951596272e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1024, 'grad_norm': 0.71875, 'learning_rate': 5.972463278216335e-05, 'epoch': 0.64}\n",
      "{'loss': 1.339, 'grad_norm': 0.6015625, 'learning_rate': 5.971327040469948e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1431, 'grad_norm': 0.66015625, 'learning_rate': 5.970190802723562e-05, 'epoch': 0.64}\n",
      "{'loss': 1.29, 'grad_norm': 0.45703125, 'learning_rate': 5.969054564977176e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1809, 'grad_norm': 0.5390625, 'learning_rate': 5.96791832723079e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0144, 'grad_norm': 0.427734375, 'learning_rate': 5.966782089484404e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2048, 'grad_norm': 0.59765625, 'learning_rate': 5.965645851738018e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0906, 'grad_norm': 0.703125, 'learning_rate': 5.964509613991632e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2299, 'grad_norm': 0.47265625, 'learning_rate': 5.963373376245246e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3429, 'grad_norm': 0.6953125, 'learning_rate': 5.96223713849886e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1213, 'grad_norm': 0.625, 'learning_rate': 5.961100900752473e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3762, 'grad_norm': 0.51953125, 'learning_rate': 5.959964663006088e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1431, 'grad_norm': 0.82421875, 'learning_rate': 5.958828425259702e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0015, 'grad_norm': 0.49609375, 'learning_rate': 5.957692187513315e-05, 'epoch': 0.64}\n",
      "{'loss': 1.189, 'grad_norm': 0.609375, 'learning_rate': 5.95655594976693e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1484, 'grad_norm': 2.09375, 'learning_rate': 5.955419712020543e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3077, 'grad_norm': 0.52734375, 'learning_rate': 5.954283474274157e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1836, 'grad_norm': 0.59765625, 'learning_rate': 5.953147236527772e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1066, 'grad_norm': 0.52734375, 'learning_rate': 5.952010998781385e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1561, 'grad_norm': 0.57421875, 'learning_rate': 5.950874761034999e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9505, 'grad_norm': 0.91796875, 'learning_rate': 5.949738523288613e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3523, 'grad_norm': 0.63671875, 'learning_rate': 5.948602285542227e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1044, 'grad_norm': 0.66796875, 'learning_rate': 5.947466047795841e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2304, 'grad_norm': 0.470703125, 'learning_rate': 5.946329810049455e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1612, 'grad_norm': 0.625, 'learning_rate': 5.945193572303069e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1306, 'grad_norm': 1.2421875, 'learning_rate': 5.944057334556683e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3055, 'grad_norm': 0.578125, 'learning_rate': 5.942921096810297e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0387, 'grad_norm': 0.9921875, 'learning_rate': 5.9417848590639113e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1936, 'grad_norm': 0.5546875, 'learning_rate': 5.940648621317525e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2626, 'grad_norm': 0.58984375, 'learning_rate': 5.939512383571139e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0846, 'grad_norm': 0.99609375, 'learning_rate': 5.938376145824752e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3297, 'grad_norm': 0.66015625, 'learning_rate': 5.937239908078367e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2099, 'grad_norm': 0.65625, 'learning_rate': 5.93610367033198e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2358, 'grad_norm': 0.54296875, 'learning_rate': 5.934967432585594e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1877, 'grad_norm': 0.59765625, 'learning_rate': 5.933831194839209e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0586, 'grad_norm': 0.98828125, 'learning_rate': 5.932694957092822e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3923, 'grad_norm': 0.65625, 'learning_rate': 5.931558719346436e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0003, 'grad_norm': 0.66015625, 'learning_rate': 5.93042248160005e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3178, 'grad_norm': 0.578125, 'learning_rate': 5.9292862438536643e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2284, 'grad_norm': 0.482421875, 'learning_rate': 5.928150006107278e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0789, 'grad_norm': 0.6640625, 'learning_rate': 5.927013768360892e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3665, 'grad_norm': 0.490234375, 'learning_rate': 5.9258775306145064e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2159, 'grad_norm': 0.51171875, 'learning_rate': 5.92474129286812e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3641, 'grad_norm': 0.470703125, 'learning_rate': 5.923605055121734e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2401, 'grad_norm': 0.6171875, 'learning_rate': 5.9224688173753484e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1089, 'grad_norm': 0.84765625, 'learning_rate': 5.921332579628962e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4046, 'grad_norm': 0.51953125, 'learning_rate': 5.920196341882576e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0868, 'grad_norm': 0.76953125, 'learning_rate': 5.919060104136189e-05, 'epoch': 0.64}\n",
      "{'loss': 1.251, 'grad_norm': 0.458984375, 'learning_rate': 5.917923866389804e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3541, 'grad_norm': 0.6796875, 'learning_rate': 5.916787628643417e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1001, 'grad_norm': 1.0234375, 'learning_rate': 5.915651390897031e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4513, 'grad_norm': 0.5390625, 'learning_rate': 5.914515153150646e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1871, 'grad_norm': 0.8828125, 'learning_rate': 5.9133789154042594e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1837, 'grad_norm': 0.458984375, 'learning_rate': 5.912242677657873e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3184, 'grad_norm': 0.6484375, 'learning_rate': 5.911106439911487e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1149, 'grad_norm': 1.2109375, 'learning_rate': 5.9099702021651014e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3068, 'grad_norm': 0.466796875, 'learning_rate': 5.908833964418715e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1608, 'grad_norm': 0.90625, 'learning_rate': 5.907697726672329e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1648, 'grad_norm': 0.69921875, 'learning_rate': 5.9065614889259434e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1857, 'grad_norm': 0.59765625, 'learning_rate': 5.905425251179557e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2074, 'grad_norm': 0.70703125, 'learning_rate': 5.904289013433171e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2576, 'grad_norm': 0.55859375, 'learning_rate': 5.9031527756867855e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2182, 'grad_norm': 0.65234375, 'learning_rate': 5.902016537940399e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3085, 'grad_norm': 0.515625, 'learning_rate': 5.900880300194013e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2258, 'grad_norm': 0.5546875, 'learning_rate': 5.899744062447626e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0741, 'grad_norm': 0.984375, 'learning_rate': 5.898607824701241e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1732, 'grad_norm': 0.60546875, 'learning_rate': 5.8974715869548544e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2449, 'grad_norm': 0.6875, 'learning_rate': 5.896335349208468e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1184, 'grad_norm': 0.458984375, 'learning_rate': 5.895199111462083e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2357, 'grad_norm': 0.640625, 'learning_rate': 5.8940628737156964e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1323, 'grad_norm': 1.1015625, 'learning_rate': 5.89292663596931e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2825, 'grad_norm': 0.62109375, 'learning_rate': 5.891790398222925e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2047, 'grad_norm': 0.6484375, 'learning_rate': 5.8906541604765385e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2713, 'grad_norm': 0.56640625, 'learning_rate': 5.889517922730152e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1609, 'grad_norm': 0.41796875, 'learning_rate': 5.888381684983766e-05, 'epoch': 0.64}\n",
      "{'loss': 1.058, 'grad_norm': 0.76171875, 'learning_rate': 5.8872454472373805e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4011, 'grad_norm': 0.5234375, 'learning_rate': 5.886109209490994e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2584, 'grad_norm': 0.7578125, 'learning_rate': 5.884972971744608e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2543, 'grad_norm': 0.546875, 'learning_rate': 5.8838367339982225e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2305, 'grad_norm': 0.55859375, 'learning_rate': 5.882700496251836e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0374, 'grad_norm': 1.1796875, 'learning_rate': 5.88156425850545e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3245, 'grad_norm': 0.5859375, 'learning_rate': 5.880428020759063e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0751, 'grad_norm': 0.8515625, 'learning_rate': 5.8792917830126784e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2189, 'grad_norm': 0.455078125, 'learning_rate': 5.8781555452662915e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2861, 'grad_norm': 0.6171875, 'learning_rate': 5.877019307519905e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1553, 'grad_norm': 0.75390625, 'learning_rate': 5.8758830697735204e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2863, 'grad_norm': 0.9375, 'learning_rate': 5.8747468320271335e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2358, 'grad_norm': 0.7734375, 'learning_rate': 5.873610594280747e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1827, 'grad_norm': 0.490234375, 'learning_rate': 5.872474356534362e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1623, 'grad_norm': 0.65234375, 'learning_rate': 5.8713381187879755e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9933, 'grad_norm': 0.625, 'learning_rate': 5.870201881041589e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3575, 'grad_norm': 0.859375, 'learning_rate': 5.869065643295203e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1773, 'grad_norm': 1.2109375, 'learning_rate': 5.8679294055488176e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2303, 'grad_norm': 0.59765625, 'learning_rate': 5.8667931678024313e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2092, 'grad_norm': 0.6171875, 'learning_rate': 5.865656930056045e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0299, 'grad_norm': 0.44140625, 'learning_rate': 5.8645206923096596e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3595, 'grad_norm': 0.5859375, 'learning_rate': 5.8633844545632734e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1156, 'grad_norm': 0.80859375, 'learning_rate': 5.862248216816887e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3927, 'grad_norm': 0.435546875, 'learning_rate': 5.8611119790705e-05, 'epoch': 0.64}\n",
      "{'loss': 1.241, 'grad_norm': 0.61328125, 'learning_rate': 5.8599757413241154e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0421, 'grad_norm': 0.828125, 'learning_rate': 5.8588395035777285e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2451, 'grad_norm': 0.4765625, 'learning_rate': 5.857703265831342e-05, 'epoch': 0.64}\n",
      "{'loss': 1.091, 'grad_norm': 0.73828125, 'learning_rate': 5.8565670280849575e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1662, 'grad_norm': 0.61328125, 'learning_rate': 5.8554307903385706e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3027, 'grad_norm': 0.66796875, 'learning_rate': 5.8542945525921843e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0447, 'grad_norm': 1.0859375, 'learning_rate': 5.853158314845799e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3371, 'grad_norm': 0.61328125, 'learning_rate': 5.8520220770994126e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1217, 'grad_norm': 0.78515625, 'learning_rate': 5.8508858393530264e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0774, 'grad_norm': 0.47265625, 'learning_rate': 5.84974960160664e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1651, 'grad_norm': 0.74609375, 'learning_rate': 5.8486133638602546e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1227, 'grad_norm': 0.60546875, 'learning_rate': 5.8474771261138684e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3849, 'grad_norm': 0.50390625, 'learning_rate': 5.846340888367482e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1117, 'grad_norm': 0.859375, 'learning_rate': 5.8452046506210967e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1814, 'grad_norm': 0.5546875, 'learning_rate': 5.8440684128747104e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3124, 'grad_norm': 0.609375, 'learning_rate': 5.842932175128324e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0407, 'grad_norm': 0.65234375, 'learning_rate': 5.8417959373819373e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3508, 'grad_norm': 0.56640625, 'learning_rate': 5.8406596996355525e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0731, 'grad_norm': 0.8046875, 'learning_rate': 5.8395234618891656e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2393, 'grad_norm': 0.44921875, 'learning_rate': 5.8383872241427794e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2495, 'grad_norm': 0.609375, 'learning_rate': 5.8372509863963945e-05, 'epoch': 0.65}\n",
      "{'loss': 1.093, 'grad_norm': 1.4765625, 'learning_rate': 5.8361147486500076e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2342, 'grad_norm': 0.462890625, 'learning_rate': 5.8349785109036214e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2111, 'grad_norm': 0.9296875, 'learning_rate': 5.833842273157236e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2096, 'grad_norm': 0.44140625, 'learning_rate': 5.8327060354108497e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1466, 'grad_norm': 0.58984375, 'learning_rate': 5.8315697976644634e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0402, 'grad_norm': 1.0078125, 'learning_rate': 5.830433559918077e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3718, 'grad_norm': 0.59375, 'learning_rate': 5.829297322171692e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3174, 'grad_norm': 0.8203125, 'learning_rate': 5.8281610844253055e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2298, 'grad_norm': 0.40234375, 'learning_rate': 5.827024846678919e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2862, 'grad_norm': 0.59765625, 'learning_rate': 5.825888608932534e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1282, 'grad_norm': 0.8828125, 'learning_rate': 5.8247523711861475e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3423, 'grad_norm': 0.51953125, 'learning_rate': 5.823616133439761e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0796, 'grad_norm': 1.2109375, 'learning_rate': 5.822479895693376e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2376, 'grad_norm': 0.7265625, 'learning_rate': 5.8213436579469895e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1913, 'grad_norm': 0.55859375, 'learning_rate': 5.8202074202006027e-05, 'epoch': 0.65}\n",
      "{'loss': 0.9361, 'grad_norm': 0.34375, 'learning_rate': 5.8190711824542164e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1747, 'grad_norm': 0.546875, 'learning_rate': 5.8179349447078316e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2149, 'grad_norm': 0.89453125, 'learning_rate': 5.816798706961445e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0119, 'grad_norm': 0.4453125, 'learning_rate': 5.8156624692150585e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2707, 'grad_norm': 0.5234375, 'learning_rate': 5.814526231468673e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1044, 'grad_norm': 0.55859375, 'learning_rate': 5.813389993722287e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2963, 'grad_norm': 0.625, 'learning_rate': 5.8122537559759005e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1033, 'grad_norm': 0.72265625, 'learning_rate': 5.811117518229514e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0266, 'grad_norm': 0.45703125, 'learning_rate': 5.809981280483129e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1051, 'grad_norm': 0.7421875, 'learning_rate': 5.8088450427367425e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0576, 'grad_norm': 0.6953125, 'learning_rate': 5.807708804990356e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4259, 'grad_norm': 0.5234375, 'learning_rate': 5.806572567243971e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1997, 'grad_norm': 0.578125, 'learning_rate': 5.8054363294975846e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1466, 'grad_norm': 0.53515625, 'learning_rate': 5.8043000917511984e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1614, 'grad_norm': 0.71875, 'learning_rate': 5.803163854004813e-05, 'epoch': 0.65}\n",
      "{'loss': 0.997, 'grad_norm': 1.0625, 'learning_rate': 5.8020276162584266e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2507, 'grad_norm': 0.515625, 'learning_rate': 5.80089137851204e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1235, 'grad_norm': 0.671875, 'learning_rate': 5.7997551407656535e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2144, 'grad_norm': 0.45703125, 'learning_rate': 5.7986189030192686e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2231, 'grad_norm': 0.5703125, 'learning_rate': 5.797482665272882e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0265, 'grad_norm': 0.70703125, 'learning_rate': 5.7963464275264955e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4716, 'grad_norm': 0.58203125, 'learning_rate': 5.79521018978011e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1148, 'grad_norm': 0.6328125, 'learning_rate': 5.794073952033724e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2906, 'grad_norm': 0.453125, 'learning_rate': 5.7929377142873376e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2491, 'grad_norm': 0.85546875, 'learning_rate': 5.7918014765409514e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2024, 'grad_norm': 1.75, 'learning_rate': 5.790665238794566e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2964, 'grad_norm': 0.546875, 'learning_rate': 5.7895290010481796e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2103, 'grad_norm': 0.6875, 'learning_rate': 5.7883927633017934e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1646, 'grad_norm': 0.4921875, 'learning_rate': 5.787256525555408e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1284, 'grad_norm': 0.75390625, 'learning_rate': 5.7861202878090216e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0516, 'grad_norm': 0.82421875, 'learning_rate': 5.7849840500626354e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4375, 'grad_norm': 0.578125, 'learning_rate': 5.78384781231625e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1534, 'grad_norm': 0.7109375, 'learning_rate': 5.782711574569864e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1818, 'grad_norm': 0.44921875, 'learning_rate': 5.781575336823477e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0904, 'grad_norm': 0.57421875, 'learning_rate': 5.7804390990770906e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1301, 'grad_norm': 1.0390625, 'learning_rate': 5.779302861330706e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4737, 'grad_norm': 0.478515625, 'learning_rate': 5.778166623584319e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1708, 'grad_norm': 0.65234375, 'learning_rate': 5.7770303858379326e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2463, 'grad_norm': 0.59765625, 'learning_rate': 5.775894148091547e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1282, 'grad_norm': 0.734375, 'learning_rate': 5.774757910345161e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1205, 'grad_norm': 0.9609375, 'learning_rate': 5.7736216725987746e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3736, 'grad_norm': 0.53515625, 'learning_rate': 5.7724854348523884e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2472, 'grad_norm': 0.74609375, 'learning_rate': 5.771349197106003e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2755, 'grad_norm': 0.4921875, 'learning_rate': 5.770212959359617e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2645, 'grad_norm': 0.59765625, 'learning_rate': 5.7690767216132304e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1034, 'grad_norm': 0.921875, 'learning_rate': 5.767940483866845e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2448, 'grad_norm': 0.58984375, 'learning_rate': 5.766804246120459e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1544, 'grad_norm': 0.859375, 'learning_rate': 5.7656680083740725e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1274, 'grad_norm': 0.47265625, 'learning_rate': 5.764531770627687e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2213, 'grad_norm': 0.91796875, 'learning_rate': 5.763395532881301e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1035, 'grad_norm': 1.125, 'learning_rate': 5.762259295134914e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2002, 'grad_norm': 0.5859375, 'learning_rate': 5.7611230573885276e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2407, 'grad_norm': 0.78125, 'learning_rate': 5.759986819642143e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2411, 'grad_norm': 0.46484375, 'learning_rate': 5.758850581895756e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2534, 'grad_norm': 0.5546875, 'learning_rate': 5.7577143441493697e-05, 'epoch': 0.65}\n",
      "{'loss': 1.099, 'grad_norm': 1.40625, 'learning_rate': 5.756578106402984e-05, 'epoch': 0.65}\n",
      "{'loss': 1.32, 'grad_norm': 0.5, 'learning_rate': 5.755441868656598e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1791, 'grad_norm': 0.71484375, 'learning_rate': 5.754305630910212e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1285, 'grad_norm': 0.52734375, 'learning_rate': 5.753169393163826e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1784, 'grad_norm': 1.0390625, 'learning_rate': 5.75203315541744e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0456, 'grad_norm': 0.408203125, 'learning_rate': 5.750896917671054e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3634, 'grad_norm': 0.50390625, 'learning_rate': 5.7497606799246675e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0421, 'grad_norm': 0.67578125, 'learning_rate': 5.748624442178282e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1909, 'grad_norm': 0.47265625, 'learning_rate': 5.747488204431896e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2601, 'grad_norm': 0.65234375, 'learning_rate': 5.7463519666855095e-05, 'epoch': 0.65}\n",
      "{'loss': 1.122, 'grad_norm': 0.91015625, 'learning_rate': 5.745215728939124e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3817, 'grad_norm': 0.58203125, 'learning_rate': 5.744079491192738e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1674, 'grad_norm': 0.64453125, 'learning_rate': 5.742943253446351e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2368, 'grad_norm': 0.53125, 'learning_rate': 5.741807015699965e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2019, 'grad_norm': 0.51171875, 'learning_rate': 5.74067077795358e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1456, 'grad_norm': 0.90234375, 'learning_rate': 5.739534540207193e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3055, 'grad_norm': 0.58203125, 'learning_rate': 5.738398302460807e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2756, 'grad_norm': 0.77734375, 'learning_rate': 5.737262064714421e-05, 'epoch': 0.65}\n",
      "{'loss': 1.218, 'grad_norm': 0.50390625, 'learning_rate': 5.736125826968035e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2536, 'grad_norm': 0.6796875, 'learning_rate': 5.734989589221649e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1496, 'grad_norm': 0.8671875, 'learning_rate': 5.733853351475263e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4241, 'grad_norm': 0.5390625, 'learning_rate': 5.732717113728877e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1532, 'grad_norm': 0.796875, 'learning_rate': 5.731580875982491e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2174, 'grad_norm': 0.5078125, 'learning_rate': 5.7304446382361046e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2623, 'grad_norm': 0.54296875, 'learning_rate': 5.729308400489719e-05, 'epoch': 0.65}\n",
      "{'loss': 0.9969, 'grad_norm': 0.8203125, 'learning_rate': 5.728172162743333e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3863, 'grad_norm': 0.59765625, 'learning_rate': 5.7270359249969466e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2373, 'grad_norm': 0.6171875, 'learning_rate': 5.725899687250561e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2196, 'grad_norm': 0.609375, 'learning_rate': 5.724763449504175e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1573, 'grad_norm': 0.6484375, 'learning_rate': 5.723627211757788e-05, 'epoch': 0.65}\n",
      "{'loss': 1.077, 'grad_norm': 1.3359375, 'learning_rate': 5.722490974011402e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3496, 'grad_norm': 0.48828125, 'learning_rate': 5.721354736265017e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2268, 'grad_norm': 0.765625, 'learning_rate': 5.72021849851863e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1389, 'grad_norm': 0.4296875, 'learning_rate': 5.719082260772244e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2419, 'grad_norm': 0.8046875, 'learning_rate': 5.717946023025858e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0523, 'grad_norm': 1.015625, 'learning_rate': 5.716809785279472e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3677, 'grad_norm': 0.4609375, 'learning_rate': 5.715673547533086e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1133, 'grad_norm': 0.6875, 'learning_rate': 5.7145373097867e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3155, 'grad_norm': 0.373046875, 'learning_rate': 5.713401072040314e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1688, 'grad_norm': 0.50390625, 'learning_rate': 5.712264834293928e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0594, 'grad_norm': 0.984375, 'learning_rate': 5.7111285965475416e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2201, 'grad_norm': 0.578125, 'learning_rate': 5.709992358801156e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0911, 'grad_norm': 0.59375, 'learning_rate': 5.70885612105477e-05, 'epoch': 0.65}\n",
      "{'loss': 1.199, 'grad_norm': 0.4921875, 'learning_rate': 5.707719883308384e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1547, 'grad_norm': 0.75390625, 'learning_rate': 5.706583645561998e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0154, 'grad_norm': 1.09375, 'learning_rate': 5.705447407815612e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3817, 'grad_norm': 0.546875, 'learning_rate': 5.704311170069225e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0728, 'grad_norm': 0.8046875, 'learning_rate': 5.70317493232284e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1921, 'grad_norm': 0.9609375, 'learning_rate': 5.702038694576454e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2435, 'grad_norm': 0.5625, 'learning_rate': 5.700902456830067e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0014, 'grad_norm': 1.0234375, 'learning_rate': 5.699766219083681e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3056, 'grad_norm': 0.73046875, 'learning_rate': 5.698629981337295e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0538, 'grad_norm': 0.71875, 'learning_rate': 5.697493743590909e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1095, 'grad_norm': 0.451171875, 'learning_rate': 5.696357505844523e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1392, 'grad_norm': 0.65625, 'learning_rate': 5.6952212680981373e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1557, 'grad_norm': 0.80859375, 'learning_rate': 5.694085030351751e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2618, 'grad_norm': 0.59375, 'learning_rate': 5.692948792605365e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2104, 'grad_norm': 0.64453125, 'learning_rate': 5.691812554858979e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1725, 'grad_norm': 0.478515625, 'learning_rate': 5.690676317112593e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2825, 'grad_norm': 0.6171875, 'learning_rate': 5.689540079366207e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0489, 'grad_norm': 0.419921875, 'learning_rate': 5.688403841619821e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2194, 'grad_norm': 0.56640625, 'learning_rate': 5.687267603873435e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0969, 'grad_norm': 0.8671875, 'learning_rate': 5.686131366127049e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1966, 'grad_norm': 0.70703125, 'learning_rate': 5.684995128380662e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2705, 'grad_norm': 0.65234375, 'learning_rate': 5.683858890634277e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0857, 'grad_norm': 0.57421875, 'learning_rate': 5.682722652887891e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3664, 'grad_norm': 0.53125, 'learning_rate': 5.681586415141504e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0022, 'grad_norm': 0.73828125, 'learning_rate': 5.680450177395118e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0818, 'grad_norm': 0.59765625, 'learning_rate': 5.6793139396487324e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1694, 'grad_norm': 0.62890625, 'learning_rate': 5.678177701902346e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0502, 'grad_norm': 0.41015625, 'learning_rate': 5.67704146415596e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2626, 'grad_norm': 0.58203125, 'learning_rate': 5.6759052264095744e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1223, 'grad_norm': 0.6640625, 'learning_rate': 5.674768988663188e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3174, 'grad_norm': 0.51171875, 'learning_rate': 5.673632750916802e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2305, 'grad_norm': 0.8125, 'learning_rate': 5.672496513170416e-05, 'epoch': 0.65}\n",
      "{'loss': 1.184, 'grad_norm': 0.47265625, 'learning_rate': 5.67136027542403e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4204, 'grad_norm': 0.58203125, 'learning_rate': 5.670224037677644e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1679, 'grad_norm': 0.96484375, 'learning_rate': 5.669087799931258e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1777, 'grad_norm': 0.478515625, 'learning_rate': 5.667951562184872e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2929, 'grad_norm': 0.69921875, 'learning_rate': 5.666815324438486e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1385, 'grad_norm': 1.046875, 'learning_rate': 5.665679086692099e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2191, 'grad_norm': 0.423828125, 'learning_rate': 5.664542848945714e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0447, 'grad_norm': 0.6953125, 'learning_rate': 5.663406611199328e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2582, 'grad_norm': 0.4375, 'learning_rate': 5.662270373452941e-05, 'epoch': 0.65}\n",
      "{'loss': 1.277, 'grad_norm': 0.6171875, 'learning_rate': 5.661134135706555e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0672, 'grad_norm': 0.77734375, 'learning_rate': 5.6599978979601694e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3369, 'grad_norm': 0.6171875, 'learning_rate': 5.658861660213783e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0994, 'grad_norm': 0.6875, 'learning_rate': 5.657725422467397e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0814, 'grad_norm': 0.443359375, 'learning_rate': 5.6565891847210115e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2551, 'grad_norm': 0.578125, 'learning_rate': 5.655452946974625e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0401, 'grad_norm': 1.046875, 'learning_rate': 5.654316709228239e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4033, 'grad_norm': 0.61328125, 'learning_rate': 5.653180471481853e-05, 'epoch': 0.65}\n",
      "{'loss': 1.142, 'grad_norm': 1.3125, 'learning_rate': 5.652044233735467e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1074, 'grad_norm': 0.42578125, 'learning_rate': 5.650907995989081e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2324, 'grad_norm': 0.57421875, 'learning_rate': 5.649771758242695e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1378, 'grad_norm': 0.890625, 'learning_rate': 5.648635520496309e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4779, 'grad_norm': 0.447265625, 'learning_rate': 5.647499282749923e-05, 'epoch': 0.65}\n",
      "{'loss': 1.271, 'grad_norm': 1.0625, 'learning_rate': 5.646363045003536e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1338, 'grad_norm': 0.55859375, 'learning_rate': 5.6452268072571514e-05, 'epoch': 0.65}\n",
      "{'loss': 1.268, 'grad_norm': 0.625, 'learning_rate': 5.644090569510765e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1205, 'grad_norm': 1.1328125, 'learning_rate': 5.642954331764378e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4609, 'grad_norm': 0.67578125, 'learning_rate': 5.641818094017992e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2396, 'grad_norm': 0.7265625, 'learning_rate': 5.6406818562716065e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1564, 'grad_norm': 0.46484375, 'learning_rate': 5.63954561852522e-05, 'epoch': 0.65}\n",
      "{'loss': 1.161, 'grad_norm': 0.65234375, 'learning_rate': 5.638409380778834e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1533, 'grad_norm': 1.0, 'learning_rate': 5.6372731430324485e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3121, 'grad_norm': 0.6875, 'learning_rate': 5.636136905286062e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0954, 'grad_norm': 0.66796875, 'learning_rate': 5.635000667539676e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1391, 'grad_norm': 0.515625, 'learning_rate': 5.6338644297932906e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2273, 'grad_norm': 0.68359375, 'learning_rate': 5.6327281920469044e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0508, 'grad_norm': 0.37109375, 'learning_rate': 5.631591954300518e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3398, 'grad_norm': 0.875, 'learning_rate': 5.630455716554132e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1252, 'grad_norm': 0.73046875, 'learning_rate': 5.6293194788077464e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2046, 'grad_norm': 0.56640625, 'learning_rate': 5.62818324106136e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2151, 'grad_norm': 0.69921875, 'learning_rate': 5.627047003314973e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1031, 'grad_norm': 1.5546875, 'learning_rate': 5.6259107655685884e-05, 'epoch': 0.65}\n",
      "{'loss': 1.3116, 'grad_norm': 0.46484375, 'learning_rate': 5.624774527822202e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1427, 'grad_norm': 0.91015625, 'learning_rate': 5.623638290075815e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1673, 'grad_norm': 0.400390625, 'learning_rate': 5.622502052329429e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2818, 'grad_norm': 0.81640625, 'learning_rate': 5.6213658145830436e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0627, 'grad_norm': 0.80859375, 'learning_rate': 5.6202295768366573e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3413, 'grad_norm': 0.78515625, 'learning_rate': 5.619093339090271e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2482, 'grad_norm': 0.76953125, 'learning_rate': 5.6179571013438856e-05, 'epoch': 0.65}\n",
      "{'loss': 1.2119, 'grad_norm': 0.859375, 'learning_rate': 5.6168208635974994e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1729, 'grad_norm': 0.68359375, 'learning_rate': 5.615684625851113e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1061, 'grad_norm': 0.77734375, 'learning_rate': 5.6145483881047276e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1711, 'grad_norm': 0.58203125, 'learning_rate': 5.6134121503583414e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0678, 'grad_norm': 0.74609375, 'learning_rate': 5.612275912611955e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2723, 'grad_norm': 0.4296875, 'learning_rate': 5.611139674865569e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2032, 'grad_norm': 0.6796875, 'learning_rate': 5.6100034371191834e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0498, 'grad_norm': 0.68359375, 'learning_rate': 5.608867199372797e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2896, 'grad_norm': 0.53125, 'learning_rate': 5.6077309616264103e-05, 'epoch': 0.66}\n",
      "{'loss': 1.154, 'grad_norm': 0.95703125, 'learning_rate': 5.6065947238800255e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0987, 'grad_norm': 0.498046875, 'learning_rate': 5.605458486133639e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2731, 'grad_norm': 0.7578125, 'learning_rate': 5.6043222483872524e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9701, 'grad_norm': 1.34375, 'learning_rate': 5.603186010640866e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3422, 'grad_norm': 0.486328125, 'learning_rate': 5.6020497728944806e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0824, 'grad_norm': 1.2109375, 'learning_rate': 5.6009135351480944e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1109, 'grad_norm': 0.48828125, 'learning_rate': 5.599777297401708e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1105, 'grad_norm': 0.67578125, 'learning_rate': 5.5986410596553227e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0932, 'grad_norm': 0.453125, 'learning_rate': 5.5975048219089364e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4464, 'grad_norm': 0.546875, 'learning_rate': 5.59636858416255e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1784, 'grad_norm': 0.84765625, 'learning_rate': 5.595232346416165e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2642, 'grad_norm': 0.5390625, 'learning_rate': 5.5940961086697785e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1971, 'grad_norm': 0.76953125, 'learning_rate': 5.592959870923392e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0966, 'grad_norm': 0.73828125, 'learning_rate': 5.591823633177006e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3603, 'grad_norm': 0.50390625, 'learning_rate': 5.5906873954306205e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1683, 'grad_norm': 0.76953125, 'learning_rate': 5.589551157684234e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2615, 'grad_norm': 0.54296875, 'learning_rate': 5.5884149199378474e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2622, 'grad_norm': 0.64453125, 'learning_rate': 5.5872786821914625e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1827, 'grad_norm': 0.859375, 'learning_rate': 5.586142444445076e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3759, 'grad_norm': 0.4765625, 'learning_rate': 5.5850062066986894e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0045, 'grad_norm': 1.0625, 'learning_rate': 5.583869968952303e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3755, 'grad_norm': 0.53515625, 'learning_rate': 5.5827337312059184e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2915, 'grad_norm': 0.734375, 'learning_rate': 5.5815974934595315e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0631, 'grad_norm': 0.82421875, 'learning_rate': 5.580461255713145e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2863, 'grad_norm': 0.5078125, 'learning_rate': 5.57932501796676e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1658, 'grad_norm': 0.6875, 'learning_rate': 5.5781887802203735e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1202, 'grad_norm': 0.490234375, 'learning_rate': 5.577052542473987e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3523, 'grad_norm': 0.51171875, 'learning_rate': 5.575916304727602e-05, 'epoch': 0.66}\n",
      "{'loss': 1.052, 'grad_norm': 0.47265625, 'learning_rate': 5.5747800669812155e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3714, 'grad_norm': 0.5078125, 'learning_rate': 5.573643829234829e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1565, 'grad_norm': 0.72265625, 'learning_rate': 5.572507591488443e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0379, 'grad_norm': 0.466796875, 'learning_rate': 5.5713713537420576e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2142, 'grad_norm': 0.8203125, 'learning_rate': 5.5702351159956714e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1526, 'grad_norm': 0.8125, 'learning_rate': 5.5690988782492845e-05, 'epoch': 0.66}\n",
      "{'loss': 1.312, 'grad_norm': 0.56640625, 'learning_rate': 5.5679626405028996e-05, 'epoch': 0.66}\n",
      "{'loss': 1.257, 'grad_norm': 0.6875, 'learning_rate': 5.5668264027565134e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0495, 'grad_norm': 0.443359375, 'learning_rate': 5.5656901650101265e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2399, 'grad_norm': 0.6640625, 'learning_rate': 5.5645539272637416e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0712, 'grad_norm': 0.87890625, 'learning_rate': 5.5634176895173554e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3476, 'grad_norm': 0.76171875, 'learning_rate': 5.5622814517709685e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0439, 'grad_norm': 0.734375, 'learning_rate': 5.561145214024582e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1378, 'grad_norm': 0.9765625, 'learning_rate': 5.560008976278197e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2224, 'grad_norm': 0.953125, 'learning_rate': 5.5588727385318106e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9878, 'grad_norm': 0.62890625, 'learning_rate': 5.5577365007854244e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2823, 'grad_norm': 0.625, 'learning_rate': 5.556600263039039e-05, 'epoch': 0.66}\n",
      "{'loss': 1.26, 'grad_norm': 0.9453125, 'learning_rate': 5.5554640252926526e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1478, 'grad_norm': 0.46875, 'learning_rate': 5.5543277875462664e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2762, 'grad_norm': 0.70703125, 'learning_rate': 5.55319154979988e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0507, 'grad_norm': 0.984375, 'learning_rate': 5.5520553120534946e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2175, 'grad_norm': 0.53515625, 'learning_rate': 5.5509190743071084e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2283, 'grad_norm': 0.76171875, 'learning_rate': 5.5497828365607215e-05, 'epoch': 0.66}\n",
      "{'loss': 1.128, 'grad_norm': 0.66015625, 'learning_rate': 5.548646598814337e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2451, 'grad_norm': 0.61328125, 'learning_rate': 5.5475103610679505e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1039, 'grad_norm': 1.1875, 'learning_rate': 5.5463741233215636e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2632, 'grad_norm': 0.50390625, 'learning_rate': 5.545237885575179e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2427, 'grad_norm': 0.6875, 'learning_rate': 5.5441016478287925e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2512, 'grad_norm': 0.6875, 'learning_rate': 5.5429654100824056e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1905, 'grad_norm': 0.74609375, 'learning_rate': 5.5418291723360194e-05, 'epoch': 0.66}\n",
      "{'loss': 1.096, 'grad_norm': 0.8203125, 'learning_rate': 5.540692934589634e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3915, 'grad_norm': 0.5, 'learning_rate': 5.5395566968432476e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1257, 'grad_norm': 0.765625, 'learning_rate': 5.5384204590968614e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1482, 'grad_norm': 0.53125, 'learning_rate': 5.537284221350476e-05, 'epoch': 0.66}\n",
      "{'loss': 1.169, 'grad_norm': 0.65234375, 'learning_rate': 5.53614798360409e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0119, 'grad_norm': 1.015625, 'learning_rate': 5.5350117458577035e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3156, 'grad_norm': 0.478515625, 'learning_rate': 5.533875508111317e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2418, 'grad_norm': 1.34375, 'learning_rate': 5.532739270364932e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2362, 'grad_norm': 0.53515625, 'learning_rate': 5.5316030326185455e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2126, 'grad_norm': 0.9296875, 'learning_rate': 5.5304667948721586e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9738, 'grad_norm': 1.015625, 'learning_rate': 5.529330557125774e-05, 'epoch': 0.66}\n",
      "{'loss': 1.243, 'grad_norm': 0.53125, 'learning_rate': 5.5281943193793875e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1207, 'grad_norm': 0.63671875, 'learning_rate': 5.5270580816330006e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1482, 'grad_norm': 0.4609375, 'learning_rate': 5.525921843886616e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1593, 'grad_norm': 0.7265625, 'learning_rate': 5.5247856061402296e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0277, 'grad_norm': 0.81640625, 'learning_rate': 5.523649368393843e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4341, 'grad_norm': 0.67578125, 'learning_rate': 5.5225131306474564e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0866, 'grad_norm': 0.78125, 'learning_rate': 5.521376892901071e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2978, 'grad_norm': 0.46875, 'learning_rate': 5.520240655154685e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2659, 'grad_norm': 0.578125, 'learning_rate': 5.5191044174082985e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0711, 'grad_norm': 0.6875, 'learning_rate': 5.517968179661913e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4083, 'grad_norm': 0.640625, 'learning_rate': 5.516831941915527e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0979, 'grad_norm': 0.609375, 'learning_rate': 5.5156957041691405e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2218, 'grad_norm': 0.5234375, 'learning_rate': 5.514559466422755e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2731, 'grad_norm': 0.625, 'learning_rate': 5.513423228676369e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0121, 'grad_norm': 1.328125, 'learning_rate': 5.5122869909299825e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2816, 'grad_norm': 0.609375, 'learning_rate': 5.5111507531835957e-05, 'epoch': 0.66}\n",
      "{'loss': 1.226, 'grad_norm': 0.828125, 'learning_rate': 5.510014515437211e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1172, 'grad_norm': 0.396484375, 'learning_rate': 5.5088782776908246e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1641, 'grad_norm': 0.69140625, 'learning_rate': 5.507742039944438e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9847, 'grad_norm': 1.0078125, 'learning_rate': 5.506605802198053e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3183, 'grad_norm': 0.62109375, 'learning_rate': 5.5054695644516666e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2051, 'grad_norm': 0.71875, 'learning_rate': 5.50433332670528e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1179, 'grad_norm': 0.45703125, 'learning_rate': 5.5031970889588935e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2087, 'grad_norm': 0.640625, 'learning_rate': 5.502060851212508e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1563, 'grad_norm': 0.91796875, 'learning_rate': 5.500924613466122e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4198, 'grad_norm': 0.470703125, 'learning_rate': 5.4997883757197355e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0558, 'grad_norm': 0.8046875, 'learning_rate': 5.49865213797335e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0745, 'grad_norm': 0.64453125, 'learning_rate': 5.497515900226964e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3112, 'grad_norm': 0.61328125, 'learning_rate': 5.4963796624805776e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2181, 'grad_norm': 1.1171875, 'learning_rate': 5.495243424734192e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3518, 'grad_norm': 0.5, 'learning_rate': 5.494107186987806e-05, 'epoch': 0.66}\n",
      "{'loss': 1.22, 'grad_norm': 0.93359375, 'learning_rate': 5.4929709492414196e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2423, 'grad_norm': 0.482421875, 'learning_rate': 5.491834711495033e-05, 'epoch': 0.66}\n",
      "{'loss': 1.246, 'grad_norm': 0.5703125, 'learning_rate': 5.490698473748648e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9883, 'grad_norm': 0.64453125, 'learning_rate': 5.4895622360022616e-05, 'epoch': 0.66}\n",
      "{'loss': 1.297, 'grad_norm': 0.62109375, 'learning_rate': 5.488425998255875e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1031, 'grad_norm': 0.80859375, 'learning_rate': 5.48728976050949e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1157, 'grad_norm': 0.470703125, 'learning_rate': 5.486153522763104e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2789, 'grad_norm': 0.6328125, 'learning_rate': 5.485017285016717e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0187, 'grad_norm': 0.890625, 'learning_rate': 5.4838810472703306e-05, 'epoch': 0.66}\n",
      "{'loss': 1.269, 'grad_norm': 0.515625, 'learning_rate': 5.482744809523945e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1899, 'grad_norm': 0.69921875, 'learning_rate': 5.481608571777559e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2636, 'grad_norm': 0.484375, 'learning_rate': 5.4804723340311726e-05, 'epoch': 0.66}\n",
      "{'loss': 1.183, 'grad_norm': 0.70703125, 'learning_rate': 5.479336096284787e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9649, 'grad_norm': 0.69140625, 'learning_rate': 5.478199858538401e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2356, 'grad_norm': 0.59375, 'learning_rate': 5.4770636207920146e-05, 'epoch': 0.66}\n",
      "{'loss': 1.148, 'grad_norm': 0.80078125, 'learning_rate': 5.475927383045629e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1684, 'grad_norm': 0.5, 'learning_rate': 5.474791145299243e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3015, 'grad_norm': 0.55078125, 'learning_rate': 5.473654907552857e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0618, 'grad_norm': 1.3359375, 'learning_rate': 5.47251866980647e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3388, 'grad_norm': 0.45703125, 'learning_rate': 5.471382432060085e-05, 'epoch': 0.66}\n",
      "{'loss': 1.032, 'grad_norm': 0.63671875, 'learning_rate': 5.470246194313699e-05, 'epoch': 0.66}\n",
      "{'loss': 1.148, 'grad_norm': 0.59375, 'learning_rate': 5.469109956567312e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2609, 'grad_norm': 0.8125, 'learning_rate': 5.467973718820927e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9763, 'grad_norm': 0.91015625, 'learning_rate': 5.466837481074541e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4018, 'grad_norm': 0.66796875, 'learning_rate': 5.465701243328154e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0926, 'grad_norm': 0.7265625, 'learning_rate': 5.4645650055817676e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2573, 'grad_norm': 0.6484375, 'learning_rate': 5.463428767835382e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1432, 'grad_norm': 0.52734375, 'learning_rate': 5.462292530088996e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0967, 'grad_norm': 1.1328125, 'learning_rate': 5.46115629234261e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2958, 'grad_norm': 0.5546875, 'learning_rate': 5.460020054596224e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0723, 'grad_norm': 0.66015625, 'learning_rate': 5.458883816849838e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2151, 'grad_norm': 0.58203125, 'learning_rate': 5.457747579103452e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3234, 'grad_norm': 0.62890625, 'learning_rate': 5.456611341357066e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1125, 'grad_norm': 1.078125, 'learning_rate': 5.45547510361068e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1688, 'grad_norm': 0.578125, 'learning_rate': 5.454338865864294e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0737, 'grad_norm': 1.28125, 'learning_rate': 5.453202628117907e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1925, 'grad_norm': 0.53125, 'learning_rate': 5.452066390371522e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1443, 'grad_norm': 0.8046875, 'learning_rate': 5.450930152625136e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1478, 'grad_norm': 0.703125, 'learning_rate': 5.449793914878749e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2285, 'grad_norm': 0.75, 'learning_rate': 5.448657677132364e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1768, 'grad_norm': 0.59765625, 'learning_rate': 5.447521439385978e-05, 'epoch': 0.66}\n",
      "{'loss': 1.239, 'grad_norm': 0.466796875, 'learning_rate': 5.446385201639591e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2232, 'grad_norm': 0.6171875, 'learning_rate': 5.445248963893206e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0612, 'grad_norm': 1.0703125, 'learning_rate': 5.444112726146819e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4003, 'grad_norm': 0.56640625, 'learning_rate': 5.442976488400433e-05, 'epoch': 0.66}\n",
      "{'loss': 1.161, 'grad_norm': 0.609375, 'learning_rate': 5.441840250654047e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2892, 'grad_norm': 0.5234375, 'learning_rate': 5.440704012907661e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1682, 'grad_norm': 0.63671875, 'learning_rate': 5.439567775161275e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1978, 'grad_norm': 0.96875, 'learning_rate': 5.438431537414889e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2783, 'grad_norm': 0.8046875, 'learning_rate': 5.437295299668503e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1922, 'grad_norm': 0.7421875, 'learning_rate': 5.436159061922117e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0865, 'grad_norm': 0.515625, 'learning_rate': 5.435022824175731e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2233, 'grad_norm': 0.9765625, 'learning_rate': 5.433886586429344e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0557, 'grad_norm': 0.91796875, 'learning_rate': 5.432750348682959e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3881, 'grad_norm': 0.55859375, 'learning_rate': 5.431614110936573e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2076, 'grad_norm': 0.83203125, 'learning_rate': 5.430477873190186e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2052, 'grad_norm': 0.50390625, 'learning_rate': 5.429341635443801e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3485, 'grad_norm': 0.55859375, 'learning_rate': 5.428205397697415e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0468, 'grad_norm': 0.60546875, 'learning_rate': 5.427069159951028e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4184, 'grad_norm': 0.578125, 'learning_rate': 5.425932922204643e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2105, 'grad_norm': 1.0390625, 'learning_rate': 5.424796684458256e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1516, 'grad_norm': 0.4765625, 'learning_rate': 5.42366044671187e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2627, 'grad_norm': 0.65625, 'learning_rate': 5.422524208965484e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0881, 'grad_norm': 1.0078125, 'learning_rate': 5.421387971219098e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4339, 'grad_norm': 0.61328125, 'learning_rate': 5.420251733472712e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1104, 'grad_norm': 0.53125, 'learning_rate': 5.419115495726326e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1834, 'grad_norm': 0.56640625, 'learning_rate': 5.41797925797994e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1589, 'grad_norm': 0.9296875, 'learning_rate': 5.416843020233554e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0069, 'grad_norm': 0.9609375, 'learning_rate': 5.415706782487168e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2183, 'grad_norm': 0.6015625, 'learning_rate': 5.414570544740781e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1708, 'grad_norm': 0.93359375, 'learning_rate': 5.413434306994396e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2092, 'grad_norm': 0.478515625, 'learning_rate': 5.41229806924801e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2144, 'grad_norm': 0.8671875, 'learning_rate': 5.411161831501623e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9355, 'grad_norm': 0.357421875, 'learning_rate': 5.410025593755238e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3507, 'grad_norm': 0.58984375, 'learning_rate': 5.408889356008852e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1572, 'grad_norm': 0.78515625, 'learning_rate': 5.407753118262465e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0929, 'grad_norm': 0.69140625, 'learning_rate': 5.40661688051608e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1293, 'grad_norm': 0.6171875, 'learning_rate': 5.405480642769693e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9899, 'grad_norm': 1.25, 'learning_rate': 5.404344405023307e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3093, 'grad_norm': 0.5703125, 'learning_rate': 5.403208167276921e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1558, 'grad_norm': 0.703125, 'learning_rate': 5.402071929530535e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3087, 'grad_norm': 0.455078125, 'learning_rate': 5.400935691784149e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2203, 'grad_norm': 0.54296875, 'learning_rate': 5.399799454037763e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0111, 'grad_norm': 1.1796875, 'learning_rate': 5.3986632162913774e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3386, 'grad_norm': 0.703125, 'learning_rate': 5.397526978544991e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2036, 'grad_norm': 0.62890625, 'learning_rate': 5.396390740798605e-05, 'epoch': 0.66}\n",
      "{'loss': 1.16, 'grad_norm': 0.5234375, 'learning_rate': 5.395254503052218e-05, 'epoch': 0.66}\n",
      "{'loss': 1.256, 'grad_norm': 0.765625, 'learning_rate': 5.394118265305833e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9679, 'grad_norm': 0.56640625, 'learning_rate': 5.392982027559447e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3891, 'grad_norm': 0.55859375, 'learning_rate': 5.39184578981306e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1396, 'grad_norm': 0.8203125, 'learning_rate': 5.390709552066675e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2214, 'grad_norm': 0.5859375, 'learning_rate': 5.389573314320289e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3114, 'grad_norm': 0.72265625, 'learning_rate': 5.388437076573902e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0006, 'grad_norm': 0.85546875, 'learning_rate': 5.387300838827517e-05, 'epoch': 0.66}\n",
      "{'loss': 1.3081, 'grad_norm': 0.6328125, 'learning_rate': 5.3861646010811304e-05, 'epoch': 0.66}\n",
      "{'loss': 1.2483, 'grad_norm': 0.7734375, 'learning_rate': 5.385028363334744e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1973, 'grad_norm': 0.5, 'learning_rate': 5.383892125588358e-05, 'epoch': 0.66}\n",
      "{'loss': 1.1509, 'grad_norm': 0.5625, 'learning_rate': 5.3827558878419724e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9973, 'grad_norm': 0.88671875, 'learning_rate': 5.381619650095586e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3561, 'grad_norm': 0.828125, 'learning_rate': 5.3804834123492e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2253, 'grad_norm': 0.59765625, 'learning_rate': 5.3793471746028144e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1717, 'grad_norm': 0.5234375, 'learning_rate': 5.378210936856428e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3085, 'grad_norm': 0.62109375, 'learning_rate': 5.377074699110042e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1509, 'grad_norm': 0.91796875, 'learning_rate': 5.3759384613636565e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3418, 'grad_norm': 0.53125, 'learning_rate': 5.37480222361727e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1975, 'grad_norm': 0.609375, 'learning_rate': 5.373665985870884e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9956, 'grad_norm': 0.470703125, 'learning_rate': 5.372529748124497e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3201, 'grad_norm': 0.76953125, 'learning_rate': 5.371393510378112e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0721, 'grad_norm': 0.703125, 'learning_rate': 5.370257272631726e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3162, 'grad_norm': 0.5, 'learning_rate': 5.369121034885339e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1826, 'grad_norm': 0.72265625, 'learning_rate': 5.367984797138954e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1212, 'grad_norm': 0.498046875, 'learning_rate': 5.3668485593925674e-05, 'epoch': 0.67}\n",
      "{'loss': 1.144, 'grad_norm': 0.703125, 'learning_rate': 5.365712321646181e-05, 'epoch': 0.67}\n",
      "{'loss': 0.973, 'grad_norm': 1.1484375, 'learning_rate': 5.364576083899795e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1409, 'grad_norm': 0.66015625, 'learning_rate': 5.3634398461534094e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2215, 'grad_norm': 0.8125, 'learning_rate': 5.362303608407023e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2857, 'grad_norm': 0.59765625, 'learning_rate': 5.361167370660637e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2193, 'grad_norm': 0.640625, 'learning_rate': 5.3600311329142515e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1361, 'grad_norm': 1.1640625, 'learning_rate': 5.358894895167865e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3699, 'grad_norm': 0.49609375, 'learning_rate': 5.357758657421479e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0872, 'grad_norm': 1.03125, 'learning_rate': 5.3566224196750935e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1025, 'grad_norm': 0.51171875, 'learning_rate': 5.355486181928707e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1852, 'grad_norm': 0.55859375, 'learning_rate': 5.354349944182321e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1362, 'grad_norm': 1.140625, 'learning_rate': 5.353213706435934e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2044, 'grad_norm': 0.765625, 'learning_rate': 5.352077468689549e-05, 'epoch': 0.67}\n",
      "{'loss': 1.17, 'grad_norm': 0.73828125, 'learning_rate': 5.350941230943163e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1241, 'grad_norm': 0.59765625, 'learning_rate': 5.349804993196776e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2389, 'grad_norm': 0.59765625, 'learning_rate': 5.3486687554503914e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1022, 'grad_norm': 0.69921875, 'learning_rate': 5.3475325177040045e-05, 'epoch': 0.67}\n",
      "{'loss': 1.431, 'grad_norm': 0.61328125, 'learning_rate': 5.346396279957618e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1721, 'grad_norm': 0.96484375, 'learning_rate': 5.345260042211232e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2393, 'grad_norm': 0.51953125, 'learning_rate': 5.3441238044648465e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3125, 'grad_norm': 0.5078125, 'learning_rate': 5.34298756671846e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1296, 'grad_norm': 0.984375, 'learning_rate': 5.341851328972074e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2193, 'grad_norm': 0.53125, 'learning_rate': 5.3407150912256885e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1424, 'grad_norm': 0.63671875, 'learning_rate': 5.339578853479302e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2437, 'grad_norm': 0.470703125, 'learning_rate': 5.338442615732916e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1621, 'grad_norm': 0.65234375, 'learning_rate': 5.3373063779865306e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0124, 'grad_norm': 0.5703125, 'learning_rate': 5.3361701402401444e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3489, 'grad_norm': 0.5078125, 'learning_rate': 5.335033902493758e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2358, 'grad_norm': 0.828125, 'learning_rate': 5.333897664747371e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1571, 'grad_norm': 0.546875, 'learning_rate': 5.3327614270009864e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2941, 'grad_norm': 0.94140625, 'learning_rate': 5.3316251892546e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9442, 'grad_norm': 1.359375, 'learning_rate': 5.330488951508213e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3864, 'grad_norm': 0.51953125, 'learning_rate': 5.3293527137618284e-05, 'epoch': 0.67}\n",
      "{'loss': 1.167, 'grad_norm': 1.0, 'learning_rate': 5.3282164760154415e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3547, 'grad_norm': 0.484375, 'learning_rate': 5.327080238269055e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4264, 'grad_norm': 0.65234375, 'learning_rate': 5.325944000522669e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9976, 'grad_norm': 0.87109375, 'learning_rate': 5.3248077627762836e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3019, 'grad_norm': 0.490234375, 'learning_rate': 5.3236715250298974e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1864, 'grad_norm': 0.609375, 'learning_rate': 5.322535287283511e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2541, 'grad_norm': 0.46484375, 'learning_rate': 5.3213990495371256e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1942, 'grad_norm': 0.6796875, 'learning_rate': 5.3202628117907394e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9772, 'grad_norm': 0.81640625, 'learning_rate': 5.319126574044353e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4895, 'grad_norm': 0.455078125, 'learning_rate': 5.3179903362979676e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1783, 'grad_norm': 0.76171875, 'learning_rate': 5.3168540985515814e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2685, 'grad_norm': 0.4296875, 'learning_rate': 5.315717860805195e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2491, 'grad_norm': 0.5390625, 'learning_rate': 5.314581623058808e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0646, 'grad_norm': 0.7421875, 'learning_rate': 5.3134453853124235e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3891, 'grad_norm': 0.6796875, 'learning_rate': 5.312309147566037e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1585, 'grad_norm': 0.7734375, 'learning_rate': 5.3111729098196504e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2061, 'grad_norm': 0.7109375, 'learning_rate': 5.3100366720732655e-05, 'epoch': 0.67}\n",
      "{'loss': 1.202, 'grad_norm': 0.73828125, 'learning_rate': 5.3089004343268786e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0644, 'grad_norm': 0.890625, 'learning_rate': 5.3077641965804924e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3354, 'grad_norm': 0.59375, 'learning_rate': 5.3066279588341075e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1267, 'grad_norm': 0.80859375, 'learning_rate': 5.3054917210877206e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1264, 'grad_norm': 0.55859375, 'learning_rate': 5.3043554833413344e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2933, 'grad_norm': 0.546875, 'learning_rate': 5.303219245594948e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1795, 'grad_norm': 0.97265625, 'learning_rate': 5.302083007848563e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3043, 'grad_norm': 0.8046875, 'learning_rate': 5.3009467701021765e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2081, 'grad_norm': 0.61328125, 'learning_rate': 5.29981053235579e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1548, 'grad_norm': 0.46484375, 'learning_rate': 5.298674294609405e-05, 'epoch': 0.67}\n",
      "{'loss': 1.208, 'grad_norm': 0.765625, 'learning_rate': 5.2975380568630185e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9911, 'grad_norm': 0.7421875, 'learning_rate': 5.296401819116632e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2724, 'grad_norm': 0.5703125, 'learning_rate': 5.2952655813702454e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0248, 'grad_norm': 0.6875, 'learning_rate': 5.2941293436238605e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0715, 'grad_norm': 0.474609375, 'learning_rate': 5.292993105877474e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2471, 'grad_norm': 0.66015625, 'learning_rate': 5.2918568681310874e-05, 'epoch': 0.67}\n",
      "{'loss': 1.059, 'grad_norm': 0.92578125, 'learning_rate': 5.2907206303847026e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3288, 'grad_norm': 0.53515625, 'learning_rate': 5.289584392638316e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0367, 'grad_norm': 1.0625, 'learning_rate': 5.2884481548919295e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2167, 'grad_norm': 0.609375, 'learning_rate': 5.2873119171455446e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3486, 'grad_norm': 0.578125, 'learning_rate': 5.286175679399158e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1281, 'grad_norm': 0.76953125, 'learning_rate': 5.2850394416527715e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4569, 'grad_norm': 0.5, 'learning_rate': 5.283903203906385e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0751, 'grad_norm': 0.79296875, 'learning_rate': 5.28276696616e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1211, 'grad_norm': 0.6171875, 'learning_rate': 5.2816307284136135e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0952, 'grad_norm': 0.59375, 'learning_rate': 5.280494490667227e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0572, 'grad_norm': 0.9609375, 'learning_rate': 5.279358252920842e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2858, 'grad_norm': 0.578125, 'learning_rate': 5.2782220151744556e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0812, 'grad_norm': 0.7421875, 'learning_rate': 5.277085777428069e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1887, 'grad_norm': 0.486328125, 'learning_rate': 5.2759495396816824e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1901, 'grad_norm': 0.640625, 'learning_rate': 5.2748133019352976e-05, 'epoch': 0.67}\n",
      "{'loss': 1.097, 'grad_norm': 0.51171875, 'learning_rate': 5.2736770641889114e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2763, 'grad_norm': 0.5390625, 'learning_rate': 5.2725408264425245e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1282, 'grad_norm': 0.78125, 'learning_rate': 5.2714045886961396e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2366, 'grad_norm': 0.54296875, 'learning_rate': 5.270268350949753e-05, 'epoch': 0.67}\n",
      "{'loss': 1.186, 'grad_norm': 0.65234375, 'learning_rate': 5.2691321132033665e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9714, 'grad_norm': 1.21875, 'learning_rate': 5.2679958754569817e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3874, 'grad_norm': 0.53515625, 'learning_rate': 5.266859637710595e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1147, 'grad_norm': 0.65234375, 'learning_rate': 5.2657233999642085e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0952, 'grad_norm': 0.5, 'learning_rate': 5.264587162217822e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1127, 'grad_norm': 0.578125, 'learning_rate': 5.263450924471437e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0684, 'grad_norm': 0.75, 'learning_rate': 5.2623146867250506e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2714, 'grad_norm': 0.412109375, 'learning_rate': 5.2611784489786644e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1665, 'grad_norm': 0.73046875, 'learning_rate': 5.260042211232279e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1137, 'grad_norm': 0.6171875, 'learning_rate': 5.2589059734858926e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1861, 'grad_norm': 0.6015625, 'learning_rate': 5.2577697357395064e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0702, 'grad_norm': 0.87109375, 'learning_rate': 5.256633497993121e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2774, 'grad_norm': 0.482421875, 'learning_rate': 5.2554972602467347e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1226, 'grad_norm': 0.5703125, 'learning_rate': 5.2543610225003484e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3242, 'grad_norm': 0.4375, 'learning_rate': 5.2532247847539615e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2656, 'grad_norm': 0.875, 'learning_rate': 5.252088547007577e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0708, 'grad_norm': 0.75, 'learning_rate': 5.25095230926119e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2867, 'grad_norm': 0.45703125, 'learning_rate': 5.2498160715148036e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2702, 'grad_norm': 0.8359375, 'learning_rate': 5.248679833768419e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1693, 'grad_norm': 0.515625, 'learning_rate': 5.247543596022032e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2955, 'grad_norm': 0.69921875, 'learning_rate': 5.2464073582756456e-05, 'epoch': 0.67}\n",
      "{'loss': 1.016, 'grad_norm': 0.8828125, 'learning_rate': 5.2452711205292594e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3012, 'grad_norm': 0.80859375, 'learning_rate': 5.244134882782874e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1016, 'grad_norm': 0.58203125, 'learning_rate': 5.2429986450364876e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1561, 'grad_norm': 0.546875, 'learning_rate': 5.2418624072901014e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2283, 'grad_norm': 0.8203125, 'learning_rate': 5.240726169543716e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0736, 'grad_norm': 1.046875, 'learning_rate': 5.23958993179733e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4362, 'grad_norm': 0.5234375, 'learning_rate': 5.2384536940509435e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2218, 'grad_norm': 1.078125, 'learning_rate': 5.237317456304558e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2321, 'grad_norm': 0.50390625, 'learning_rate': 5.236181218558172e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2965, 'grad_norm': 0.62109375, 'learning_rate': 5.2350449808117855e-05, 'epoch': 0.67}\n",
      "{'loss': 1.096, 'grad_norm': 1.5546875, 'learning_rate': 5.2339087430653986e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2853, 'grad_norm': 0.83203125, 'learning_rate': 5.232772505319014e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2002, 'grad_norm': 0.65234375, 'learning_rate': 5.231636267572627e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9996, 'grad_norm': 0.53515625, 'learning_rate': 5.2305000298262406e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1795, 'grad_norm': 0.96484375, 'learning_rate': 5.229363792079856e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9903, 'grad_norm': 0.69921875, 'learning_rate': 5.228227554333469e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1873, 'grad_norm': 0.546875, 'learning_rate': 5.227091316587083e-05, 'epoch': 0.67}\n",
      "{'loss': 1.094, 'grad_norm': 0.84765625, 'learning_rate': 5.2259550788406965e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1485, 'grad_norm': 0.5859375, 'learning_rate': 5.224818841094311e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2373, 'grad_norm': 0.6484375, 'learning_rate': 5.223682603347925e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0051, 'grad_norm': 0.8046875, 'learning_rate': 5.2225463656015385e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4117, 'grad_norm': 0.73046875, 'learning_rate': 5.221410127855153e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1833, 'grad_norm': 0.64453125, 'learning_rate': 5.220273890108767e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2721, 'grad_norm': 0.4609375, 'learning_rate': 5.2191376523623805e-05, 'epoch': 0.67}\n",
      "{'loss': 1.322, 'grad_norm': 0.51171875, 'learning_rate': 5.218001414615995e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0171, 'grad_norm': 0.6875, 'learning_rate': 5.216865176869609e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2558, 'grad_norm': 0.46484375, 'learning_rate': 5.2157289391232226e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0944, 'grad_norm': 0.765625, 'learning_rate': 5.214592701376836e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0485, 'grad_norm': 0.53515625, 'learning_rate': 5.213456463630451e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1692, 'grad_norm': 0.47265625, 'learning_rate': 5.212320225884064e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0472, 'grad_norm': 1.078125, 'learning_rate': 5.211183988137678e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3167, 'grad_norm': 0.478515625, 'learning_rate': 5.210047750391293e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1695, 'grad_norm': 1.0703125, 'learning_rate': 5.208911512644906e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1586, 'grad_norm': 0.52734375, 'learning_rate': 5.20777527489852e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2403, 'grad_norm': 0.7109375, 'learning_rate': 5.2066390371521335e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1181, 'grad_norm': 0.8359375, 'learning_rate': 5.205502799405748e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2338, 'grad_norm': 0.59765625, 'learning_rate': 5.204366561659362e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0651, 'grad_norm': 0.68359375, 'learning_rate': 5.2032303239129756e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2538, 'grad_norm': 0.458984375, 'learning_rate': 5.20209408616659e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3425, 'grad_norm': 0.494140625, 'learning_rate': 5.200957848420204e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0687, 'grad_norm': 1.1015625, 'learning_rate': 5.1998216106738176e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4467, 'grad_norm': 0.47265625, 'learning_rate': 5.198685372927432e-05, 'epoch': 0.67}\n",
      "{'loss': 1.141, 'grad_norm': 0.94921875, 'learning_rate': 5.197549135181046e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0806, 'grad_norm': 0.609375, 'learning_rate': 5.1964128974346596e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2564, 'grad_norm': 0.6484375, 'learning_rate': 5.195276659688273e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0584, 'grad_norm': 0.95703125, 'learning_rate': 5.194140421941888e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2989, 'grad_norm': 0.53125, 'learning_rate': 5.193004184195501e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0703, 'grad_norm': 0.875, 'learning_rate': 5.191867946449115e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1738, 'grad_norm': 0.51953125, 'learning_rate': 5.19073170870273e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1399, 'grad_norm': 0.6328125, 'learning_rate': 5.189595470956343e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0218, 'grad_norm': 0.75390625, 'learning_rate': 5.188459233209957e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3365, 'grad_norm': 0.51953125, 'learning_rate': 5.187322995463571e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1606, 'grad_norm': 0.703125, 'learning_rate': 5.186186757717185e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2155, 'grad_norm': 0.7265625, 'learning_rate': 5.185050519970799e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0859, 'grad_norm': 0.5703125, 'learning_rate': 5.1839142822244126e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0259, 'grad_norm': 0.78125, 'learning_rate': 5.182778044478027e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2964, 'grad_norm': 0.51953125, 'learning_rate': 5.181641806731641e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1653, 'grad_norm': 0.75, 'learning_rate': 5.1805055689852547e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2398, 'grad_norm': 0.435546875, 'learning_rate': 5.179369331238869e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2198, 'grad_norm': 0.57421875, 'learning_rate': 5.178233093492483e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1987, 'grad_norm': 1.34375, 'learning_rate': 5.177096855746097e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3861, 'grad_norm': 0.765625, 'learning_rate': 5.17596061799971e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1506, 'grad_norm': 0.8125, 'learning_rate': 5.174824380253325e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1429, 'grad_norm': 0.78125, 'learning_rate': 5.173688142506938e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1892, 'grad_norm': 0.609375, 'learning_rate': 5.172551904760552e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0665, 'grad_norm': 0.91015625, 'learning_rate': 5.171415667014167e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2848, 'grad_norm': 0.6484375, 'learning_rate': 5.17027942926778e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1314, 'grad_norm': 1.0234375, 'learning_rate': 5.169143191521394e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1729, 'grad_norm': 0.482421875, 'learning_rate': 5.168006953775008e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1655, 'grad_norm': 0.6484375, 'learning_rate': 5.166870716028622e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1155, 'grad_norm': 0.78125, 'learning_rate': 5.165734478282236e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3258, 'grad_norm': 0.58984375, 'learning_rate': 5.16459824053585e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0202, 'grad_norm': 0.6953125, 'learning_rate': 5.163462002789464e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2201, 'grad_norm': 0.4453125, 'learning_rate': 5.162325765043078e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2219, 'grad_norm': 0.59375, 'learning_rate': 5.161189527296692e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1373, 'grad_norm': 0.890625, 'learning_rate': 5.160053289550306e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3103, 'grad_norm': 0.48828125, 'learning_rate': 5.15891705180392e-05, 'epoch': 0.67}\n",
      "{'loss': 1.2558, 'grad_norm': 0.73046875, 'learning_rate': 5.157780814057534e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1341, 'grad_norm': 0.671875, 'learning_rate': 5.156644576311147e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3263, 'grad_norm': 1.0703125, 'learning_rate': 5.155508338564762e-05, 'epoch': 0.67}\n",
      "{'loss': 0.922, 'grad_norm': 0.703125, 'learning_rate': 5.154372100818375e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4248, 'grad_norm': 0.5390625, 'learning_rate': 5.153235863071989e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1729, 'grad_norm': 0.6875, 'learning_rate': 5.152099625325604e-05, 'epoch': 0.68}\n",
      "{'loss': 1.295, 'grad_norm': 0.55859375, 'learning_rate': 5.150963387579217e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2444, 'grad_norm': 0.70703125, 'learning_rate': 5.149827149832831e-05, 'epoch': 0.68}\n",
      "{'loss': 1.109, 'grad_norm': 0.78125, 'learning_rate': 5.1486909120864454e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2614, 'grad_norm': 0.6953125, 'learning_rate': 5.147554674340059e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0874, 'grad_norm': 0.8203125, 'learning_rate': 5.146418436593673e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1765, 'grad_norm': 0.5703125, 'learning_rate': 5.145282198847287e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1852, 'grad_norm': 0.87890625, 'learning_rate': 5.144145961100901e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0157, 'grad_norm': 0.9609375, 'learning_rate': 5.143009723354515e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4074, 'grad_norm': 0.68359375, 'learning_rate': 5.141873485608129e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1518, 'grad_norm': 0.765625, 'learning_rate': 5.140737247861743e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1793, 'grad_norm': 0.5625, 'learning_rate': 5.139601010115357e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1808, 'grad_norm': 0.71875, 'learning_rate': 5.138464772368971e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1419, 'grad_norm': 0.94140625, 'learning_rate': 5.137328534622584e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4542, 'grad_norm': 0.6328125, 'learning_rate': 5.136192296876199e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2168, 'grad_norm': 0.77734375, 'learning_rate': 5.135056059129812e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3035, 'grad_norm': 0.57421875, 'learning_rate': 5.133919821383426e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2063, 'grad_norm': 0.578125, 'learning_rate': 5.132783583637041e-05, 'epoch': 0.68}\n",
      "{'loss': 1.058, 'grad_norm': 0.59765625, 'learning_rate': 5.131647345890654e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2621, 'grad_norm': 0.953125, 'learning_rate': 5.130511108144268e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1483, 'grad_norm': 0.875, 'learning_rate': 5.1293748703978825e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1488, 'grad_norm': 0.427734375, 'learning_rate': 5.128238632651496e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3214, 'grad_norm': 0.67578125, 'learning_rate': 5.12710239490511e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0126, 'grad_norm': 0.84375, 'learning_rate': 5.125966157158724e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3493, 'grad_norm': 0.55078125, 'learning_rate': 5.124829919412338e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2426, 'grad_norm': 0.87109375, 'learning_rate': 5.123693681665952e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2708, 'grad_norm': 0.470703125, 'learning_rate': 5.122557443919566e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1875, 'grad_norm': 0.66796875, 'learning_rate': 5.12142120617318e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9951, 'grad_norm': 0.5703125, 'learning_rate': 5.120284968426794e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3508, 'grad_norm': 0.443359375, 'learning_rate': 5.119148730680408e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1547, 'grad_norm': 0.578125, 'learning_rate': 5.1180124929340223e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3358, 'grad_norm': 0.443359375, 'learning_rate': 5.116876255187636e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1796, 'grad_norm': 0.703125, 'learning_rate': 5.115740017441249e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0545, 'grad_norm': 0.58984375, 'learning_rate': 5.114603779694863e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2542, 'grad_norm': 0.5078125, 'learning_rate': 5.113467541948478e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1788, 'grad_norm': 0.79296875, 'learning_rate': 5.112331304202091e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1446, 'grad_norm': 0.61328125, 'learning_rate': 5.111195066455705e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1762, 'grad_norm': 0.65234375, 'learning_rate': 5.1100588287093195e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1558, 'grad_norm': 0.88671875, 'learning_rate': 5.108922590962933e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4676, 'grad_norm': 0.73828125, 'learning_rate': 5.107786353216547e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1464, 'grad_norm': 0.6484375, 'learning_rate': 5.106650115470161e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1692, 'grad_norm': 0.41796875, 'learning_rate': 5.105513877723775e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2786, 'grad_norm': 0.8984375, 'learning_rate': 5.104377639977389e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0479, 'grad_norm': 0.451171875, 'learning_rate': 5.103241402231003e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3011, 'grad_norm': 0.5078125, 'learning_rate': 5.1021051644846174e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1866, 'grad_norm': 0.94921875, 'learning_rate': 5.100968926738231e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1266, 'grad_norm': 0.515625, 'learning_rate': 5.099832688991845e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1187, 'grad_norm': 0.6484375, 'learning_rate': 5.0986964512454594e-05, 'epoch': 0.68}\n",
      "{'loss': 1.097, 'grad_norm': 1.0078125, 'learning_rate': 5.097560213499073e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2751, 'grad_norm': 0.57421875, 'learning_rate': 5.096423975752686e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1225, 'grad_norm': 0.61328125, 'learning_rate': 5.0952877380063e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0881, 'grad_norm': 0.35546875, 'learning_rate': 5.094151500259915e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2969, 'grad_norm': 0.703125, 'learning_rate': 5.093015262513528e-05, 'epoch': 0.68}\n",
      "{'loss': 1.179, 'grad_norm': 0.95703125, 'learning_rate': 5.091879024767142e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2602, 'grad_norm': 0.57421875, 'learning_rate': 5.0907427870207566e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3054, 'grad_norm': 0.92578125, 'learning_rate': 5.0896065492743704e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2443, 'grad_norm': 0.44140625, 'learning_rate': 5.088470311527984e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2787, 'grad_norm': 0.73046875, 'learning_rate': 5.087334073781598e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1396, 'grad_norm': 1.0078125, 'learning_rate': 5.0861978360352124e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3364, 'grad_norm': 1.0390625, 'learning_rate': 5.085061598288826e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0784, 'grad_norm': 0.8359375, 'learning_rate': 5.08392536054244e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2184, 'grad_norm': 0.38671875, 'learning_rate': 5.0827891227960544e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3516, 'grad_norm': 0.5703125, 'learning_rate': 5.081652885049668e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1008, 'grad_norm': 0.80078125, 'learning_rate': 5.080516647303282e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4255, 'grad_norm': 0.9609375, 'learning_rate': 5.0793804095568965e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2472, 'grad_norm': 1.078125, 'learning_rate': 5.07824417181051e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3198, 'grad_norm': 0.5859375, 'learning_rate': 5.0771079340641234e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2862, 'grad_norm': 0.62890625, 'learning_rate': 5.075971696317737e-05, 'epoch': 0.68}\n",
      "{'loss': 0.992, 'grad_norm': 0.474609375, 'learning_rate': 5.074835458571352e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3348, 'grad_norm': 0.54296875, 'learning_rate': 5.0736992208249654e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0866, 'grad_norm': 1.0078125, 'learning_rate': 5.072562983078579e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2811, 'grad_norm': 0.5390625, 'learning_rate': 5.0714267453321936e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2915, 'grad_norm': 0.66796875, 'learning_rate': 5.0702905075858074e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9617, 'grad_norm': 0.921875, 'learning_rate': 5.069154269839421e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2875, 'grad_norm': 0.546875, 'learning_rate': 5.068018032093036e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2063, 'grad_norm': 1.25, 'learning_rate': 5.0668817943466495e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1941, 'grad_norm': 0.52734375, 'learning_rate': 5.065745556600263e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1937, 'grad_norm': 0.61328125, 'learning_rate': 5.064609318853877e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2, 'grad_norm': 1.0859375, 'learning_rate': 5.0634730811074915e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3653, 'grad_norm': 0.44140625, 'learning_rate': 5.062336843361105e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1291, 'grad_norm': 0.74609375, 'learning_rate': 5.061200605614719e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2571, 'grad_norm': 0.58984375, 'learning_rate': 5.0600643678683335e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1122, 'grad_norm': 0.703125, 'learning_rate': 5.058928130121947e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1498, 'grad_norm': 0.65625, 'learning_rate': 5.0577918923755604e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4909, 'grad_norm': 0.54296875, 'learning_rate': 5.056655654629174e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1822, 'grad_norm': 1.4296875, 'learning_rate': 5.0555194168827893e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1021, 'grad_norm': 0.470703125, 'learning_rate': 5.0543831791364025e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2132, 'grad_norm': 0.55859375, 'learning_rate': 5.053246941390016e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1179, 'grad_norm': 1.0078125, 'learning_rate': 5.052110703643631e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2829, 'grad_norm': 0.46875, 'learning_rate': 5.0509744658972445e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0871, 'grad_norm': 0.88671875, 'learning_rate': 5.049838228150858e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2574, 'grad_norm': 0.50390625, 'learning_rate': 5.048701990404473e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2892, 'grad_norm': 0.58984375, 'learning_rate': 5.0475657526580865e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1064, 'grad_norm': 1.2890625, 'learning_rate': 5.0464295149117e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4513, 'grad_norm': 0.69921875, 'learning_rate': 5.045293277165314e-05, 'epoch': 0.68}\n",
      "{'loss': 1.061, 'grad_norm': 0.8984375, 'learning_rate': 5.0441570394189286e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2072, 'grad_norm': 0.54296875, 'learning_rate': 5.0430208016725423e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0607, 'grad_norm': 0.62109375, 'learning_rate': 5.041884563926156e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0843, 'grad_norm': 1.046875, 'learning_rate': 5.0407483261797706e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3319, 'grad_norm': 0.82421875, 'learning_rate': 5.0396120884333844e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1216, 'grad_norm': 0.65625, 'learning_rate': 5.0384758506869975e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2065, 'grad_norm': 0.58203125, 'learning_rate': 5.037339612940611e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1547, 'grad_norm': 0.72265625, 'learning_rate': 5.0362033751942264e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9925, 'grad_norm': 0.91796875, 'learning_rate': 5.0350671374478395e-05, 'epoch': 0.68}\n",
      "{'loss': 1.216, 'grad_norm': 0.515625, 'learning_rate': 5.033930899701453e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0662, 'grad_norm': 1.1640625, 'learning_rate': 5.032794661955068e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2121, 'grad_norm': 0.439453125, 'learning_rate': 5.0316584242086816e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2049, 'grad_norm': 0.7265625, 'learning_rate': 5.030522186462295e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0481, 'grad_norm': 0.984375, 'learning_rate': 5.02938594871591e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2708, 'grad_norm': 0.89453125, 'learning_rate': 5.0282497109695236e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1762, 'grad_norm': 1.2734375, 'learning_rate': 5.0271134732231374e-05, 'epoch': 0.68}\n",
      "{'loss': 1.083, 'grad_norm': 0.423828125, 'learning_rate': 5.025977235476751e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2709, 'grad_norm': 0.63671875, 'learning_rate': 5.0248409977303656e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0549, 'grad_norm': 0.51953125, 'learning_rate': 5.0237047599839794e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4138, 'grad_norm': 0.77734375, 'learning_rate': 5.022568522237593e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1966, 'grad_norm': 0.72265625, 'learning_rate': 5.0214322844912077e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1927, 'grad_norm': 0.404296875, 'learning_rate': 5.0202960467448214e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3204, 'grad_norm': 0.56640625, 'learning_rate': 5.0191598089984345e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1476, 'grad_norm': 0.67578125, 'learning_rate': 5.018023571252048e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2357, 'grad_norm': 0.58984375, 'learning_rate': 5.0168873335056635e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2564, 'grad_norm': 0.73828125, 'learning_rate': 5.0157510957592766e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2685, 'grad_norm': 0.48046875, 'learning_rate': 5.0146148580128904e-05, 'epoch': 0.68}\n",
      "{'loss': 1.198, 'grad_norm': 0.609375, 'learning_rate': 5.013478620266505e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0811, 'grad_norm': 0.6484375, 'learning_rate': 5.0123423825201186e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3017, 'grad_norm': 0.69921875, 'learning_rate': 5.0112061447737324e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1924, 'grad_norm': 0.73046875, 'learning_rate': 5.010069907027347e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1223, 'grad_norm': 0.439453125, 'learning_rate': 5.0089336692809606e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3267, 'grad_norm': 0.59375, 'learning_rate': 5.0077974315345744e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0497, 'grad_norm': 0.87109375, 'learning_rate': 5.006661193788188e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3983, 'grad_norm': 0.76171875, 'learning_rate': 5.005524956041803e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1666, 'grad_norm': 0.71875, 'learning_rate': 5.0043887182954165e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1088, 'grad_norm': 0.55078125, 'learning_rate': 5.00325248054903e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1762, 'grad_norm': 0.69140625, 'learning_rate': 5.002116242802645e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0771, 'grad_norm': 0.98828125, 'learning_rate': 5.0009800050562585e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3531, 'grad_norm': 0.76171875, 'learning_rate': 4.9998437673098716e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1927, 'grad_norm': 0.71875, 'learning_rate': 4.998707529563486e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9682, 'grad_norm': 0.55859375, 'learning_rate': 4.9975712918171005e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2392, 'grad_norm': 0.73828125, 'learning_rate': 4.9964350540707136e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1671, 'grad_norm': 0.6796875, 'learning_rate': 4.995298816324328e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3625, 'grad_norm': 0.69140625, 'learning_rate': 4.994162578577942e-05, 'epoch': 0.68}\n",
      "{'loss': 1.216, 'grad_norm': 0.87109375, 'learning_rate': 4.993026340831556e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2153, 'grad_norm': 0.4765625, 'learning_rate': 4.99189010308517e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2152, 'grad_norm': 0.8828125, 'learning_rate': 4.990753865338784e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0746, 'grad_norm': 1.3671875, 'learning_rate': 4.989617627592398e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3771, 'grad_norm': 0.59375, 'learning_rate': 4.9884813898460115e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2027, 'grad_norm': 0.7265625, 'learning_rate': 4.987345152099625e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0301, 'grad_norm': 0.462890625, 'learning_rate': 4.98620891435324e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3141, 'grad_norm': 0.60546875, 'learning_rate': 4.9850726766068535e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0475, 'grad_norm': 0.44921875, 'learning_rate': 4.983936438860467e-05, 'epoch': 0.68}\n",
      "{'loss': 1.251, 'grad_norm': 0.59765625, 'learning_rate': 4.982800201114081e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1359, 'grad_norm': 0.71875, 'learning_rate': 4.9816639633676956e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1132, 'grad_norm': 0.4453125, 'learning_rate': 4.980527725621309e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1419, 'grad_norm': 0.60546875, 'learning_rate': 4.979391487874923e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0279, 'grad_norm': 0.86328125, 'learning_rate': 4.9782552501285376e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4721, 'grad_norm': 0.62109375, 'learning_rate': 4.977119012382151e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1064, 'grad_norm': 0.7578125, 'learning_rate': 4.975982774635765e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1638, 'grad_norm': 0.52734375, 'learning_rate': 4.974846536889379e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2059, 'grad_norm': 0.62890625, 'learning_rate': 4.973710299142993e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0777, 'grad_norm': 1.25, 'learning_rate': 4.972574061396607e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2907, 'grad_norm': 0.66796875, 'learning_rate': 4.971437823650221e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1153, 'grad_norm': 0.71484375, 'learning_rate': 4.970301585903835e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1797, 'grad_norm': 0.51953125, 'learning_rate': 4.9691653481574486e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3198, 'grad_norm': 0.5390625, 'learning_rate': 4.9680291104110623e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0539, 'grad_norm': 0.70703125, 'learning_rate': 4.966892872664677e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3715, 'grad_norm': 0.5078125, 'learning_rate': 4.9657566349182906e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1212, 'grad_norm': 0.78125, 'learning_rate': 4.9646203971719044e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1303, 'grad_norm': 0.515625, 'learning_rate': 4.963484159425518e-05, 'epoch': 0.68}\n",
      "{'loss': 1.238, 'grad_norm': 0.8203125, 'learning_rate': 4.9623479216791326e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9827, 'grad_norm': 1.3671875, 'learning_rate': 4.9612116839327464e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3326, 'grad_norm': 0.52734375, 'learning_rate': 4.96007544618636e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1682, 'grad_norm': 0.80078125, 'learning_rate': 4.958939208439975e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1013, 'grad_norm': 0.5390625, 'learning_rate': 4.957802970693588e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2055, 'grad_norm': 0.58984375, 'learning_rate': 4.956666732947202e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0353, 'grad_norm': 0.96875, 'learning_rate': 4.955530495200816e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3413, 'grad_norm': 0.5703125, 'learning_rate': 4.95439425745443e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0913, 'grad_norm': 1.2109375, 'learning_rate': 4.953258019708044e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1883, 'grad_norm': 0.51171875, 'learning_rate': 4.952121781961658e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2419, 'grad_norm': 0.6328125, 'learning_rate': 4.950985544215272e-05, 'epoch': 0.68}\n",
      "{'loss': 1.082, 'grad_norm': 0.55859375, 'learning_rate': 4.9498493064688856e-05, 'epoch': 0.68}\n",
      "{'loss': 1.474, 'grad_norm': 0.61328125, 'learning_rate': 4.9487130687224994e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0866, 'grad_norm': 0.80859375, 'learning_rate': 4.947576830976114e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0381, 'grad_norm': 0.5234375, 'learning_rate': 4.9464405932297277e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2847, 'grad_norm': 0.90625, 'learning_rate': 4.9453043554833414e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1698, 'grad_norm': 1.109375, 'learning_rate': 4.944168117736955e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2377, 'grad_norm': 0.52734375, 'learning_rate': 4.94303187999057e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1376, 'grad_norm': 0.69140625, 'learning_rate': 4.9418956422441835e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2423, 'grad_norm': 0.5234375, 'learning_rate': 4.940759404497797e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3351, 'grad_norm': 0.55078125, 'learning_rate': 4.939623166751412e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9809, 'grad_norm': 0.87890625, 'learning_rate': 4.938486929005025e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2212, 'grad_norm': 0.474609375, 'learning_rate': 4.937350691258639e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0932, 'grad_norm': 0.43359375, 'learning_rate': 4.936214453512253e-05, 'epoch': 0.68}\n",
      "{'loss': 1.133, 'grad_norm': 0.423828125, 'learning_rate': 4.935078215765867e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1346, 'grad_norm': 0.60546875, 'learning_rate': 4.933941978019481e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0866, 'grad_norm': 1.0078125, 'learning_rate': 4.932805740273095e-05, 'epoch': 0.68}\n",
      "{'loss': 1.4186, 'grad_norm': 0.6484375, 'learning_rate': 4.931669502526709e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0629, 'grad_norm': 0.73828125, 'learning_rate': 4.930533264780323e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2435, 'grad_norm': 0.640625, 'learning_rate': 4.9293970270339365e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2055, 'grad_norm': 0.62890625, 'learning_rate': 4.928260789287551e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1711, 'grad_norm': 0.9609375, 'learning_rate': 4.927124551541165e-05, 'epoch': 0.68}\n",
      "{'loss': 1.2129, 'grad_norm': 0.734375, 'learning_rate': 4.9259883137947785e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1126, 'grad_norm': 0.59765625, 'learning_rate': 4.924852076048392e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1733, 'grad_norm': 0.69140625, 'learning_rate': 4.923715838302007e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1573, 'grad_norm': 0.78515625, 'learning_rate': 4.9225796005556205e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9603, 'grad_norm': 0.70703125, 'learning_rate': 4.921443362809234e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3993, 'grad_norm': 0.54296875, 'learning_rate': 4.920307125062849e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2412, 'grad_norm': 0.76953125, 'learning_rate': 4.919170887316462e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2455, 'grad_norm': 0.5078125, 'learning_rate': 4.9180346495700764e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2838, 'grad_norm': 0.5625, 'learning_rate': 4.916898411823691e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1806, 'grad_norm': 1.078125, 'learning_rate': 4.915762174077304e-05, 'epoch': 0.69}\n",
      "{'loss': 1.236, 'grad_norm': 0.6171875, 'learning_rate': 4.9146259363309184e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1159, 'grad_norm': 0.67578125, 'learning_rate': 4.913489698584532e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1467, 'grad_norm': 0.451171875, 'learning_rate': 4.912353460838146e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2723, 'grad_norm': 0.5703125, 'learning_rate': 4.91121722309176e-05, 'epoch': 0.69}\n",
      "{'loss': 1.148, 'grad_norm': 1.3359375, 'learning_rate': 4.9100809853453735e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2658, 'grad_norm': 0.71484375, 'learning_rate': 4.908944747598988e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1424, 'grad_norm': 0.8515625, 'learning_rate': 4.907808509852602e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1186, 'grad_norm': 0.4140625, 'learning_rate': 4.9066722721062156e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2326, 'grad_norm': 0.5703125, 'learning_rate': 4.9055360343598294e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0242, 'grad_norm': 1.2421875, 'learning_rate': 4.904399796613444e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3217, 'grad_norm': 0.5390625, 'learning_rate': 4.9032635588670576e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2351, 'grad_norm': 0.91796875, 'learning_rate': 4.9021273211206714e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1773, 'grad_norm': 0.490234375, 'learning_rate': 4.900991083374286e-05, 'epoch': 0.69}\n",
      "{'loss': 1.111, 'grad_norm': 0.72265625, 'learning_rate': 4.899854845627899e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1606, 'grad_norm': 0.8984375, 'learning_rate': 4.8987186078815134e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3656, 'grad_norm': 0.57421875, 'learning_rate': 4.897582370135128e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1742, 'grad_norm': 1.2890625, 'learning_rate': 4.896446132388741e-05, 'epoch': 0.69}\n",
      "{'loss': 1.276, 'grad_norm': 0.5, 'learning_rate': 4.8953098946423555e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1855, 'grad_norm': 0.65234375, 'learning_rate': 4.894173656895969e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0816, 'grad_norm': 0.97265625, 'learning_rate': 4.893037419149583e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3586, 'grad_norm': 0.58984375, 'learning_rate': 4.8919011814031975e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1759, 'grad_norm': 0.62890625, 'learning_rate': 4.8907649436568106e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1982, 'grad_norm': 0.5, 'learning_rate': 4.889628705910425e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1278, 'grad_norm': 0.68359375, 'learning_rate': 4.888492468164039e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9908, 'grad_norm': 0.859375, 'learning_rate': 4.8873562304176526e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3179, 'grad_norm': 0.53515625, 'learning_rate': 4.8862199926712664e-05, 'epoch': 0.69}\n",
      "{'loss': 1.135, 'grad_norm': 0.75, 'learning_rate': 4.885083754924881e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1247, 'grad_norm': 0.51171875, 'learning_rate': 4.883947517178495e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2628, 'grad_norm': 0.54296875, 'learning_rate': 4.8828112794321085e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0615, 'grad_norm': 0.7421875, 'learning_rate': 4.881675041685723e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2869, 'grad_norm': 0.65625, 'learning_rate': 4.880538803939336e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1535, 'grad_norm': 0.71484375, 'learning_rate': 4.8794025661929505e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3022, 'grad_norm': 0.49609375, 'learning_rate': 4.878266328446565e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2761, 'grad_norm': 0.6015625, 'learning_rate': 4.877130090700178e-05, 'epoch': 0.69}\n",
      "{'loss': 1.049, 'grad_norm': 0.87890625, 'learning_rate': 4.8759938529537925e-05, 'epoch': 0.69}\n",
      "{'loss': 1.4215, 'grad_norm': 0.5703125, 'learning_rate': 4.874857615207406e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1527, 'grad_norm': 0.78125, 'learning_rate': 4.87372137746102e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2515, 'grad_norm': 0.412109375, 'learning_rate': 4.8725851397146346e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2069, 'grad_norm': 0.6875, 'learning_rate': 4.8714489019682477e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0241, 'grad_norm': 1.109375, 'learning_rate': 4.870312664221862e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3912, 'grad_norm': 0.6484375, 'learning_rate': 4.869176426475476e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2044, 'grad_norm': 0.578125, 'learning_rate': 4.86804018872909e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2255, 'grad_norm': 0.52734375, 'learning_rate': 4.866903950982704e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2506, 'grad_norm': 0.70703125, 'learning_rate': 4.865767713236318e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2528, 'grad_norm': 0.62109375, 'learning_rate': 4.864631475489932e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3831, 'grad_norm': 0.78125, 'learning_rate': 4.8634952377435455e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1443, 'grad_norm': 0.7890625, 'learning_rate': 4.86235899999716e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2486, 'grad_norm': 0.54296875, 'learning_rate': 4.861222762250773e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2478, 'grad_norm': 0.98828125, 'learning_rate': 4.8600865245043875e-05, 'epoch': 0.69}\n",
      "{'loss': 1.063, 'grad_norm': 0.89453125, 'learning_rate': 4.858950286758002e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2822, 'grad_norm': 0.53125, 'learning_rate': 4.857814049011615e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9862, 'grad_norm': 0.75390625, 'learning_rate': 4.8566778112652296e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2249, 'grad_norm': 0.58984375, 'learning_rate': 4.8555415735188434e-05, 'epoch': 0.69}\n",
      "{'loss': 1.173, 'grad_norm': 0.62890625, 'learning_rate': 4.854405335772457e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0587, 'grad_norm': 1.0, 'learning_rate': 4.8532690980260716e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2788, 'grad_norm': 0.54296875, 'learning_rate': 4.852132860279685e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1971, 'grad_norm': 0.83984375, 'learning_rate': 4.850996622533299e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2393, 'grad_norm': 0.42578125, 'learning_rate': 4.849860384786913e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2757, 'grad_norm': 0.58203125, 'learning_rate': 4.848724147040527e-05, 'epoch': 0.69}\n",
      "{'loss': 1.127, 'grad_norm': 0.56640625, 'learning_rate': 4.847587909294141e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3722, 'grad_norm': 0.59375, 'learning_rate': 4.846451671547755e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1353, 'grad_norm': 0.91796875, 'learning_rate': 4.845315433801369e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1049, 'grad_norm': 0.5859375, 'learning_rate': 4.8441791960549826e-05, 'epoch': 0.69}\n",
      "{'loss': 1.05, 'grad_norm': 0.65234375, 'learning_rate': 4.843042958308597e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1904, 'grad_norm': 0.75, 'learning_rate': 4.84190672056221e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1594, 'grad_norm': 0.72265625, 'learning_rate': 4.8407704828158246e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2873, 'grad_norm': 0.57421875, 'learning_rate': 4.839634245069439e-05, 'epoch': 0.69}\n",
      "{'loss': 1.209, 'grad_norm': 0.486328125, 'learning_rate': 4.838498007323052e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2631, 'grad_norm': 0.609375, 'learning_rate': 4.8373617695766666e-05, 'epoch': 0.69}\n",
      "{'loss': 1.088, 'grad_norm': 1.953125, 'learning_rate': 4.8362255318302804e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1575, 'grad_norm': 0.71875, 'learning_rate': 4.835089294083894e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1027, 'grad_norm': 0.78125, 'learning_rate': 4.833953056337509e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1245, 'grad_norm': 0.5859375, 'learning_rate': 4.832816818591122e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1941, 'grad_norm': 0.75390625, 'learning_rate': 4.831680580844736e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0676, 'grad_norm': 0.71875, 'learning_rate': 4.83054434309835e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3516, 'grad_norm': 0.546875, 'learning_rate': 4.829408105351964e-05, 'epoch': 0.69}\n",
      "{'loss': 1.185, 'grad_norm': 0.62890625, 'learning_rate': 4.828271867605578e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2242, 'grad_norm': 0.6015625, 'learning_rate': 4.827135629859192e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2196, 'grad_norm': 0.55859375, 'learning_rate': 4.825999392112806e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9984, 'grad_norm': 1.25, 'learning_rate': 4.8248631543664196e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3554, 'grad_norm': 0.50390625, 'learning_rate': 4.823726916620034e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1836, 'grad_norm': 0.7421875, 'learning_rate': 4.822590678873648e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2269, 'grad_norm': 0.42578125, 'learning_rate': 4.821454441127262e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3166, 'grad_norm': 0.53125, 'learning_rate': 4.820318203380876e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0386, 'grad_norm': 0.4921875, 'learning_rate': 4.819181965634489e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2669, 'grad_norm': 0.53125, 'learning_rate': 4.818045727888104e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1752, 'grad_norm': 0.97265625, 'learning_rate': 4.8169094901417175e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2352, 'grad_norm': 0.4453125, 'learning_rate': 4.815773252395331e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1604, 'grad_norm': 0.62890625, 'learning_rate': 4.814637014648946e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9935, 'grad_norm': 0.90234375, 'learning_rate': 4.813500776902559e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1649, 'grad_norm': 0.5078125, 'learning_rate': 4.812364539156173e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1814, 'grad_norm': 0.8984375, 'learning_rate': 4.811228301409787e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2029, 'grad_norm': 0.4375, 'learning_rate': 4.810092063663401e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2105, 'grad_norm': 0.69921875, 'learning_rate': 4.8089558259170153e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1611, 'grad_norm': 0.90625, 'learning_rate': 4.807819588170629e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2368, 'grad_norm': 0.5625, 'learning_rate': 4.806683350424243e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0832, 'grad_norm': 0.73046875, 'learning_rate': 4.805547112677857e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2612, 'grad_norm': 0.474609375, 'learning_rate': 4.804410874931471e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3412, 'grad_norm': 0.69140625, 'learning_rate': 4.803274637185085e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1004, 'grad_norm': 1.15625, 'learning_rate': 4.802138399438699e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3101, 'grad_norm': 0.73828125, 'learning_rate': 4.801002161692313e-05, 'epoch': 0.69}\n",
      "{'loss': 1.206, 'grad_norm': 0.51953125, 'learning_rate': 4.799865923945926e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2953, 'grad_norm': 0.5390625, 'learning_rate': 4.798729686199541e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1347, 'grad_norm': 0.89453125, 'learning_rate': 4.7975934484531546e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0885, 'grad_norm': 0.9296875, 'learning_rate': 4.7964572107067683e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3071, 'grad_norm': 0.5078125, 'learning_rate': 4.795320972960383e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2029, 'grad_norm': 0.9453125, 'learning_rate': 4.794184735213996e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2054, 'grad_norm': 0.48828125, 'learning_rate': 4.7930484974676104e-05, 'epoch': 0.69}\n",
      "{'loss': 1.117, 'grad_norm': 0.61328125, 'learning_rate': 4.791912259721224e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0181, 'grad_norm': 1.015625, 'learning_rate': 4.790776021974838e-05, 'epoch': 0.69}\n",
      "{'loss': 1.325, 'grad_norm': 0.55078125, 'learning_rate': 4.7896397842284524e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2668, 'grad_norm': 0.76953125, 'learning_rate': 4.788503546482066e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2892, 'grad_norm': 0.51953125, 'learning_rate': 4.78736730873568e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1972, 'grad_norm': 0.62109375, 'learning_rate': 4.786231070989294e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0391, 'grad_norm': 0.94921875, 'learning_rate': 4.785094833242908e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2725, 'grad_norm': 0.5703125, 'learning_rate': 4.783958595496522e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1859, 'grad_norm': 0.70703125, 'learning_rate': 4.782822357750136e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1091, 'grad_norm': 0.640625, 'learning_rate': 4.78168612000375e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2251, 'grad_norm': 0.6875, 'learning_rate': 4.7805498822573634e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0506, 'grad_norm': 0.54296875, 'learning_rate': 4.779413644510978e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1908, 'grad_norm': 0.671875, 'learning_rate': 4.7782774067645916e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2202, 'grad_norm': 0.8515625, 'learning_rate': 4.7771411690182054e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2031, 'grad_norm': 0.5078125, 'learning_rate': 4.77600493127182e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2221, 'grad_norm': 0.5546875, 'learning_rate': 4.774868693525433e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0288, 'grad_norm': 0.78515625, 'learning_rate': 4.7737324557790474e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3499, 'grad_norm': 0.498046875, 'learning_rate': 4.772596218032662e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1904, 'grad_norm': 0.75390625, 'learning_rate': 4.771459980286275e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1856, 'grad_norm': 0.53515625, 'learning_rate': 4.7703237425398895e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2185, 'grad_norm': 0.7734375, 'learning_rate': 4.769187504793503e-05, 'epoch': 0.69}\n",
      "{'loss': 1.09, 'grad_norm': 0.91796875, 'learning_rate': 4.768051267047117e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3105, 'grad_norm': 0.578125, 'learning_rate': 4.766915029300731e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1486, 'grad_norm': 0.82421875, 'learning_rate': 4.765778791554345e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2425, 'grad_norm': 0.71875, 'learning_rate': 4.764642553807959e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2712, 'grad_norm': 0.6484375, 'learning_rate': 4.763506316061573e-05, 'epoch': 0.69}\n",
      "{'loss': 1.089, 'grad_norm': 0.92578125, 'learning_rate': 4.762370078315187e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2567, 'grad_norm': 0.57421875, 'learning_rate': 4.7612338405688004e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1442, 'grad_norm': 0.8671875, 'learning_rate': 4.760097602822415e-05, 'epoch': 0.69}\n",
      "{'loss': 1.137, 'grad_norm': 0.4609375, 'learning_rate': 4.758961365076029e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1522, 'grad_norm': 0.6796875, 'learning_rate': 4.7578251273296425e-05, 'epoch': 0.69}\n",
      "{'loss': 1.106, 'grad_norm': 0.80078125, 'learning_rate': 4.756688889583257e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3446, 'grad_norm': 0.6171875, 'learning_rate': 4.75555265183687e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2108, 'grad_norm': 0.63671875, 'learning_rate': 4.7544164140904845e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2059, 'grad_norm': 0.4375, 'learning_rate': 4.753280176344099e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3337, 'grad_norm': 0.5859375, 'learning_rate': 4.752143938597712e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0527, 'grad_norm': 1.328125, 'learning_rate': 4.7510077008513265e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3965, 'grad_norm': 0.69140625, 'learning_rate': 4.74987146310494e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0928, 'grad_norm': 0.75390625, 'learning_rate': 4.748735225358554e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2524, 'grad_norm': 0.439453125, 'learning_rate': 4.747598987612168e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1274, 'grad_norm': 0.546875, 'learning_rate': 4.7464627498657824e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9805, 'grad_norm': 0.78515625, 'learning_rate': 4.745326512119396e-05, 'epoch': 0.69}\n",
      "{'loss': 1.4999, 'grad_norm': 0.51171875, 'learning_rate': 4.74419027437301e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1082, 'grad_norm': 0.59765625, 'learning_rate': 4.7430540366266244e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1036, 'grad_norm': 0.51171875, 'learning_rate': 4.7419177988802375e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2293, 'grad_norm': 0.72265625, 'learning_rate': 4.740781561133852e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0563, 'grad_norm': 0.80078125, 'learning_rate': 4.739645323387466e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4079, 'grad_norm': 0.95703125, 'learning_rate': 4.7385090856410795e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1298, 'grad_norm': 0.640625, 'learning_rate': 4.737372847894694e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2583, 'grad_norm': 0.5390625, 'learning_rate': 4.736236610148308e-05, 'epoch': 0.69}\n",
      "{'loss': 1.126, 'grad_norm': 0.8515625, 'learning_rate': 4.7351003724019216e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0729, 'grad_norm': 0.98046875, 'learning_rate': 4.733964134655536e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2854, 'grad_norm': 0.5703125, 'learning_rate': 4.732827896909149e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1818, 'grad_norm': 0.69140625, 'learning_rate': 4.7316916591627636e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0976, 'grad_norm': 0.546875, 'learning_rate': 4.7305554214163774e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2833, 'grad_norm': 0.53515625, 'learning_rate': 4.729419183669991e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9655, 'grad_norm': 0.4765625, 'learning_rate': 4.7282829459236056e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3193, 'grad_norm': 0.6640625, 'learning_rate': 4.7271467081772194e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0528, 'grad_norm': 0.71875, 'learning_rate': 4.726010470430833e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3021, 'grad_norm': 0.451171875, 'learning_rate': 4.724874232684447e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2253, 'grad_norm': 0.80078125, 'learning_rate': 4.7237379949380615e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0991, 'grad_norm': 1.0390625, 'learning_rate': 4.7226017571916746e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2367, 'grad_norm': 0.6171875, 'learning_rate': 4.721465519445289e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2463, 'grad_norm': 0.73828125, 'learning_rate': 4.720329281698903e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0853, 'grad_norm': 0.470703125, 'learning_rate': 4.7191930439525166e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0838, 'grad_norm': 0.54296875, 'learning_rate': 4.718056806206131e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0999, 'grad_norm': 0.90234375, 'learning_rate': 4.716920568459745e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3553, 'grad_norm': 0.78125, 'learning_rate': 4.7157843307133586e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0776, 'grad_norm': 0.8359375, 'learning_rate': 4.714648092966973e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1292, 'grad_norm': 0.60546875, 'learning_rate': 4.713511855220586e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2187, 'grad_norm': 0.64453125, 'learning_rate': 4.7123756174742007e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0932, 'grad_norm': 1.125, 'learning_rate': 4.7112393797278144e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3441, 'grad_norm': 0.46484375, 'learning_rate': 4.710103141981428e-05, 'epoch': 0.69}\n",
      "{'loss': 1.25, 'grad_norm': 0.64453125, 'learning_rate': 4.708966904235043e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2176, 'grad_norm': 0.54296875, 'learning_rate': 4.7078306664886565e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3044, 'grad_norm': 0.828125, 'learning_rate': 4.70669442874227e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9936, 'grad_norm': 1.0078125, 'learning_rate': 4.705558190995884e-05, 'epoch': 0.69}\n",
      "{'loss': 1.336, 'grad_norm': 0.51953125, 'learning_rate': 4.7044219532494985e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2399, 'grad_norm': 0.5546875, 'learning_rate': 4.703285715503112e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2314, 'grad_norm': 0.50390625, 'learning_rate': 4.702149477756726e-05, 'epoch': 0.69}\n",
      "{'loss': 1.2032, 'grad_norm': 0.609375, 'learning_rate': 4.70101324001034e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0581, 'grad_norm': 0.98828125, 'learning_rate': 4.6998770022639537e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3796, 'grad_norm': 0.65625, 'learning_rate': 4.698740764517568e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1746, 'grad_norm': 0.76953125, 'learning_rate': 4.697604526771182e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3921, 'grad_norm': 0.48046875, 'learning_rate': 4.696468289024796e-05, 'epoch': 0.69}\n",
      "{'loss': 1.3166, 'grad_norm': 0.55859375, 'learning_rate': 4.69533205127841e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1289, 'grad_norm': 0.93359375, 'learning_rate': 4.694195813532023e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3423, 'grad_norm': 0.65234375, 'learning_rate': 4.693059575785638e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2, 'grad_norm': 0.79296875, 'learning_rate': 4.6919233380392515e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2178, 'grad_norm': 0.498046875, 'learning_rate': 4.690787100292865e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1974, 'grad_norm': 0.5625, 'learning_rate': 4.68965086254648e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0153, 'grad_norm': 0.51953125, 'learning_rate': 4.6885146248000935e-05, 'epoch': 0.7}\n",
      "{'loss': 1.269, 'grad_norm': 0.6796875, 'learning_rate': 4.687378387053707e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9952, 'grad_norm': 0.890625, 'learning_rate': 4.686242149307321e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2337, 'grad_norm': 0.63671875, 'learning_rate': 4.6851059115609356e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1465, 'grad_norm': 0.6171875, 'learning_rate': 4.6839696738145494e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0548, 'grad_norm': 0.7109375, 'learning_rate': 4.682833436068163e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3341, 'grad_norm': 0.53125, 'learning_rate': 4.681697198321777e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1089, 'grad_norm': 0.609375, 'learning_rate': 4.680560960575391e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2226, 'grad_norm': 0.5078125, 'learning_rate': 4.679424722829005e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1177, 'grad_norm': 0.68359375, 'learning_rate': 4.678288485082619e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1138, 'grad_norm': 1.0078125, 'learning_rate': 4.677152247336233e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1989, 'grad_norm': 0.5234375, 'learning_rate': 4.676016009589847e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1625, 'grad_norm': 0.68359375, 'learning_rate': 4.67487977184346e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2176, 'grad_norm': 0.7109375, 'learning_rate': 4.673743534097075e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1738, 'grad_norm': 0.76171875, 'learning_rate': 4.6726072963506886e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0485, 'grad_norm': 1.15625, 'learning_rate': 4.6714710586043024e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3051, 'grad_norm': 0.51953125, 'learning_rate': 4.670334820857917e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0148, 'grad_norm': 0.6953125, 'learning_rate': 4.6691985831115306e-05, 'epoch': 0.7}\n",
      "{'loss': 1.301, 'grad_norm': 0.5859375, 'learning_rate': 4.6680623453651444e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1387, 'grad_norm': 0.58984375, 'learning_rate': 4.666926107618758e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0583, 'grad_norm': 1.0234375, 'learning_rate': 4.6657898698723726e-05, 'epoch': 0.7}\n",
      "{'loss': 1.4112, 'grad_norm': 0.6328125, 'learning_rate': 4.6646536321259864e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2758, 'grad_norm': 0.7421875, 'learning_rate': 4.6635173943796e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1865, 'grad_norm': 0.60546875, 'learning_rate': 4.662381156633214e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1664, 'grad_norm': 0.75390625, 'learning_rate': 4.661244918886828e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1053, 'grad_norm': 1.21875, 'learning_rate': 4.660108681140442e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3843, 'grad_norm': 0.484375, 'learning_rate': 4.658972443394056e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2512, 'grad_norm': 0.8203125, 'learning_rate': 4.65783620564767e-05, 'epoch': 0.7}\n",
      "{'loss': 1.172, 'grad_norm': 0.5078125, 'learning_rate': 4.656699967901284e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1823, 'grad_norm': 0.63671875, 'learning_rate': 4.6555637301548974e-05, 'epoch': 0.7}\n",
      "{'loss': 1.016, 'grad_norm': 0.5625, 'learning_rate': 4.654427492408512e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2572, 'grad_norm': 0.8046875, 'learning_rate': 4.6532912546621256e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0089, 'grad_norm': 0.66015625, 'learning_rate': 4.6521550169157394e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2785, 'grad_norm': 0.453125, 'learning_rate': 4.651018779169354e-05, 'epoch': 0.7}\n",
      "{'loss': 1.218, 'grad_norm': 0.64453125, 'learning_rate': 4.649882541422968e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1784, 'grad_norm': 0.74609375, 'learning_rate': 4.6487463036765815e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3752, 'grad_norm': 0.6484375, 'learning_rate': 4.647610065930195e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2171, 'grad_norm': 0.8125, 'learning_rate': 4.64647382818381e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0998, 'grad_norm': 0.466796875, 'learning_rate': 4.6453375904374235e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1068, 'grad_norm': 0.60546875, 'learning_rate': 4.644201352691037e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0529, 'grad_norm': 1.6640625, 'learning_rate': 4.643065114944651e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2597, 'grad_norm': 0.53515625, 'learning_rate': 4.641928877198265e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2148, 'grad_norm': 1.421875, 'learning_rate': 4.640792639451879e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3005, 'grad_norm': 0.55859375, 'learning_rate': 4.639656401705493e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2468, 'grad_norm': 0.52734375, 'learning_rate': 4.638520163959107e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0766, 'grad_norm': 0.80078125, 'learning_rate': 4.6373839262127213e-05, 'epoch': 0.7}\n",
      "{'loss': 1.4501, 'grad_norm': 0.48828125, 'learning_rate': 4.6362476884663344e-05, 'epoch': 0.7}\n",
      "{'loss': 1.279, 'grad_norm': 1.015625, 'learning_rate': 4.635111450719949e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2163, 'grad_norm': 0.435546875, 'learning_rate': 4.6339752129735634e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2388, 'grad_norm': 0.58984375, 'learning_rate': 4.6328389752271765e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9849, 'grad_norm': 0.875, 'learning_rate': 4.631702737480791e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2, 'grad_norm': 0.53515625, 'learning_rate': 4.630566499734405e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2046, 'grad_norm': 0.8046875, 'learning_rate': 4.6294302619880185e-05, 'epoch': 0.7}\n",
      "{'loss': 1.093, 'grad_norm': 0.470703125, 'learning_rate': 4.628294024241632e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1536, 'grad_norm': 0.65234375, 'learning_rate': 4.627157786495247e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0987, 'grad_norm': 1.8828125, 'learning_rate': 4.6260215487488606e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2627, 'grad_norm': 0.68359375, 'learning_rate': 4.624885311002474e-05, 'epoch': 0.7}\n",
      "{'loss': 1.228, 'grad_norm': 0.75390625, 'learning_rate': 4.623749073256088e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1722, 'grad_norm': 0.435546875, 'learning_rate': 4.622612835509702e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2337, 'grad_norm': 0.73046875, 'learning_rate': 4.6214765977633164e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1302, 'grad_norm': 0.94140625, 'learning_rate': 4.62034036001693e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3412, 'grad_norm': 0.703125, 'learning_rate': 4.619204122270544e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1681, 'grad_norm': 0.85546875, 'learning_rate': 4.6180678845241584e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2759, 'grad_norm': 0.4765625, 'learning_rate': 4.6169316467777715e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3347, 'grad_norm': 0.51171875, 'learning_rate': 4.615795409031386e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0589, 'grad_norm': 1.28125, 'learning_rate': 4.6146591712850004e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3743, 'grad_norm': 0.59375, 'learning_rate': 4.6135229335386135e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0952, 'grad_norm': 0.80859375, 'learning_rate': 4.612386695792228e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2595, 'grad_norm': 0.47265625, 'learning_rate': 4.611250458045842e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1063, 'grad_norm': 0.5703125, 'learning_rate': 4.6101142202994556e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9961, 'grad_norm': 0.75390625, 'learning_rate': 4.60897798255307e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3635, 'grad_norm': 0.59375, 'learning_rate': 4.607841744806684e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0723, 'grad_norm': 0.59765625, 'learning_rate': 4.6067055070602976e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1969, 'grad_norm': 0.58203125, 'learning_rate': 4.6055692693139114e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1899, 'grad_norm': 0.69140625, 'learning_rate': 4.604433031567525e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0708, 'grad_norm': 0.8828125, 'learning_rate': 4.603296793821139e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3026, 'grad_norm': 0.5703125, 'learning_rate': 4.6021605560747534e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0713, 'grad_norm': 0.69921875, 'learning_rate': 4.601024318328367e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1209, 'grad_norm': 0.494140625, 'learning_rate': 4.599888080581981e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2579, 'grad_norm': 0.5234375, 'learning_rate': 4.5987518428355955e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0684, 'grad_norm': 1.09375, 'learning_rate': 4.5976156050892086e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3187, 'grad_norm': 0.58203125, 'learning_rate': 4.596479367342823e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1116, 'grad_norm': 0.9140625, 'learning_rate': 4.5953431295964375e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2116, 'grad_norm': 0.453125, 'learning_rate': 4.5942068918500506e-05, 'epoch': 0.7}\n",
      "{'loss': 1.29, 'grad_norm': 0.640625, 'learning_rate': 4.593070654103665e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9906, 'grad_norm': 0.8984375, 'learning_rate': 4.591934416357279e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3689, 'grad_norm': 0.53515625, 'learning_rate': 4.5907981786108926e-05, 'epoch': 0.7}\n",
      "{'loss': 1.177, 'grad_norm': 0.7265625, 'learning_rate': 4.589661940864507e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1251, 'grad_norm': 0.58203125, 'learning_rate': 4.588525703118121e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2394, 'grad_norm': 0.61328125, 'learning_rate': 4.587389465371735e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0141, 'grad_norm': 0.83984375, 'learning_rate': 4.5862532276253485e-05, 'epoch': 0.7}\n",
      "{'loss': 1.4305, 'grad_norm': 0.54296875, 'learning_rate': 4.585116989878962e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2334, 'grad_norm': 0.75, 'learning_rate': 4.583980752132577e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2421, 'grad_norm': 0.44140625, 'learning_rate': 4.5828445143861905e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3077, 'grad_norm': 0.71875, 'learning_rate': 4.581708276639804e-05, 'epoch': 0.7}\n",
      "{'loss': 1.046, 'grad_norm': 0.5390625, 'learning_rate': 4.580572038893418e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3802, 'grad_norm': 0.451171875, 'learning_rate': 4.5794358011470325e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0989, 'grad_norm': 0.7109375, 'learning_rate': 4.5782995634006456e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3231, 'grad_norm': 0.48046875, 'learning_rate': 4.57716332565426e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2956, 'grad_norm': 0.67578125, 'learning_rate': 4.5760270879078746e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0151, 'grad_norm': 1.1328125, 'learning_rate': 4.574890850161488e-05, 'epoch': 0.7}\n",
      "{'loss': 1.4303, 'grad_norm': 0.59765625, 'learning_rate': 4.573754612415102e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1079, 'grad_norm': 0.70703125, 'learning_rate': 4.572618374668716e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1834, 'grad_norm': 0.451171875, 'learning_rate': 4.57148213692233e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1794, 'grad_norm': 0.52734375, 'learning_rate': 4.570345899175944e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1024, 'grad_norm': 0.5625, 'learning_rate': 4.569209661429558e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2457, 'grad_norm': 0.451171875, 'learning_rate': 4.568073423683172e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1239, 'grad_norm': 0.76953125, 'learning_rate': 4.5669371859367855e-05, 'epoch': 0.7}\n",
      "{'loss': 1.166, 'grad_norm': 0.48046875, 'learning_rate': 4.565800948190399e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2807, 'grad_norm': 0.5546875, 'learning_rate': 4.564664710444014e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0521, 'grad_norm': 1.0234375, 'learning_rate': 4.5635284726976276e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3143, 'grad_norm': 0.41796875, 'learning_rate': 4.5623922349512413e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1705, 'grad_norm': 0.66796875, 'learning_rate': 4.561255997204855e-05, 'epoch': 0.7}\n",
      "{'loss': 1.269, 'grad_norm': 0.4375, 'learning_rate': 4.5601197594584696e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2873, 'grad_norm': 0.55078125, 'learning_rate': 4.558983521712083e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0644, 'grad_norm': 0.73828125, 'learning_rate': 4.557847283965697e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2779, 'grad_norm': 0.62109375, 'learning_rate': 4.5567110462193116e-05, 'epoch': 0.7}\n",
      "{'loss': 1.257, 'grad_norm': 0.76171875, 'learning_rate': 4.555574808472925e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1641, 'grad_norm': 0.54296875, 'learning_rate': 4.554438570726539e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2638, 'grad_norm': 0.4921875, 'learning_rate': 4.553302332980153e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1398, 'grad_norm': 0.890625, 'learning_rate': 4.552166095233767e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2792, 'grad_norm': 0.494140625, 'learning_rate': 4.551029857487381e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0538, 'grad_norm': 0.7109375, 'learning_rate': 4.549893619740995e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2645, 'grad_norm': 0.578125, 'learning_rate': 4.548757381994609e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1826, 'grad_norm': 0.5234375, 'learning_rate': 4.5476211442482226e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9838, 'grad_norm': 0.57421875, 'learning_rate': 4.5464849065018364e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2686, 'grad_norm': 0.69140625, 'learning_rate': 4.545348668755451e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2282, 'grad_norm': 0.7109375, 'learning_rate': 4.5442124310090646e-05, 'epoch': 0.7}\n",
      "{'loss': 1.079, 'grad_norm': 0.6328125, 'learning_rate': 4.5430761932626784e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1953, 'grad_norm': 0.6484375, 'learning_rate': 4.541939955516292e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0647, 'grad_norm': 1.484375, 'learning_rate': 4.5408037177699067e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3281, 'grad_norm': 0.60546875, 'learning_rate': 4.5396674800235204e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1039, 'grad_norm': 0.91796875, 'learning_rate': 4.538531242277134e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2015, 'grad_norm': 0.54296875, 'learning_rate': 4.537395004530749e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2526, 'grad_norm': 0.625, 'learning_rate': 4.536258766784362e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0672, 'grad_norm': 1.8203125, 'learning_rate': 4.535122529037976e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2972, 'grad_norm': 0.48046875, 'learning_rate': 4.53398629129159e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1497, 'grad_norm': 0.6953125, 'learning_rate': 4.532850053545204e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0711, 'grad_norm': 0.470703125, 'learning_rate': 4.531713815798818e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1732, 'grad_norm': 0.72265625, 'learning_rate': 4.530577578052432e-05, 'epoch': 0.7}\n",
      "{'loss': 0.96, 'grad_norm': 0.921875, 'learning_rate': 4.529441340306046e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3089, 'grad_norm': 0.62890625, 'learning_rate': 4.5283051025596597e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1337, 'grad_norm': 0.77734375, 'learning_rate': 4.5271688648132734e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1861, 'grad_norm': 0.4921875, 'learning_rate': 4.526032627066888e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1787, 'grad_norm': 0.59375, 'learning_rate': 4.524896389320502e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0592, 'grad_norm': 1.4140625, 'learning_rate': 4.5237601515741155e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2831, 'grad_norm': 0.69921875, 'learning_rate': 4.522623913827729e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2393, 'grad_norm': 0.88671875, 'learning_rate': 4.521487676081344e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2292, 'grad_norm': 0.546875, 'learning_rate': 4.5203514383349575e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1988, 'grad_norm': 0.57421875, 'learning_rate': 4.519215200588571e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9731, 'grad_norm': 0.828125, 'learning_rate': 4.518078962842186e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2369, 'grad_norm': 0.56640625, 'learning_rate': 4.516942725095799e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0702, 'grad_norm': 0.890625, 'learning_rate': 4.515806487349413e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1802, 'grad_norm': 0.470703125, 'learning_rate': 4.514670249603027e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2561, 'grad_norm': 0.74609375, 'learning_rate': 4.513534011856641e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9998, 'grad_norm': 0.96875, 'learning_rate': 4.5123977741102554e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2854, 'grad_norm': 0.57421875, 'learning_rate': 4.511261536363869e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0985, 'grad_norm': 1.1328125, 'learning_rate': 4.510125298617483e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1366, 'grad_norm': 0.609375, 'learning_rate': 4.508989060871097e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2696, 'grad_norm': 0.97265625, 'learning_rate': 4.5078528231247105e-05, 'epoch': 0.7}\n",
      "{'loss': 1.031, 'grad_norm': 0.6328125, 'learning_rate': 4.506716585378325e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3329, 'grad_norm': 0.6015625, 'learning_rate': 4.505580347631939e-05, 'epoch': 0.7}\n",
      "{'loss': 1.105, 'grad_norm': 0.63671875, 'learning_rate': 4.5044441098855525e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2727, 'grad_norm': 0.5, 'learning_rate': 4.503307872139166e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2498, 'grad_norm': 0.61328125, 'learning_rate': 4.502171634392781e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1337, 'grad_norm': 1.375, 'learning_rate': 4.5010353966463946e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2412, 'grad_norm': 0.80078125, 'learning_rate': 4.4998991589000084e-05, 'epoch': 0.7}\n",
      "{'loss': 1.142, 'grad_norm': 0.84765625, 'learning_rate': 4.498762921153623e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1673, 'grad_norm': 0.60546875, 'learning_rate': 4.497626683407236e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2019, 'grad_norm': 0.8828125, 'learning_rate': 4.4964904456608504e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0034, 'grad_norm': 1.171875, 'learning_rate': 4.495354207914464e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3913, 'grad_norm': 0.51171875, 'learning_rate': 4.494217970168078e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1642, 'grad_norm': 0.6640625, 'learning_rate': 4.4930817324216924e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1597, 'grad_norm': 0.54296875, 'learning_rate': 4.491945494675306e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1888, 'grad_norm': 0.5859375, 'learning_rate': 4.49080925692892e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0552, 'grad_norm': 0.5546875, 'learning_rate': 4.4896730191825345e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1851, 'grad_norm': 0.50390625, 'learning_rate': 4.4885367814361476e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1573, 'grad_norm': 0.7734375, 'learning_rate': 4.487400543689762e-05, 'epoch': 0.7}\n",
      "{'loss': 1.167, 'grad_norm': 0.4375, 'learning_rate': 4.486264305943376e-05, 'epoch': 0.7}\n",
      "{'loss': 1.196, 'grad_norm': 0.8046875, 'learning_rate': 4.4851280681969896e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1463, 'grad_norm': 0.8828125, 'learning_rate': 4.4839918304506034e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2281, 'grad_norm': 0.6015625, 'learning_rate': 4.482855592704218e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0463, 'grad_norm': 0.73828125, 'learning_rate': 4.4817193549578316e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1723, 'grad_norm': 0.40625, 'learning_rate': 4.4805831172114454e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1095, 'grad_norm': 0.546875, 'learning_rate': 4.47944687946506e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0457, 'grad_norm': 0.8203125, 'learning_rate': 4.478310641718673e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2864, 'grad_norm': 0.6484375, 'learning_rate': 4.4771744039722874e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1906, 'grad_norm': 0.75390625, 'learning_rate': 4.476038166225901e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1271, 'grad_norm': 0.51171875, 'learning_rate': 4.474901928479515e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2401, 'grad_norm': 0.7109375, 'learning_rate': 4.4737656907331295e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0597, 'grad_norm': 0.7578125, 'learning_rate': 4.472629452986743e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2676, 'grad_norm': 0.6328125, 'learning_rate': 4.471493215240357e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1169, 'grad_norm': 1.03125, 'learning_rate': 4.4703569774939715e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1356, 'grad_norm': 0.5625, 'learning_rate': 4.4692207397475846e-05, 'epoch': 0.7}\n",
      "{'loss': 1.2324, 'grad_norm': 1.0234375, 'learning_rate': 4.468084502001199e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0255, 'grad_norm': 0.58984375, 'learning_rate': 4.466948264254813e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3555, 'grad_norm': 0.49609375, 'learning_rate': 4.4658120265084267e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1186, 'grad_norm': 0.82421875, 'learning_rate': 4.4646757887620404e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2887, 'grad_norm': 0.50390625, 'learning_rate': 4.463539551015655e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2322, 'grad_norm': 0.6953125, 'learning_rate': 4.462403313269269e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9731, 'grad_norm': 0.75390625, 'learning_rate': 4.4612670755228825e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3735, 'grad_norm': 0.486328125, 'learning_rate': 4.460130837776497e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1647, 'grad_norm': 0.80078125, 'learning_rate': 4.45899460003011e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2682, 'grad_norm': 0.61328125, 'learning_rate': 4.4578583622837245e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1356, 'grad_norm': 0.83203125, 'learning_rate': 4.456722124537338e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9485, 'grad_norm': 0.58984375, 'learning_rate': 4.455585886790952e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3393, 'grad_norm': 0.515625, 'learning_rate': 4.4544496490445665e-05, 'epoch': 0.71}\n",
      "{'loss': 1.108, 'grad_norm': 0.76953125, 'learning_rate': 4.45331341129818e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1881, 'grad_norm': 0.62109375, 'learning_rate': 4.452177173551794e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2486, 'grad_norm': 0.6328125, 'learning_rate': 4.4510409358054086e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0611, 'grad_norm': 1.3125, 'learning_rate': 4.449904698059022e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3022, 'grad_norm': 0.6640625, 'learning_rate': 4.448768460312636e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1898, 'grad_norm': 0.58984375, 'learning_rate': 4.44763222256625e-05, 'epoch': 0.71}\n",
      "{'loss': 1.117, 'grad_norm': 0.53515625, 'learning_rate': 4.446495984819864e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1587, 'grad_norm': 0.80859375, 'learning_rate': 4.445359747073478e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0748, 'grad_norm': 0.447265625, 'learning_rate': 4.444223509327092e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4399, 'grad_norm': 0.74609375, 'learning_rate': 4.443087271580706e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2541, 'grad_norm': 0.7734375, 'learning_rate': 4.4419510338343195e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0717, 'grad_norm': 0.73046875, 'learning_rate': 4.440814796087934e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1733, 'grad_norm': 0.6171875, 'learning_rate': 4.439678558341547e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0656, 'grad_norm': 1.4375, 'learning_rate': 4.4385423205951616e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4536, 'grad_norm': 0.54296875, 'learning_rate': 4.4374060828487754e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1158, 'grad_norm': 0.65625, 'learning_rate': 4.436269845102389e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1123, 'grad_norm': 0.58203125, 'learning_rate': 4.4351336073560036e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1669, 'grad_norm': 0.6875, 'learning_rate': 4.4339973696096174e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0472, 'grad_norm': 0.6796875, 'learning_rate': 4.432861131863231e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2784, 'grad_norm': 0.5859375, 'learning_rate': 4.4317248941168456e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2, 'grad_norm': 0.75, 'learning_rate': 4.430588656370459e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1237, 'grad_norm': 0.5, 'learning_rate': 4.429452418624073e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2137, 'grad_norm': 0.671875, 'learning_rate': 4.428316180877687e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1423, 'grad_norm': 0.451171875, 'learning_rate': 4.427179943131301e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4107, 'grad_norm': 0.48046875, 'learning_rate': 4.426043705384915e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1937, 'grad_norm': 0.66015625, 'learning_rate': 4.424907467638529e-05, 'epoch': 0.71}\n",
      "{'loss': 1.111, 'grad_norm': 0.404296875, 'learning_rate': 4.423771229892143e-05, 'epoch': 0.71}\n",
      "{'loss': 1.439, 'grad_norm': 0.671875, 'learning_rate': 4.4226349921457566e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0808, 'grad_norm': 1.015625, 'learning_rate': 4.421498754399371e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4054, 'grad_norm': 0.65234375, 'learning_rate': 4.420362516652985e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1723, 'grad_norm': 1.0625, 'learning_rate': 4.4192262789065986e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1918, 'grad_norm': 0.578125, 'learning_rate': 4.4180900411602124e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2254, 'grad_norm': 0.8671875, 'learning_rate': 4.416953803413826e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0998, 'grad_norm': 0.91796875, 'learning_rate': 4.415817565667441e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3995, 'grad_norm': 0.392578125, 'learning_rate': 4.4146813279210545e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1505, 'grad_norm': 0.8046875, 'learning_rate': 4.413545090174668e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2464, 'grad_norm': 0.78515625, 'learning_rate': 4.412408852428283e-05, 'epoch': 0.71}\n",
      "{'loss': 1.074, 'grad_norm': 0.66015625, 'learning_rate': 4.411272614681896e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1638, 'grad_norm': 1.15625, 'learning_rate': 4.41013637693551e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2502, 'grad_norm': 0.66796875, 'learning_rate': 4.409000139189124e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2308, 'grad_norm': 0.87890625, 'learning_rate': 4.407863901442738e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2088, 'grad_norm': 0.474609375, 'learning_rate': 4.406727663696352e-05, 'epoch': 0.71}\n",
      "{'loss': 1.17, 'grad_norm': 0.55078125, 'learning_rate': 4.405591425949966e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0619, 'grad_norm': 1.1328125, 'learning_rate': 4.40445518820358e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4106, 'grad_norm': 0.58984375, 'learning_rate': 4.403318950457194e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0953, 'grad_norm': 0.50390625, 'learning_rate': 4.402182712710808e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2509, 'grad_norm': 0.44921875, 'learning_rate': 4.401046474964422e-05, 'epoch': 0.71}\n",
      "{'loss': 1.204, 'grad_norm': 0.5390625, 'learning_rate': 4.399910237218036e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9917, 'grad_norm': 1.09375, 'learning_rate': 4.3987739994716495e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.355, 'grad_norm': 0.53125, 'learning_rate': 4.397637761725263e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1692, 'grad_norm': 1.125, 'learning_rate': 4.396501523978878e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1848, 'grad_norm': 0.443359375, 'learning_rate': 4.3953652862324915e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2551, 'grad_norm': 0.515625, 'learning_rate': 4.394229048486105e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0605, 'grad_norm': 0.69921875, 'learning_rate': 4.39309281073972e-05, 'epoch': 0.71}\n",
      "{'loss': 1.253, 'grad_norm': 0.70703125, 'learning_rate': 4.391956572993333e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1907, 'grad_norm': 1.078125, 'learning_rate': 4.3908203352469473e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3336, 'grad_norm': 0.515625, 'learning_rate': 4.389684097500561e-05, 'epoch': 0.71}\n",
      "{'loss': 1.24, 'grad_norm': 0.8203125, 'learning_rate': 4.388547859754175e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1347, 'grad_norm': 1.5546875, 'learning_rate': 4.3874116220077894e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2804, 'grad_norm': 0.53515625, 'learning_rate': 4.386275384261403e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1095, 'grad_norm': 0.6015625, 'learning_rate': 4.385139146515017e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1929, 'grad_norm': 0.61328125, 'learning_rate': 4.384002908768631e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2242, 'grad_norm': 0.546875, 'learning_rate': 4.382866671022245e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1204, 'grad_norm': 1.28125, 'learning_rate': 4.381730433275859e-05, 'epoch': 0.71}\n",
      "{'loss': 1.244, 'grad_norm': 0.455078125, 'learning_rate': 4.380594195529473e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1406, 'grad_norm': 0.8046875, 'learning_rate': 4.3794579577830865e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0975, 'grad_norm': 0.50390625, 'learning_rate': 4.3783217200367e-05, 'epoch': 0.71}\n",
      "{'loss': 1.28, 'grad_norm': 0.53125, 'learning_rate': 4.377185482290315e-05, 'epoch': 0.71}\n",
      "{'loss': 1.097, 'grad_norm': 0.4296875, 'learning_rate': 4.3760492445439286e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2623, 'grad_norm': 0.53125, 'learning_rate': 4.3749130067975424e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1359, 'grad_norm': 0.9296875, 'learning_rate': 4.373776769051157e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1609, 'grad_norm': 0.39453125, 'learning_rate': 4.37264053130477e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2621, 'grad_norm': 0.63671875, 'learning_rate': 4.3715042935583844e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0879, 'grad_norm': 0.578125, 'learning_rate': 4.370368055811998e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3903, 'grad_norm': 0.6015625, 'learning_rate': 4.369231818065612e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2497, 'grad_norm': 0.7578125, 'learning_rate': 4.3680955803192264e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2832, 'grad_norm': 0.671875, 'learning_rate': 4.36695934257284e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2993, 'grad_norm': 0.6796875, 'learning_rate': 4.365823104826454e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9951, 'grad_norm': 1.4140625, 'learning_rate': 4.364686867080068e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4835, 'grad_norm': 0.640625, 'learning_rate': 4.363550629333682e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1924, 'grad_norm': 0.734375, 'learning_rate': 4.362414391587296e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2618, 'grad_norm': 0.462890625, 'learning_rate': 4.36127815384091e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3095, 'grad_norm': 0.734375, 'learning_rate': 4.3601419160945236e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0386, 'grad_norm': 0.765625, 'learning_rate': 4.3590056783481374e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4588, 'grad_norm': 0.609375, 'learning_rate': 4.357869440601752e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1098, 'grad_norm': 0.75, 'learning_rate': 4.3567332028553656e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1371, 'grad_norm': 0.515625, 'learning_rate': 4.3555969651089794e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4131, 'grad_norm': 0.82421875, 'learning_rate': 4.354460727362594e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9839, 'grad_norm': 0.5703125, 'learning_rate': 4.353324489616207e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3047, 'grad_norm': 0.9765625, 'learning_rate': 4.3521882518698215e-05, 'epoch': 0.71}\n",
      "{'loss': 1.309, 'grad_norm': 0.61328125, 'learning_rate': 4.351052014123436e-05, 'epoch': 0.71}\n",
      "{'loss': 1.11, 'grad_norm': 0.58203125, 'learning_rate': 4.349915776377049e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2483, 'grad_norm': 0.58203125, 'learning_rate': 4.3487795386306635e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1338, 'grad_norm': 0.92578125, 'learning_rate': 4.347643300884277e-05, 'epoch': 0.71}\n",
      "{'loss': 1.288, 'grad_norm': 0.5546875, 'learning_rate': 4.346507063137891e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1814, 'grad_norm': 1.2421875, 'learning_rate': 4.345370825391505e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0438, 'grad_norm': 0.490234375, 'learning_rate': 4.344234587645119e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2235, 'grad_norm': 0.56640625, 'learning_rate': 4.343098349898733e-05, 'epoch': 0.71}\n",
      "{'loss': 1.199, 'grad_norm': 1.25, 'learning_rate': 4.341962112152347e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3293, 'grad_norm': 0.5625, 'learning_rate': 4.340825874405961e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1363, 'grad_norm': 1.28125, 'learning_rate': 4.3396896366595745e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0146, 'grad_norm': 0.5, 'learning_rate': 4.338553398913189e-05, 'epoch': 0.71}\n",
      "{'loss': 1.332, 'grad_norm': 0.75390625, 'learning_rate': 4.337417161166803e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1802, 'grad_norm': 1.734375, 'learning_rate': 4.3362809234204165e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2327, 'grad_norm': 0.765625, 'learning_rate': 4.335144685674031e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1953, 'grad_norm': 0.60546875, 'learning_rate': 4.334008447927644e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1194, 'grad_norm': 0.4921875, 'learning_rate': 4.3328722101812585e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2071, 'grad_norm': 0.6796875, 'learning_rate': 4.331735972434873e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0533, 'grad_norm': 0.76953125, 'learning_rate': 4.330599734688486e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3126, 'grad_norm': 0.546875, 'learning_rate': 4.3294634969421006e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2395, 'grad_norm': 0.82421875, 'learning_rate': 4.3283272591957143e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2711, 'grad_norm': 0.443359375, 'learning_rate': 4.327191021449328e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2669, 'grad_norm': 0.7421875, 'learning_rate': 4.3260547837029426e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0982, 'grad_norm': 1.1796875, 'learning_rate': 4.3249185459565564e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3724, 'grad_norm': 0.482421875, 'learning_rate': 4.32378230821017e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0925, 'grad_norm': 1.578125, 'learning_rate': 4.322646070463784e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2456, 'grad_norm': 0.470703125, 'learning_rate': 4.321509832717398e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0895, 'grad_norm': 0.62890625, 'learning_rate': 4.3203735949710115e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0037, 'grad_norm': 0.90234375, 'learning_rate': 4.319237357224626e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3039, 'grad_norm': 0.546875, 'learning_rate': 4.31810111947824e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0564, 'grad_norm': 0.93359375, 'learning_rate': 4.3169648817318536e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1529, 'grad_norm': 0.51953125, 'learning_rate': 4.315828643985468e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1799, 'grad_norm': 0.64453125, 'learning_rate': 4.314692406239081e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1172, 'grad_norm': 0.6640625, 'learning_rate': 4.3135561684926956e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3261, 'grad_norm': 0.609375, 'learning_rate': 4.31241993074631e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1965, 'grad_norm': 0.74609375, 'learning_rate': 4.311283692999923e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2792, 'grad_norm': 0.4765625, 'learning_rate': 4.3101474552535376e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2312, 'grad_norm': 0.9296875, 'learning_rate': 4.3090112175071514e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1019, 'grad_norm': 0.984375, 'learning_rate': 4.307874979760765e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2748, 'grad_norm': 0.54296875, 'learning_rate': 4.3067387420143797e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3351, 'grad_norm': 0.74609375, 'learning_rate': 4.3056025042679934e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0857, 'grad_norm': 0.439453125, 'learning_rate': 4.304466266521607e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2682, 'grad_norm': 0.53515625, 'learning_rate': 4.303330028775221e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0752, 'grad_norm': 0.52734375, 'learning_rate': 4.302193791028835e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2372, 'grad_norm': 0.52734375, 'learning_rate': 4.3010575532824486e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0351, 'grad_norm': 0.55078125, 'learning_rate': 4.299921315536063e-05, 'epoch': 0.71}\n",
      "{'loss': 1.064, 'grad_norm': 0.44140625, 'learning_rate': 4.298785077789677e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1797, 'grad_norm': 0.58984375, 'learning_rate': 4.2976488400432906e-05, 'epoch': 0.71}\n",
      "{'loss': 0.977, 'grad_norm': 1.2109375, 'learning_rate': 4.296512602296905e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2177, 'grad_norm': 0.5234375, 'learning_rate': 4.295376364550518e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0556, 'grad_norm': 0.74609375, 'learning_rate': 4.2942401268041327e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2352, 'grad_norm': 0.51953125, 'learning_rate': 4.293103889057747e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2333, 'grad_norm': 0.68359375, 'learning_rate': 4.29196765131136e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0906, 'grad_norm': 1.21875, 'learning_rate': 4.290831413564975e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4394, 'grad_norm': 0.53125, 'learning_rate': 4.2896951758185885e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0585, 'grad_norm': 0.8984375, 'learning_rate': 4.288558938072202e-05, 'epoch': 0.71}\n",
      "{'loss': 1.311, 'grad_norm': 0.50390625, 'learning_rate': 4.287422700325817e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1353, 'grad_norm': 0.7265625, 'learning_rate': 4.2862864625794305e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0894, 'grad_norm': 1.3203125, 'learning_rate': 4.285150224833044e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2574, 'grad_norm': 0.4375, 'learning_rate': 4.284013987086658e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2627, 'grad_norm': 0.78515625, 'learning_rate': 4.282877749340272e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1946, 'grad_norm': 0.52734375, 'learning_rate': 4.281741511593886e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2962, 'grad_norm': 0.8046875, 'learning_rate': 4.2806052738475e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0036, 'grad_norm': 1.09375, 'learning_rate': 4.279469036101114e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2425, 'grad_norm': 0.6640625, 'learning_rate': 4.278332798354728e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0717, 'grad_norm': 1.03125, 'learning_rate': 4.277196560608342e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1962, 'grad_norm': 0.4296875, 'learning_rate': 4.276060322861955e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2255, 'grad_norm': 0.76171875, 'learning_rate': 4.27492408511557e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1016, 'grad_norm': 1.1953125, 'learning_rate': 4.273787847369184e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2655, 'grad_norm': 0.60546875, 'learning_rate': 4.272651609622797e-05, 'epoch': 0.71}\n",
      "{'loss': 1.188, 'grad_norm': 0.85546875, 'learning_rate': 4.271515371876412e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2401, 'grad_norm': 0.494140625, 'learning_rate': 4.2703791341300255e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1986, 'grad_norm': 0.5390625, 'learning_rate': 4.269242896383639e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0672, 'grad_norm': 1.0234375, 'learning_rate': 4.268106658637254e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3274, 'grad_norm': 0.62890625, 'learning_rate': 4.2669704208908676e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2096, 'grad_norm': 0.5703125, 'learning_rate': 4.2658341831444814e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2967, 'grad_norm': 0.5625, 'learning_rate': 4.264697945398095e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1688, 'grad_norm': 0.59375, 'learning_rate': 4.263561707651709e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1352, 'grad_norm': 0.703125, 'learning_rate': 4.2624254699053234e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2158, 'grad_norm': 0.74609375, 'learning_rate': 4.261289232158937e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1566, 'grad_norm': 0.828125, 'learning_rate': 4.260152994412551e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2595, 'grad_norm': 0.49609375, 'learning_rate': 4.259016756666165e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1989, 'grad_norm': 0.6015625, 'learning_rate': 4.257880518919779e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0482, 'grad_norm': 1.5078125, 'learning_rate': 4.256744281173393e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.232, 'grad_norm': 0.765625, 'learning_rate': 4.255608043427007e-05, 'epoch': 0.71}\n",
      "{'loss': 1.1862, 'grad_norm': 0.84375, 'learning_rate': 4.254471805680621e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2216, 'grad_norm': 0.474609375, 'learning_rate': 4.2533355679342344e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2508, 'grad_norm': 0.435546875, 'learning_rate': 4.252199330187849e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0901, 'grad_norm': 0.92578125, 'learning_rate': 4.2510630924414626e-05, 'epoch': 0.71}\n",
      "{'loss': 1.499, 'grad_norm': 1.140625, 'learning_rate': 4.2499268546950764e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3, 'grad_norm': 0.65234375, 'learning_rate': 4.248790616948691e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3186, 'grad_norm': 0.6328125, 'learning_rate': 4.2476543792023046e-05, 'epoch': 0.71}\n",
      "{'loss': 1.191, 'grad_norm': 0.65625, 'learning_rate': 4.2465181414559184e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0677, 'grad_norm': 0.97265625, 'learning_rate': 4.245381903709532e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3338, 'grad_norm': 0.8828125, 'learning_rate': 4.244245665963146e-05, 'epoch': 0.71}\n",
      "{'loss': 1.141, 'grad_norm': 0.78125, 'learning_rate': 4.2431094282167605e-05, 'epoch': 0.71}\n",
      "{'loss': 1.146, 'grad_norm': 0.609375, 'learning_rate': 4.241973190470374e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3385, 'grad_norm': 0.68359375, 'learning_rate': 4.240836952723988e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0413, 'grad_norm': 0.6015625, 'learning_rate': 4.239700714977602e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3504, 'grad_norm': 0.5078125, 'learning_rate': 4.238564477231216e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2027, 'grad_norm': 1.140625, 'learning_rate': 4.23742823948483e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1703, 'grad_norm': 0.51171875, 'learning_rate': 4.236292001738444e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3483, 'grad_norm': 0.7890625, 'learning_rate': 4.235155763992058e-05, 'epoch': 0.72}\n",
      "{'loss': 1.145, 'grad_norm': 1.0234375, 'learning_rate': 4.2340195262456714e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2944, 'grad_norm': 0.6640625, 'learning_rate': 4.232883288499286e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0493, 'grad_norm': 1.0390625, 'learning_rate': 4.2317470507529003e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1809, 'grad_norm': 0.48046875, 'learning_rate': 4.2306108130065134e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2185, 'grad_norm': 0.6484375, 'learning_rate': 4.229474575260128e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0966, 'grad_norm': 1.078125, 'learning_rate': 4.228338337513742e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1998, 'grad_norm': 0.46875, 'learning_rate': 4.2272020997673555e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1972, 'grad_norm': 0.66796875, 'learning_rate': 4.226065862020969e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2425, 'grad_norm': 0.578125, 'learning_rate': 4.224929624274583e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2423, 'grad_norm': 0.546875, 'learning_rate': 4.2237933865281975e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0669, 'grad_norm': 1.0078125, 'learning_rate': 4.222657148781811e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2668, 'grad_norm': 0.65234375, 'learning_rate': 4.221520911035425e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2518, 'grad_norm': 0.72265625, 'learning_rate': 4.220384673289039e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2207, 'grad_norm': 0.49609375, 'learning_rate': 4.219248435542653e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1295, 'grad_norm': 0.578125, 'learning_rate': 4.218112197796267e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9285, 'grad_norm': 0.984375, 'learning_rate': 4.216975960049881e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3398, 'grad_norm': 0.83984375, 'learning_rate': 4.2158397223034954e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1025, 'grad_norm': 0.7578125, 'learning_rate': 4.2147034845571085e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3235, 'grad_norm': 0.48046875, 'learning_rate': 4.213567246810723e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1436, 'grad_norm': 0.62890625, 'learning_rate': 4.2124310090643374e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0418, 'grad_norm': 0.5859375, 'learning_rate': 4.2112947713179505e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4374, 'grad_norm': 0.71484375, 'learning_rate': 4.210158533571565e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0821, 'grad_norm': 0.76171875, 'learning_rate': 4.209022295825179e-05, 'epoch': 0.72}\n",
      "{'loss': 1.168, 'grad_norm': 0.57421875, 'learning_rate': 4.2078860580787925e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2985, 'grad_norm': 0.78515625, 'learning_rate': 4.206749820332406e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1005, 'grad_norm': 0.83203125, 'learning_rate': 4.20561358258602e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4035, 'grad_norm': 0.6484375, 'learning_rate': 4.2044773448396346e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1412, 'grad_norm': 0.59375, 'learning_rate': 4.2033411070932484e-05, 'epoch': 0.72}\n",
      "{'loss': 1.323, 'grad_norm': 0.455078125, 'learning_rate': 4.202204869346862e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2646, 'grad_norm': 0.74609375, 'learning_rate': 4.201068631600476e-05, 'epoch': 0.72}\n",
      "{'loss': 0.965, 'grad_norm': 0.90625, 'learning_rate': 4.1999323938540904e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2919, 'grad_norm': 0.58984375, 'learning_rate': 4.198796156107704e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1119, 'grad_norm': 0.546875, 'learning_rate': 4.197659918361318e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1368, 'grad_norm': 0.515625, 'learning_rate': 4.1965236806149324e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2038, 'grad_norm': 0.66015625, 'learning_rate': 4.1953874428685455e-05, 'epoch': 0.72}\n",
      "{'loss': 0.956, 'grad_norm': 1.0625, 'learning_rate': 4.19425120512216e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3422, 'grad_norm': 0.52734375, 'learning_rate': 4.1931149673757745e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2131, 'grad_norm': 0.8359375, 'learning_rate': 4.1919787296293876e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2325, 'grad_norm': 0.53125, 'learning_rate': 4.190842491883002e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1881, 'grad_norm': 0.578125, 'learning_rate': 4.189706254136616e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0575, 'grad_norm': 0.78125, 'learning_rate': 4.1885700163902296e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3855, 'grad_norm': 0.8828125, 'learning_rate': 4.187433778643844e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1404, 'grad_norm': 0.671875, 'learning_rate': 4.186297540897457e-05, 'epoch': 0.72}\n",
      "{'loss': 1.315, 'grad_norm': 0.458984375, 'learning_rate': 4.1851613031510716e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0906, 'grad_norm': 0.78125, 'learning_rate': 4.1840250654046854e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1297, 'grad_norm': 0.70703125, 'learning_rate': 4.182888827658299e-05, 'epoch': 0.72}\n",
      "{'loss': 1.315, 'grad_norm': 0.53125, 'learning_rate': 4.181752589911913e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2294, 'grad_norm': 0.7421875, 'learning_rate': 4.1806163521655275e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1218, 'grad_norm': 0.6640625, 'learning_rate': 4.179480114419141e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1907, 'grad_norm': 0.56640625, 'learning_rate': 4.178343876672755e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0417, 'grad_norm': 1.8984375, 'learning_rate': 4.1772076389263695e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3, 'grad_norm': 0.4765625, 'learning_rate': 4.1760714011799826e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1137, 'grad_norm': 0.89453125, 'learning_rate': 4.174935163433597e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2054, 'grad_norm': 0.447265625, 'learning_rate': 4.1737989256872115e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1225, 'grad_norm': 0.5234375, 'learning_rate': 4.1726626879408246e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0374, 'grad_norm': 0.68359375, 'learning_rate': 4.171526450194439e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3583, 'grad_norm': 0.53515625, 'learning_rate': 4.170390212448053e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1763, 'grad_norm': 0.76171875, 'learning_rate': 4.169253974701667e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1012, 'grad_norm': 0.470703125, 'learning_rate': 4.168117736955281e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2721, 'grad_norm': 0.6640625, 'learning_rate': 4.166981499208894e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1415, 'grad_norm': 0.7734375, 'learning_rate': 4.165845261462509e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3365, 'grad_norm': 0.8046875, 'learning_rate': 4.1647090237161225e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0975, 'grad_norm': 0.6640625, 'learning_rate': 4.163572785969736e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1472, 'grad_norm': 0.58203125, 'learning_rate': 4.162436548223351e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1277, 'grad_norm': 0.6953125, 'learning_rate': 4.1613003104769645e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0402, 'grad_norm': 1.171875, 'learning_rate': 4.160164072730578e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2892, 'grad_norm': 0.6484375, 'learning_rate': 4.159027834984192e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1583, 'grad_norm': 0.671875, 'learning_rate': 4.1578915972378066e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1659, 'grad_norm': 0.4375, 'learning_rate': 4.15675535949142e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0962, 'grad_norm': 1.03125, 'learning_rate': 4.155619121745034e-05, 'epoch': 0.72}\n",
      "{'loss': 1.141, 'grad_norm': 0.97265625, 'learning_rate': 4.1544828839986486e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2775, 'grad_norm': 0.4765625, 'learning_rate': 4.153346646252262e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1911, 'grad_norm': 0.67578125, 'learning_rate': 4.152210408505876e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2266, 'grad_norm': 0.94140625, 'learning_rate': 4.15107417075949e-05, 'epoch': 0.72}\n",
      "{'loss': 1.194, 'grad_norm': 0.9921875, 'learning_rate': 4.149937933013104e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0803, 'grad_norm': 0.9375, 'learning_rate': 4.148801695266718e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2317, 'grad_norm': 0.62109375, 'learning_rate': 4.147665457520331e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1027, 'grad_norm': 0.74609375, 'learning_rate': 4.146529219773946e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1104, 'grad_norm': 0.578125, 'learning_rate': 4.1453929820275596e-05, 'epoch': 0.72}\n",
      "{'loss': 1.204, 'grad_norm': 0.66015625, 'learning_rate': 4.144256744281173e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0379, 'grad_norm': 0.92578125, 'learning_rate': 4.143120506534788e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4479, 'grad_norm': 0.50390625, 'learning_rate': 4.1419842687884016e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2463, 'grad_norm': 0.77734375, 'learning_rate': 4.1408480310420154e-05, 'epoch': 0.72}\n",
      "{'loss': 1.144, 'grad_norm': 0.515625, 'learning_rate': 4.139711793295629e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2118, 'grad_norm': 0.6015625, 'learning_rate': 4.1385755555492436e-05, 'epoch': 0.72}\n",
      "{'loss': 0.988, 'grad_norm': 0.482421875, 'learning_rate': 4.1374393178028574e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3163, 'grad_norm': 0.546875, 'learning_rate': 4.136303080056471e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1604, 'grad_norm': 0.78515625, 'learning_rate': 4.1351668423100857e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1089, 'grad_norm': 0.44921875, 'learning_rate': 4.134030604563699e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3479, 'grad_norm': 0.5625, 'learning_rate': 4.132894366817313e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1665, 'grad_norm': 0.6484375, 'learning_rate': 4.131758129070927e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3468, 'grad_norm': 0.66015625, 'learning_rate': 4.130621891324541e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0856, 'grad_norm': 0.6875, 'learning_rate': 4.129485653578155e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1815, 'grad_norm': 0.8046875, 'learning_rate': 4.128349415831769e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1995, 'grad_norm': 0.7578125, 'learning_rate': 4.127213178085383e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1919, 'grad_norm': 1.1796875, 'learning_rate': 4.1260769403389966e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3103, 'grad_norm': 0.65625, 'learning_rate': 4.1249407025926104e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1846, 'grad_norm': 1.171875, 'learning_rate': 4.123804464846225e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1608, 'grad_norm': 0.58984375, 'learning_rate': 4.1226682270998387e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2101, 'grad_norm': 0.9453125, 'learning_rate': 4.1215319893534524e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0473, 'grad_norm': 0.87890625, 'learning_rate': 4.120395751607066e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3722, 'grad_norm': 0.515625, 'learning_rate': 4.119259513860681e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1209, 'grad_norm': 0.86328125, 'learning_rate': 4.1181232761142945e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1976, 'grad_norm': 0.515625, 'learning_rate': 4.116987038367908e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2311, 'grad_norm': 0.71484375, 'learning_rate': 4.115850800621523e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0947, 'grad_norm': 0.68359375, 'learning_rate': 4.114714562875136e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3774, 'grad_norm': 0.486328125, 'learning_rate': 4.11357832512875e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1695, 'grad_norm': 0.73828125, 'learning_rate': 4.112442087382364e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9999, 'grad_norm': 0.4609375, 'learning_rate': 4.111305849635978e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1822, 'grad_norm': 0.625, 'learning_rate': 4.110169611889592e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0251, 'grad_norm': 0.84765625, 'learning_rate': 4.109033374143206e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2869, 'grad_norm': 0.62109375, 'learning_rate': 4.10789713639682e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0727, 'grad_norm': 0.71484375, 'learning_rate': 4.106760898650434e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1171, 'grad_norm': 0.49609375, 'learning_rate': 4.1056246609040475e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2673, 'grad_norm': 0.498046875, 'learning_rate': 4.104488423157662e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0261, 'grad_norm': 0.82421875, 'learning_rate': 4.103352185411276e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3298, 'grad_norm': 0.50390625, 'learning_rate': 4.1022159476648895e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1387, 'grad_norm': 0.6796875, 'learning_rate': 4.101079709918503e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2851, 'grad_norm': 0.6171875, 'learning_rate': 4.099943472172118e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2137, 'grad_norm': 0.86328125, 'learning_rate': 4.0988072344257315e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0551, 'grad_norm': 0.8046875, 'learning_rate': 4.097670996679345e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3711, 'grad_norm': 0.60546875, 'learning_rate': 4.09653475893296e-05, 'epoch': 0.72}\n",
      "{'loss': 1.214, 'grad_norm': 0.68359375, 'learning_rate': 4.095398521186573e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1414, 'grad_norm': 0.578125, 'learning_rate': 4.0942622834401874e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1823, 'grad_norm': 0.70703125, 'learning_rate': 4.093126045693801e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0984, 'grad_norm': 0.71484375, 'learning_rate': 4.091989807947415e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3118, 'grad_norm': 0.490234375, 'learning_rate': 4.0908535702010294e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4208, 'grad_norm': 0.6328125, 'learning_rate': 4.089717332454643e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2132, 'grad_norm': 0.470703125, 'learning_rate': 4.088581094708257e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2468, 'grad_norm': 0.58984375, 'learning_rate': 4.087444856961871e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1015, 'grad_norm': 0.81640625, 'learning_rate': 4.0863086192154845e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.298, 'grad_norm': 0.55859375, 'learning_rate': 4.085172381469099e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1679, 'grad_norm': 0.74609375, 'learning_rate': 4.084036143722713e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2568, 'grad_norm': 0.4140625, 'learning_rate': 4.0828999059763266e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2784, 'grad_norm': 0.67578125, 'learning_rate': 4.0817636682299403e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1088, 'grad_norm': 2.125, 'learning_rate': 4.080627430483555e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4682, 'grad_norm': 0.609375, 'learning_rate': 4.0794911927371686e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1757, 'grad_norm': 0.8515625, 'learning_rate': 4.0783549549907824e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2136, 'grad_norm': 0.4140625, 'learning_rate': 4.077218717244397e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3997, 'grad_norm': 0.609375, 'learning_rate': 4.07608247949801e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1534, 'grad_norm': 1.0234375, 'learning_rate': 4.0749462417516244e-05, 'epoch': 0.72}\n",
      "{'loss': 1.372, 'grad_norm': 0.515625, 'learning_rate': 4.073810004005238e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0845, 'grad_norm': 0.79296875, 'learning_rate': 4.072673766258852e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3129, 'grad_norm': 0.419921875, 'learning_rate': 4.0715375285124664e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2369, 'grad_norm': 0.578125, 'learning_rate': 4.07040129076608e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0435, 'grad_norm': 0.58984375, 'learning_rate': 4.069265053019694e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1542, 'grad_norm': 0.578125, 'learning_rate': 4.0681288152733085e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1315, 'grad_norm': 0.73046875, 'learning_rate': 4.0669925775269216e-05, 'epoch': 0.72}\n",
      "{'loss': 1.085, 'grad_norm': 0.50390625, 'learning_rate': 4.065856339780536e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1469, 'grad_norm': 0.59375, 'learning_rate': 4.06472010203415e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1906, 'grad_norm': 1.046875, 'learning_rate': 4.0635838642877636e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2923, 'grad_norm': 0.494140625, 'learning_rate': 4.0624476265413774e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1951, 'grad_norm': 1.015625, 'learning_rate': 4.061311388794992e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1408, 'grad_norm': 0.44140625, 'learning_rate': 4.0601751510486057e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1625, 'grad_norm': 0.6015625, 'learning_rate': 4.0590389133022194e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0555, 'grad_norm': 0.95703125, 'learning_rate': 4.057902675555834e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3317, 'grad_norm': 0.5625, 'learning_rate': 4.056766437809447e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1119, 'grad_norm': 0.81640625, 'learning_rate': 4.0556302000630615e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3388, 'grad_norm': 0.58203125, 'learning_rate': 4.054493962316675e-05, 'epoch': 0.72}\n",
      "{'loss': 1.224, 'grad_norm': 0.5625, 'learning_rate': 4.053357724570289e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0658, 'grad_norm': 0.83984375, 'learning_rate': 4.0522214868239035e-05, 'epoch': 0.72}\n",
      "{'loss': 1.349, 'grad_norm': 0.55859375, 'learning_rate': 4.051085249077517e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2071, 'grad_norm': 1.015625, 'learning_rate': 4.049949011331131e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2585, 'grad_norm': 0.5078125, 'learning_rate': 4.0488127735847455e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3603, 'grad_norm': 0.5390625, 'learning_rate': 4.0476765358383587e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0569, 'grad_norm': 2.078125, 'learning_rate': 4.046540298091973e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3624, 'grad_norm': 0.55859375, 'learning_rate': 4.045404060345587e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1119, 'grad_norm': 0.734375, 'learning_rate': 4.044267822599201e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2604, 'grad_norm': 0.470703125, 'learning_rate': 4.043131584852815e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1549, 'grad_norm': 0.58984375, 'learning_rate': 4.041995347106429e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0995, 'grad_norm': 1.0625, 'learning_rate': 4.040859109360043e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3613, 'grad_norm': 0.54296875, 'learning_rate': 4.0397228716136565e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1057, 'grad_norm': 0.8515625, 'learning_rate': 4.038586633867271e-05, 'epoch': 0.72}\n",
      "{'loss': 1.182, 'grad_norm': 0.70703125, 'learning_rate': 4.037450396120884e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1238, 'grad_norm': 0.63671875, 'learning_rate': 4.0363141583744985e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0474, 'grad_norm': 0.96875, 'learning_rate': 4.035177920628112e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4085, 'grad_norm': 0.5078125, 'learning_rate': 4.034041682881726e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1416, 'grad_norm': 0.71484375, 'learning_rate': 4.0329054451353406e-05, 'epoch': 0.72}\n",
      "{'loss': 1.206, 'grad_norm': 0.396484375, 'learning_rate': 4.0317692073889544e-05, 'epoch': 0.72}\n",
      "{'loss': 1.344, 'grad_norm': 0.578125, 'learning_rate': 4.030632969642568e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9583, 'grad_norm': 0.61328125, 'learning_rate': 4.0294967318961826e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3189, 'grad_norm': 0.5859375, 'learning_rate': 4.028360494149796e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1535, 'grad_norm': 0.609375, 'learning_rate': 4.02722425640341e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1997, 'grad_norm': 0.421875, 'learning_rate': 4.026088018657024e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2746, 'grad_norm': 1.046875, 'learning_rate': 4.024951780910638e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1576, 'grad_norm': 0.68359375, 'learning_rate': 4.023815543164252e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3856, 'grad_norm': 0.5625, 'learning_rate': 4.022679305417866e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1148, 'grad_norm': 0.6015625, 'learning_rate': 4.02154306767148e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2845, 'grad_norm': 0.53125, 'learning_rate': 4.0204068299250936e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1449, 'grad_norm': 0.62890625, 'learning_rate': 4.019270592178708e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0885, 'grad_norm': 1.1640625, 'learning_rate': 4.018134354432321e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4069, 'grad_norm': 0.62890625, 'learning_rate': 4.0169981166859356e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1509, 'grad_norm': 0.66015625, 'learning_rate': 4.0158618789395494e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1976, 'grad_norm': 0.4921875, 'learning_rate': 4.014725641193163e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1625, 'grad_norm': 0.55859375, 'learning_rate': 4.0135894034467776e-05, 'epoch': 0.72}\n",
      "{'loss': 0.98, 'grad_norm': 0.486328125, 'learning_rate': 4.0124531657003914e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3465, 'grad_norm': 0.5703125, 'learning_rate': 4.011316927954005e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1616, 'grad_norm': 0.61328125, 'learning_rate': 4.01018069020762e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1697, 'grad_norm': 0.458984375, 'learning_rate': 4.009044452461233e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1341, 'grad_norm': 1.0625, 'learning_rate': 4.007908214714847e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1938, 'grad_norm': 0.984375, 'learning_rate': 4.006771976968461e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3333, 'grad_norm': 0.50390625, 'learning_rate': 4.005635739222075e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1744, 'grad_norm': 0.7421875, 'learning_rate': 4.004499501475689e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1403, 'grad_norm': 0.490234375, 'learning_rate': 4.003363263729303e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2818, 'grad_norm': 0.73046875, 'learning_rate': 4.002227025982917e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0546, 'grad_norm': 1.1484375, 'learning_rate': 4.0010907882365306e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4224, 'grad_norm': 0.66015625, 'learning_rate': 3.999954550490145e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0992, 'grad_norm': 0.69140625, 'learning_rate': 3.998818312743759e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2841, 'grad_norm': 0.48828125, 'learning_rate': 3.997682074997373e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2005, 'grad_norm': 0.60546875, 'learning_rate': 3.9965458372509865e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0059, 'grad_norm': 1.1015625, 'learning_rate': 3.9954095995046e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3097, 'grad_norm': 0.5703125, 'learning_rate': 3.994273361758215e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2587, 'grad_norm': 0.69140625, 'learning_rate': 3.9931371240118285e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1995, 'grad_norm': 0.52734375, 'learning_rate': 3.992000886265442e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1567, 'grad_norm': 0.68359375, 'learning_rate': 3.990864648519057e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1029, 'grad_norm': 0.90625, 'learning_rate': 3.98972841077267e-05, 'epoch': 0.73}\n",
      "{'loss': 1.4776, 'grad_norm': 0.515625, 'learning_rate': 3.988592173026284e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9891, 'grad_norm': 0.875, 'learning_rate': 3.987455935279898e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2181, 'grad_norm': 0.65625, 'learning_rate': 3.986319697533512e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1232, 'grad_norm': 0.6015625, 'learning_rate': 3.9851834597871263e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9874, 'grad_norm': 0.66015625, 'learning_rate': 3.98404722204074e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2119, 'grad_norm': 0.63671875, 'learning_rate': 3.982910984294354e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1825, 'grad_norm': 0.79296875, 'learning_rate': 3.981774746547968e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2497, 'grad_norm': 0.43359375, 'learning_rate': 3.980638508801582e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2432, 'grad_norm': 0.498046875, 'learning_rate': 3.979502271055196e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1382, 'grad_norm': 0.96875, 'learning_rate': 3.97836603330881e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3758, 'grad_norm': 0.75, 'learning_rate': 3.9772297955624235e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1146, 'grad_norm': 0.8984375, 'learning_rate': 3.976093557816037e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2223, 'grad_norm': 0.5703125, 'learning_rate': 3.974957320069652e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2056, 'grad_norm': 0.48828125, 'learning_rate': 3.9738210823232655e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0887, 'grad_norm': 0.84375, 'learning_rate': 3.972684844576879e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4085, 'grad_norm': 0.55859375, 'learning_rate': 3.971548606830494e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1222, 'grad_norm': 0.83203125, 'learning_rate': 3.970412369084107e-05, 'epoch': 0.73}\n",
      "{'loss': 1.167, 'grad_norm': 0.65234375, 'learning_rate': 3.9692761313377214e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2325, 'grad_norm': 0.51953125, 'learning_rate': 3.968139893591335e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0914, 'grad_norm': 1.0625, 'learning_rate': 3.967003655844949e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3221, 'grad_norm': 0.50390625, 'learning_rate': 3.9658674180985634e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1502, 'grad_norm': 0.62890625, 'learning_rate': 3.964731180352177e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1849, 'grad_norm': 0.478515625, 'learning_rate': 3.963594942605791e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2083, 'grad_norm': 0.66015625, 'learning_rate': 3.962458704859405e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0467, 'grad_norm': 0.9453125, 'learning_rate': 3.961322467113019e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3274, 'grad_norm': 0.53125, 'learning_rate': 3.960186229366633e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1581, 'grad_norm': 0.625, 'learning_rate': 3.959049991620247e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1276, 'grad_norm': 0.48828125, 'learning_rate': 3.9579137538738606e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2516, 'grad_norm': 0.6171875, 'learning_rate': 3.9567775161274744e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0876, 'grad_norm': 1.4140625, 'learning_rate': 3.955641278381089e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3939, 'grad_norm': 0.51953125, 'learning_rate': 3.9545050406347026e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2185, 'grad_norm': 0.90625, 'learning_rate': 3.9533688028883164e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3309, 'grad_norm': 0.462890625, 'learning_rate': 3.952232565141931e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2002, 'grad_norm': 0.71875, 'learning_rate': 3.951096327395544e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9968, 'grad_norm': 0.87890625, 'learning_rate': 3.9499600896491584e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2978, 'grad_norm': 0.57421875, 'learning_rate': 3.948823851902773e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2003, 'grad_norm': 0.85546875, 'learning_rate': 3.947687614156386e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1136, 'grad_norm': 0.44140625, 'learning_rate': 3.9465513764100005e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1786, 'grad_norm': 0.87890625, 'learning_rate': 3.945415138663614e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0792, 'grad_norm': 0.9609375, 'learning_rate': 3.944278900917228e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4265, 'grad_norm': 0.484375, 'learning_rate': 3.943142663170842e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1802, 'grad_norm': 1.1171875, 'learning_rate': 3.942006425424456e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1591, 'grad_norm': 0.58203125, 'learning_rate': 3.94087018767807e-05, 'epoch': 0.73}\n",
      "{'loss': 1.266, 'grad_norm': 0.6328125, 'learning_rate': 3.939733949931684e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1179, 'grad_norm': 1.1484375, 'learning_rate': 3.9385977121852976e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2312, 'grad_norm': 0.57421875, 'learning_rate': 3.9374614744389114e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1264, 'grad_norm': 0.73046875, 'learning_rate': 3.936325236692526e-05, 'epoch': 0.73}\n",
      "{'loss': 1.268, 'grad_norm': 0.5078125, 'learning_rate': 3.93518899894614e-05, 'epoch': 0.73}\n",
      "{'loss': 1.225, 'grad_norm': 0.54296875, 'learning_rate': 3.9340527611997535e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9892, 'grad_norm': 0.96875, 'learning_rate': 3.932916523453368e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2699, 'grad_norm': 0.5234375, 'learning_rate': 3.931780285706981e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2365, 'grad_norm': 0.76953125, 'learning_rate': 3.9306440479605955e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2147, 'grad_norm': 0.6328125, 'learning_rate': 3.92950781021421e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1593, 'grad_norm': 0.85546875, 'learning_rate': 3.928371572467823e-05, 'epoch': 0.73}\n",
      "{'loss': 1.074, 'grad_norm': 1.2421875, 'learning_rate': 3.9272353347214375e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2873, 'grad_norm': 0.75390625, 'learning_rate': 3.926099096975051e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2042, 'grad_norm': 0.79296875, 'learning_rate': 3.924962859228665e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3785, 'grad_norm': 0.66015625, 'learning_rate': 3.923826621482279e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2456, 'grad_norm': 0.71484375, 'learning_rate': 3.9226903837358933e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0577, 'grad_norm': 1.8046875, 'learning_rate': 3.921554145989507e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2381, 'grad_norm': 0.50390625, 'learning_rate': 3.920417908243121e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1227, 'grad_norm': 1.046875, 'learning_rate': 3.919281670496735e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2133, 'grad_norm': 0.765625, 'learning_rate': 3.9181454327503485e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1815, 'grad_norm': 0.9765625, 'learning_rate': 3.917009195003963e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1777, 'grad_norm': 0.94140625, 'learning_rate': 3.915872957257577e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4, 'grad_norm': 0.6171875, 'learning_rate': 3.9147367195111905e-05, 'epoch': 0.73}\n",
      "{'loss': 1.142, 'grad_norm': 0.75, 'learning_rate': 3.913600481764805e-05, 'epoch': 0.73}\n",
      "{'loss': 1.252, 'grad_norm': 0.40625, 'learning_rate': 3.912464244018418e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3511, 'grad_norm': 0.6875, 'learning_rate': 3.9113280062720326e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1441, 'grad_norm': 0.65625, 'learning_rate': 3.910191768525647e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2252, 'grad_norm': 0.51171875, 'learning_rate': 3.90905553077926e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0579, 'grad_norm': 0.7109375, 'learning_rate': 3.9079192930328746e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1276, 'grad_norm': 0.5078125, 'learning_rate': 3.9067830552864884e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2395, 'grad_norm': 0.7578125, 'learning_rate': 3.905646817540102e-05, 'epoch': 0.73}\n",
      "{'loss': 1.132, 'grad_norm': 1.21875, 'learning_rate': 3.9045105797937166e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3903, 'grad_norm': 0.52734375, 'learning_rate': 3.9033743420473304e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1238, 'grad_norm': 0.6484375, 'learning_rate': 3.902238104300944e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1279, 'grad_norm': 0.44921875, 'learning_rate': 3.901101866554558e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1925, 'grad_norm': 0.5, 'learning_rate': 3.899965628808172e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9252, 'grad_norm': 1.2109375, 'learning_rate': 3.8988293910617856e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3475, 'grad_norm': 0.53515625, 'learning_rate': 3.8976931533154e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1312, 'grad_norm': 1.0390625, 'learning_rate': 3.896556915569014e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1172, 'grad_norm': 0.5625, 'learning_rate': 3.8954206778226276e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2179, 'grad_norm': 0.82421875, 'learning_rate': 3.894284440076242e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0773, 'grad_norm': 0.78515625, 'learning_rate': 3.893148202329855e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3104, 'grad_norm': 0.5859375, 'learning_rate': 3.8920119645834696e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1071, 'grad_norm': 0.66796875, 'learning_rate': 3.890875726837084e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1118, 'grad_norm': 0.51953125, 'learning_rate': 3.889739489090697e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2045, 'grad_norm': 0.59765625, 'learning_rate': 3.8886032513443117e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9864, 'grad_norm': 0.9296875, 'learning_rate': 3.8874670135979254e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3107, 'grad_norm': 0.53125, 'learning_rate': 3.886330775851539e-05, 'epoch': 0.73}\n",
      "{'loss': 1.257, 'grad_norm': 1.0, 'learning_rate': 3.885194538105154e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1246, 'grad_norm': 0.46875, 'learning_rate': 3.8840583003587675e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1513, 'grad_norm': 0.703125, 'learning_rate': 3.882922062612381e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9582, 'grad_norm': 1.03125, 'learning_rate': 3.881785824865995e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2015, 'grad_norm': 0.5390625, 'learning_rate': 3.880649587119609e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2371, 'grad_norm': 0.71484375, 'learning_rate': 3.879513349373223e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2714, 'grad_norm': 0.57421875, 'learning_rate': 3.878377111626837e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2735, 'grad_norm': 0.67578125, 'learning_rate': 3.877240873880451e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0482, 'grad_norm': 0.58203125, 'learning_rate': 3.8761046361340646e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3505, 'grad_norm': 0.5546875, 'learning_rate': 3.874968398387679e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1604, 'grad_norm': 0.6640625, 'learning_rate': 3.873832160641292e-05, 'epoch': 0.73}\n",
      "{'loss': 1.226, 'grad_norm': 0.53515625, 'learning_rate': 3.872695922894907e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1326, 'grad_norm': 0.73046875, 'learning_rate': 3.871559685148521e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0896, 'grad_norm': 0.98828125, 'learning_rate': 3.870423447402134e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3364, 'grad_norm': 0.55859375, 'learning_rate': 3.869287209655749e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1192, 'grad_norm': 0.6171875, 'learning_rate': 3.8681509719093625e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1895, 'grad_norm': 0.5546875, 'learning_rate': 3.867014734162976e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1054, 'grad_norm': 0.77734375, 'learning_rate': 3.865878496416591e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2222, 'grad_norm': 1.0859375, 'learning_rate': 3.8647422586702045e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3886, 'grad_norm': 0.64453125, 'learning_rate': 3.863606020923818e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2041, 'grad_norm': 0.72265625, 'learning_rate': 3.862469783177432e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2014, 'grad_norm': 0.4765625, 'learning_rate': 3.861333545431046e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3038, 'grad_norm': 0.97265625, 'learning_rate': 3.8601973076846604e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0591, 'grad_norm': 0.828125, 'learning_rate': 3.859061069938274e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4267, 'grad_norm': 0.578125, 'learning_rate': 3.857924832191888e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1789, 'grad_norm': 0.71875, 'learning_rate': 3.856788594445502e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3866, 'grad_norm': 0.51953125, 'learning_rate': 3.855652356699116e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1165, 'grad_norm': 0.59765625, 'learning_rate': 3.85451611895273e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1342, 'grad_norm': 0.5546875, 'learning_rate': 3.853379881206344e-05, 'epoch': 0.73}\n",
      "{'loss': 1.4142, 'grad_norm': 0.431640625, 'learning_rate': 3.852243643459958e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1621, 'grad_norm': 0.7109375, 'learning_rate': 3.851107405713571e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1549, 'grad_norm': 0.56640625, 'learning_rate': 3.849971167967186e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0969, 'grad_norm': 0.8125, 'learning_rate': 3.8488349302207996e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1928, 'grad_norm': 1.09375, 'learning_rate': 3.8476986924744134e-05, 'epoch': 0.73}\n",
      "{'loss': 1.4019, 'grad_norm': 0.6171875, 'learning_rate': 3.846562454728028e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1217, 'grad_norm': 0.83984375, 'learning_rate': 3.8454262169816416e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1881, 'grad_norm': 0.5625, 'learning_rate': 3.8442899792352554e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1381, 'grad_norm': 0.494140625, 'learning_rate': 3.843153741488869e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0421, 'grad_norm': 0.94921875, 'learning_rate': 3.842017503742483e-05, 'epoch': 0.73}\n",
      "{'loss': 1.227, 'grad_norm': 0.498046875, 'learning_rate': 3.8408812659960974e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0658, 'grad_norm': 1.0625, 'learning_rate': 3.839745028249711e-05, 'epoch': 0.73}\n",
      "{'loss': 1.17, 'grad_norm': 0.6484375, 'learning_rate': 3.838608790503325e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1604, 'grad_norm': 0.63671875, 'learning_rate': 3.837472552756939e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2192, 'grad_norm': 0.56640625, 'learning_rate': 3.836336315010553e-05, 'epoch': 0.73}\n",
      "{'loss': 1.295, 'grad_norm': 0.6484375, 'learning_rate': 3.835200077264167e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2121, 'grad_norm': 0.7890625, 'learning_rate': 3.834063839517781e-05, 'epoch': 0.73}\n",
      "{'loss': 1.08, 'grad_norm': 0.451171875, 'learning_rate': 3.832927601771395e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0686, 'grad_norm': 0.69921875, 'learning_rate': 3.8317913640250084e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9425, 'grad_norm': 1.1953125, 'learning_rate': 3.830655126278623e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3916, 'grad_norm': 0.5859375, 'learning_rate': 3.8295188885322366e-05, 'epoch': 0.73}\n",
      "{'loss': 1.077, 'grad_norm': 0.75390625, 'learning_rate': 3.8283826507858504e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3104, 'grad_norm': 0.5078125, 'learning_rate': 3.827246413039465e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2206, 'grad_norm': 0.5546875, 'learning_rate': 3.826110175293079e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0575, 'grad_norm': 0.78125, 'learning_rate': 3.8249739375466924e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2685, 'grad_norm': 0.78125, 'learning_rate': 3.823837699800306e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1188, 'grad_norm': 0.75390625, 'learning_rate': 3.82270146205392e-05, 'epoch': 0.73}\n",
      "{'loss': 1.103, 'grad_norm': 0.53515625, 'learning_rate': 3.8215652243075345e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1974, 'grad_norm': 0.625, 'learning_rate': 3.820428986561148e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0663, 'grad_norm': 0.7265625, 'learning_rate': 3.819292748814762e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3415, 'grad_norm': 0.55859375, 'learning_rate': 3.818156511068376e-05, 'epoch': 0.73}\n",
      "{'loss': 1.145, 'grad_norm': 0.86328125, 'learning_rate': 3.81702027332199e-05, 'epoch': 0.73}\n",
      "{'loss': 1.324, 'grad_norm': 0.59375, 'learning_rate': 3.815884035575604e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2022, 'grad_norm': 0.50390625, 'learning_rate': 3.814747797829218e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9839, 'grad_norm': 0.8984375, 'learning_rate': 3.813611560082832e-05, 'epoch': 0.73}\n",
      "{'loss': 1.4059, 'grad_norm': 0.71875, 'learning_rate': 3.8124753223364454e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1048, 'grad_norm': 0.80078125, 'learning_rate': 3.81133908459006e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2486, 'grad_norm': 0.546875, 'learning_rate': 3.810202846843674e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2327, 'grad_norm': 0.66015625, 'learning_rate': 3.8090666090972875e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1311, 'grad_norm': 1.078125, 'learning_rate': 3.807930371350902e-05, 'epoch': 0.73}\n",
      "{'loss': 1.4429, 'grad_norm': 0.65625, 'learning_rate': 3.806794133604516e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2241, 'grad_norm': 0.671875, 'learning_rate': 3.8056578958581295e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1546, 'grad_norm': 0.65625, 'learning_rate': 3.804521658111743e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2179, 'grad_norm': 0.7109375, 'learning_rate': 3.803385420365357e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1387, 'grad_norm': 0.95703125, 'learning_rate': 3.8022491826189715e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2758, 'grad_norm': 0.486328125, 'learning_rate': 3.801112944872585e-05, 'epoch': 0.73}\n",
      "{'loss': 1.116, 'grad_norm': 0.87109375, 'learning_rate': 3.799976707126199e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1694, 'grad_norm': 0.578125, 'learning_rate': 3.798840469379813e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1288, 'grad_norm': 0.88671875, 'learning_rate': 3.7977042316334274e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0436, 'grad_norm': 1.0, 'learning_rate': 3.796567993887041e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5158, 'grad_norm': 0.54296875, 'learning_rate': 3.795431756140655e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0713, 'grad_norm': 0.7109375, 'learning_rate': 3.7942955183942694e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1634, 'grad_norm': 0.66796875, 'learning_rate': 3.7931592806478825e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1912, 'grad_norm': 0.56640625, 'learning_rate': 3.792023042901497e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0588, 'grad_norm': 0.99609375, 'learning_rate': 3.790886805155111e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2054, 'grad_norm': 0.7265625, 'learning_rate': 3.7897505674087245e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2058, 'grad_norm': 1.09375, 'learning_rate': 3.788614329662339e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2869, 'grad_norm': 0.63671875, 'learning_rate': 3.787478091915953e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2213, 'grad_norm': 0.5859375, 'learning_rate': 3.7863418541695666e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0679, 'grad_norm': 1.6015625, 'learning_rate': 3.785205616423181e-05, 'epoch': 0.73}\n",
      "{'loss': 1.3321, 'grad_norm': 0.59375, 'learning_rate': 3.784069378676794e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0804, 'grad_norm': 0.671875, 'learning_rate': 3.7829331409304086e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1867, 'grad_norm': 0.5234375, 'learning_rate': 3.7817969031840224e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2606, 'grad_norm': 0.78125, 'learning_rate': 3.780660665437636e-05, 'epoch': 0.73}\n",
      "{'loss': 1.1304, 'grad_norm': 0.96875, 'learning_rate': 3.77952442769125e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2965, 'grad_norm': 0.4609375, 'learning_rate': 3.7783881899448644e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0904, 'grad_norm': 0.703125, 'learning_rate': 3.777251952198478e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2628, 'grad_norm': 0.49609375, 'learning_rate': 3.776115714452092e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1604, 'grad_norm': 0.5625, 'learning_rate': 3.7749794767057065e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0752, 'grad_norm': 0.75, 'learning_rate': 3.7738432389593196e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3373, 'grad_norm': 0.56640625, 'learning_rate': 3.772707001212934e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1033, 'grad_norm': 0.78515625, 'learning_rate': 3.771570763466548e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1388, 'grad_norm': 0.4921875, 'learning_rate': 3.7704345257201616e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2611, 'grad_norm': 0.625, 'learning_rate': 3.769298287973776e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1521, 'grad_norm': 0.8515625, 'learning_rate': 3.76816205022739e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2331, 'grad_norm': 0.73046875, 'learning_rate': 3.7670258124810036e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1549, 'grad_norm': 0.765625, 'learning_rate': 3.765889574734618e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2162, 'grad_norm': 0.44921875, 'learning_rate': 3.764753336988231e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1872, 'grad_norm': 0.5625, 'learning_rate': 3.763617099241846e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0603, 'grad_norm': 1.5, 'learning_rate': 3.7624808614954595e-05, 'epoch': 0.74}\n",
      "{'loss': 1.468, 'grad_norm': 0.49609375, 'learning_rate': 3.761344623749073e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1538, 'grad_norm': 0.66796875, 'learning_rate': 3.760208386002688e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1214, 'grad_norm': 0.70703125, 'learning_rate': 3.7590721482563015e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2752, 'grad_norm': 0.76171875, 'learning_rate': 3.757935910509915e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0411, 'grad_norm': 0.87109375, 'learning_rate': 3.756799672763529e-05, 'epoch': 0.74}\n",
      "{'loss': 1.4012, 'grad_norm': 0.8046875, 'learning_rate': 3.7556634350171435e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2429, 'grad_norm': 0.75, 'learning_rate': 3.7545271972707566e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1702, 'grad_norm': 0.58203125, 'learning_rate': 3.753390959524371e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1505, 'grad_norm': 0.71875, 'learning_rate': 3.752254721777985e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0332, 'grad_norm': 1.0390625, 'learning_rate': 3.751118484031599e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3946, 'grad_norm': 0.5234375, 'learning_rate': 3.749982246285213e-05, 'epoch': 0.74}\n",
      "{'loss': 1.203, 'grad_norm': 0.96484375, 'learning_rate': 3.748846008538827e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2128, 'grad_norm': 0.57421875, 'learning_rate': 3.747709770792441e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1976, 'grad_norm': 0.6171875, 'learning_rate': 3.746573533046055e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0337, 'grad_norm': 6.34375, 'learning_rate': 3.745437295299668e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4116, 'grad_norm': 0.494140625, 'learning_rate': 3.744301057553283e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1801, 'grad_norm': 0.89453125, 'learning_rate': 3.7431648198068965e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3251, 'grad_norm': 0.625, 'learning_rate': 3.74202858206051e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2871, 'grad_norm': 0.83984375, 'learning_rate': 3.740892344314125e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0355, 'grad_norm': 1.109375, 'learning_rate': 3.7397561065677386e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3323, 'grad_norm': 0.67578125, 'learning_rate': 3.738619868821352e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2878, 'grad_norm': 0.71484375, 'learning_rate': 3.737483631074966e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1697, 'grad_norm': 0.455078125, 'learning_rate': 3.7363473933285806e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1596, 'grad_norm': 1.09375, 'learning_rate': 3.735211155582194e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1056, 'grad_norm': 0.98046875, 'learning_rate': 3.734074917835808e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3495, 'grad_norm': 0.65234375, 'learning_rate': 3.732938680089422e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1598, 'grad_norm': 0.78125, 'learning_rate': 3.731802442343036e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1847, 'grad_norm': 0.4765625, 'learning_rate': 3.73066620459665e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3082, 'grad_norm': 0.5234375, 'learning_rate': 3.729529966850264e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0117, 'grad_norm': 0.80078125, 'learning_rate': 3.728393729103878e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3308, 'grad_norm': 0.59375, 'learning_rate': 3.727257491357492e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1639, 'grad_norm': 0.73828125, 'learning_rate': 3.726121253611105e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2056, 'grad_norm': 0.412109375, 'learning_rate': 3.72498501586472e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2759, 'grad_norm': 0.53125, 'learning_rate': 3.7238487781183336e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1235, 'grad_norm': 0.8125, 'learning_rate': 3.7227125403719474e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3366, 'grad_norm': 0.50390625, 'learning_rate': 3.721576302625562e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3046, 'grad_norm': 0.68359375, 'learning_rate': 3.7204400648791756e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2565, 'grad_norm': 0.474609375, 'learning_rate': 3.7193038271327894e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2186, 'grad_norm': 0.53125, 'learning_rate': 3.718167589386403e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0822, 'grad_norm': 0.71484375, 'learning_rate': 3.7170313516400176e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4091, 'grad_norm': 0.52734375, 'learning_rate': 3.7158951138936314e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1195, 'grad_norm': 0.75390625, 'learning_rate': 3.714758876147245e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0902, 'grad_norm': 0.4140625, 'learning_rate': 3.713622638400859e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2694, 'grad_norm': 0.66015625, 'learning_rate': 3.712486400654473e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0419, 'grad_norm': 1.3125, 'learning_rate': 3.711350162908087e-05, 'epoch': 0.74}\n",
      "{'loss': 1.5013, 'grad_norm': 0.63671875, 'learning_rate': 3.710213925161701e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2039, 'grad_norm': 0.83984375, 'learning_rate': 3.709077687415315e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1987, 'grad_norm': 0.609375, 'learning_rate': 3.707941449668929e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2515, 'grad_norm': 0.53515625, 'learning_rate': 3.7068052119225424e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0603, 'grad_norm': 1.2734375, 'learning_rate': 3.705668974176157e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2833, 'grad_norm': 0.609375, 'learning_rate': 3.7045327364297706e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2442, 'grad_norm': 0.6875, 'learning_rate': 3.7033964986833844e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2375, 'grad_norm': 0.427734375, 'learning_rate': 3.702260260936999e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2489, 'grad_norm': 0.62109375, 'learning_rate': 3.701124023190613e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0573, 'grad_norm': 0.5703125, 'learning_rate': 3.6999877854442265e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2215, 'grad_norm': 0.5625, 'learning_rate': 3.69885154769784e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0742, 'grad_norm': 0.75, 'learning_rate': 3.697715309951455e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1589, 'grad_norm': 0.470703125, 'learning_rate': 3.6965790722050685e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3189, 'grad_norm': 0.734375, 'learning_rate': 3.695442834458682e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1111, 'grad_norm': 0.953125, 'learning_rate': 3.694306596712296e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3094, 'grad_norm': 0.50390625, 'learning_rate': 3.69317035896591e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0904, 'grad_norm': 0.6171875, 'learning_rate': 3.692034121219524e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1836, 'grad_norm': 0.45703125, 'learning_rate': 3.690897883473138e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1009, 'grad_norm': 0.74609375, 'learning_rate': 3.689761645726752e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2005, 'grad_norm': 1.4453125, 'learning_rate': 3.6886254079803664e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2509, 'grad_norm': 0.6328125, 'learning_rate': 3.6874891702339795e-05, 'epoch': 0.74}\n",
      "{'loss': 1.201, 'grad_norm': 0.7109375, 'learning_rate': 3.686352932487594e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1371, 'grad_norm': 0.4296875, 'learning_rate': 3.685216694741208e-05, 'epoch': 0.74}\n",
      "{'loss': 1.354, 'grad_norm': 0.68359375, 'learning_rate': 3.6840804569948215e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0579, 'grad_norm': 1.0625, 'learning_rate': 3.682944219248436e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2983, 'grad_norm': 0.48046875, 'learning_rate': 3.68180798150205e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0853, 'grad_norm': 1.6484375, 'learning_rate': 3.6806717437556635e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1867, 'grad_norm': 0.40625, 'learning_rate': 3.679535506009277e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2374, 'grad_norm': 0.6484375, 'learning_rate': 3.678399268262892e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0825, 'grad_norm': 0.77734375, 'learning_rate': 3.6772630305165056e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2813, 'grad_norm': 0.63671875, 'learning_rate': 3.6761267927701193e-05, 'epoch': 0.74}\n",
      "{'loss': 1.157, 'grad_norm': 1.1171875, 'learning_rate': 3.674990555023733e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1251, 'grad_norm': 0.83984375, 'learning_rate': 3.673854317277347e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2128, 'grad_norm': 0.5625, 'learning_rate': 3.6727180795309614e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0269, 'grad_norm': 0.6953125, 'learning_rate': 3.671581841784575e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1924, 'grad_norm': 0.75, 'learning_rate': 3.670445604038189e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2367, 'grad_norm': 0.90234375, 'learning_rate': 3.6693093662918034e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1873, 'grad_norm': 0.62890625, 'learning_rate': 3.6681731285454165e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1896, 'grad_norm': 0.7421875, 'learning_rate': 3.667036890799031e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0324, 'grad_norm': 0.65625, 'learning_rate': 3.6659006530526454e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2976, 'grad_norm': 0.59375, 'learning_rate': 3.6647644153062586e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1079, 'grad_norm': 0.5625, 'learning_rate': 3.663628177559873e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1161, 'grad_norm': 0.64453125, 'learning_rate': 3.662491939813487e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1067, 'grad_norm': 0.70703125, 'learning_rate': 3.6613557020671006e-05, 'epoch': 0.74}\n",
      "{'loss': 0.9373, 'grad_norm': 1.0546875, 'learning_rate': 3.6602194643207144e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2504, 'grad_norm': 0.458984375, 'learning_rate': 3.659083226574329e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2047, 'grad_norm': 0.75390625, 'learning_rate': 3.6579469888279426e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1507, 'grad_norm': 0.443359375, 'learning_rate': 3.6568107510815564e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0868, 'grad_norm': 0.7734375, 'learning_rate': 3.65567451333517e-05, 'epoch': 0.74}\n",
      "{'loss': 0.977, 'grad_norm': 0.7578125, 'learning_rate': 3.654538275588784e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3919, 'grad_norm': 0.6484375, 'learning_rate': 3.6534020378423984e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1362, 'grad_norm': 0.65625, 'learning_rate': 3.652265800096012e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2773, 'grad_norm': 0.43359375, 'learning_rate': 3.651129562349626e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3062, 'grad_norm': 0.765625, 'learning_rate': 3.6499933246032405e-05, 'epoch': 0.74}\n",
      "{'loss': 1.093, 'grad_norm': 1.4453125, 'learning_rate': 3.6488570868568536e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3426, 'grad_norm': 0.609375, 'learning_rate': 3.647720849110468e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1268, 'grad_norm': 0.72265625, 'learning_rate': 3.6465846113640825e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3051, 'grad_norm': 0.482421875, 'learning_rate': 3.6454483736176956e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2756, 'grad_norm': 0.60546875, 'learning_rate': 3.64431213587131e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0923, 'grad_norm': 1.0, 'learning_rate': 3.643175898124924e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3231, 'grad_norm': 0.6484375, 'learning_rate': 3.6420396603785377e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1235, 'grad_norm': 0.84375, 'learning_rate': 3.6409034226321514e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2279, 'grad_norm': 0.60546875, 'learning_rate': 3.639767184885766e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1371, 'grad_norm': 0.6640625, 'learning_rate': 3.63863094713938e-05, 'epoch': 0.74}\n",
      "{'loss': 1.132, 'grad_norm': 0.96875, 'learning_rate': 3.6374947093929935e-05, 'epoch': 0.74}\n",
      "{'loss': 1.4334, 'grad_norm': 0.52734375, 'learning_rate': 3.636358471646607e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1492, 'grad_norm': 0.76953125, 'learning_rate': 3.635222233900221e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3343, 'grad_norm': 0.50390625, 'learning_rate': 3.6340859961538355e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2514, 'grad_norm': 0.6484375, 'learning_rate': 3.632949758407449e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2562, 'grad_norm': 1.1640625, 'learning_rate': 3.631813520661063e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4155, 'grad_norm': 0.515625, 'learning_rate': 3.6306772829146775e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0533, 'grad_norm': 0.8125, 'learning_rate': 3.6295410451682906e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1851, 'grad_norm': 0.640625, 'learning_rate': 3.628404807421905e-05, 'epoch': 0.74}\n",
      "{'loss': 1.198, 'grad_norm': 0.7109375, 'learning_rate': 3.6272685696755196e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0764, 'grad_norm': 1.015625, 'learning_rate': 3.626132331929133e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3909, 'grad_norm': 0.6484375, 'learning_rate': 3.624996094182747e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3159, 'grad_norm': 0.640625, 'learning_rate': 3.623859856436361e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3087, 'grad_norm': 0.5, 'learning_rate': 3.622723618689975e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2139, 'grad_norm': 1.0703125, 'learning_rate': 3.621587380943589e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0233, 'grad_norm': 0.85546875, 'learning_rate': 3.620451143197203e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2895, 'grad_norm': 0.78125, 'learning_rate': 3.619314905450817e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1403, 'grad_norm': 0.734375, 'learning_rate': 3.6181786677044305e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0732, 'grad_norm': 0.61328125, 'learning_rate': 3.617042429958044e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3432, 'grad_norm': 0.71484375, 'learning_rate': 3.615906192211658e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0524, 'grad_norm': 0.87890625, 'learning_rate': 3.6147699544652726e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3956, 'grad_norm': 0.62890625, 'learning_rate': 3.6136337167188864e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0997, 'grad_norm': 0.6796875, 'learning_rate': 3.6124974789725e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1662, 'grad_norm': 0.47265625, 'learning_rate': 3.6113612412261146e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2164, 'grad_norm': 0.52734375, 'learning_rate': 3.610225003479728e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0315, 'grad_norm': 0.70703125, 'learning_rate': 3.609088765733342e-05, 'epoch': 0.74}\n",
      "{'loss': 1.279, 'grad_norm': 0.5625, 'learning_rate': 3.6079525279869566e-05, 'epoch': 0.74}\n",
      "{'loss': 1.208, 'grad_norm': 0.671875, 'learning_rate': 3.60681629024057e-05, 'epoch': 0.74}\n",
      "{'loss': 1.148, 'grad_norm': 0.427734375, 'learning_rate': 3.605680052494184e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2398, 'grad_norm': 0.83984375, 'learning_rate': 3.604543814747798e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0791, 'grad_norm': 0.92578125, 'learning_rate': 3.603407577001412e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5051, 'grad_norm': 1.0390625, 'learning_rate': 3.602271339255026e-05, 'epoch': 0.74}\n",
      "{'loss': 1.229, 'grad_norm': 0.62890625, 'learning_rate': 3.60113510150864e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2778, 'grad_norm': 0.55078125, 'learning_rate': 3.599998863762254e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2654, 'grad_norm': 0.578125, 'learning_rate': 3.5988626260158676e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0742, 'grad_norm': 1.03125, 'learning_rate': 3.5977263882694814e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2522, 'grad_norm': 0.609375, 'learning_rate': 3.596590150523096e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0576, 'grad_norm': 0.58984375, 'learning_rate': 3.5954539127767096e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0935, 'grad_norm': 0.4375, 'learning_rate': 3.5943176750303234e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1799, 'grad_norm': 0.53125, 'learning_rate': 3.593181437283937e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0202, 'grad_norm': 0.73046875, 'learning_rate': 3.592045199537552e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3142, 'grad_norm': 0.5078125, 'learning_rate': 3.590908961791165e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0482, 'grad_norm': 0.7265625, 'learning_rate': 3.589772724044779e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1429, 'grad_norm': 0.486328125, 'learning_rate': 3.588636486298394e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2468, 'grad_norm': 0.7421875, 'learning_rate': 3.587500248552007e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2078, 'grad_norm': 0.859375, 'learning_rate': 3.586364010805621e-05, 'epoch': 0.74}\n",
      "{'loss': 1.4305, 'grad_norm': 0.52734375, 'learning_rate': 3.585227773059235e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1226, 'grad_norm': 0.67578125, 'learning_rate': 3.584091535312849e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2871, 'grad_norm': 0.462890625, 'learning_rate': 3.582955297566463e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2453, 'grad_norm': 0.57421875, 'learning_rate': 3.581819059820077e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0033, 'grad_norm': 0.90234375, 'learning_rate': 3.580682822073691e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3195, 'grad_norm': 0.65234375, 'learning_rate': 3.5795465843273047e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2808, 'grad_norm': 0.84375, 'learning_rate': 3.5784103465809184e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0744, 'grad_norm': 0.453125, 'learning_rate': 3.577274108834533e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2688, 'grad_norm': 0.67578125, 'learning_rate': 3.576137871088147e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0863, 'grad_norm': 1.3046875, 'learning_rate': 3.5750016333417605e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.398, 'grad_norm': 0.69921875, 'learning_rate': 3.573865395595374e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1997, 'grad_norm': 0.72265625, 'learning_rate': 3.572729157848989e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1625, 'grad_norm': 0.41796875, 'learning_rate': 3.5715929201026025e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2447, 'grad_norm': 0.5390625, 'learning_rate': 3.570456682356216e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0399, 'grad_norm': 0.94140625, 'learning_rate': 3.569320444609831e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2555, 'grad_norm': 0.6484375, 'learning_rate': 3.568184206863444e-05, 'epoch': 0.74}\n",
      "{'loss': 1.149, 'grad_norm': 1.1640625, 'learning_rate': 3.567047969117058e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1554, 'grad_norm': 0.5546875, 'learning_rate': 3.565911731370672e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2713, 'grad_norm': 0.7265625, 'learning_rate': 3.564775493624286e-05, 'epoch': 0.74}\n",
      "{'loss': 0.9897, 'grad_norm': 0.9296875, 'learning_rate': 3.5636392558779004e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2809, 'grad_norm': 0.55078125, 'learning_rate': 3.562503018131514e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1307, 'grad_norm': 0.67578125, 'learning_rate': 3.561366780385128e-05, 'epoch': 0.74}\n",
      "{'loss': 1.165, 'grad_norm': 0.453125, 'learning_rate': 3.560230542638742e-05, 'epoch': 0.74}\n",
      "{'loss': 1.298, 'grad_norm': 0.5625, 'learning_rate': 3.5590943048923555e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1449, 'grad_norm': 1.890625, 'learning_rate': 3.55795806714597e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2673, 'grad_norm': 0.54296875, 'learning_rate': 3.556821829399584e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2255, 'grad_norm': 0.7578125, 'learning_rate': 3.5556855916531975e-05, 'epoch': 0.74}\n",
      "{'loss': 1.2772, 'grad_norm': 0.53125, 'learning_rate': 3.554549353906811e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1577, 'grad_norm': 0.5625, 'learning_rate': 3.553413116160426e-05, 'epoch': 0.74}\n",
      "{'loss': 0.9954, 'grad_norm': 0.8671875, 'learning_rate': 3.5522768784140396e-05, 'epoch': 0.74}\n",
      "{'loss': 1.3102, 'grad_norm': 0.9375, 'learning_rate': 3.5511406406676534e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1648, 'grad_norm': 0.62890625, 'learning_rate': 3.550004402921268e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1249, 'grad_norm': 0.6328125, 'learning_rate': 3.548868165174881e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2741, 'grad_norm': 0.5078125, 'learning_rate': 3.5477319274284954e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1497, 'grad_norm': 0.63671875, 'learning_rate': 3.546595689682109e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2058, 'grad_norm': 0.53515625, 'learning_rate': 3.545459451935723e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1979, 'grad_norm': 0.58984375, 'learning_rate': 3.5443232141893374e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0432, 'grad_norm': 0.50390625, 'learning_rate': 3.543186976442951e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1943, 'grad_norm': 0.546875, 'learning_rate': 3.542050738696565e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0487, 'grad_norm': 0.89453125, 'learning_rate': 3.540914500950179e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3124, 'grad_norm': 0.55078125, 'learning_rate': 3.5397782632037926e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1076, 'grad_norm': 0.81640625, 'learning_rate': 3.538642025457407e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2253, 'grad_norm': 0.53515625, 'learning_rate': 3.537505787711021e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1213, 'grad_norm': 0.54296875, 'learning_rate': 3.5363695499646346e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0539, 'grad_norm': 0.8125, 'learning_rate': 3.5352333122182484e-05, 'epoch': 0.75}\n",
      "{'loss': 1.268, 'grad_norm': 0.58203125, 'learning_rate': 3.534097074471863e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0785, 'grad_norm': 0.91015625, 'learning_rate': 3.5329608367254766e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1844, 'grad_norm': 0.427734375, 'learning_rate': 3.5318245989790904e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2415, 'grad_norm': 0.455078125, 'learning_rate': 3.530688361232705e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1328, 'grad_norm': 1.5390625, 'learning_rate': 3.529552123486318e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4376, 'grad_norm': 0.5625, 'learning_rate': 3.5284158857399325e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1536, 'grad_norm': 0.7109375, 'learning_rate': 3.527279647993547e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1615, 'grad_norm': 0.451171875, 'learning_rate': 3.52614341024716e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1713, 'grad_norm': 0.796875, 'learning_rate': 3.5250071725007745e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0833, 'grad_norm': 0.75390625, 'learning_rate': 3.523870934754388e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2592, 'grad_norm': 0.61328125, 'learning_rate': 3.522734697008002e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2234, 'grad_norm': 0.91015625, 'learning_rate': 3.521598459261616e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2334, 'grad_norm': 0.482421875, 'learning_rate': 3.52046222151523e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1607, 'grad_norm': 0.68359375, 'learning_rate': 3.519325983768844e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1462, 'grad_norm': 1.1171875, 'learning_rate': 3.518189746022458e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.383, 'grad_norm': 0.490234375, 'learning_rate': 3.517053508276072e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2505, 'grad_norm': 0.88671875, 'learning_rate': 3.5159172705296855e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3052, 'grad_norm': 0.490234375, 'learning_rate': 3.5147810327833e-05, 'epoch': 0.75}\n",
      "{'loss': 1.202, 'grad_norm': 0.90234375, 'learning_rate': 3.513644795036914e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0967, 'grad_norm': 0.8515625, 'learning_rate': 3.5125085572905275e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3968, 'grad_norm': 0.53125, 'learning_rate': 3.511372319544142e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0801, 'grad_norm': 0.8046875, 'learning_rate': 3.510236081797755e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1634, 'grad_norm': 0.5, 'learning_rate': 3.5090998440513695e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2657, 'grad_norm': 0.60546875, 'learning_rate': 3.507963606304984e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9669, 'grad_norm': 0.5390625, 'learning_rate': 3.506827368558597e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3332, 'grad_norm': 0.5546875, 'learning_rate': 3.5056911308122116e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1622, 'grad_norm': 1.4609375, 'learning_rate': 3.5045548930658253e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1909, 'grad_norm': 0.546875, 'learning_rate': 3.503418655319439e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2441, 'grad_norm': 0.66015625, 'learning_rate': 3.5022824175730536e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1295, 'grad_norm': 0.8984375, 'learning_rate': 3.5011461798266674e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3246, 'grad_norm': 0.53515625, 'learning_rate': 3.500009942080281e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1477, 'grad_norm': 0.6796875, 'learning_rate': 3.498873704333895e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0188, 'grad_norm': 0.53125, 'learning_rate': 3.497737466587509e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2118, 'grad_norm': 0.58984375, 'learning_rate': 3.4966012288411225e-05, 'epoch': 0.75}\n",
      "{'loss': 1.091, 'grad_norm': 1.125, 'learning_rate': 3.495464991094737e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4583, 'grad_norm': 0.66796875, 'learning_rate': 3.494328753348351e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2825, 'grad_norm': 0.80859375, 'learning_rate': 3.4931925156019646e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3309, 'grad_norm': 0.498046875, 'learning_rate': 3.492056277855579e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1962, 'grad_norm': 0.54296875, 'learning_rate': 3.490920040109192e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0875, 'grad_norm': 0.71484375, 'learning_rate': 3.4897838023628066e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2471, 'grad_norm': 0.640625, 'learning_rate': 3.488647564616421e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1512, 'grad_norm': 1.0, 'learning_rate': 3.487511326870034e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1463, 'grad_norm': 0.50390625, 'learning_rate': 3.4863750891236486e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1584, 'grad_norm': 0.5390625, 'learning_rate': 3.4852388513772624e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1165, 'grad_norm': 1.34375, 'learning_rate': 3.484102613630876e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4046, 'grad_norm': 0.7578125, 'learning_rate': 3.4829663758844907e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2423, 'grad_norm': 0.94140625, 'learning_rate': 3.4818301381381044e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1832, 'grad_norm': 0.6171875, 'learning_rate': 3.480693900391718e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1578, 'grad_norm': 0.63671875, 'learning_rate': 3.479557662645332e-05, 'epoch': 0.75}\n",
      "{'loss': 0.979, 'grad_norm': 0.7890625, 'learning_rate': 3.478421424898946e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3112, 'grad_norm': 0.5859375, 'learning_rate': 3.47728518715256e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1143, 'grad_norm': 0.86328125, 'learning_rate': 3.476148949406174e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1123, 'grad_norm': 0.478515625, 'learning_rate': 3.475012711659788e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2952, 'grad_norm': 0.68359375, 'learning_rate': 3.4738764739134016e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9303, 'grad_norm': 0.76953125, 'learning_rate': 3.472740236167016e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3968, 'grad_norm': 0.53515625, 'learning_rate': 3.471603998420629e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0711, 'grad_norm': 0.9140625, 'learning_rate': 3.4704677606742436e-05, 'epoch': 0.75}\n",
      "{'loss': 1.251, 'grad_norm': 0.443359375, 'learning_rate': 3.469331522927858e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1192, 'grad_norm': 0.91015625, 'learning_rate': 3.468195285181471e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0575, 'grad_norm': 0.7734375, 'learning_rate': 3.467059047435086e-05, 'epoch': 0.75}\n",
      "{'loss': 1.293, 'grad_norm': 0.7109375, 'learning_rate': 3.4659228096886995e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2137, 'grad_norm': 0.6484375, 'learning_rate': 3.464786571942313e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2395, 'grad_norm': 0.5546875, 'learning_rate': 3.463650334195928e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2218, 'grad_norm': 0.59765625, 'learning_rate': 3.4625140964495415e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0185, 'grad_norm': 1.21875, 'learning_rate': 3.461377858703155e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4302, 'grad_norm': 0.421875, 'learning_rate': 3.460241620956769e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2721, 'grad_norm': 0.6328125, 'learning_rate': 3.459105383210383e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2144, 'grad_norm': 0.478515625, 'learning_rate': 3.457969145463997e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2411, 'grad_norm': 1.015625, 'learning_rate': 3.456832907717611e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9698, 'grad_norm': 0.53515625, 'learning_rate': 3.455696669971225e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2885, 'grad_norm': 0.6328125, 'learning_rate': 3.454560432224839e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1536, 'grad_norm': 0.7578125, 'learning_rate': 3.453424194478453e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0863, 'grad_norm': 0.431640625, 'learning_rate': 3.452287956732066e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2141, 'grad_norm': 0.54296875, 'learning_rate': 3.451151718985681e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0198, 'grad_norm': 0.48046875, 'learning_rate': 3.450015481239295e-05, 'epoch': 0.75}\n",
      "{'loss': 1.351, 'grad_norm': 0.55078125, 'learning_rate': 3.448879243492908e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1046, 'grad_norm': 0.63671875, 'learning_rate': 3.447743005746523e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1228, 'grad_norm': 0.48046875, 'learning_rate': 3.4466067680001365e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2281, 'grad_norm': 0.5703125, 'learning_rate': 3.44547053025375e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0092, 'grad_norm': 1.1640625, 'learning_rate': 3.444334292507365e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4253, 'grad_norm': 0.431640625, 'learning_rate': 3.4431980547609786e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1135, 'grad_norm': 0.5625, 'learning_rate': 3.4420618170145923e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1576, 'grad_norm': 0.6953125, 'learning_rate': 3.440925579268206e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2641, 'grad_norm': 0.60546875, 'learning_rate': 3.43978934152182e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1244, 'grad_norm': 1.03125, 'learning_rate': 3.4386531037754344e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4538, 'grad_norm': 0.431640625, 'learning_rate': 3.437516866029048e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1489, 'grad_norm': 0.7109375, 'learning_rate': 3.436380628282662e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2375, 'grad_norm': 0.5625, 'learning_rate': 3.435244390536276e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3051, 'grad_norm': 0.7109375, 'learning_rate': 3.43410815278989e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9444, 'grad_norm': 0.703125, 'learning_rate': 3.432971915043504e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2721, 'grad_norm': 0.46484375, 'learning_rate': 3.431835677297118e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2517, 'grad_norm': 0.66796875, 'learning_rate': 3.430699439550732e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3837, 'grad_norm': 0.478515625, 'learning_rate': 3.4295632018043453e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1659, 'grad_norm': 0.7265625, 'learning_rate': 3.42842696405796e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0589, 'grad_norm': 0.57421875, 'learning_rate': 3.4272907263115736e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2937, 'grad_norm': 0.6015625, 'learning_rate': 3.4261544885651874e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2239, 'grad_norm': 0.70703125, 'learning_rate': 3.425018250818802e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2477, 'grad_norm': 0.4296875, 'learning_rate': 3.4238820130724156e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2482, 'grad_norm': 0.65234375, 'learning_rate': 3.4227457753260294e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0206, 'grad_norm': 1.1953125, 'learning_rate': 3.421609537579643e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3013, 'grad_norm': 0.53515625, 'learning_rate': 3.420473299833257e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1042, 'grad_norm': 0.67578125, 'learning_rate': 3.4193370620868714e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3082, 'grad_norm': 0.4921875, 'learning_rate': 3.418200824340485e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2378, 'grad_norm': 0.6171875, 'learning_rate': 3.417064586594099e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1042, 'grad_norm': 1.1484375, 'learning_rate': 3.415928348847713e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2221, 'grad_norm': 0.640625, 'learning_rate': 3.414792111101327e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0601, 'grad_norm': 0.6640625, 'learning_rate': 3.413655873354941e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2064, 'grad_norm': 0.5546875, 'learning_rate': 3.412519635608555e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2213, 'grad_norm': 0.5625, 'learning_rate': 3.411383397862169e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9236, 'grad_norm': 1.015625, 'learning_rate': 3.4102471601157824e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2863, 'grad_norm': 0.66015625, 'learning_rate': 3.409110922369397e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1049, 'grad_norm': 0.6484375, 'learning_rate': 3.4079746846230107e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2385, 'grad_norm': 0.478515625, 'learning_rate': 3.4068384468766244e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2095, 'grad_norm': 0.640625, 'learning_rate': 3.405702209130239e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1016, 'grad_norm': 0.5, 'learning_rate': 3.404565971383853e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3119, 'grad_norm': 0.515625, 'learning_rate': 3.4034297336374665e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0981, 'grad_norm': 0.64453125, 'learning_rate': 3.40229349589108e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1031, 'grad_norm': 0.578125, 'learning_rate': 3.401157258144694e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2054, 'grad_norm': 0.74609375, 'learning_rate': 3.4000210203983085e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1055, 'grad_norm': 1.0, 'learning_rate': 3.398884782651922e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3329, 'grad_norm': 0.5625, 'learning_rate': 3.397748544905536e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2858, 'grad_norm': 0.69140625, 'learning_rate': 3.39661230715915e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2753, 'grad_norm': 0.46875, 'learning_rate': 3.395476069412764e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3102, 'grad_norm': 0.82421875, 'learning_rate': 3.394339831666378e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9701, 'grad_norm': 1.015625, 'learning_rate': 3.393203593919992e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2864, 'grad_norm': 0.52734375, 'learning_rate': 3.3920673561736064e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0745, 'grad_norm': 0.73046875, 'learning_rate': 3.3909311184272195e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2994, 'grad_norm': 0.416015625, 'learning_rate': 3.389794880680834e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1687, 'grad_norm': 0.73046875, 'learning_rate': 3.388658642934448e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0352, 'grad_norm': 1.0, 'learning_rate': 3.3875224051880615e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4475, 'grad_norm': 0.50390625, 'learning_rate': 3.386386167441676e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1809, 'grad_norm': 0.6875, 'learning_rate': 3.38524992969529e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1546, 'grad_norm': 0.61328125, 'learning_rate': 3.3841136919489035e-05, 'epoch': 0.75}\n",
      "{'loss': 1.125, 'grad_norm': 0.640625, 'learning_rate': 3.382977454202518e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0632, 'grad_norm': 0.92578125, 'learning_rate': 3.381841216456131e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3849, 'grad_norm': 0.474609375, 'learning_rate': 3.3807049787097456e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2392, 'grad_norm': 0.7265625, 'learning_rate': 3.3795687409633594e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1891, 'grad_norm': 0.5234375, 'learning_rate': 3.378432503216973e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1322, 'grad_norm': 0.57421875, 'learning_rate': 3.377296265470587e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9463, 'grad_norm': 1.3125, 'learning_rate': 3.3761600277242014e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.337, 'grad_norm': 0.56640625, 'learning_rate': 3.375023789977815e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1664, 'grad_norm': 0.78125, 'learning_rate': 3.373887552231429e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0758, 'grad_norm': 0.455078125, 'learning_rate': 3.3727513144850434e-05, 'epoch': 0.75}\n",
      "{'loss': 1.245, 'grad_norm': 0.75390625, 'learning_rate': 3.3716150767386565e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9619, 'grad_norm': 0.87890625, 'learning_rate': 3.370478838992271e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2429, 'grad_norm': 0.578125, 'learning_rate': 3.369342601245885e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1766, 'grad_norm': 0.88671875, 'learning_rate': 3.3682063634994986e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2141, 'grad_norm': 0.490234375, 'learning_rate': 3.367070125753113e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3156, 'grad_norm': 0.6015625, 'learning_rate': 3.365933888006727e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1362, 'grad_norm': 0.8828125, 'learning_rate': 3.3647976502603406e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4763, 'grad_norm': 0.5, 'learning_rate': 3.363661412513955e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1856, 'grad_norm': 1.6484375, 'learning_rate': 3.362525174767568e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2654, 'grad_norm': 0.5390625, 'learning_rate': 3.3613889370211826e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1101, 'grad_norm': 0.486328125, 'learning_rate': 3.3602526992747964e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0295, 'grad_norm': 1.0703125, 'learning_rate': 3.35911646152841e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4035, 'grad_norm': 0.412109375, 'learning_rate': 3.357980223782024e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1988, 'grad_norm': 0.7265625, 'learning_rate': 3.3568439860356385e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1639, 'grad_norm': 0.5078125, 'learning_rate': 3.355707748289252e-05, 'epoch': 0.75}\n",
      "{'loss': 1.195, 'grad_norm': 0.88671875, 'learning_rate': 3.354571510542866e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0804, 'grad_norm': 0.859375, 'learning_rate': 3.3534352727964805e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3035, 'grad_norm': 0.55078125, 'learning_rate': 3.3522990350500936e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1048, 'grad_norm': 0.7734375, 'learning_rate': 3.351162797303708e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0946, 'grad_norm': 0.6015625, 'learning_rate': 3.350026559557322e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2182, 'grad_norm': 0.6015625, 'learning_rate': 3.3488903218109356e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1637, 'grad_norm': 1.046875, 'learning_rate': 3.34775408406455e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1533, 'grad_norm': 0.578125, 'learning_rate': 3.346617846318164e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0882, 'grad_norm': 0.66796875, 'learning_rate': 3.345481608571778e-05, 'epoch': 0.75}\n",
      "{'loss': 1.331, 'grad_norm': 0.61328125, 'learning_rate': 3.344345370825392e-05, 'epoch': 0.75}\n",
      "{'loss': 1.236, 'grad_norm': 0.55859375, 'learning_rate': 3.343209133079005e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0177, 'grad_norm': 1.0390625, 'learning_rate': 3.34207289533262e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2975, 'grad_norm': 0.5234375, 'learning_rate': 3.3409366575862335e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0713, 'grad_norm': 1.015625, 'learning_rate': 3.339800419839847e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1342, 'grad_norm': 0.462890625, 'learning_rate': 3.338664182093462e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2729, 'grad_norm': 0.5234375, 'learning_rate': 3.3375279443470755e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0595, 'grad_norm': 0.478515625, 'learning_rate': 3.336391706600689e-05, 'epoch': 0.75}\n",
      "{'loss': 1.39, 'grad_norm': 0.62890625, 'learning_rate': 3.335255468854303e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9151, 'grad_norm': 0.890625, 'learning_rate': 3.3341192311079176e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2144, 'grad_norm': 0.55859375, 'learning_rate': 3.3329829933615307e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3321, 'grad_norm': 0.51171875, 'learning_rate': 3.331846755615145e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0072, 'grad_norm': 0.75390625, 'learning_rate': 3.330710517868759e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4975, 'grad_norm': 0.5078125, 'learning_rate': 3.329574280122373e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1688, 'grad_norm': 0.482421875, 'learning_rate': 3.328438042375987e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3226, 'grad_norm': 0.466796875, 'learning_rate': 3.327301804629601e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2803, 'grad_norm': 0.5703125, 'learning_rate': 3.326165566883215e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1508, 'grad_norm': 0.76953125, 'learning_rate': 3.325029329136829e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2845, 'grad_norm': 0.59765625, 'learning_rate': 3.323893091390442e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0651, 'grad_norm': 0.63671875, 'learning_rate': 3.322756853644057e-05, 'epoch': 0.75}\n",
      "{'loss': 1.1644, 'grad_norm': 0.46484375, 'learning_rate': 3.3216206158976705e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1031, 'grad_norm': 0.62890625, 'learning_rate': 3.320484378151284e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1567, 'grad_norm': 1.0234375, 'learning_rate': 3.319348140404899e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4286, 'grad_norm': 0.57421875, 'learning_rate': 3.3182119026585126e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1723, 'grad_norm': 0.6796875, 'learning_rate': 3.3170756649121264e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1444, 'grad_norm': 0.5703125, 'learning_rate': 3.31593942716574e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2707, 'grad_norm': 0.53515625, 'learning_rate': 3.3148031894193546e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1448, 'grad_norm': 1.1640625, 'learning_rate': 3.3136669516729684e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3154, 'grad_norm': 0.63671875, 'learning_rate': 3.312530713926582e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2156, 'grad_norm': 0.74609375, 'learning_rate': 3.311394476180196e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2796, 'grad_norm': 0.546875, 'learning_rate': 3.31025823843381e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1924, 'grad_norm': 0.58984375, 'learning_rate': 3.309122000687424e-05, 'epoch': 0.76}\n",
      "{'loss': 1.063, 'grad_norm': 0.7109375, 'learning_rate': 3.307985762941038e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3104, 'grad_norm': 0.62109375, 'learning_rate': 3.306849525194652e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1232, 'grad_norm': 0.73828125, 'learning_rate': 3.305713287448266e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3105, 'grad_norm': 0.50390625, 'learning_rate': 3.3045770497018794e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2557, 'grad_norm': 0.6953125, 'learning_rate': 3.303440811955494e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9731, 'grad_norm': 0.73828125, 'learning_rate': 3.3023045742091076e-05, 'epoch': 0.76}\n",
      "{'loss': 1.411, 'grad_norm': 0.53125, 'learning_rate': 3.3011683364627214e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1563, 'grad_norm': 0.71484375, 'learning_rate': 3.300032098716336e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1948, 'grad_norm': 0.486328125, 'learning_rate': 3.2988958609699496e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1839, 'grad_norm': 0.75, 'learning_rate': 3.2977596232235634e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0882, 'grad_norm': 0.8828125, 'learning_rate': 3.296623385477177e-05, 'epoch': 0.76}\n",
      "{'loss': 1.4294, 'grad_norm': 0.62109375, 'learning_rate': 3.295487147730792e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1051, 'grad_norm': 0.7890625, 'learning_rate': 3.2943509099844055e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0554, 'grad_norm': 0.60546875, 'learning_rate': 3.293214672238019e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3121, 'grad_norm': 0.65625, 'learning_rate': 3.292078434491633e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0099, 'grad_norm': 1.5234375, 'learning_rate': 3.290942196745247e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2684, 'grad_norm': 0.486328125, 'learning_rate': 3.289805958998861e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2039, 'grad_norm': 0.9609375, 'learning_rate': 3.288669721252475e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2138, 'grad_norm': 0.6484375, 'learning_rate': 3.287533483506089e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3463, 'grad_norm': 1.0859375, 'learning_rate': 3.286397245759703e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9791, 'grad_norm': 0.85546875, 'learning_rate': 3.2852610080133164e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3291, 'grad_norm': 0.52734375, 'learning_rate': 3.284124770266931e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1198, 'grad_norm': 1.328125, 'learning_rate': 3.282988532520545e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1625, 'grad_norm': 0.53515625, 'learning_rate': 3.2818522947741585e-05, 'epoch': 0.76}\n",
      "{'loss': 1.183, 'grad_norm': 0.92578125, 'learning_rate': 3.280716057027773e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0292, 'grad_norm': 0.52734375, 'learning_rate': 3.279579819281387e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2826, 'grad_norm': 0.59375, 'learning_rate': 3.2784435815350005e-05, 'epoch': 0.76}\n",
      "{'loss': 1.203, 'grad_norm': 0.70703125, 'learning_rate': 3.277307343788614e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1411, 'grad_norm': 0.5625, 'learning_rate': 3.276171106042229e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3274, 'grad_norm': 0.5546875, 'learning_rate': 3.2750348682958425e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9823, 'grad_norm': 0.484375, 'learning_rate': 3.273898630549456e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3812, 'grad_norm': 0.5546875, 'learning_rate': 3.27276239280307e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2013, 'grad_norm': 0.98828125, 'learning_rate': 3.271626155056684e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1564, 'grad_norm': 0.53125, 'learning_rate': 3.2704899173102983e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1589, 'grad_norm': 0.58984375, 'learning_rate': 3.269353679563912e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0729, 'grad_norm': 0.5546875, 'learning_rate': 3.268217441817526e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3999, 'grad_norm': 0.5859375, 'learning_rate': 3.2670812040711404e-05, 'epoch': 0.76}\n",
      "{'loss': 1.179, 'grad_norm': 0.74609375, 'learning_rate': 3.2659449663247535e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0748, 'grad_norm': 0.39453125, 'learning_rate': 3.264808728578368e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2205, 'grad_norm': 0.6015625, 'learning_rate': 3.263672490831982e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0322, 'grad_norm': 0.83984375, 'learning_rate': 3.2625362530855955e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3524, 'grad_norm': 0.53515625, 'learning_rate': 3.26140001533921e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1647, 'grad_norm': 0.625, 'learning_rate': 3.260263777592824e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1881, 'grad_norm': 0.55078125, 'learning_rate': 3.2591275398464376e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2593, 'grad_norm': 0.474609375, 'learning_rate': 3.2579913021000513e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1256, 'grad_norm': 1.296875, 'learning_rate': 3.256855064353666e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3874, 'grad_norm': 0.46484375, 'learning_rate': 3.2557188266072796e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1488, 'grad_norm': 0.97265625, 'learning_rate': 3.2545825888608934e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1197, 'grad_norm': 0.4765625, 'learning_rate': 3.253446351114507e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2446, 'grad_norm': 0.5625, 'learning_rate': 3.252310113368121e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1731, 'grad_norm': 0.86328125, 'learning_rate': 3.2511738756217354e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2493, 'grad_norm': 0.455078125, 'learning_rate': 3.250037637875349e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0432, 'grad_norm': 0.74609375, 'learning_rate': 3.248901400128963e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2641, 'grad_norm': 0.52734375, 'learning_rate': 3.2477651623825774e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2309, 'grad_norm': 0.91796875, 'learning_rate': 3.2466289246361905e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0601, 'grad_norm': 1.125, 'learning_rate': 3.245492686889805e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2861, 'grad_norm': 0.52734375, 'learning_rate': 3.2443564491434195e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1124, 'grad_norm': 0.71484375, 'learning_rate': 3.2432202113970326e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2208, 'grad_norm': 0.71875, 'learning_rate': 3.242083973650647e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1309, 'grad_norm': 0.5859375, 'learning_rate': 3.240947735904261e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0154, 'grad_norm': 0.53515625, 'learning_rate': 3.2398114981578746e-05, 'epoch': 0.76}\n",
      "{'loss': 1.5474, 'grad_norm': 0.482421875, 'learning_rate': 3.2386752604114884e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1376, 'grad_norm': 1.1875, 'learning_rate': 3.237539022665103e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2309, 'grad_norm': 0.4609375, 'learning_rate': 3.2364027849187167e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3049, 'grad_norm': 0.61328125, 'learning_rate': 3.2352665471723304e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0065, 'grad_norm': 1.0234375, 'learning_rate': 3.234130309425944e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4113, 'grad_norm': 0.609375, 'learning_rate': 3.232994071679558e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1753, 'grad_norm': 0.6953125, 'learning_rate': 3.2318578339331725e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2376, 'grad_norm': 0.48828125, 'learning_rate': 3.230721596186786e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2418, 'grad_norm': 0.6015625, 'learning_rate': 3.2295853584404e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9823, 'grad_norm': 0.62109375, 'learning_rate': 3.2284491206940145e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3735, 'grad_norm': 1.1015625, 'learning_rate': 3.2273128829476276e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1843, 'grad_norm': 0.74609375, 'learning_rate': 3.226176645201242e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1903, 'grad_norm': 0.4375, 'learning_rate': 3.2250404074548565e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2619, 'grad_norm': 0.828125, 'learning_rate': 3.2239041697084696e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0162, 'grad_norm': 0.4453125, 'learning_rate': 3.222767931962084e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3707, 'grad_norm': 0.640625, 'learning_rate': 3.221631694215698e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1861, 'grad_norm': 0.5859375, 'learning_rate': 3.220495456469312e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1757, 'grad_norm': 0.71484375, 'learning_rate': 3.219359218722926e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2969, 'grad_norm': 0.66796875, 'learning_rate': 3.21822298097654e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9891, 'grad_norm': 0.921875, 'learning_rate': 3.217086743230154e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2837, 'grad_norm': 0.625, 'learning_rate': 3.2159505054837675e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0897, 'grad_norm': 0.84375, 'learning_rate': 3.214814267737381e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3136, 'grad_norm': 0.5078125, 'learning_rate': 3.213678029990995e-05, 'epoch': 0.76}\n",
      "{'loss': 1.207, 'grad_norm': 0.68359375, 'learning_rate': 3.2125417922446095e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9671, 'grad_norm': 0.7421875, 'learning_rate': 3.211405554498223e-05, 'epoch': 0.76}\n",
      "{'loss': 1.373, 'grad_norm': 0.51171875, 'learning_rate': 3.210269316751837e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1465, 'grad_norm': 0.7421875, 'learning_rate': 3.2091330790054516e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2552, 'grad_norm': 0.490234375, 'learning_rate': 3.207996841259065e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1217, 'grad_norm': 0.640625, 'learning_rate': 3.206860603512679e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0527, 'grad_norm': 0.734375, 'learning_rate': 3.2057243657662936e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4326, 'grad_norm': 0.49609375, 'learning_rate': 3.204588128019907e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1783, 'grad_norm': 0.66015625, 'learning_rate': 3.203451890273521e-05, 'epoch': 0.76}\n",
      "{'loss': 1.043, 'grad_norm': 0.51171875, 'learning_rate': 3.202315652527135e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3246, 'grad_norm': 0.69921875, 'learning_rate': 3.201179414780749e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9719, 'grad_norm': 1.2734375, 'learning_rate': 3.200043177034363e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2653, 'grad_norm': 0.57421875, 'learning_rate': 3.198906939287977e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2263, 'grad_norm': 0.6953125, 'learning_rate': 3.197770701541591e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0839, 'grad_norm': 0.51171875, 'learning_rate': 3.1966344637952046e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2685, 'grad_norm': 0.53125, 'learning_rate': 3.1954982260488183e-05, 'epoch': 0.76}\n",
      "{'loss': 1.136, 'grad_norm': 0.98046875, 'learning_rate': 3.194361988302433e-05, 'epoch': 0.76}\n",
      "{'loss': 1.4026, 'grad_norm': 0.75390625, 'learning_rate': 3.1932257505560466e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0995, 'grad_norm': 0.6796875, 'learning_rate': 3.1920895128096604e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0343, 'grad_norm': 0.478515625, 'learning_rate': 3.190953275063274e-05, 'epoch': 0.76}\n",
      "{'loss': 1.203, 'grad_norm': 0.59765625, 'learning_rate': 3.1898170373168886e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0201, 'grad_norm': 0.79296875, 'learning_rate': 3.188680799570502e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3599, 'grad_norm': 0.52734375, 'learning_rate': 3.187544561824116e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2244, 'grad_norm': 0.91796875, 'learning_rate': 3.186408324077731e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2327, 'grad_norm': 0.48046875, 'learning_rate': 3.185272086331344e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2687, 'grad_norm': 0.55078125, 'learning_rate': 3.184135848584958e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0278, 'grad_norm': 1.0625, 'learning_rate': 3.182999610838572e-05, 'epoch': 0.76}\n",
      "{'loss': 1.229, 'grad_norm': 0.5859375, 'learning_rate': 3.181863373092186e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1301, 'grad_norm': 0.6953125, 'learning_rate': 3.1807271353458e-05, 'epoch': 0.76}\n",
      "{'loss': 1.24, 'grad_norm': 0.478515625, 'learning_rate': 3.179590897599414e-05, 'epoch': 0.76}\n",
      "{'loss': 1.181, 'grad_norm': 0.609375, 'learning_rate': 3.178454659853028e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0995, 'grad_norm': 0.6875, 'learning_rate': 3.1773184221066416e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3926, 'grad_norm': 0.486328125, 'learning_rate': 3.1761821843602554e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1761, 'grad_norm': 0.71484375, 'learning_rate': 3.17504594661387e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1842, 'grad_norm': 0.494140625, 'learning_rate': 3.1739097088674837e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2539, 'grad_norm': 0.64453125, 'learning_rate': 3.1727734711210974e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1074, 'grad_norm': 0.50390625, 'learning_rate': 3.171637233374711e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3265, 'grad_norm': 0.4609375, 'learning_rate': 3.170500995628326e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0903, 'grad_norm': 0.7421875, 'learning_rate': 3.169364757881939e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3326, 'grad_norm': 0.546875, 'learning_rate': 3.168228520135553e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1559, 'grad_norm': 0.63671875, 'learning_rate': 3.167092282389168e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0934, 'grad_norm': 1.59375, 'learning_rate': 3.165956044642781e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2972, 'grad_norm': 0.56640625, 'learning_rate': 3.164819806896395e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1168, 'grad_norm': 0.66015625, 'learning_rate': 3.163683569150009e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1535, 'grad_norm': 0.59765625, 'learning_rate': 3.162547331403623e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1276, 'grad_norm': 0.54296875, 'learning_rate': 3.161411093657237e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0999, 'grad_norm': 1.1328125, 'learning_rate': 3.160274855910851e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2745, 'grad_norm': 0.5, 'learning_rate': 3.159138618164465e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1126, 'grad_norm': 1.0859375, 'learning_rate': 3.158002380418079e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2241, 'grad_norm': 0.44140625, 'learning_rate': 3.1568661426716925e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2827, 'grad_norm': 0.6171875, 'learning_rate': 3.155729904925307e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0989, 'grad_norm': 0.9921875, 'learning_rate': 3.154593667178921e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3431, 'grad_norm': 0.52734375, 'learning_rate': 3.1534574294325345e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1164, 'grad_norm': 1.015625, 'learning_rate': 3.152321191686148e-05, 'epoch': 0.76}\n",
      "{'loss': 1.142, 'grad_norm': 0.490234375, 'learning_rate': 3.151184953939763e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3246, 'grad_norm': 0.70703125, 'learning_rate': 3.1500487161933765e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0792, 'grad_norm': 0.78515625, 'learning_rate': 3.14891247844699e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.348, 'grad_norm': 0.515625, 'learning_rate': 3.147776240700605e-05, 'epoch': 0.76}\n",
      "{'loss': 1.141, 'grad_norm': 0.73046875, 'learning_rate': 3.146640002954218e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2305, 'grad_norm': 0.435546875, 'learning_rate': 3.1455037652078324e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1309, 'grad_norm': 0.61328125, 'learning_rate': 3.144367527461446e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9818, 'grad_norm': 0.6640625, 'learning_rate': 3.14323128971506e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2837, 'grad_norm': 0.8671875, 'learning_rate': 3.1420950519686744e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1635, 'grad_norm': 0.73046875, 'learning_rate': 3.140958814222288e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2585, 'grad_norm': 0.54296875, 'learning_rate': 3.139822576475902e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2235, 'grad_norm': 0.6328125, 'learning_rate': 3.138686338729516e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9174, 'grad_norm': 0.92578125, 'learning_rate': 3.1375501009831295e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3451, 'grad_norm': 0.671875, 'learning_rate': 3.136413863236744e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1386, 'grad_norm': 0.80078125, 'learning_rate': 3.135277625490358e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2232, 'grad_norm': 0.4375, 'learning_rate': 3.1341413877439716e-05, 'epoch': 0.76}\n",
      "{'loss': 1.201, 'grad_norm': 0.60546875, 'learning_rate': 3.1330051499975854e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1278, 'grad_norm': 0.87890625, 'learning_rate': 3.1318689122512e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3201, 'grad_norm': 0.64453125, 'learning_rate': 3.1307326745048136e-05, 'epoch': 0.76}\n",
      "{'loss': 1.122, 'grad_norm': 0.6953125, 'learning_rate': 3.1295964367584274e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1941, 'grad_norm': 0.51171875, 'learning_rate': 3.128460199012042e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1639, 'grad_norm': 0.69921875, 'learning_rate': 3.127323961265655e-05, 'epoch': 0.76}\n",
      "{'loss': 1.123, 'grad_norm': 0.5546875, 'learning_rate': 3.1261877235192694e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3063, 'grad_norm': 0.45703125, 'learning_rate': 3.125051485772883e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2926, 'grad_norm': 0.890625, 'learning_rate': 3.123915248026497e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1558, 'grad_norm': 0.59375, 'learning_rate': 3.1227790102801115e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3121, 'grad_norm': 0.609375, 'learning_rate': 3.121642772533725e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9183, 'grad_norm': 1.078125, 'learning_rate': 3.120506534787339e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4021, 'grad_norm': 0.5625, 'learning_rate': 3.119370297040953e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0802, 'grad_norm': 0.625, 'learning_rate': 3.1182340592945666e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3774, 'grad_norm': 0.5234375, 'learning_rate': 3.117097821548181e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1577, 'grad_norm': 0.71875, 'learning_rate': 3.115961583801795e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0621, 'grad_norm': 0.9296875, 'learning_rate': 3.1148253460554086e-05, 'epoch': 0.76}\n",
      "{'loss': 1.4086, 'grad_norm': 0.6171875, 'learning_rate': 3.1136891083090224e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1775, 'grad_norm': 0.69921875, 'learning_rate': 3.112552870562637e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1081, 'grad_norm': 0.640625, 'learning_rate': 3.111416632816251e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1626, 'grad_norm': 0.828125, 'learning_rate': 3.1102803950698645e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1133, 'grad_norm': 1.1484375, 'learning_rate': 3.109144157323479e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1992, 'grad_norm': 0.609375, 'learning_rate': 3.108007919577092e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1228, 'grad_norm': 1.1640625, 'learning_rate': 3.1068716818307065e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3652, 'grad_norm': 0.54296875, 'learning_rate': 3.10573544408432e-05, 'epoch': 0.76}\n",
      "{'loss': 1.174, 'grad_norm': 0.62109375, 'learning_rate': 3.104599206337934e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1078, 'grad_norm': 0.83203125, 'learning_rate': 3.1034629685915485e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3134, 'grad_norm': 0.64453125, 'learning_rate': 3.102326730845162e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1941, 'grad_norm': 1.34375, 'learning_rate': 3.101190493098776e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1233, 'grad_norm': 0.80078125, 'learning_rate': 3.1000542553523906e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1534, 'grad_norm': 0.625, 'learning_rate': 3.098918017606004e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0314, 'grad_norm': 1.015625, 'learning_rate': 3.097781779859618e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1321, 'grad_norm': 0.890625, 'learning_rate': 3.096645542113232e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1977, 'grad_norm': 0.6953125, 'learning_rate': 3.095509304366846e-05, 'epoch': 0.76}\n",
      "{'loss': 1.2086, 'grad_norm': 0.45703125, 'learning_rate': 3.0943730666204595e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1479, 'grad_norm': 0.73828125, 'learning_rate': 3.093236828874074e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1201, 'grad_norm': 1.0546875, 'learning_rate': 3.092100591127688e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4454, 'grad_norm': 0.546875, 'learning_rate': 3.0909643533813015e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2429, 'grad_norm': 1.0234375, 'learning_rate': 3.089828115634916e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1962, 'grad_norm': 0.478515625, 'learning_rate': 3.088691877888529e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3269, 'grad_norm': 0.58203125, 'learning_rate': 3.0875556401421435e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1157, 'grad_norm': 1.0234375, 'learning_rate': 3.086419402395757e-05, 'epoch': 0.77}\n",
      "{'loss': 1.4751, 'grad_norm': 0.498046875, 'learning_rate': 3.085283164649371e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1675, 'grad_norm': 0.625, 'learning_rate': 3.0841469269029856e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1568, 'grad_norm': 0.43359375, 'learning_rate': 3.0830106891565994e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2353, 'grad_norm': 0.6328125, 'learning_rate': 3.081874451410213e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1173, 'grad_norm': 0.75390625, 'learning_rate': 3.0807382136638276e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2169, 'grad_norm': 0.62890625, 'learning_rate': 3.079601975917441e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1889, 'grad_norm': 0.6953125, 'learning_rate': 3.078465738171055e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2072, 'grad_norm': 0.5390625, 'learning_rate': 3.077329500424669e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3036, 'grad_norm': 0.70703125, 'learning_rate': 3.076193262678283e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9876, 'grad_norm': 1.0078125, 'learning_rate': 3.0750570249318965e-05, 'epoch': 0.77}\n",
      "{'loss': 1.5917, 'grad_norm': 0.59765625, 'learning_rate': 3.073920787185511e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2044, 'grad_norm': 0.76171875, 'learning_rate': 3.072784549439125e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3513, 'grad_norm': 0.388671875, 'learning_rate': 3.0716483116927386e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2687, 'grad_norm': 0.65234375, 'learning_rate': 3.070512073946353e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0906, 'grad_norm': 0.8984375, 'learning_rate': 3.069375836199966e-05, 'epoch': 0.77}\n",
      "{'loss': 1.4297, 'grad_norm': 0.54296875, 'learning_rate': 3.0682395984535806e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1348, 'grad_norm': 0.66796875, 'learning_rate': 3.0671033607071944e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2016, 'grad_norm': 0.515625, 'learning_rate': 3.065967122960808e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3908, 'grad_norm': 0.70703125, 'learning_rate': 3.0648308852144226e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1778, 'grad_norm': 0.953125, 'learning_rate': 3.0636946474680364e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4479, 'grad_norm': 0.59375, 'learning_rate': 3.06255840972165e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0509, 'grad_norm': 0.76171875, 'learning_rate': 3.061422171975265e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0787, 'grad_norm': 0.490234375, 'learning_rate': 3.060285934228878e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2417, 'grad_norm': 0.55078125, 'learning_rate': 3.059149696482492e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1136, 'grad_norm': 1.1484375, 'learning_rate': 3.058013458736106e-05, 'epoch': 0.77}\n",
      "{'loss': 1.233, 'grad_norm': 0.447265625, 'learning_rate': 3.05687722098972e-05, 'epoch': 0.77}\n",
      "{'loss': 1.024, 'grad_norm': 0.859375, 'learning_rate': 3.055740983243334e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2678, 'grad_norm': 0.51953125, 'learning_rate': 3.054604745496948e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1643, 'grad_norm': 0.6875, 'learning_rate': 3.053468507750562e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2144, 'grad_norm': 0.9296875, 'learning_rate': 3.0523322700041756e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3191, 'grad_norm': 0.85546875, 'learning_rate': 3.0511960322577898e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0867, 'grad_norm': 0.8125, 'learning_rate': 3.0500597945114036e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1689, 'grad_norm': 0.74609375, 'learning_rate': 3.0489235567650177e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1044, 'grad_norm': 0.6015625, 'learning_rate': 3.0477873190186318e-05, 'epoch': 0.77}\n",
      "{'loss': 0.931, 'grad_norm': 1.0703125, 'learning_rate': 3.0466510812722452e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3793, 'grad_norm': 0.53515625, 'learning_rate': 3.0455148435258597e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1321, 'grad_norm': 0.73828125, 'learning_rate': 3.044378605779473e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1664, 'grad_norm': 0.4921875, 'learning_rate': 3.0432423680330873e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1089, 'grad_norm': 0.59765625, 'learning_rate': 3.0421061302867014e-05, 'epoch': 0.77}\n",
      "{'loss': 0.937, 'grad_norm': 0.79296875, 'learning_rate': 3.0409698925403152e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3363, 'grad_norm': 0.482421875, 'learning_rate': 3.0398336547939293e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0993, 'grad_norm': 0.67578125, 'learning_rate': 3.038697417047543e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1662, 'grad_norm': 0.46875, 'learning_rate': 3.0375611793011572e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2157, 'grad_norm': 0.7109375, 'learning_rate': 3.0364249415547713e-05, 'epoch': 0.77}\n",
      "{'loss': 1.109, 'grad_norm': 1.453125, 'learning_rate': 3.0352887038083848e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3209, 'grad_norm': 0.59375, 'learning_rate': 3.0341524660619993e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2913, 'grad_norm': 1.0078125, 'learning_rate': 3.0330162283156127e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3085, 'grad_norm': 0.494140625, 'learning_rate': 3.0318799905692268e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1606, 'grad_norm': 0.56640625, 'learning_rate': 3.030743752822841e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1989, 'grad_norm': 0.7890625, 'learning_rate': 3.0296075150764547e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3448, 'grad_norm': 0.55078125, 'learning_rate': 3.028471277330069e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1306, 'grad_norm': 0.5703125, 'learning_rate': 3.0273350395836823e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2501, 'grad_norm': 0.455078125, 'learning_rate': 3.0261988018372968e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2108, 'grad_norm': 0.63671875, 'learning_rate': 3.0250625640909102e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0821, 'grad_norm': 0.93359375, 'learning_rate': 3.0239263263445243e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2591, 'grad_norm': 0.5703125, 'learning_rate': 3.0227900885981385e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1008, 'grad_norm': 0.78125, 'learning_rate': 3.0216538508517523e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1751, 'grad_norm': 0.45703125, 'learning_rate': 3.0205176131053664e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2628, 'grad_norm': 0.7109375, 'learning_rate': 3.01938137535898e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0049, 'grad_norm': 0.77734375, 'learning_rate': 3.0182451376125943e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3271, 'grad_norm': 0.5703125, 'learning_rate': 3.0171088998662084e-05, 'epoch': 0.77}\n",
      "{'loss': 1.045, 'grad_norm': 0.71484375, 'learning_rate': 3.015972662119822e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0891, 'grad_norm': 0.4765625, 'learning_rate': 3.0148364243734363e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2268, 'grad_norm': 0.62890625, 'learning_rate': 3.0137001866270498e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0977, 'grad_norm': 1.28125, 'learning_rate': 3.012563948880664e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2673, 'grad_norm': 0.57421875, 'learning_rate': 3.011427711134278e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0503, 'grad_norm': 0.74609375, 'learning_rate': 3.0102914733878918e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3454, 'grad_norm': 0.44921875, 'learning_rate': 3.009155235641506e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2906, 'grad_norm': 0.58984375, 'learning_rate': 3.0080189978951194e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2207, 'grad_norm': 1.21875, 'learning_rate': 3.006882760148734e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3326, 'grad_norm': 0.84765625, 'learning_rate': 3.005746522402348e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0805, 'grad_norm': 1.1171875, 'learning_rate': 3.0046102846559614e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2439, 'grad_norm': 0.462890625, 'learning_rate': 3.0034740469095755e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2447, 'grad_norm': 0.59765625, 'learning_rate': 3.0023378091631893e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9586, 'grad_norm': 0.66796875, 'learning_rate': 3.0012015714168034e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2468, 'grad_norm': 0.546875, 'learning_rate': 3.0000653336704172e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1724, 'grad_norm': 0.9921875, 'learning_rate': 2.9989290959240313e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2801, 'grad_norm': 0.45703125, 'learning_rate': 2.9977928581776455e-05, 'epoch': 0.77}\n",
      "{'loss': 1.279, 'grad_norm': 0.5546875, 'learning_rate': 2.996656620431259e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9678, 'grad_norm': 0.62890625, 'learning_rate': 2.9955203826848734e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2911, 'grad_norm': 0.490234375, 'learning_rate': 2.9943841449384868e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1335, 'grad_norm': 0.89453125, 'learning_rate': 2.993247907192101e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0632, 'grad_norm': 0.451171875, 'learning_rate': 2.992111669445715e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1532, 'grad_norm': 0.55078125, 'learning_rate': 2.990975431699329e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1283, 'grad_norm': 1.453125, 'learning_rate': 2.989839193952943e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2297, 'grad_norm': 0.6171875, 'learning_rate': 2.9887029562065564e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0843, 'grad_norm': 0.51953125, 'learning_rate': 2.987566718460171e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3181, 'grad_norm': 0.5078125, 'learning_rate': 2.986430480713785e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2116, 'grad_norm': 0.66796875, 'learning_rate': 2.9852942429673985e-05, 'epoch': 0.77}\n",
      "{'loss': 0.991, 'grad_norm': 1.1328125, 'learning_rate': 2.9841580052210126e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2906, 'grad_norm': 0.53515625, 'learning_rate': 2.9830217674746264e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2141, 'grad_norm': 0.90625, 'learning_rate': 2.9818855297282405e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1069, 'grad_norm': 0.5, 'learning_rate': 2.9807492919818543e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2543, 'grad_norm': 0.6015625, 'learning_rate': 2.9796130542354684e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1334, 'grad_norm': 1.21875, 'learning_rate': 2.9784768164890825e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3059, 'grad_norm': 0.5703125, 'learning_rate': 2.977340578742696e-05, 'epoch': 0.77}\n",
      "{'loss': 1.177, 'grad_norm': 0.77734375, 'learning_rate': 2.9762043409963104e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2416, 'grad_norm': 0.462890625, 'learning_rate': 2.975068103249924e-05, 'epoch': 0.77}\n",
      "{'loss': 1.218, 'grad_norm': 0.734375, 'learning_rate': 2.973931865503538e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0653, 'grad_norm': 1.0390625, 'learning_rate': 2.972795627757152e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3921, 'grad_norm': 0.51171875, 'learning_rate': 2.971659390010766e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0743, 'grad_norm': 0.55078125, 'learning_rate': 2.97052315226438e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1806, 'grad_norm': 0.42578125, 'learning_rate': 2.9693869145179935e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2009, 'grad_norm': 0.55859375, 'learning_rate': 2.968250676771608e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1196, 'grad_norm': 0.74609375, 'learning_rate': 2.967114439025222e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1773, 'grad_norm': 0.5546875, 'learning_rate': 2.9659782012788355e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2294, 'grad_norm': 0.91796875, 'learning_rate': 2.9648419635324497e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2142, 'grad_norm': 0.515625, 'learning_rate': 2.9637057257860634e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1706, 'grad_norm': 0.5234375, 'learning_rate': 2.9625694880396776e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0702, 'grad_norm': 0.5703125, 'learning_rate': 2.9614332502932917e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3131, 'grad_norm': 0.546875, 'learning_rate': 2.9602970125469055e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1095, 'grad_norm': 0.58984375, 'learning_rate': 2.9591607748005196e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1131, 'grad_norm': 0.5703125, 'learning_rate': 2.958024537054133e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1875, 'grad_norm': 0.6015625, 'learning_rate': 2.9568882993077475e-05, 'epoch': 0.77}\n",
      "{'loss': 1.08, 'grad_norm': 0.9609375, 'learning_rate': 2.955752061561361e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3496, 'grad_norm': 0.55859375, 'learning_rate': 2.954615823814975e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2414, 'grad_norm': 0.796875, 'learning_rate': 2.9534795860685892e-05, 'epoch': 0.77}\n",
      "{'loss': 1.24, 'grad_norm': 0.375, 'learning_rate': 2.952343348322203e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1864, 'grad_norm': 0.81640625, 'learning_rate': 2.951207110575817e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0863, 'grad_norm': 0.97265625, 'learning_rate': 2.950070872829431e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4088, 'grad_norm': 0.5078125, 'learning_rate': 2.948934635083045e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1801, 'grad_norm': 1.140625, 'learning_rate': 2.947798397336659e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0985, 'grad_norm': 0.56640625, 'learning_rate': 2.9466621595902726e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2387, 'grad_norm': 0.83984375, 'learning_rate': 2.9455259218438867e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1111, 'grad_norm': 0.75390625, 'learning_rate': 2.9443896840975005e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3535, 'grad_norm': 0.5625, 'learning_rate': 2.9432534463511146e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2015, 'grad_norm': 0.609375, 'learning_rate': 2.9421172086047288e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1969, 'grad_norm': 0.46484375, 'learning_rate': 2.9409809708583425e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3113, 'grad_norm': 0.62109375, 'learning_rate': 2.9398447331119567e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1076, 'grad_norm': 0.8125, 'learning_rate': 2.93870849536557e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3518, 'grad_norm': 0.6875, 'learning_rate': 2.9375722576191846e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2027, 'grad_norm': 0.7265625, 'learning_rate': 2.9364360198727987e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1526, 'grad_norm': 0.46875, 'learning_rate': 2.935299782126412e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2394, 'grad_norm': 0.546875, 'learning_rate': 2.9341635443800263e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1072, 'grad_norm': 1.171875, 'learning_rate': 2.93302730663364e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3363, 'grad_norm': 0.58203125, 'learning_rate': 2.9318910688872542e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0821, 'grad_norm': 0.796875, 'learning_rate': 2.930754831140868e-05, 'epoch': 0.77}\n",
      "{'loss': 1.25, 'grad_norm': 0.466796875, 'learning_rate': 2.929618593394482e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2947, 'grad_norm': 0.478515625, 'learning_rate': 2.9284823556480962e-05, 'epoch': 0.77}\n",
      "{'loss': 1.088, 'grad_norm': 1.6015625, 'learning_rate': 2.9273461179017097e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2959, 'grad_norm': 0.62890625, 'learning_rate': 2.9262098801553238e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1197, 'grad_norm': 0.92578125, 'learning_rate': 2.9250736424089376e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9955, 'grad_norm': 0.6953125, 'learning_rate': 2.9239374046625517e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2571, 'grad_norm': 0.55078125, 'learning_rate': 2.9228011669161658e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0499, 'grad_norm': 1.609375, 'learning_rate': 2.9216649291697796e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2923, 'grad_norm': 0.68359375, 'learning_rate': 2.9205286914233937e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1497, 'grad_norm': 0.6953125, 'learning_rate': 2.9193924536770072e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1505, 'grad_norm': 0.43359375, 'learning_rate': 2.9182562159306216e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3248, 'grad_norm': 0.8359375, 'learning_rate': 2.9171199781842358e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0703, 'grad_norm': 0.8515625, 'learning_rate': 2.9159837404378492e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2904, 'grad_norm': 0.75, 'learning_rate': 2.9148475026914633e-05, 'epoch': 0.77}\n",
      "{'loss': 1.229, 'grad_norm': 0.7734375, 'learning_rate': 2.913711264945077e-05, 'epoch': 0.77}\n",
      "{'loss': 1.101, 'grad_norm': 0.65625, 'learning_rate': 2.9125750271986912e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2482, 'grad_norm': 0.6640625, 'learning_rate': 2.9114387894523054e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0537, 'grad_norm': 0.63671875, 'learning_rate': 2.910302551705919e-05, 'epoch': 0.77}\n",
      "{'loss': 1.4232, 'grad_norm': 0.59375, 'learning_rate': 2.9091663139595333e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1957, 'grad_norm': 0.5625, 'learning_rate': 2.9080300762131467e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0751, 'grad_norm': 0.470703125, 'learning_rate': 2.906893838466761e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3219, 'grad_norm': 0.7734375, 'learning_rate': 2.9057576007203746e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0614, 'grad_norm': 2.109375, 'learning_rate': 2.9046213629739888e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2821, 'grad_norm': 0.7109375, 'learning_rate': 2.903485125227603e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1284, 'grad_norm': 1.4765625, 'learning_rate': 2.9023488874812167e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2403, 'grad_norm': 0.66796875, 'learning_rate': 2.9012126497348308e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1548, 'grad_norm': 0.6953125, 'learning_rate': 2.9000764119884442e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1424, 'grad_norm': 0.99609375, 'learning_rate': 2.8989401742420587e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3241, 'grad_norm': 0.62890625, 'learning_rate': 2.8978039364956728e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0688, 'grad_norm': 0.671875, 'learning_rate': 2.8966676987492863e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0678, 'grad_norm': 0.56640625, 'learning_rate': 2.8955314610029004e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2582, 'grad_norm': 0.59765625, 'learning_rate': 2.8943952232565142e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1564, 'grad_norm': 0.69140625, 'learning_rate': 2.8932589855101283e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4457, 'grad_norm': 0.5546875, 'learning_rate': 2.8921227477637424e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2111, 'grad_norm': 0.7265625, 'learning_rate': 2.8909865100173562e-05, 'epoch': 0.77}\n",
      "{'loss': 1.225, 'grad_norm': 0.486328125, 'learning_rate': 2.8898502722709703e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2232, 'grad_norm': 0.66015625, 'learning_rate': 2.8887140345245838e-05, 'epoch': 0.77}\n",
      "{'loss': 1.009, 'grad_norm': 0.66015625, 'learning_rate': 2.887577796778198e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3009, 'grad_norm': 0.796875, 'learning_rate': 2.8864415590318117e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1793, 'grad_norm': 0.67578125, 'learning_rate': 2.8853053212854258e-05, 'epoch': 0.77}\n",
      "{'loss': 1.255, 'grad_norm': 0.65625, 'learning_rate': 2.88416908353904e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2718, 'grad_norm': 0.80078125, 'learning_rate': 2.8830328457926537e-05, 'epoch': 0.77}\n",
      "{'loss': 1.048, 'grad_norm': 1.4296875, 'learning_rate': 2.881896608046268e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3967, 'grad_norm': 0.5390625, 'learning_rate': 2.8807603702998813e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2379, 'grad_norm': 0.9609375, 'learning_rate': 2.8796241325534958e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1573, 'grad_norm': 0.490234375, 'learning_rate': 2.87848789480711e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1878, 'grad_norm': 0.5859375, 'learning_rate': 2.8773516570607233e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0558, 'grad_norm': 0.7578125, 'learning_rate': 2.8762154193143375e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3581, 'grad_norm': 0.69140625, 'learning_rate': 2.8750791815679512e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0686, 'grad_norm': 0.703125, 'learning_rate': 2.8739429438215654e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1619, 'grad_norm': 0.388671875, 'learning_rate': 2.8728067060751795e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2147, 'grad_norm': 0.67578125, 'learning_rate': 2.8716704683287933e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9464, 'grad_norm': 0.703125, 'learning_rate': 2.8705342305824074e-05, 'epoch': 0.77}\n",
      "{'loss': 1.4166, 'grad_norm': 1.0625, 'learning_rate': 2.869397992836021e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1767, 'grad_norm': 0.6796875, 'learning_rate': 2.868261755089635e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1892, 'grad_norm': 0.6171875, 'learning_rate': 2.8671255173432494e-05, 'epoch': 0.77}\n",
      "{'loss': 1.1757, 'grad_norm': 0.54296875, 'learning_rate': 2.865989279596863e-05, 'epoch': 0.77}\n",
      "{'loss': 1.027, 'grad_norm': 1.25, 'learning_rate': 2.864853041850477e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2891, 'grad_norm': 0.50390625, 'learning_rate': 2.8637168041040908e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2116, 'grad_norm': 0.74609375, 'learning_rate': 2.862580566357705e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3547, 'grad_norm': 0.7265625, 'learning_rate': 2.8614443286113184e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1855, 'grad_norm': 0.56640625, 'learning_rate': 2.8603080908649328e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0642, 'grad_norm': 1.3203125, 'learning_rate': 2.859171853118547e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3384, 'grad_norm': 0.7265625, 'learning_rate': 2.8580356153721604e-05, 'epoch': 0.78}\n",
      "{'loss': 1.096, 'grad_norm': 0.6171875, 'learning_rate': 2.8568993776257745e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2526, 'grad_norm': 0.46484375, 'learning_rate': 2.8557631398793883e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2663, 'grad_norm': 0.5625, 'learning_rate': 2.8546269021330024e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0785, 'grad_norm': 0.71484375, 'learning_rate': 2.8534906643866166e-05, 'epoch': 0.78}\n",
      "{'loss': 1.365, 'grad_norm': 0.490234375, 'learning_rate': 2.8523544266402303e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1393, 'grad_norm': 0.8046875, 'learning_rate': 2.8512181888938445e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0591, 'grad_norm': 0.51171875, 'learning_rate': 2.850081951147458e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3199, 'grad_norm': 0.8203125, 'learning_rate': 2.848945713401072e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0969, 'grad_norm': 0.90234375, 'learning_rate': 2.8478094756546865e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3498, 'grad_norm': 0.5390625, 'learning_rate': 2.8466732379083e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0599, 'grad_norm': 0.71484375, 'learning_rate': 2.845537000161914e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3321, 'grad_norm': 0.66796875, 'learning_rate': 2.844400762415528e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1511, 'grad_norm': 0.57421875, 'learning_rate': 2.843264524669142e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2123, 'grad_norm': 1.0, 'learning_rate': 2.842128286922756e-05, 'epoch': 0.78}\n",
      "{'loss': 1.131, 'grad_norm': 0.52734375, 'learning_rate': 2.84099204917637e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1311, 'grad_norm': 0.88671875, 'learning_rate': 2.839855811429984e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2172, 'grad_norm': 0.640625, 'learning_rate': 2.8387195736835975e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1087, 'grad_norm': 0.7109375, 'learning_rate': 2.8375833359372116e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0775, 'grad_norm': 0.8984375, 'learning_rate': 2.8364470981908254e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3086, 'grad_norm': 0.55078125, 'learning_rate': 2.8353108604444395e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2954, 'grad_norm': 0.65234375, 'learning_rate': 2.8341746226980536e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2135, 'grad_norm': 0.6953125, 'learning_rate': 2.8330383849516674e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2895, 'grad_norm': 0.578125, 'learning_rate': 2.8319021472052815e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0748, 'grad_norm': 0.5546875, 'learning_rate': 2.830765909458895e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3914, 'grad_norm': 0.52734375, 'learning_rate': 2.829629671712509e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0303, 'grad_norm': 0.94921875, 'learning_rate': 2.8284934339661236e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2068, 'grad_norm': 0.451171875, 'learning_rate': 2.827357196219737e-05, 'epoch': 0.78}\n",
      "{'loss': 1.224, 'grad_norm': 0.5703125, 'learning_rate': 2.826220958473351e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1219, 'grad_norm': 1.015625, 'learning_rate': 2.825084720726965e-05, 'epoch': 0.78}\n",
      "{'loss': 1.388, 'grad_norm': 0.62109375, 'learning_rate': 2.823948482980579e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2626, 'grad_norm': 0.6796875, 'learning_rate': 2.822812245234193e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0881, 'grad_norm': 0.515625, 'learning_rate': 2.821676007487807e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2143, 'grad_norm': 0.58984375, 'learning_rate': 2.820539769741421e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1598, 'grad_norm': 1.4453125, 'learning_rate': 2.8194035319950345e-05, 'epoch': 0.78}\n",
      "{'loss': 1.5065, 'grad_norm': 0.56640625, 'learning_rate': 2.8182672942486486e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1229, 'grad_norm': 0.703125, 'learning_rate': 2.817131056502263e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2506, 'grad_norm': 0.51953125, 'learning_rate': 2.8159948187558766e-05, 'epoch': 0.78}\n",
      "{'loss': 1.312, 'grad_norm': 0.77734375, 'learning_rate': 2.8148585810094907e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1708, 'grad_norm': 1.0234375, 'learning_rate': 2.8137223432631045e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2819, 'grad_norm': 0.65625, 'learning_rate': 2.8125861055167186e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2089, 'grad_norm': 0.76171875, 'learning_rate': 2.811449867770332e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1553, 'grad_norm': 0.6171875, 'learning_rate': 2.810313630023946e-05, 'epoch': 0.78}\n",
      "{'loss': 1.314, 'grad_norm': 0.6484375, 'learning_rate': 2.8091773922775606e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1032, 'grad_norm': 0.94921875, 'learning_rate': 2.808041154531174e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3866, 'grad_norm': 0.61328125, 'learning_rate': 2.8069049167847882e-05, 'epoch': 0.78}\n",
      "{'loss': 1.046, 'grad_norm': 0.7578125, 'learning_rate': 2.805768679038402e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1796, 'grad_norm': 0.4609375, 'learning_rate': 2.804632441292016e-05, 'epoch': 0.78}\n",
      "{'loss': 1.226, 'grad_norm': 0.7109375, 'learning_rate': 2.8034962035456302e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2174, 'grad_norm': 0.47265625, 'learning_rate': 2.802359965799244e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3647, 'grad_norm': 0.57421875, 'learning_rate': 2.801223728052858e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0721, 'grad_norm': 1.015625, 'learning_rate': 2.8000874903064716e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1863, 'grad_norm': 0.46875, 'learning_rate': 2.7989512525600857e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2029, 'grad_norm': 0.63671875, 'learning_rate': 2.7978150148137002e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0249, 'grad_norm': 0.6015625, 'learning_rate': 2.7966787770673136e-05, 'epoch': 0.78}\n",
      "{'loss': 1.4517, 'grad_norm': 0.640625, 'learning_rate': 2.7955425393209277e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1763, 'grad_norm': 0.71875, 'learning_rate': 2.7944063015745415e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1745, 'grad_norm': 0.6640625, 'learning_rate': 2.7932700638281557e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3116, 'grad_norm': 0.64453125, 'learning_rate': 2.792133826081769e-05, 'epoch': 0.78}\n",
      "{'loss': 0.9703, 'grad_norm': 0.53125, 'learning_rate': 2.7909975883353832e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3563, 'grad_norm': 0.59765625, 'learning_rate': 2.7898613505889977e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1939, 'grad_norm': 0.71484375, 'learning_rate': 2.788725112842611e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3024, 'grad_norm': 0.66015625, 'learning_rate': 2.7875888750962253e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2363, 'grad_norm': 0.5390625, 'learning_rate': 2.786452637349839e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0473, 'grad_norm': 1.8125, 'learning_rate': 2.785316399603453e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3801, 'grad_norm': 0.66796875, 'learning_rate': 2.7841801618570673e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1394, 'grad_norm': 0.78125, 'learning_rate': 2.783043924110681e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0328, 'grad_norm': 0.578125, 'learning_rate': 2.7819076863642952e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2826, 'grad_norm': 0.65234375, 'learning_rate': 2.7807714486179086e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0289, 'grad_norm': 1.0546875, 'learning_rate': 2.7796352108715228e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5148, 'grad_norm': 0.765625, 'learning_rate': 2.7784989731251372e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1392, 'grad_norm': 1.03125, 'learning_rate': 2.7773627353787507e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0864, 'grad_norm': 0.62890625, 'learning_rate': 2.7762264976323648e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2787, 'grad_norm': 0.5546875, 'learning_rate': 2.7750902598859786e-05, 'epoch': 0.78}\n",
      "{'loss': 1.089, 'grad_norm': 1.171875, 'learning_rate': 2.7739540221395927e-05, 'epoch': 0.78}\n",
      "{'loss': 1.4158, 'grad_norm': 0.5625, 'learning_rate': 2.772817784393207e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0573, 'grad_norm': 0.71875, 'learning_rate': 2.7716815466468203e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1606, 'grad_norm': 0.6953125, 'learning_rate': 2.7705453089004347e-05, 'epoch': 0.78}\n",
      "{'loss': 1.072, 'grad_norm': 0.66796875, 'learning_rate': 2.7694090711540482e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2099, 'grad_norm': 0.4765625, 'learning_rate': 2.7682728334076623e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3911, 'grad_norm': 0.482421875, 'learning_rate': 2.767136595661276e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2124, 'grad_norm': 0.640625, 'learning_rate': 2.7660003579148902e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1295, 'grad_norm': 0.5390625, 'learning_rate': 2.7648641201685044e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1494, 'grad_norm': 0.6015625, 'learning_rate': 2.763727882422118e-05, 'epoch': 0.78}\n",
      "{'loss': 0.9205, 'grad_norm': 0.69921875, 'learning_rate': 2.7625916446757323e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3068, 'grad_norm': 0.54296875, 'learning_rate': 2.7614554069293457e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2017, 'grad_norm': 0.9453125, 'learning_rate': 2.76031916918296e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1832, 'grad_norm': 0.4375, 'learning_rate': 2.7591829314365743e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2269, 'grad_norm': 0.89453125, 'learning_rate': 2.7580466936901877e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0573, 'grad_norm': 1.140625, 'learning_rate': 2.756910455943802e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3829, 'grad_norm': 0.515625, 'learning_rate': 2.7557742181974157e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2016, 'grad_norm': 0.828125, 'learning_rate': 2.7546379804510298e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2911, 'grad_norm': 0.51171875, 'learning_rate': 2.753501742704644e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1329, 'grad_norm': 0.6484375, 'learning_rate': 2.7523655049582573e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0777, 'grad_norm': 0.81640625, 'learning_rate': 2.7512292672118718e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4161, 'grad_norm': 0.8671875, 'learning_rate': 2.7500930294654853e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1972, 'grad_norm': 0.78125, 'learning_rate': 2.7489567917190994e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2838, 'grad_norm': 0.478515625, 'learning_rate': 2.7478205539727135e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2293, 'grad_norm': 0.58984375, 'learning_rate': 2.7466843162263273e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0325, 'grad_norm': 0.66796875, 'learning_rate': 2.7455480784799414e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2673, 'grad_norm': 0.61328125, 'learning_rate': 2.7444118407335552e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1897, 'grad_norm': 1.078125, 'learning_rate': 2.7432756029871693e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1367, 'grad_norm': 0.486328125, 'learning_rate': 2.7421393652407828e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1328, 'grad_norm': 0.6015625, 'learning_rate': 2.741003127494397e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0365, 'grad_norm': 1.4296875, 'learning_rate': 2.7398668897480114e-05, 'epoch': 0.78}\n",
      "{'loss': 1.4415, 'grad_norm': 0.68359375, 'learning_rate': 2.7387306520016248e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0772, 'grad_norm': 0.6953125, 'learning_rate': 2.737594414255239e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1202, 'grad_norm': 0.431640625, 'learning_rate': 2.7364581765088527e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2328, 'grad_norm': 0.54296875, 'learning_rate': 2.735321938762467e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0021, 'grad_norm': 1.0234375, 'learning_rate': 2.734185701016081e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3138, 'grad_norm': 0.50390625, 'learning_rate': 2.7330494632696944e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0943, 'grad_norm': 1.1640625, 'learning_rate': 2.731913225523309e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3607, 'grad_norm': 0.40234375, 'learning_rate': 2.7307769877769223e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1993, 'grad_norm': 0.67578125, 'learning_rate': 2.7296407500305364e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0861, 'grad_norm': 0.9921875, 'learning_rate': 2.7285045122841506e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3176, 'grad_norm': 0.51171875, 'learning_rate': 2.7273682745377644e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0781, 'grad_norm': 1.4453125, 'learning_rate': 2.7262320367913785e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1024, 'grad_norm': 0.76953125, 'learning_rate': 2.7250957990449923e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2143, 'grad_norm': 0.5625, 'learning_rate': 2.7239595612986064e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0287, 'grad_norm': 0.828125, 'learning_rate': 2.7228233235522205e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3681, 'grad_norm': 0.61328125, 'learning_rate': 2.721687085805834e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1235, 'grad_norm': 0.734375, 'learning_rate': 2.7205508480594484e-05, 'epoch': 0.78}\n",
      "{'loss': 1.191, 'grad_norm': 0.416015625, 'learning_rate': 2.719414610313062e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3161, 'grad_norm': 0.625, 'learning_rate': 2.718278372566676e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0725, 'grad_norm': 1.34375, 'learning_rate': 2.7171421348202898e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3352, 'grad_norm': 0.5859375, 'learning_rate': 2.716005897073904e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0773, 'grad_norm': 0.8515625, 'learning_rate': 2.714869659327518e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1473, 'grad_norm': 0.47265625, 'learning_rate': 2.7137334215811315e-05, 'epoch': 0.78}\n",
      "{'loss': 1.168, 'grad_norm': 0.95703125, 'learning_rate': 2.712597183834746e-05, 'epoch': 0.78}\n",
      "{'loss': 0.998, 'grad_norm': 0.85546875, 'learning_rate': 2.7114609460883594e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3031, 'grad_norm': 0.5703125, 'learning_rate': 2.7103247083419735e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0915, 'grad_norm': 0.70703125, 'learning_rate': 2.7091884705955876e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3387, 'grad_norm': 0.63671875, 'learning_rate': 2.7080522328492014e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0994, 'grad_norm': 0.67578125, 'learning_rate': 2.7069159951028155e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0042, 'grad_norm': 1.1875, 'learning_rate': 2.7057797573564293e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3614, 'grad_norm': 0.5625, 'learning_rate': 2.7046435196100435e-05, 'epoch': 0.78}\n",
      "{'loss': 1.216, 'grad_norm': 0.734375, 'learning_rate': 2.7035072818636576e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2253, 'grad_norm': 0.466796875, 'learning_rate': 2.702371044117271e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2625, 'grad_norm': 0.61328125, 'learning_rate': 2.7012348063708855e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0496, 'grad_norm': 1.0078125, 'learning_rate': 2.700098568624499e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2849, 'grad_norm': 0.494140625, 'learning_rate': 2.698962330878113e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1527, 'grad_norm': 0.80859375, 'learning_rate': 2.697826093131727e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1824, 'grad_norm': 0.58203125, 'learning_rate': 2.696689855385341e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2586, 'grad_norm': 0.65625, 'learning_rate': 2.695553617638955e-05, 'epoch': 0.78}\n",
      "{'loss': 1.087, 'grad_norm': 0.984375, 'learning_rate': 2.6944173798925685e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4553, 'grad_norm': 0.5625, 'learning_rate': 2.693281142146183e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1293, 'grad_norm': 1.234375, 'learning_rate': 2.6921449043997964e-05, 'epoch': 0.78}\n",
      "{'loss': 1.029, 'grad_norm': 0.50390625, 'learning_rate': 2.6910086666534106e-05, 'epoch': 0.78}\n",
      "{'loss': 1.179, 'grad_norm': 0.828125, 'learning_rate': 2.6898724289070247e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0229, 'grad_norm': 0.984375, 'learning_rate': 2.6887361911606385e-05, 'epoch': 0.78}\n",
      "{'loss': 1.597, 'grad_norm': 0.5234375, 'learning_rate': 2.6875999534142526e-05, 'epoch': 0.78}\n",
      "{'loss': 1.087, 'grad_norm': 0.6953125, 'learning_rate': 2.6864637156678664e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2699, 'grad_norm': 0.498046875, 'learning_rate': 2.6853274779214805e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2339, 'grad_norm': 0.5859375, 'learning_rate': 2.6841912401750946e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0958, 'grad_norm': 0.875, 'learning_rate': 2.683055002428708e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3729, 'grad_norm': 0.5390625, 'learning_rate': 2.6819187646823225e-05, 'epoch': 0.78}\n",
      "{'loss': 1.109, 'grad_norm': 0.73828125, 'learning_rate': 2.680782526935936e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1847, 'grad_norm': 0.6875, 'learning_rate': 2.67964628918955e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2185, 'grad_norm': 0.68359375, 'learning_rate': 2.6785100514431642e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0826, 'grad_norm': 0.58203125, 'learning_rate': 2.677373813696778e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3405, 'grad_norm': 0.81640625, 'learning_rate': 2.676237575950392e-05, 'epoch': 0.78}\n",
      "{'loss': 1.176, 'grad_norm': 0.5, 'learning_rate': 2.6751013382040056e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2438, 'grad_norm': 0.5078125, 'learning_rate': 2.67396510045762e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1541, 'grad_norm': 0.62890625, 'learning_rate': 2.6728288627112335e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1127, 'grad_norm': 0.62890625, 'learning_rate': 2.6716926249648476e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3665, 'grad_norm': 0.52734375, 'learning_rate': 2.6705563872184618e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0818, 'grad_norm': 0.96484375, 'learning_rate': 2.6694201494720755e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2717, 'grad_norm': 0.6328125, 'learning_rate': 2.6682839117256897e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2536, 'grad_norm': 0.67578125, 'learning_rate': 2.6671476739793035e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1233, 'grad_norm': 1.0390625, 'learning_rate': 2.6660114362329176e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3717, 'grad_norm': 0.435546875, 'learning_rate': 2.6648751984865317e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1753, 'grad_norm': 0.66015625, 'learning_rate': 2.663738960740145e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1287, 'grad_norm': 0.53515625, 'learning_rate': 2.6626027229937596e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1409, 'grad_norm': 0.50390625, 'learning_rate': 2.661466485247373e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0016, 'grad_norm': 0.46875, 'learning_rate': 2.6603302475009872e-05, 'epoch': 0.78}\n",
      "{'loss': 1.5041, 'grad_norm': 0.50390625, 'learning_rate': 2.6591940097546013e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3052, 'grad_norm': 1.3359375, 'learning_rate': 2.658057772008215e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3089, 'grad_norm': 0.458984375, 'learning_rate': 2.6569215342618292e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1535, 'grad_norm': 0.78125, 'learning_rate': 2.655785296515443e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1213, 'grad_norm': 0.57421875, 'learning_rate': 2.654649058769057e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3554, 'grad_norm': 0.51953125, 'learning_rate': 2.6535128210226713e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1182, 'grad_norm': 1.0234375, 'learning_rate': 2.6523765832762847e-05, 'epoch': 0.78}\n",
      "{'loss': 1.271, 'grad_norm': 0.5234375, 'learning_rate': 2.6512403455298988e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1839, 'grad_norm': 1.0234375, 'learning_rate': 2.6501041077835126e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1799, 'grad_norm': 0.71875, 'learning_rate': 2.6489678700371267e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3785, 'grad_norm': 0.52734375, 'learning_rate': 2.6478316322907405e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1661, 'grad_norm': 0.7109375, 'learning_rate': 2.6466953945443546e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2146, 'grad_norm': 0.515625, 'learning_rate': 2.6455591567979688e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3533, 'grad_norm': 0.5625, 'learning_rate': 2.6444229190515822e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1983, 'grad_norm': 1.15625, 'learning_rate': 2.6432866813051967e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3184, 'grad_norm': 0.50390625, 'learning_rate': 2.64215044355881e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2249, 'grad_norm': 0.6875, 'learning_rate': 2.6410142058124242e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2389, 'grad_norm': 0.53125, 'learning_rate': 2.6398779680660384e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1226, 'grad_norm': 0.58203125, 'learning_rate': 2.638741730319652e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1025, 'grad_norm': 1.0546875, 'learning_rate': 2.6376054925732663e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2963, 'grad_norm': 0.609375, 'learning_rate': 2.63646925482688e-05, 'epoch': 0.78}\n",
      "{'loss': 1.2213, 'grad_norm': 0.62890625, 'learning_rate': 2.6353330170804942e-05, 'epoch': 0.78}\n",
      "{'loss': 1.3105, 'grad_norm': 0.57421875, 'learning_rate': 2.6341967793341083e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2695, 'grad_norm': 0.92578125, 'learning_rate': 2.6330605415877218e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9697, 'grad_norm': 0.77734375, 'learning_rate': 2.631924303841336e-05, 'epoch': 0.79}\n",
      "{'loss': 1.315, 'grad_norm': 0.55078125, 'learning_rate': 2.6307880660949497e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1028, 'grad_norm': 0.80859375, 'learning_rate': 2.6296518283485638e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1555, 'grad_norm': 0.55078125, 'learning_rate': 2.628515590602178e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2171, 'grad_norm': 0.703125, 'learning_rate': 2.6273793528557917e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0787, 'grad_norm': 1.7734375, 'learning_rate': 2.6262431151094058e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2955, 'grad_norm': 0.5859375, 'learning_rate': 2.6251068773630193e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0491, 'grad_norm': 0.9609375, 'learning_rate': 2.6239706396166337e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1783, 'grad_norm': 0.58203125, 'learning_rate': 2.6228344018702472e-05, 'epoch': 0.79}\n",
      "{'loss': 1.076, 'grad_norm': 0.63671875, 'learning_rate': 2.6216981641238613e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0706, 'grad_norm': 0.96484375, 'learning_rate': 2.6205619263774754e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3902, 'grad_norm': 0.5078125, 'learning_rate': 2.6194256886310892e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1073, 'grad_norm': 0.7265625, 'learning_rate': 2.6182894508847033e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2073, 'grad_norm': 0.609375, 'learning_rate': 2.617153213138317e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0612, 'grad_norm': 0.6015625, 'learning_rate': 2.6160169753919313e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0495, 'grad_norm': 1.0859375, 'learning_rate': 2.6148807376455454e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3299, 'grad_norm': 0.6640625, 'learning_rate': 2.6137444998991588e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1516, 'grad_norm': 0.78125, 'learning_rate': 2.612608262152773e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2566, 'grad_norm': 0.486328125, 'learning_rate': 2.6114720244063867e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2517, 'grad_norm': 0.54296875, 'learning_rate': 2.610335786660001e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0237, 'grad_norm': 0.89453125, 'learning_rate': 2.609199548913615e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.326, 'grad_norm': 0.52734375, 'learning_rate': 2.6080633111672288e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1729, 'grad_norm': 0.57421875, 'learning_rate': 2.606927073420843e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2033, 'grad_norm': 0.470703125, 'learning_rate': 2.6057908356744563e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2477, 'grad_norm': 0.69921875, 'learning_rate': 2.6046545979280708e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0034, 'grad_norm': 0.71484375, 'learning_rate': 2.6035183601816842e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4225, 'grad_norm': 0.498046875, 'learning_rate': 2.6023821224352984e-05, 'epoch': 0.79}\n",
      "{'loss': 1.315, 'grad_norm': 1.1953125, 'learning_rate': 2.6012458846889125e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1174, 'grad_norm': 0.466796875, 'learning_rate': 2.6001096469425263e-05, 'epoch': 0.79}\n",
      "{'loss': 1.135, 'grad_norm': 0.8046875, 'learning_rate': 2.5989734091961404e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0726, 'grad_norm': 1.1328125, 'learning_rate': 2.5978371714497542e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3948, 'grad_norm': 0.6875, 'learning_rate': 2.5967009337033683e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1243, 'grad_norm': 0.68359375, 'learning_rate': 2.5955646959569824e-05, 'epoch': 0.79}\n",
      "{'loss': 1.221, 'grad_norm': 0.59765625, 'learning_rate': 2.594428458210596e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2414, 'grad_norm': 0.7265625, 'learning_rate': 2.59329222046421e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1837, 'grad_norm': 0.97265625, 'learning_rate': 2.5921559827178238e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1822, 'grad_norm': 0.60546875, 'learning_rate': 2.591019744971438e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2301, 'grad_norm': 0.78125, 'learning_rate': 2.589883507225052e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1689, 'grad_norm': 0.421875, 'learning_rate': 2.5887472694786658e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1543, 'grad_norm': 0.50390625, 'learning_rate': 2.58761103173228e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1187, 'grad_norm': 1.0078125, 'learning_rate': 2.5864747939858934e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2833, 'grad_norm': 0.61328125, 'learning_rate': 2.585338556239508e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1199, 'grad_norm': 0.62109375, 'learning_rate': 2.584202318493122e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4008, 'grad_norm': 0.78125, 'learning_rate': 2.5830660807467354e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2736, 'grad_norm': 0.63671875, 'learning_rate': 2.5819298430003496e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0427, 'grad_norm': 1.2109375, 'learning_rate': 2.5807936052539633e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3608, 'grad_norm': 0.58203125, 'learning_rate': 2.5796573675075775e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0993, 'grad_norm': 0.7890625, 'learning_rate': 2.5785211297611913e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2385, 'grad_norm': 0.63671875, 'learning_rate': 2.5773848920148054e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2018, 'grad_norm': 0.69921875, 'learning_rate': 2.5762486542684195e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1274, 'grad_norm': 0.80078125, 'learning_rate': 2.575112416522033e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3608, 'grad_norm': 0.55859375, 'learning_rate': 2.573976178775647e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0818, 'grad_norm': 0.91015625, 'learning_rate': 2.572839941029261e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2164, 'grad_norm': 0.6484375, 'learning_rate': 2.571703703282875e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2284, 'grad_norm': 0.5078125, 'learning_rate': 2.570567465536489e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1471, 'grad_norm': 1.0234375, 'learning_rate': 2.569431227790103e-05, 'epoch': 0.79}\n",
      "{'loss': 1.5519, 'grad_norm': 0.578125, 'learning_rate': 2.568294990043717e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1469, 'grad_norm': 0.921875, 'learning_rate': 2.5671587522973305e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2034, 'grad_norm': 0.369140625, 'learning_rate': 2.566022514550945e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2911, 'grad_norm': 0.734375, 'learning_rate': 2.564886276804559e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2096, 'grad_norm': 0.93359375, 'learning_rate': 2.5637500390581725e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3096, 'grad_norm': 0.62109375, 'learning_rate': 2.5626138013117866e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1801, 'grad_norm': 1.578125, 'learning_rate': 2.5614775635654004e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2728, 'grad_norm': 0.474609375, 'learning_rate': 2.5603413258190145e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3788, 'grad_norm': 0.75390625, 'learning_rate': 2.5592050880726287e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0821, 'grad_norm': 0.73828125, 'learning_rate': 2.5580688503262424e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4391, 'grad_norm': 0.55859375, 'learning_rate': 2.5569326125798566e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0792, 'grad_norm': 0.79296875, 'learning_rate': 2.55579637483347e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2227, 'grad_norm': 0.451171875, 'learning_rate': 2.554660137087084e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2749, 'grad_norm': 0.609375, 'learning_rate': 2.553523899340698e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9527, 'grad_norm': 0.478515625, 'learning_rate': 2.552387661594312e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4294, 'grad_norm': 0.4921875, 'learning_rate': 2.5512514238479262e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1358, 'grad_norm': 1.1171875, 'learning_rate': 2.55011518610154e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1605, 'grad_norm': 0.5234375, 'learning_rate': 2.548978948355154e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0973, 'grad_norm': 0.86328125, 'learning_rate': 2.5478427106087675e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0617, 'grad_norm': 1.140625, 'learning_rate': 2.546706472862382e-05, 'epoch': 0.79}\n",
      "{'loss': 1.372, 'grad_norm': 0.482421875, 'learning_rate': 2.545570235115996e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2436, 'grad_norm': 0.79296875, 'learning_rate': 2.5444339973696096e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1332, 'grad_norm': 0.44140625, 'learning_rate': 2.5432977596232237e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1633, 'grad_norm': 1.0859375, 'learning_rate': 2.5421615218768375e-05, 'epoch': 0.79}\n",
      "{'loss': 1.023, 'grad_norm': 1.203125, 'learning_rate': 2.5410252841304516e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3297, 'grad_norm': 0.640625, 'learning_rate': 2.5398890463840657e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2554, 'grad_norm': 1.0078125, 'learning_rate': 2.5387528086376795e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2392, 'grad_norm': 0.57421875, 'learning_rate': 2.5376165708912936e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1685, 'grad_norm': 0.89453125, 'learning_rate': 2.536480333144907e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9949, 'grad_norm': 0.7421875, 'learning_rate': 2.5353440953985212e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3433, 'grad_norm': 0.55078125, 'learning_rate': 2.5342078576521357e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1262, 'grad_norm': 0.73046875, 'learning_rate': 2.533071619905749e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3577, 'grad_norm': 0.51953125, 'learning_rate': 2.5319353821593632e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2806, 'grad_norm': 0.6015625, 'learning_rate': 2.530799144412977e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0647, 'grad_norm': 0.66796875, 'learning_rate': 2.529662906666591e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2864, 'grad_norm': 0.6015625, 'learning_rate': 2.5285266689202046e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2238, 'grad_norm': 0.6953125, 'learning_rate': 2.527390431173819e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1219, 'grad_norm': 0.515625, 'learning_rate': 2.5262541934274332e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3858, 'grad_norm': 0.51953125, 'learning_rate': 2.5251179556810466e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0705, 'grad_norm': 0.69140625, 'learning_rate': 2.5239817179346607e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4044, 'grad_norm': 0.474609375, 'learning_rate': 2.5228454801882745e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1139, 'grad_norm': 0.7734375, 'learning_rate': 2.5217092424418887e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2659, 'grad_norm': 0.48046875, 'learning_rate': 2.5205730046955028e-05, 'epoch': 0.79}\n",
      "{'loss': 1.227, 'grad_norm': 0.609375, 'learning_rate': 2.5194367669491166e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0668, 'grad_norm': 0.76171875, 'learning_rate': 2.5183005292027307e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3759, 'grad_norm': 0.5703125, 'learning_rate': 2.517164291456344e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1727, 'grad_norm': 0.65625, 'learning_rate': 2.5160280537099583e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1154, 'grad_norm': 0.47265625, 'learning_rate': 2.5148918159635727e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2892, 'grad_norm': 0.62109375, 'learning_rate': 2.5137555782171862e-05, 'epoch': 0.79}\n",
      "{'loss': 1.152, 'grad_norm': 1.171875, 'learning_rate': 2.5126193404708003e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3237, 'grad_norm': 0.6015625, 'learning_rate': 2.511483102724414e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1652, 'grad_norm': 0.859375, 'learning_rate': 2.5103468649780282e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1352, 'grad_norm': 0.47265625, 'learning_rate': 2.5092106272316417e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2866, 'grad_norm': 0.59375, 'learning_rate': 2.508074389485256e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9891, 'grad_norm': 0.97265625, 'learning_rate': 2.5069381517388702e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1926, 'grad_norm': 0.55859375, 'learning_rate': 2.5058019139924837e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1783, 'grad_norm': 1.0078125, 'learning_rate': 2.5046656762460978e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2601, 'grad_norm': 0.8359375, 'learning_rate': 2.5035294384997116e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1992, 'grad_norm': 0.6796875, 'learning_rate': 2.5023932007533257e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1411, 'grad_norm': 0.98046875, 'learning_rate': 2.50125696300694e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2508, 'grad_norm': 0.5234375, 'learning_rate': 2.5001207252605536e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0935, 'grad_norm': 0.640625, 'learning_rate': 2.4989844875141678e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2085, 'grad_norm': 0.5234375, 'learning_rate': 2.4978482497677815e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1429, 'grad_norm': 0.9453125, 'learning_rate': 2.4967120120213953e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9873, 'grad_norm': 1.0546875, 'learning_rate': 2.4955757742750094e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2057, 'grad_norm': 0.59375, 'learning_rate': 2.4944395365286232e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1679, 'grad_norm': 0.6796875, 'learning_rate': 2.4933032987822374e-05, 'epoch': 0.79}\n",
      "{'loss': 1.189, 'grad_norm': 0.47265625, 'learning_rate': 2.4921670610358515e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1961, 'grad_norm': 0.625, 'learning_rate': 2.4910308232894653e-05, 'epoch': 0.79}\n",
      "{'loss': 1.165, 'grad_norm': 0.86328125, 'learning_rate': 2.489894585543079e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4236, 'grad_norm': 0.5390625, 'learning_rate': 2.4887583477966932e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1676, 'grad_norm': 1.2265625, 'learning_rate': 2.487622110050307e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2783, 'grad_norm': 0.47265625, 'learning_rate': 2.486485872303921e-05, 'epoch': 0.79}\n",
      "{'loss': 1.316, 'grad_norm': 0.6484375, 'learning_rate': 2.485349634557535e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1079, 'grad_norm': 0.95703125, 'learning_rate': 2.484213396811149e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4104, 'grad_norm': 0.6015625, 'learning_rate': 2.4830771590647628e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1613, 'grad_norm': 0.78125, 'learning_rate': 2.4819409213183766e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2317, 'grad_norm': 0.49609375, 'learning_rate': 2.4808046835719907e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1907, 'grad_norm': 0.68359375, 'learning_rate': 2.4796684458256048e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0976, 'grad_norm': 1.390625, 'learning_rate': 2.4785322080792186e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2092, 'grad_norm': 0.515625, 'learning_rate': 2.4773959703328324e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0722, 'grad_norm': 0.69921875, 'learning_rate': 2.4762597325864465e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1807, 'grad_norm': 0.466796875, 'learning_rate': 2.4751234948400603e-05, 'epoch': 0.79}\n",
      "{'loss': 1.205, 'grad_norm': 0.49609375, 'learning_rate': 2.4739872570936744e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1443, 'grad_norm': 1.0546875, 'learning_rate': 2.4728510193472885e-05, 'epoch': 0.79}\n",
      "{'loss': 1.256, 'grad_norm': 0.61328125, 'learning_rate': 2.4717147816009023e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1284, 'grad_norm': 0.61328125, 'learning_rate': 2.470578543854516e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1796, 'grad_norm': 0.50390625, 'learning_rate': 2.4694423061081302e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2298, 'grad_norm': 0.61328125, 'learning_rate': 2.468306068361744e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0611, 'grad_norm': 0.87890625, 'learning_rate': 2.467169830615358e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4408, 'grad_norm': 0.53125, 'learning_rate': 2.466033592868972e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1511, 'grad_norm': 0.6796875, 'learning_rate': 2.464897355122586e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2388, 'grad_norm': 0.5859375, 'learning_rate': 2.4637611173762e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2623, 'grad_norm': 0.9453125, 'learning_rate': 2.4626248796298136e-05, 'epoch': 0.79}\n",
      "{'loss': 1.118, 'grad_norm': 1.1484375, 'learning_rate': 2.461488641883428e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2886, 'grad_norm': 0.470703125, 'learning_rate': 2.460352404137042e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3084, 'grad_norm': 0.65234375, 'learning_rate': 2.4592161663906557e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3172, 'grad_norm': 0.49609375, 'learning_rate': 2.4580799286442695e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3626, 'grad_norm': 0.5, 'learning_rate': 2.4569436908978836e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0878, 'grad_norm': 1.5859375, 'learning_rate': 2.4558074531514974e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3427, 'grad_norm': 0.69140625, 'learning_rate': 2.4546712154051115e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1163, 'grad_norm': 0.74609375, 'learning_rate': 2.4535349776587256e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0612, 'grad_norm': 0.6328125, 'learning_rate': 2.4523987399123394e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3074, 'grad_norm': 0.6796875, 'learning_rate': 2.4512625021659532e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0305, 'grad_norm': 0.96484375, 'learning_rate': 2.4501262644195673e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2908, 'grad_norm': 0.4609375, 'learning_rate': 2.4489900266731814e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0858, 'grad_norm': 0.9140625, 'learning_rate': 2.4478537889267952e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0976, 'grad_norm': 0.494140625, 'learning_rate': 2.446717551180409e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2003, 'grad_norm': 0.65625, 'learning_rate': 2.445581313434023e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0603, 'grad_norm': 1.0390625, 'learning_rate': 2.444445075687637e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2566, 'grad_norm': 0.44140625, 'learning_rate': 2.4433088379412507e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2317, 'grad_norm': 0.87890625, 'learning_rate': 2.442172600194865e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1956, 'grad_norm': 0.51171875, 'learning_rate': 2.441036362448479e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2973, 'grad_norm': 0.55859375, 'learning_rate': 2.4399001247020927e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1255, 'grad_norm': 1.2890625, 'learning_rate': 2.4387638869557065e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.458, 'grad_norm': 0.82421875, 'learning_rate': 2.4376276492093206e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1572, 'grad_norm': 0.71875, 'learning_rate': 2.4364914114629348e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1918, 'grad_norm': 0.53515625, 'learning_rate': 2.4353551737165485e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3005, 'grad_norm': 0.97265625, 'learning_rate': 2.4342189359701627e-05, 'epoch': 0.79}\n",
      "{'loss': 0.935, 'grad_norm': 0.84765625, 'learning_rate': 2.4330826982237765e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4292, 'grad_norm': 0.61328125, 'learning_rate': 2.4319464604773902e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0934, 'grad_norm': 0.8046875, 'learning_rate': 2.4308102227310044e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1165, 'grad_norm': 0.458984375, 'learning_rate': 2.4296739849846185e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2968, 'grad_norm': 0.5625, 'learning_rate': 2.4285377472382323e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9577, 'grad_norm': 0.5703125, 'learning_rate': 2.427401509491846e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4195, 'grad_norm': 0.7265625, 'learning_rate': 2.4262652717454602e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0743, 'grad_norm': 0.73046875, 'learning_rate': 2.425129033999074e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1132, 'grad_norm': 0.59375, 'learning_rate': 2.423992796252688e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1819, 'grad_norm': 0.83984375, 'learning_rate': 2.4228565585063022e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1076, 'grad_norm': 0.72265625, 'learning_rate': 2.421720320759916e-05, 'epoch': 0.79}\n",
      "{'loss': 1.3433, 'grad_norm': 0.54296875, 'learning_rate': 2.4205840830135298e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2243, 'grad_norm': 0.7421875, 'learning_rate': 2.4194478452671436e-05, 'epoch': 0.79}\n",
      "{'loss': 1.186, 'grad_norm': 0.75, 'learning_rate': 2.4183116075207577e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1966, 'grad_norm': 0.62890625, 'learning_rate': 2.4171753697743718e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9974, 'grad_norm': 0.73828125, 'learning_rate': 2.4160391320279856e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2552, 'grad_norm': 0.62109375, 'learning_rate': 2.4149028942815997e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1443, 'grad_norm': 0.71875, 'learning_rate': 2.4137666565352135e-05, 'epoch': 0.79}\n",
      "{'loss': 1.145, 'grad_norm': 0.52734375, 'learning_rate': 2.4126304187888273e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1194, 'grad_norm': 0.7109375, 'learning_rate': 2.4114941810424414e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0099, 'grad_norm': 1.2265625, 'learning_rate': 2.4103579432960556e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2993, 'grad_norm': 0.4765625, 'learning_rate': 2.4092217055496693e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1599, 'grad_norm': 0.84765625, 'learning_rate': 2.408085467803283e-05, 'epoch': 0.79}\n",
      "{'loss': 1.2235, 'grad_norm': 0.6015625, 'learning_rate': 2.4069492300568972e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1969, 'grad_norm': 0.875, 'learning_rate': 2.405812992310511e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0034, 'grad_norm': 0.70703125, 'learning_rate': 2.404676754564125e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3536, 'grad_norm': 0.609375, 'learning_rate': 2.4035405168177393e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1303, 'grad_norm': 0.8828125, 'learning_rate': 2.402404279071353e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3232, 'grad_norm': 0.6484375, 'learning_rate': 2.401268041324967e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2824, 'grad_norm': 0.68359375, 'learning_rate': 2.4001318035785806e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0439, 'grad_norm': 0.9453125, 'learning_rate': 2.398995565832195e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3491, 'grad_norm': 0.66015625, 'learning_rate': 2.397859328085809e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1094, 'grad_norm': 0.75, 'learning_rate': 2.3967230903394227e-05, 'epoch': 0.8}\n",
      "{'loss': 1.182, 'grad_norm': 0.51171875, 'learning_rate': 2.3955868525930368e-05, 'epoch': 0.8}\n",
      "{'loss': 1.118, 'grad_norm': 0.61328125, 'learning_rate': 2.3944506148466506e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1329, 'grad_norm': 0.85546875, 'learning_rate': 2.3933143771002644e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3239, 'grad_norm': 0.5546875, 'learning_rate': 2.3921781393538785e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1602, 'grad_norm': 1.1640625, 'learning_rate': 2.3910419016074926e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2969, 'grad_norm': 0.48046875, 'learning_rate': 2.3899056638611064e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2639, 'grad_norm': 0.67578125, 'learning_rate': 2.3887694261147202e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0529, 'grad_norm': 1.0390625, 'learning_rate': 2.3876331883683343e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2633, 'grad_norm': 0.53125, 'learning_rate': 2.386496950621948e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1696, 'grad_norm': 0.6640625, 'learning_rate': 2.3853607128755622e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0791, 'grad_norm': 0.5546875, 'learning_rate': 2.3842244751291763e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2056, 'grad_norm': 0.93359375, 'learning_rate': 2.38308823738279e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0615, 'grad_norm': 0.6484375, 'learning_rate': 2.381951999636404e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4636, 'grad_norm': 0.5, 'learning_rate': 2.3808157618900177e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1827, 'grad_norm': 0.671875, 'learning_rate': 2.379679524143632e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2326, 'grad_norm': 0.47265625, 'learning_rate': 2.378543286397246e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3258, 'grad_norm': 0.7109375, 'learning_rate': 2.3774070486508597e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9867, 'grad_norm': 0.828125, 'learning_rate': 2.376270810904474e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3248, 'grad_norm': 0.5703125, 'learning_rate': 2.3751345731580876e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2427, 'grad_norm': 0.78515625, 'learning_rate': 2.3739983354117014e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2135, 'grad_norm': 0.4453125, 'learning_rate': 2.3728620976653156e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1848, 'grad_norm': 0.71875, 'learning_rate': 2.3717258599189297e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0303, 'grad_norm': 0.7734375, 'learning_rate': 2.3705896221725435e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2875, 'grad_norm': 0.92578125, 'learning_rate': 2.3694533844261573e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1618, 'grad_norm': 0.90234375, 'learning_rate': 2.3683171466797714e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1749, 'grad_norm': 0.56640625, 'learning_rate': 2.3671809089333855e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0769, 'grad_norm': 0.75390625, 'learning_rate': 2.3660446711869993e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0081, 'grad_norm': 0.70703125, 'learning_rate': 2.3649084334406134e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2045, 'grad_norm': 0.56640625, 'learning_rate': 2.3637721956942272e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1503, 'grad_norm': 0.63671875, 'learning_rate': 2.362635957947841e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3672, 'grad_norm': 0.56640625, 'learning_rate': 2.361499720201455e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3628, 'grad_norm': 0.578125, 'learning_rate': 2.3603634824550692e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0279, 'grad_norm': 1.078125, 'learning_rate': 2.359227244708683e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3991, 'grad_norm': 0.6015625, 'learning_rate': 2.3580910069622968e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1549, 'grad_norm': 0.94921875, 'learning_rate': 2.356954769215911e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1577, 'grad_norm': 0.4765625, 'learning_rate': 2.3558185314695247e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1678, 'grad_norm': 0.625, 'learning_rate': 2.354682293723139e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1029, 'grad_norm': 1.234375, 'learning_rate': 2.3535460559767526e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2276, 'grad_norm': 0.609375, 'learning_rate': 2.3524098182303667e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0768, 'grad_norm': 0.796875, 'learning_rate': 2.3512735804839805e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2469, 'grad_norm': 0.48828125, 'learning_rate': 2.3501373427375943e-05, 'epoch': 0.8}\n",
      "{'loss': 1.4802, 'grad_norm': 0.7734375, 'learning_rate': 2.3490011049912084e-05, 'epoch': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model_save_dir = \"./results/gugugo-experi_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(lora_model_save_dir, save_embedding_layers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(lora_model_save_dir)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aeolian83/llama_ko_sft_kullm_experi_05/commit/a660bf40db1d86c7dea85c0a2328f73c61f61596', commit_message='Upload tokenizer', commit_description='', oid='a660bf40db1d86c7dea85c0a2328f73c61f61596', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('aeolian83/llama_ko_sft_gugugo_experi_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
