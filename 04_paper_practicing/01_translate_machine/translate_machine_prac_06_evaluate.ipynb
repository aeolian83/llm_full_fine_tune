{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"Machin Translator_01\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "with open('./data/train_data_300.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "len(train_data)\n",
    "\n",
    "with open('./data/validation_data_28.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "len(test_data)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# DatasetDict로 \"train\"과 \"test\" 데이터셋 묶기\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Checkpoints Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/translate_machine_llama3ko_intsuct_origindata300_02\"\n",
    "\n",
    "# 디렉터리 이름에서 숫자를 추출하는 함수\n",
    "def extract_number(directory_name):\n",
    "    return int(directory_name.split('-')[-1])\n",
    "\n",
    "\n",
    "# 디렉터리 항목을 숫자 순서대로 정렬\n",
    "checkpoints = []\n",
    "with os.scandir(checkpoint_dir) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_dir():\n",
    "            checkpoints.append(entry.name)\n",
    "\n",
    "# 숫자 순서대로 정렬\n",
    "checkpoints.sort(key=extract_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30dc8bd4a34542f3b418610a3f347e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-110\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-120\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-130\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-140\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-150\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-160\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-170\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-180\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-190\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-200\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-210\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-220\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-230\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-240\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-250\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-260\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-270\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-280\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-290\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "checkpoint-300\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "model_id = \"beomi/Llama-3-KoEn-8B-Instruct-preview\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\"\n",
    "\n",
    "def make_prompt(text):\n",
    "    return f'''Translate input sentence to Korean\n",
    "### Input: {text}\n",
    "### Translated:'''\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def counter_terms(terms, text):\n",
    "    if not isinstance(terms, list):\n",
    "        terms = terms.split(\", \")\n",
    "\n",
    "    return sum(text.lower().count(term.lower()) for term in terms)\n",
    "\n",
    "\n",
    "def generate_and_evaluate(data, model, tokenizer):\n",
    "    prompt = make_prompt(data['english'])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, pad_token_id=tokenizer.pad_token_id)\n",
    "    prediction = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "\n",
    "    label_clean = clean_text(data['korean'])\n",
    "    prediction_clean = clean_text(prediction)\n",
    "\n",
    "    input_count = counter_terms(data['terms'], data['english'])\n",
    "    predic_count = counter_terms(data['terms'], prediction)\n",
    "\n",
    "    weight = 1.0 if predic_count > input_count else predic_count / input_count\n",
    "\n",
    "    result = metric.compute(predictions=[prediction_clean], references=[label_clean])\n",
    "\n",
    "    return result['score'], weight, weight * result['score']\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map='auto', cache_dir=cache_model_dir)\n",
    "\n",
    "bleu_scores = {}\n",
    "\n",
    "for checkpoint in checkpoints[10:]:\n",
    "    print('#' * 50)\n",
    "    print(checkpoint)\n",
    "    print('#' * 50)\n",
    "\n",
    "    peft_model_id = os.path.join(checkpoint_dir, checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    loaded_model = PeftModel.from_pretrained(model=model, model_id=peft_model_id)\n",
    "\n",
    "    results = [generate_and_evaluate(data, loaded_model, tokenizer) for data in dataset_dict['test']]\n",
    "\n",
    "    bleus, weights, weighted_bleus = zip(*results)\n",
    "\n",
    "    score = {\n",
    "        \"bleu\": np.mean(bleus),\n",
    "        \"weight\": np.mean(weights),\n",
    "        \"weighted_bleus\": np.mean(weighted_bleus)\n",
    "    }\n",
    "\n",
    "    bleu_scores[checkpoint] = score\n",
    "\n",
    "    del peft_model_id, tokenizer, loaded_model\n",
    "\n",
    "df = pd.DataFrame.from_dict(bleu_scores, orient='index')\n",
    "\n",
    "# 종료 시간 기록\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     bleu    weight  weighted_bleus\n",
      "checkpoint-110  47.655708  0.783929       38.020108\n",
      "checkpoint-120  48.512326  0.868778       42.207067\n",
      "checkpoint-130  47.967690  0.883362       42.108866\n",
      "checkpoint-140  49.140856  0.860785       42.241602\n",
      "checkpoint-150  48.969970  0.921641       44.913353\n",
      "checkpoint-160  48.083907  0.888605       42.533465\n",
      "checkpoint-170  48.533218  0.855456       41.175765\n",
      "checkpoint-180  48.648087  0.822208       39.730117\n",
      "checkpoint-190  50.512783  0.836097       42.451408\n",
      "checkpoint-200  48.490799  0.896896       43.562024\n",
      "checkpoint-210  48.102794  0.882015       43.000055\n",
      "checkpoint-220  49.040487  0.850425       41.481763\n",
      "checkpoint-230  48.273156  0.900085       43.524116\n",
      "checkpoint-240  51.532903  0.858078       44.728627\n",
      "checkpoint-250  51.716001  0.852126       44.119474\n",
      "checkpoint-260  50.759023  0.840363       42.638950\n",
      "checkpoint-270  49.657846  0.904549       44.878553\n",
      "checkpoint-280  49.732695  0.891156       44.511654\n",
      "checkpoint-290  49.988116  0.818396       41.291096\n",
      "checkpoint-300  50.137529  0.860544       43.517167\n",
      "실행 시간: 11937.921903371811 초\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "\n",
    "# 실행 시간 계산\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"실행 시간: {execution_time} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
