{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: utf-8 -*- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight and Bias Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"translate_machine_llama3ko_with_orgin_data_300\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/train_data_300.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "len(train_data)\n",
    "\n",
    "with open('./data/validation_data_28.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# DatasetDictë¡œ \"train\"ê³¼ \"test\" ë°ì´í„°ì…‹ ë¬¶ê¸°\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Explainable AI is becoming increasingly important as AI systems are integrated into various industries. With the rise of cloud computing, massive datasets can be processed and analyzed more efficiently, but this often comes at the cost of transparency. By combining explainable AI with cloud computing, organizations can ensure that their AI models are both powerful and understandable. Meanwhile, edge computing allows for data processing closer to the source, which can enhance real-time decision-making capabilities. Integrating explainable AI with edge computing can further improve the trustworthiness and reliability of these real-time systems.',\n",
       " 'korean': 'ì„¤ëª… ê°€ëŠ¥í•œ AI(explainable AI)ëŠ” AI ì‹œìŠ¤í…œì´ ë‹¤ì–‘í•œ ì‚°ì—…ì— í†µí•©ë¨ì— ë”°ë¼ ì ì  ë” ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤. í´ë¼ìš°ë“œ ì»´í“¨íŒ…(cloud computing)ì˜ ë°œì „ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” ì¢…ì¢… íˆ¬ëª…ì„±ì˜ ëŒ€ê°€ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì„¤ëª… ê°€ëŠ¥í•œ AI(explainable AI)ì™€ í´ë¼ìš°ë“œ ì»´í“¨íŒ…(cloud computing)ì„ ê²°í•©í•˜ë©´ ì¡°ì§ì€ ê°•ë ¥í•˜ë©´ì„œë„ ì´í•´í•  ìˆ˜ ìˆëŠ” AI ëª¨ë¸ì„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œí¸, ì—£ì§€ ì»´í“¨íŒ…(edge computing)ì€ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì†ŒìŠ¤ì— ë” ê°€ê¹ê²Œ í•˜ì—¬ ì‹¤ì‹œê°„ ì˜ì‚¬ ê²°ì • ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„¤ëª… ê°€ëŠ¥í•œ AI(explainable AI)ì™€ ì—£ì§€ ì»´í“¨íŒ…(edge computing)ì„ í†µí•©í•˜ë©´ ì´ëŸ¬í•œ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œì˜ ì‹ ë¢°ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',\n",
       " 'terms': 'explainable AI, cloud computing, edge computing'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/Llama-3-KoEn-8B\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for 4-bit QLoRA Training(4bit QLoRA í•™ìŠµì„ ìœ„í•œ ì„¤ì •)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Nvidiaì˜ Ampere ì•„í‚¤í…ì²˜ ì´í›„ ê°€ì†ê¸°ëŠ” bf16ìœ¼ë¡œ ì†ë„ í–¥ìƒì„ ê¾€í• ìˆ˜ ìˆë‹¤. \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# bnb_4bit_quant_type=\"nf4\" ì„¤ì •ìƒ ê¸°ë³¸ê°’ì€ bnb_4bit_quant_type=\"fp4\"ì´ë‚˜ í—ˆê¹…í˜ì´ìŠ¤ ì €ìë“¤ì— ì˜í•˜ë©´\n",
    "# ê²½í—˜ì  ê²°ê³¼ë¡œ \"nf4\"ê°€ ê²°ê³¼ê°€ ë” ì¢‹ì•˜ë‹¤ê³  í•œë‹¤. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# bnb_4bit_use_double_quant=Trueë¡œ í•˜ë©´ ë§¤ê°œë³€ìˆ˜ë‹¨ 0.4bitì„ ì¶”ê°€ë¡œ ì ˆì•½ í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34dc4a5c7714a7482d723addb31135f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# model.config.pretraining_tp = 1\n",
    "# ì¢…ì¢… QLoRA ì½”ë“œì— ì´ ì½”ë“œê°€ ë³´ì´ëŠ”ë° ë³‘ë ¬ í•™ìŠµì— ì“°ì´ëŠ” ì½”ë“œë¡œ ë³´ì¸ë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_model_dir)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) # pad_tokenì´ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ embeddingê³¼ language modeling headë¥¼ resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Formatting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'korean', 'terms'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english', 'korean', 'terms'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting function\n",
    "def formatting_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"english\"])):\n",
    "        text = f\"Translate input sentence to Korean \\n### Input: {example['english'][i]} \\n### Translated: {example['korean'][i]}\" + tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "response_template = \" \\n### Translated:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# https://huggingface.co/docs/trl/sft_trainer#using-tokenids-directly-for-responsetemplate ì°¸ê³ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_template = \" \\n### Translated:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "# response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "# # data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Argument Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoint/translate_machine_llama3ko_nonintsuct_origindata300_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = checkpoint_dir\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_steps = 20\n",
    "save_total_limit=5\n",
    "num_train_epochs = 2\n",
    "logging_steps = 20\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/transformers/training_args.py:1815: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cc5496fdc24bc29c7c55916588e47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be94812a6a1494c888599182238113b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114492888898693, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/llm_practicing/04_paper_practicing/01_translate_machine/wandb/run-20240628_210159-i8glg0xt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/i8glg0xt' target=\"_blank\">ancient-pond-8</a></strong> to <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/i8glg0xt' target=\"_blank\">https://wandb.ai/aeolian83/translate_machine_llama3ko_with_orgin_data_300/runs/i8glg0xt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/aeolian83/anaconda3/envs/llm_for_p311/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.19696848551432292, metrics={'train_runtime': 499.2441, 'train_samples_per_second': 1.202, 'train_steps_per_second': 0.601, 'total_flos': 9466790000295936.0, 'train_loss': 0.19696848551432292, 'epoch': 2.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "lora_model_save_dir = \"./results/translate_machine_llama3ko_origindata300_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "trainer.save_model(lora_model_save_dir)\n",
    "\n",
    "# model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(lora_model_save_dir, save_embedding_layers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/Llama-3-KoEn-8B\"\n",
    "device_map = {\"\": 0}\n",
    "cache_model_dir=\"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f940e671c74cc2bfeb6f057f1bab8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=cache_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained(lora_model_save_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=lora_model_save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[128000,  28573,   1988,  11914,    311,  16526,    720,  14711,   5688,\n",
       "              25,  18185,    872,   6205,   4367,     11,   1057,   4211,    656,\n",
       "             539,    617,  15022,   1515,  29736,     82,   7863,    311,   1023,\n",
       "           29736,   6108,   4211,     13,    720,  14711,   4149,  22851,     25,\n",
       "             220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[128000,  28573,   1988,  11914,    311,  16526,    720,  14711,   5688,\n",
       "              25,   5751,   4211,    656,     11,   4869,     11,    617,   1515,\n",
       "           29736,     82,   2731,   1109,    279,   3544,  17989,  86997,   5962,\n",
       "           12939,  25936,    706,   1027,   5068,    311,   8356,    369,   4907,\n",
       "            3196,   4211,    323,   5573,  12864,     13,    720,  14711,   4149,\n",
       "           22851,     25,    220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[128000,  28573,   1988,  11914,    311,  16526,    720,  14711,   5688,\n",
       "              25,   1226,   5357,    389,  10128,    306,  29469,   7713,  27972,\n",
       "            2533,    814,    649,   2804,    264,   7029,   2134,    315,   1803,\n",
       "            1413,   9256,     13,   1115,    990,   5039,    430,   5042,   7060,\n",
       "            2442,  38302,    264,   2678,    961,    315,    279,   1803,   1413,\n",
       "            1646,     13,    720,  14711,   4149,  22851,     25,    220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]])}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting function\n",
    "def formatting_func_inference(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"Input\"])):\n",
    "        text = f\"Translate input sentence to Korean \\n### Input: {example['Input'][i]} \\n### Translated: \"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        output_texts.append(inputs)\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "example = {\"Input\": [\n",
    "    \"Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models.\",\n",
    "    \"Our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching.\",\n",
    "    \"We focus on Latent Diffusion Models since they can perform a wide range of generative tasks. This work shows that simply fine-tuning a small part of the generative model.\",\n",
    "]}\n",
    "\n",
    "tokenized_example = formatting_func_inference(example)\n",
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate input sentence to Korean \n",
      "### Input: Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models. \n",
      "### Translated:  ìš°ë¦¬ ëª¨ë¸ì€ ìƒ˜í”Œ í’ˆì§ˆì—ë„ ë¶ˆêµ¬í•˜ê³ , ë‹¤ë¥¸ ê°€ëŠ¥ì„± ê¸°ë°˜ ëª¨ë¸ì— ë¹„í•´ ê²½ìŸë ¥ ìˆëŠ” ë¡œê·¸ ê°€ëŠ¥ì„±ì„ ê°–ì§€ ì•ŠìŠµë‹ˆë‹¤.- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "\n",
      "####################################################################################################\n",
      "Translate input sentence to Korean \n",
      "### Input: Our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching. \n",
      "### Translated:  ìš°ë¦¬ì˜ ëª¨ë¸ì€ ê·¸ëŸ¬ë‚˜ ì—ë„ˆì§€ ê¸°ë°˜ ëª¨ë¸ê³¼ ìŠ¤ì½”ì–´ ë§¤ì¹­ì— ëŒ€í•´ ë³´ê³ ëœ ê²ƒë³´ë‹¤ ë” í° ì¶”ì •ì¹˜ë¡œ ì¤‘ìš”ë„ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ì—¬ ë¡œê·¸ ê°€ëŠ¥ì„±(log likelihoods)ì„ ë” ì˜ ê°–ìŠµë‹ˆë‹¤.- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "-\n",
      "####################################################################################################\n",
      "Translate input sentence to Korean \n",
      "### Input: We focus on Latent Diffusion Models since they can perform a wide range of generative tasks. This work shows that simply fine-tuning a small part of the generative model. \n",
      "### Translated:  ìš°ë¦¬ëŠ” ì ì¬ í™•ì‚° ëª¨ë¸(latent diffusion models)ì„ ì¤‘ì ì ìœ¼ë¡œ ì—°êµ¬í•©ë‹ˆë‹¤. ì´ëŠ” ìƒì„± ëª¨ë¸ì˜ ì‘ì€ ë¶€ë¶„ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ë‹¤ì–‘í•œ ìƒì„± ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "- 11:00 pm\n",
      "- 23rd\n",
      "- 11:00 pm\n",
      "- 24th\n",
      "- 11:00 pm\n",
      "- 25th\n",
      "- 11:00 pm\n",
      "- 26th\n",
      "- 11:00 pm\n",
      "- 27th\n",
      "- 11:00 pm\n",
      "- 28th\n",
      "- 11:00 pm\n",
      "- 29th\n",
      "- 11:00 pm\n",
      "- 30th\n",
      "- 11:00 pm- 1st\n",
      "- 11:00 pm\n",
      "- 2nd\n",
      "- 11:00 pm\n",
      "- 3rd\n",
      "- 11:00 pm\n",
      "- 4th\n",
      "- 11:00 pm\n",
      "- 5th\n",
      "- 11:00 pm\n",
      "- 6th\n",
      "- 11:00 pm\n",
      "- 7th\n",
      "- 11:00 pm\n",
      "- 8th\n",
      "- 11:00 pm\n",
      "- 9th\n",
      "- 11:00 pm\n",
      "- 10th\n",
      "- 11:00 pm\n",
      "- 11th\n",
      "- 11:00 pm\n",
      "- 12th\n",
      "- 11:00 pm\n",
      "- 13th\n",
      "- 11:00 pm\n",
      "- 14th\n",
      "- 11:00 pm\n",
      "- 15th\n",
      "- 11:00 pm\n",
      "- 16th\n",
      "- 11:00 pm\n",
      "- 17th\n",
      "- 11:00 pm\n",
      "- 18th\n",
      "- 11:00 pm\n",
      "- 19th\n",
      "- 11:00 pm\n",
      "- 20th\n",
      "- 11:00 pm\n",
      "- 21st\n",
      "- 11:00 pm\n",
      "- 22nd\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "with torch.cuda.amp.autocast():\n",
    "    for ex in tokenized_example:\n",
    "        ex = ex.to(model.device)\n",
    "        pred = model.generate(\n",
    "            **ex,\n",
    "            max_new_tokens=1024,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        outputs.append(pred)\n",
    "\n",
    "outputs = [tokenizer.batch_decode(t, skip_special_tokens=True)[0] for t in outputs]\n",
    "for o in outputs:\n",
    "    print(o)\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
