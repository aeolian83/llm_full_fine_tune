{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepare\n",
    "\n",
    "## 1. Download Korean Data from aihub.go.kr\n",
    "- The data downloaded from aihub.go.kr is separated into zip files by folder hierarchy, so it must be fully extracted without distinction before working only with the json files.\n",
    "- Then, parse the json to collect only the Korean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Check if the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/mnt/t7/dnn_data/korean_data/data\"\n",
    "target_path = \"/mnt/t7/dnn/llm_practicing/korean_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(base_path):\n",
    "    # Zip 파일 경로를 찾습니다.\n",
    "    zip_files = glob.glob(f'{base_path}/**/*.zip', recursive=True)\n",
    "    for zip_file in zip_files:\n",
    "        # Zip 파일을 압축 해제합니다.\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            extract_path = os.path.dirname(zip_file)\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "def find_json_files(base_path):\n",
    "    # JSON 파일 경로를 찾습니다.\n",
    "    return glob.glob(f'{base_path}/**/*.json', recursive=True)\n",
    "\n",
    "def contains_korean(text):\n",
    "    return bool(re.search(\"[가-힣]\", text))\n",
    "\n",
    "def find_korean_values(data, result):\n",
    "    if isinstance(data, dict):\n",
    "        for value in data.values():\n",
    "            find_korean_values(value, result)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            find_korean_values(item, result)\n",
    "    elif isinstance(data, str) and contains_korean(data):\n",
    "        result.append({\"text\": data})\n",
    "\n",
    "def save_as_jsonl(data, target_path):\n",
    "    for top_folder, texts in data.items():\n",
    "        file_name = f\"{top_folder}.jsonl\"\n",
    "        with open(os.path.join(target_path, file_name), 'w', encoding='utf-8') as file:\n",
    "            for item in texts:\n",
    "                json_record = json.dumps(item, ensure_ascii=False)\n",
    "                file.write(json_record + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_json_file(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        try:\n",
    "            file_content = file.read()\n",
    "            # 유효하지 않은 제어 문자 제거 또는 이스케이프 처리\n",
    "            # file_content = file_content.replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n",
    "            return json.loads(file_content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {json_file}: {e}\")\n",
    "            return None  # 또는 적절한 오류 처리\n",
    "\n",
    "# def process_json_files(json_files, base_path):\n",
    "#     organized_data = {}\n",
    "#     for json_file in json_files:\n",
    "#         print(json_file)\n",
    "#         top_folder = json_file[len(base_path):].split(os.sep)[1]\n",
    "#         if top_folder not in organized_data:\n",
    "#             organized_data[top_folder] = []\n",
    "        \n",
    "#         with open(json_file, 'r', encoding='utf-8') as file:\n",
    "#             data = json.load(file)\n",
    "#             result = []\n",
    "#             find_korean_values(data, result)\n",
    "#             organized_data[top_folder].extend(result)\n",
    "#     return organized_data\n",
    "\n",
    "# process_json_files 함수 내에서 json.load(file) 대신 safe_load_json_file을 사용\n",
    "def process_json_files(json_files, base_path):\n",
    "    organized_data = {}\n",
    "    for json_file in json_files:\n",
    "        top_folder = json_file[len(base_path):].split(os.sep)[1]\n",
    "        if top_folder not in organized_data:\n",
    "            organized_data[top_folder] = []\n",
    "        \n",
    "        data = safe_load_json_file(json_file)\n",
    "        if data:  # 데이터가 성공적으로 로드된 경우에만 처리\n",
    "            result = []\n",
    "            find_korean_values(data, result)\n",
    "            organized_data[top_folder].extend(result)\n",
    "    return organized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(base_path, target_path):\n",
    "    extract_zip_files(base_path)\n",
    "    json_files = find_json_files(base_path)\n",
    "    korean_texts = process_json_files(json_files, base_path)\n",
    "    save_as_jsonl(korean_texts, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "That compression method is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(base_path, target_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(base_path, target_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mextract_zip_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     json_files \u001b[38;5;241m=\u001b[39m find_json_files(base_path)\n\u001b[1;32m      4\u001b[0m     korean_texts \u001b[38;5;241m=\u001b[39m process_json_files(json_files, base_path)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mextract_zip_files\u001b[0;34m(base_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zip_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      7\u001b[0m     extract_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(zip_file)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mzip_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dps_for_p380/lib/python3.8/zipfile.py:1628\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1625\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zipinfo \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[0;32m-> 1628\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dps_for_p380/lib/python3.8/zipfile.py:1681\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         os\u001b[38;5;241m.\u001b[39mmkdir(targetpath)\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[0;32m-> 1681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmember\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[1;32m   1682\u001b[0m      \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[1;32m   1683\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(source, target)\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "File \u001b[0;32m~/anaconda3/envs/dps_for_p380/lib/python3.8/zipfile.py:1552\u001b[0m, in \u001b[0;36mZipFile.open\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m h[\u001b[38;5;241m11\u001b[39m] \u001b[38;5;241m!=\u001b[39m check_byte:\n\u001b[1;32m   1550\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad password for file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[0;32m-> 1552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mZipExtFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzef_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m     zef_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/dps_for_p380/lib/python3.8/zipfile.py:805\u001b[0m, in \u001b[0;36mZipExtFile.__init__\u001b[0;34m(self, fileobj, mode, zipinfo, decrypter, close_fileobj)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m=\u001b[39m zipinfo\u001b[38;5;241m.\u001b[39mcompress_size\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_left \u001b[38;5;241m=\u001b[39m zipinfo\u001b[38;5;241m.\u001b[39mfile_size\n\u001b[0;32m--> 805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor \u001b[38;5;241m=\u001b[39m \u001b[43m_get_decompressor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compress_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dps_for_p380/lib/python3.8/zipfile.py:706\u001b[0m, in \u001b[0;36m_get_decompressor\u001b[0;34m(compress_type)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_decompressor\u001b[39m(compress_type):\n\u001b[0;32m--> 706\u001b[0m     \u001b[43m_check_compression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompress_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compress_type \u001b[38;5;241m==\u001b[39m ZIP_STORED:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dps_for_p380/lib/python3.8/zipfile.py:686\u001b[0m, in \u001b[0;36m_check_compression\u001b[0;34m(compression)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    684\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompression requires the (missing) lzma module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat compression method is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: That compression method is not supported"
     ]
    }
   ],
   "source": [
    "main(base_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_files = find_json_files(base_path)\n",
    "# korean_texts = process_json_files(json_files, base_path)\n",
    "# save_as_jsonl(korean_texts, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for json_file in json_files:\n",
    "#     check_st = safe_load_json_file(json_file)\n",
    "#     if check_st is not None:\n",
    "#         print(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_json_file(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        try:\n",
    "            file_content = file.read()\n",
    "            # 유효하지 않은 제어 문자 제거 또는 이스케이프 처리\n",
    "            file_content = file_content.replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n",
    "            return json.loads(file_content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {json_file}: {e}\")\n",
    "            return None  # 또는 적절한 오류 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['021.도서자료 기계독해', '022.요약문 및 레포트 생성 데이터', '024.에세이 글 평가 데이터', '025.일상생활 및 구어체 한-영 번역 병렬 말뭉치 데이터', '029.대규모 구매도서 기반 한국어 말뭉치 데이터', '030.웹데이터 기반 한국어 말뭉치 데이터', '142.한국어 지식기반 관계 데이터', '150.숫자연산 기계독해 데이터', '152.기술과학 문서 기계독해 데이터', '153.기술과학 요약 데이터', '156.전문분야 영-한, 중-한 번역 말뭉치(식품)', '157.추상 요약 사실성 검증 데이터', '160.문화, 게임 콘텐츠 분야 용어 말뭉치', '308.AI 허브 데이터 활용을 위한 기계 번역앱 구축과 번역기 평가 및 신규 말뭉치 구축', '기계독해', '도서자료 요약', '법률 지식베이스', '일반상식', '전문분야 말뭉치', '전문분야 한영 말뭉치', '한국어-영어 번역 말뭉치(기술과학)', '한국어-영어 번역 말뭉치(사회과학)', '한국어-영어 번역(병렬) 말뭉치', '한국어-일본어 번역 말뭉치', '한국어-중국어 번역 말뭉치(기술과학)', '한국어-중국어 번역 말뭉치(사회과학)']\n"
     ]
    }
   ],
   "source": [
    "dir_list = os.listdir(base_path)\n",
    "\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Modify the code to work by top_folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(base_path):\n",
    "    # Zip 파일 경로를 찾습니다.\n",
    "    zip_files = glob.glob(f'{base_path}/**/*.zip', recursive=True)\n",
    "    for zip_file in zip_files:\n",
    "        # Zip 파일을 압축 해제합니다.\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            extract_path = os.path.dirname(zip_file)\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "def find_json_files(base_path):\n",
    "    # JSON 파일 경로를 찾습니다.\n",
    "    return glob.glob(f'{base_path}/**/*.json', recursive=True)\n",
    "\n",
    "def contains_korean(text):\n",
    "    return bool(re.search(\"[가-힣]\", text))\n",
    "\n",
    "def find_korean_values(data, result):\n",
    "    if isinstance(data, dict):\n",
    "        for value in data.values():\n",
    "            find_korean_values(value, result)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            find_korean_values(item, result)\n",
    "    elif isinstance(data, str) and contains_korean(data):\n",
    "        result.append({\"text\": data})\n",
    "\n",
    "def safe_load_json_file(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        try:\n",
    "            file_content = file.read()\n",
    "            return json.loads(file_content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {json_file}: {e}\")\n",
    "            return None  # 또는 적절한 오류 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_files(json_files):\n",
    "    organized_data = []\n",
    "    for json_file in json_files:\n",
    "        data = safe_load_json_file(json_file)\n",
    "        if data:  # 데이터가 성공적으로 로드된 경우에만 처리\n",
    "            result = []\n",
    "            find_korean_values(data, result)\n",
    "            organized_data.extend(result)\n",
    "    return organized_data\n",
    "\n",
    "def save_as_jsonl(data, top_dir, target_path):\n",
    "    file_name = f\"{top_dir}.jsonl\"\n",
    "    with open(os.path.join(target_path, file_name), 'w', encoding='utf-8') as file:\n",
    "        for item in data:\n",
    "            json_record = json.dumps(item, ensure_ascii=False)\n",
    "            file.write(json_record + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(base_path, target_path):\n",
    "    top_dir_list = [dir for dir in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, dir))]\n",
    "\n",
    "    for top_dir in top_dir_list:\n",
    "        base_target_path = os.path.join(base_path, top_dir)\n",
    "        extract_zip_files(base_target_path)\n",
    "        json_files = find_json_files(base_target_path)\n",
    "        korean_texts = process_json_files(json_files)\n",
    "        save_as_jsonl(korean_texts, top_dir, target_path)\n",
    "        print(top_dir, \"is done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['029.대규모 구매도서 기반 한국어 말뭉치 데이터', '030.웹데이터 기반 한국어 말뭉치 데이터', '142.한국어 지식기반 관계 데이터', '150.숫자연산 기계독해 데이터', '152.기술과학 문서 기계독해 데이터', '153.기술과학 요약 데이터', '156.전문분야 영-한, 중-한 번역 말뭉치(식품)', '157.추상 요약 사실성 검증 데이터', '160.문화, 게임 콘텐츠 분야 용어 말뭉치', '308.AI 허브 데이터 활용을 위한 기계 번역앱 구축과 번역기 평가 및 신규 말뭉치 구축', '기계독해', '도서자료 요약', '법률 지식베이스', '일반상식', '전문분야 말뭉치', '전문분야 한영 말뭉치', '한국어-영어 번역 말뭉치(기술과학)', '한국어-영어 번역 말뭉치(사회과학)', '한국어-영어 번역(병렬) 말뭉치', '한국어-일본어 번역 말뭉치', '한국어-중국어 번역 말뭉치(기술과학)', '한국어-중국어 번역 말뭉치(사회과학)']\n"
     ]
    }
   ],
   "source": [
    "top_dir_list = [dir for dir in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, dir))]\n",
    "print(top_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_target_path = os.path.join(base_path, top_dir_list[1])\n",
    "# extract_zip_files(base_target_path)\n",
    "# json_files = find_json_files(base_target_path)\n",
    "# korean_texts = process_json_files(json_files)\n",
    "# save_as_jsonl(korean_texts, top_dir_list[1], target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f7da63f4eb0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aeolian83/anaconda3/envs/dps_for_p380/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "main(base_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Change the file handling process to resolve system memory leak issues:\n",
    "\n",
    "1) Instead of processing all text as a list, handle each JSON file separately, creating a temporary jsonl file before generating the final jsonl file.\n",
    "2) Improve processing speed by parallel processing (multiprocessing) the handling of JSON files.\n",
    "    - Reduced a task that took over 2 hours to under 20 minutes.\n",
    "3) Handle decompression of compressed files through multiprocessing as well.\n",
    "    - Reduced decompression time from 11 minutes to about 5 minutes and 40 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import zipfile\n",
    "import re\n",
    "from multiprocessing import Pool, current_process\n",
    "import shutil\n",
    "from tqdm import tqdm  # tqdm 라이브러리를 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import zipfile\n",
    "import re\n",
    "from multiprocessing import Pool, current_process\n",
    "import shutil\n",
    "from tqdm import tqdm  # tqdm 라이브러리를 임포트\n",
    "\n",
    "\n",
    "def extract_zip_file(zip_file_info):\n",
    "    zip_file, extract_path = zip_file_info\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "\n",
    "def extract_all_zip_files(base_path):\n",
    "    zip_files = glob.glob(f\"{base_path}/**/*.zip\", recursive=True)\n",
    "    zip_file_info_list = [\n",
    "        (zip_file, os.path.dirname(zip_file)) for zip_file in zip_files\n",
    "    ]\n",
    "\n",
    "    # ZIP 파일을 병렬로 압축 해제\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        list(\n",
    "            tqdm(\n",
    "                pool.imap(extract_zip_file, zip_file_info_list),\n",
    "                total=len(zip_file_info_list),\n",
    "                desc=\"Extracting ZIP files\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def find_all_json_files(base_path):\n",
    "    return glob.glob(f\"{base_path}/**/*.json\", recursive=True)\n",
    "\n",
    "\n",
    "def contains_korean(text):\n",
    "    return bool(re.search(\"[가-힣]\", text))\n",
    "\n",
    "\n",
    "def find_korean_values(data, result):\n",
    "    if isinstance(data, dict):\n",
    "        for value in data.values():\n",
    "            find_korean_values(value, result)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            find_korean_values(item, result)\n",
    "    elif isinstance(data, str) and contains_korean(data):\n",
    "        result.append({\"text\": data})\n",
    "\n",
    "\n",
    "def safe_load_json_file(json_file):\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from {json_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_json_file(args):\n",
    "    json_file, temp_dir = args\n",
    "    data = safe_load_json_file(json_file)\n",
    "    temp_file_path = None  # 초기화 수정\n",
    "\n",
    "    if data:\n",
    "        result = []\n",
    "        find_korean_values(data, result)\n",
    "        if result:\n",
    "            temp_file_path = os.path.join(\n",
    "                temp_dir, f\"temp_{current_process().pid}.jsonl\"\n",
    "            )\n",
    "            with open(temp_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "                for item in result:\n",
    "                    json_record = json.dumps(item, ensure_ascii=False)\n",
    "                    file.write(json_record + \"\\n\")\n",
    "\n",
    "    # temp_file_path가 성공적으로 생성되었는지 확인\n",
    "    if temp_file_path:\n",
    "        return temp_file_path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def merge_temp_files(temp_files, final_path):\n",
    "    with open(final_path, \"w\", encoding=\"utf-8\") as final_file:\n",
    "        for temp_file in temp_files:\n",
    "            if os.path.exists(temp_file):\n",
    "                with open(temp_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    shutil.copyfileobj(f, final_file)\n",
    "                os.remove(temp_file)  # Remove temp file after merging\n",
    "\n",
    "\n",
    "def main(base_path, target_path, top_dir):\n",
    "    # extract_all_zip_files(base_path)\n",
    "    extract_zip_files(base_path)\n",
    "    json_files = find_all_json_files(base_path)\n",
    "    temp_dir = os.path.join(target_path, \"temp\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    args_list = [(json_file, temp_dir) for json_file in json_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        temp_files = list(\n",
    "            tqdm(\n",
    "                pool.imap(process_json_file, args_list),\n",
    "                total=len(args_list),\n",
    "                desc=\"Processing JSON files\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    final_file_name = f\"{top_dir}.jsonl\"\n",
    "    final_path = os.path.join(target_path, final_file_name)\n",
    "    merge_temp_files(set(filter(None, temp_files)), final_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"/mnt/t7/dnn_data/korean_data/data\"\n",
    "    target_path = \"/mnt/t7/dnn/llm_practicing/korean_data\"\n",
    "\n",
    "    top_dir_list = [\n",
    "        dir\n",
    "        for dir in os.listdir(base_path)\n",
    "        if os.path.isdir(os.path.join(base_path, dir))\n",
    "    ]\n",
    "\n",
    "    for top_dir in tqdm(top_dir_list):\n",
    "        base_target_path = os.path.join(base_path, top_dir)\n",
    "        main(base_target_path, target_path, top_dir)\n",
    "        print(top_dir, \"is done!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dps_for_p380",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
